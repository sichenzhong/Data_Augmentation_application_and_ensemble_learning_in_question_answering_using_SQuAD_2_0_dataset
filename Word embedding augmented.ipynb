{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word embedding augmented.ipynb","provenance":[],"machine_shape":"hm","mount_file_id":"1BdrYi5-fmZ9gAZSJHxu1WUknSs671X8G","authorship_tag":"ABX9TyORE0aQvRsppMyVUGClTdQs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"53b5723253a14aa58ce325a34c55deb0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_498f613046fa4b219b76d475ac850a59","IPY_MODEL_164f38a7c1a84762bf85bb6f01b01ec3","IPY_MODEL_339fea78df5341b1a4a6b0ff9b590e0a"],"layout":"IPY_MODEL_f591460c52b042dabcb6369eee0d54f2"}},"498f613046fa4b219b76d475ac850a59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77100f42d27541d0a030d91378c2ce30","placeholder":"​","style":"IPY_MODEL_4d9cefdc1ba7433fb474987026c24109","value":"Downloading data files: 100%"}},"164f38a7c1a84762bf85bb6f01b01ec3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c38606e113f4fca8cc5fea742e30cdb","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bafdd201914b4e5e8a7bacaeb4b18754","value":2}},"339fea78df5341b1a4a6b0ff9b590e0a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d2244abca21429190077b5487714eaa","placeholder":"​","style":"IPY_MODEL_302bdb70ce84405abdeee85b2c33c09f","value":" 2/2 [00:00&lt;00:00, 84.35it/s]"}},"f591460c52b042dabcb6369eee0d54f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77100f42d27541d0a030d91378c2ce30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d9cefdc1ba7433fb474987026c24109":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c38606e113f4fca8cc5fea742e30cdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bafdd201914b4e5e8a7bacaeb4b18754":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d2244abca21429190077b5487714eaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"302bdb70ce84405abdeee85b2c33c09f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"219ceddb2eac4034a6874af046aa4c50":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_20e9e091d1cb496aa2aa632b30aa4eac","IPY_MODEL_4ccb6d78c2a649ebbeb7df0cb16da6a9","IPY_MODEL_c491ce4df0f84438a466e6afdcc1e554"],"layout":"IPY_MODEL_c6f4385cf6eb440299fdf0a9db9db70a"}},"20e9e091d1cb496aa2aa632b30aa4eac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4d6cc8bda694166948aa342e965c122","placeholder":"​","style":"IPY_MODEL_37df5cd7ae9a41e48d5396368f431a66","value":"Extracting data files: 100%"}},"4ccb6d78c2a649ebbeb7df0cb16da6a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac306dd4e5f947bcad6fbde244e48832","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3bce3dc579a94ddb890c4a8170839e7a","value":2}},"c491ce4df0f84438a466e6afdcc1e554":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da2f9cdb4c424521976b351e16d71f3a","placeholder":"​","style":"IPY_MODEL_07a01fc6296948d3aa815a8ae0a7ab3a","value":" 2/2 [00:00&lt;00:00, 44.93it/s]"}},"c6f4385cf6eb440299fdf0a9db9db70a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4d6cc8bda694166948aa342e965c122":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37df5cd7ae9a41e48d5396368f431a66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac306dd4e5f947bcad6fbde244e48832":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bce3dc579a94ddb890c4a8170839e7a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da2f9cdb4c424521976b351e16d71f3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07a01fc6296948d3aa815a8ae0a7ab3a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"564187fc18724d9e9237f9e37b577bfd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10f07942f31343b2b24051ecd4343744","IPY_MODEL_2e9ef56d440b4c26bff07569aa481c30","IPY_MODEL_dfa8d428ca64453086553fa7667916ee"],"layout":"IPY_MODEL_89d7417b82384f2fbb9ce5fb4892c644"}},"10f07942f31343b2b24051ecd4343744":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9080046ae703471eb43365962a07b3f4","placeholder":"​","style":"IPY_MODEL_a06b968632ab41e2a73d8ec91a8b2793","value":"Generating train split: "}},"2e9ef56d440b4c26bff07569aa481c30":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_59749782ef00464cb2781072280ab60b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_336e8111f3284225bee52eb453a342ff","value":1}},"dfa8d428ca64453086553fa7667916ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d53805b5deb4e94b927f41f4d542e18","placeholder":"​","style":"IPY_MODEL_e82e5a081fa144d9a137a6206375a812","value":" 129325/0 [00:10&lt;00:00, 15224.72 examples/s]"}},"89d7417b82384f2fbb9ce5fb4892c644":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9080046ae703471eb43365962a07b3f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a06b968632ab41e2a73d8ec91a8b2793":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59749782ef00464cb2781072280ab60b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"336e8111f3284225bee52eb453a342ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d53805b5deb4e94b927f41f4d542e18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e82e5a081fa144d9a137a6206375a812":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"576bfd8bbca543308fb421a79d680dd8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2aa9dd2bcf8e409a9e4c3e44c634f2b2","IPY_MODEL_1f7e0eaff73b4974a8ca67337b66b5b7","IPY_MODEL_6e9e8c516e274edebbab2489cd29dc29"],"layout":"IPY_MODEL_203b962a7f1e431aa2a2213d27000548"}},"2aa9dd2bcf8e409a9e4c3e44c634f2b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0f285b79acb4ab69bf9db389b2cbd79","placeholder":"​","style":"IPY_MODEL_2814935a07c94979af5422ea855ba76a","value":"Generating validation split: "}},"1f7e0eaff73b4974a8ca67337b66b5b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4f4841398fe4f76bf4f6d0a07b75108","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_37fd4fb1a6514392aab1eefdcbb4246a","value":1}},"6e9e8c516e274edebbab2489cd29dc29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_634f9caaf63e440d8ef0110746d0164f","placeholder":"​","style":"IPY_MODEL_ce457d54c2b247c18f97e79707d530a0","value":" 11656/0 [00:00&lt;00:00, 14275.27 examples/s]"}},"203b962a7f1e431aa2a2213d27000548":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0f285b79acb4ab69bf9db389b2cbd79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2814935a07c94979af5422ea855ba76a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a4f4841398fe4f76bf4f6d0a07b75108":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"37fd4fb1a6514392aab1eefdcbb4246a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"634f9caaf63e440d8ef0110746d0164f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce457d54c2b247c18f97e79707d530a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"710533fabc184bbba06cb74ac6dd31ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1bca0364d956417380484be8696fb3d7","IPY_MODEL_71cbee9cf3fc475fa1271dab6409ad68","IPY_MODEL_ee3310cd20904acd9f8f3435548d5413"],"layout":"IPY_MODEL_a39582ad48e84e6ab607f6dcdc089a97"}},"1bca0364d956417380484be8696fb3d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ea8204b674f4878a9a64718f27ed98c","placeholder":"​","style":"IPY_MODEL_0dbbe850fcee4889b351907fb50c22f7","value":"100%"}},"71cbee9cf3fc475fa1271dab6409ad68":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d37ed23cbc10446c9f4fc632cd729308","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_849d0aae16fd4ffca7b5e7ac270cb0dd","value":2}},"ee3310cd20904acd9f8f3435548d5413":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1beb52ad61494d38875ee00ea769ac48","placeholder":"​","style":"IPY_MODEL_2f2f16fef5754b91aafba2422bc6303f","value":" 2/2 [00:00&lt;00:00, 63.64it/s]"}},"a39582ad48e84e6ab607f6dcdc089a97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ea8204b674f4878a9a64718f27ed98c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dbbe850fcee4889b351907fb50c22f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d37ed23cbc10446c9f4fc632cd729308":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"849d0aae16fd4ffca7b5e7ac270cb0dd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1beb52ad61494d38875ee00ea769ac48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f2f16fef5754b91aafba2422bc6303f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a648710f3853408f8c98e6f145fd8418":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f16e5afd7d584a6385a75c9879dc6f48","IPY_MODEL_d4fcb1b6b62843d3996d07ad20a57866","IPY_MODEL_4f5642ce2ab444b59304285762768a51"],"layout":"IPY_MODEL_0fae5b53f42d46f4af7f3656194fb7e3"}},"f16e5afd7d584a6385a75c9879dc6f48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3abc2abeac8458ab9a93e3a4a9c5a95","placeholder":"​","style":"IPY_MODEL_7bf0b7800146463cb8ffc3f2d352e3c5","value":"Pushing dataset shards to the dataset hub: 100%"}},"d4fcb1b6b62843d3996d07ad20a57866":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_714868d458144630919f6b8acdcf1601","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5d2c85fc71cb47699f167318ef165239","value":1}},"4f5642ce2ab444b59304285762768a51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79a08eb572594f55bd38adc2f2097344","placeholder":"​","style":"IPY_MODEL_dfc8056e4c2f42228d93bab8783bf15d","value":" 1/1 [00:06&lt;00:00,  6.01s/it]"}},"0fae5b53f42d46f4af7f3656194fb7e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3abc2abeac8458ab9a93e3a4a9c5a95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bf0b7800146463cb8ffc3f2d352e3c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"714868d458144630919f6b8acdcf1601":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d2c85fc71cb47699f167318ef165239":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"79a08eb572594f55bd38adc2f2097344":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfc8056e4c2f42228d93bab8783bf15d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"168ee085f5aa40b9a02345d4fb78f08d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eedd7c64adef4924a2901c0a299979b7","IPY_MODEL_9734aada5dfe4e5d9d69b18b0ad0481b","IPY_MODEL_f7af7d2a28134f2bb9015167c19da080"],"layout":"IPY_MODEL_9eaf480fdb4b4573b6fd750d6b1297bd"}},"eedd7c64adef4924a2901c0a299979b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f33da811c51943faa058d2f51471be71","placeholder":"​","style":"IPY_MODEL_42103bf303c240eabc6a0485c534ab9f","value":"Pushing dataset shards to the dataset hub: 100%"}},"9734aada5dfe4e5d9d69b18b0ad0481b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d74468620ab4843a739ff2626fc1d52","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d4d22b861f84907b47cd9a11214d229","value":1}},"f7af7d2a28134f2bb9015167c19da080":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e804a4e7b2a34bc0a9567f214fe27408","placeholder":"​","style":"IPY_MODEL_05c76a3bbe0a4792b5631b795a73723a","value":" 1/1 [00:01&lt;00:00,  1.89s/it]"}},"9eaf480fdb4b4573b6fd750d6b1297bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f33da811c51943faa058d2f51471be71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42103bf303c240eabc6a0485c534ab9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d74468620ab4843a739ff2626fc1d52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d4d22b861f84907b47cd9a11214d229":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e804a4e7b2a34bc0a9567f214fe27408":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05c76a3bbe0a4792b5631b795a73723a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70oLi9mZP6oK","executionInfo":{"status":"ok","timestamp":1649211697755,"user_tz":240,"elapsed":338,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"df3459cb-4db3-4d76-9524-53e75226286d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Apr  6 02:21:36 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGqcN-zXTvvo","executionInfo":{"status":"ok","timestamp":1649211707572,"user_tz":240,"elapsed":9819,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"c2851128-7ae7-4629-e27c-f11dce23e54a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n","\u001b[?25l\r\u001b[K     |█                               | 10 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 325 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 39.3 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 61.3 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Collecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.0-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 6.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 64.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 64.6 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 58.6 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 61.3 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tI_RBT1FSotu","outputId":"22b9252e-f313-424c-e801-044fb2ead1d5","executionInfo":{"status":"ok","timestamp":1649211734381,"user_tz":240,"elapsed":26815,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-9lyofnx8\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-9lyofnx8\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.63.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 2.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (1.21.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (3.6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 54.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.11.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 58.8 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (0.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0.dev0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0.dev0) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0.dev0) (3.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.1.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.18.0.dev0-py3-none-any.whl size=3958915 sha256=aee8c8e70fca0f2fea9458d87d398eadd1fbde5cb221ee8d43fbd0cb5af29f25\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ig_e2_9c/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0.dev0\n"]}]},{"cell_type":"code","source":["import torch\n","import datetime\n","import json\n","import os\n","import time\n","import datasets\n","import pprint\n","import random\n","import string\n","import sys\n","import transformers\n","from datasets import load_dataset\n","from datasets.tasks import QuestionAnsweringExtractive"],"metadata":{"id":"DZ3Ma-pCRJDJ","executionInfo":{"status":"ok","timestamp":1649211743171,"user_tz":240,"elapsed":8797,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available()\n","                      else 'cpu')"],"metadata":{"id":"HNMUVyBpRGw8","executionInfo":{"status":"ok","timestamp":1649211743172,"user_tz":240,"elapsed":7,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/huggingface/transformers.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUVkgX-IQIiR","executionInfo":{"status":"ok","timestamp":1649211750550,"user_tz":240,"elapsed":7384,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"e6da274e-90c8-4e08-ec58-9235a7fb2137"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 108469, done.\u001b[K\n","remote: Counting objects: 100% (8/8), done.\u001b[K\n","remote: Compressing objects: 100% (8/8), done.\u001b[K\n","remote: Total 108469 (delta 0), reused 4 (delta 0), pack-reused 108461\u001b[K\n","Receiving objects: 100% (108469/108469), 95.18 MiB | 29.06 MiB/s, done.\n","Resolving deltas: 100% (79034/79034), done.\n"]}]},{"cell_type":"code","source":["dataset = load_dataset('/content/drive/MyDrive/QA/squad_v2_word2vec_aug.py')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":148,"referenced_widgets":["53b5723253a14aa58ce325a34c55deb0","498f613046fa4b219b76d475ac850a59","164f38a7c1a84762bf85bb6f01b01ec3","339fea78df5341b1a4a6b0ff9b590e0a","f591460c52b042dabcb6369eee0d54f2","77100f42d27541d0a030d91378c2ce30","4d9cefdc1ba7433fb474987026c24109","4c38606e113f4fca8cc5fea742e30cdb","bafdd201914b4e5e8a7bacaeb4b18754","8d2244abca21429190077b5487714eaa","302bdb70ce84405abdeee85b2c33c09f","219ceddb2eac4034a6874af046aa4c50","20e9e091d1cb496aa2aa632b30aa4eac","4ccb6d78c2a649ebbeb7df0cb16da6a9","c491ce4df0f84438a466e6afdcc1e554","c6f4385cf6eb440299fdf0a9db9db70a","d4d6cc8bda694166948aa342e965c122","37df5cd7ae9a41e48d5396368f431a66","ac306dd4e5f947bcad6fbde244e48832","3bce3dc579a94ddb890c4a8170839e7a","da2f9cdb4c424521976b351e16d71f3a","07a01fc6296948d3aa815a8ae0a7ab3a","564187fc18724d9e9237f9e37b577bfd","10f07942f31343b2b24051ecd4343744","2e9ef56d440b4c26bff07569aa481c30","dfa8d428ca64453086553fa7667916ee","89d7417b82384f2fbb9ce5fb4892c644","9080046ae703471eb43365962a07b3f4","a06b968632ab41e2a73d8ec91a8b2793","59749782ef00464cb2781072280ab60b","336e8111f3284225bee52eb453a342ff","3d53805b5deb4e94b927f41f4d542e18","e82e5a081fa144d9a137a6206375a812","576bfd8bbca543308fb421a79d680dd8","2aa9dd2bcf8e409a9e4c3e44c634f2b2","1f7e0eaff73b4974a8ca67337b66b5b7","6e9e8c516e274edebbab2489cd29dc29","203b962a7f1e431aa2a2213d27000548","c0f285b79acb4ab69bf9db389b2cbd79","2814935a07c94979af5422ea855ba76a","a4f4841398fe4f76bf4f6d0a07b75108","37fd4fb1a6514392aab1eefdcbb4246a","634f9caaf63e440d8ef0110746d0164f","ce457d54c2b247c18f97e79707d530a0","710533fabc184bbba06cb74ac6dd31ea","1bca0364d956417380484be8696fb3d7","71cbee9cf3fc475fa1271dab6409ad68","ee3310cd20904acd9f8f3435548d5413","a39582ad48e84e6ab607f6dcdc089a97","8ea8204b674f4878a9a64718f27ed98c","0dbbe850fcee4889b351907fb50c22f7","d37ed23cbc10446c9f4fc632cd729308","849d0aae16fd4ffca7b5e7ac270cb0dd","1beb52ad61494d38875ee00ea769ac48","2f2f16fef5754b91aafba2422bc6303f"]},"id":"A2gdlPTvcmfL","executionInfo":{"status":"ok","timestamp":1648601077697,"user_tz":240,"elapsed":13153,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"c990537d-75b3-4111-b86b-b3e416eb8a69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset squad_v2_aug/squad_v2 to /root/.cache/huggingface/datasets/squad_v2_aug/squad_v2/2.0.0/9a5193b8c1e21274c16ea8ce275c9ef705559bfccfe1bbe1d82b0cbc31a7245b...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53b5723253a14aa58ce325a34c55deb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"219ceddb2eac4034a6874af046aa4c50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"564187fc18724d9e9237f9e37b577bfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"576bfd8bbca543308fb421a79d680dd8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset squad_v2_aug downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2_aug/squad_v2/2.0.0/9a5193b8c1e21274c16ea8ce275c9ef705559bfccfe1bbe1d82b0cbc31a7245b. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"710533fabc184bbba06cb74ac6dd31ea"}},"metadata":{}}]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttpAfwxBWdR0","executionInfo":{"status":"ok","timestamp":1648601131434,"user_tz":240,"elapsed":53740,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"1fadfd97-3f93-4158-dbc8-83f8521d7c57"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/token.\n","        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n","        \n","Token: \n","Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}]},{"cell_type":"code","source":["dataset.push_to_hub(\"sichenzhong/squad_v2_word2vec_aug\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":150,"referenced_widgets":["a648710f3853408f8c98e6f145fd8418","f16e5afd7d584a6385a75c9879dc6f48","d4fcb1b6b62843d3996d07ad20a57866","4f5642ce2ab444b59304285762768a51","0fae5b53f42d46f4af7f3656194fb7e3","f3abc2abeac8458ab9a93e3a4a9c5a95","7bf0b7800146463cb8ffc3f2d352e3c5","714868d458144630919f6b8acdcf1601","5d2c85fc71cb47699f167318ef165239","79a08eb572594f55bd38adc2f2097344","dfc8056e4c2f42228d93bab8783bf15d","168ee085f5aa40b9a02345d4fb78f08d","eedd7c64adef4924a2901c0a299979b7","9734aada5dfe4e5d9d69b18b0ad0481b","f7af7d2a28134f2bb9015167c19da080","9eaf480fdb4b4573b6fd750d6b1297bd","f33da811c51943faa058d2f51471be71","42103bf303c240eabc6a0485c534ab9f","1d74468620ab4843a739ff2626fc1d52","0d4d22b861f84907b47cd9a11214d229","e804a4e7b2a34bc0a9567f214fe27408","05c76a3bbe0a4792b5631b795a73723a"]},"id":"WGAeWENJWR_F","executionInfo":{"status":"ok","timestamp":1648601141985,"user_tz":240,"elapsed":10561,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"175432bd-231a-4bff-e0ea-bb31907e7037"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Pushing split train to the Hub.\n","The repository already exists: the `private` keyword argument will be ignored.\n"]},{"output_type":"display_data","data":{"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a648710f3853408f8c98e6f145fd8418"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Pushing split validation to the Hub.\n","The repository already exists: the `private` keyword argument will be ignored.\n"]},{"output_type":"display_data","data":{"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"168ee085f5aa40b9a02345d4fb78f08d"}},"metadata":{}}]},{"cell_type":"code","source":["%cd /content/transformers/examples/pytorch/question-answering/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uLaizsXQrk9","executionInfo":{"status":"ok","timestamp":1649211750550,"user_tz":240,"elapsed":17,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"53963e41-eea0-497d-eb37-c02346327225"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/transformers/examples/pytorch/question-answering\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path bert-base-cased \\\n","  --dataset_name sichenzhong/squad_v2_word2vec_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 3 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aI-ipSbaHqF6","executionInfo":{"status":"ok","timestamp":1649228315074,"user_tz":240,"elapsed":16563763,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"b9a83651-3f46-43db-a6eb-76ed90779f32"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["04/06/2022 02:22:34 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 02:22:34 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/runs/Apr06_02-22-34_e998dce90490,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 02:22:35 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/sichenzhong/squad_v2_word2vec_aug/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpbea1lt8n\n","Downloading: 100% 2.13k/2.13k [00:00<00:00, 2.84MB/s]\n","04/06/2022 02:22:35 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/sichenzhong/squad_v2_word2vec_aug/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/092f4304f430e105096247458cd8596544dacdfb865c2251f6f5efa61eed1e89.47f464735ab4095336165fae0b1d933d4b0378a112edc84d594e79894a2c1de3\n","04/06/2022 02:22:35 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/092f4304f430e105096247458cd8596544dacdfb865c2251f6f5efa61eed1e89.47f464735ab4095336165fae0b1d933d4b0378a112edc84d594e79894a2c1de3\n","04/06/2022 02:22:35 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a\n","04/06/2022 02:22:35 - INFO - datasets.builder - Generating dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","Downloading and preparing dataset squad_v2_aug/squad_v2 (download: 19.66 MiB, generated: 125.16 MiB, post-processed: Unknown size, total: 144.81 MiB) to /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n","04/06/2022 02:22:35 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]04/06/2022 02:22:35 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/sichenzhong/squad_v2_word2vec_aug/resolve/caaf42b1456b58f07e576df3e2d3348b4ef7bf6d/data/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp29wpcby9\n","\n","Downloading data:   0% 0.00/19.4M [00:00<?, ?B/s]\u001b[A\n","Downloading data:  13% 2.43M/19.4M [00:00<00:00, 24.3MB/s]\u001b[A\n","Downloading data:  35% 6.86M/19.4M [00:00<00:00, 36.1MB/s]\u001b[A\n","Downloading data:  61% 11.7M/19.4M [00:00<00:00, 41.7MB/s]\u001b[A\n","Downloading data: 100% 19.4M/19.4M [00:00<00:00, 42.3MB/s]\n","04/06/2022 02:22:36 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/sichenzhong/squad_v2_word2vec_aug/resolve/caaf42b1456b58f07e576df3e2d3348b4ef7bf6d/data/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/672dda4eb4a03c06c5a9bff752f8fad5f0bea581fe42a1510a058ad372555dc2\n","04/06/2022 02:22:36 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/672dda4eb4a03c06c5a9bff752f8fad5f0bea581fe42a1510a058ad372555dc2\n","Downloading data files:  50% 1/2 [00:01<00:01,  1.15s/it]04/06/2022 02:22:37 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/sichenzhong/squad_v2_word2vec_aug/resolve/caaf42b1456b58f07e576df3e2d3348b4ef7bf6d/data/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp4hdu268w\n","\n","Downloading data: 100% 1.26M/1.26M [00:00<00:00, 12.9MB/s]\n","04/06/2022 02:22:37 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/sichenzhong/squad_v2_word2vec_aug/resolve/caaf42b1456b58f07e576df3e2d3348b4ef7bf6d/data/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/0cc581d38bcdb7b3ae4aef18ee636bd53b8d6b3d0cc595d048ff9f284dc69ad5\n","04/06/2022 02:22:37 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/0cc581d38bcdb7b3ae4aef18ee636bd53b8d6b3d0cc595d048ff9f284dc69ad5\n","Downloading data files: 100% 2/2 [00:01<00:00,  1.02it/s]\n","04/06/2022 02:22:37 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","04/06/2022 02:22:37 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1113.43it/s]\n","04/06/2022 02:22:37 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n","04/06/2022 02:22:37 - INFO - datasets.builder - Generating train split\n","04/06/2022 02:22:37 - INFO - datasets.builder - Generating validation split\n","04/06/2022 02:22:37 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 290.06it/s]\n","[INFO|hub.py:583] 2022-04-06 02:22:38,097 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplr2bvgaw\n","Downloading: 100% 570/570 [00:00<00:00, 621kB/s]\n","[INFO|hub.py:587] 2022-04-06 02:22:38,239 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|hub.py:595] 2022-04-06 02:22:38,239 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:654] 2022-04-06 02:22:38,239 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 02:22:38,240 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 02:22:38,383 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4vo25jdm\n","Downloading: 100% 29.0/29.0 [00:00<00:00, 28.2kB/s]\n","[INFO|hub.py:587] 2022-04-06 02:22:38,516 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|hub.py:595] 2022-04-06 02:22:38,516 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:654] 2022-04-06 02:22:38,650 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 02:22:38,651 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 02:22:38,926 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwbvk8538\n","Downloading: 100% 208k/208k [00:00<00:00, 1.81MB/s]\n","[INFO|hub.py:587] 2022-04-06 02:22:39,183 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:595] 2022-04-06 02:22:39,183 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:583] 2022-04-06 02:22:39,349 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn_9bzdqh\n","Downloading: 100% 426k/426k [00:00<00:00, 3.20MB/s]\n","[INFO|hub.py:587] 2022-04-06 02:22:39,624 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|hub.py:595] 2022-04-06 02:22:39,624 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 02:22:40,035 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 02:22:40,035 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 02:22:40,035 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 02:22:40,035 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 02:22:40,035 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:654] 2022-04-06 02:22:40,174 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 02:22:40,175 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 02:22:40,364 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwgs1mv4c\n","Downloading: 100% 416M/416M [00:08<00:00, 48.6MB/s]\n","[INFO|hub.py:587] 2022-04-06 02:22:49,416 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|hub.py:595] 2022-04-06 02:22:49,416 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|modeling_utils.py:1772] 2022-04-06 02:22:49,417 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:2050] 2022-04-06 02:22:53,053 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-06 02:22:53,053 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/06/2022 02:22:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-9dd8dacc9a010e92.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:49<00:00,  2.63ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/06/2022 02:23:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-a1cd7a64dab8fb21.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:13<00:00,  6.13s/ba]\n","04/06/2022 02:24:56 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpenlcmw8e\n","Downloading builder script: 6.46kB [00:00, 4.86MB/s]       \n","04/06/2022 02:24:56 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/06/2022 02:24:56 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/06/2022 02:24:57 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpo_riwa77\n","Downloading extra modules: 11.3kB [00:00, 12.4MB/s]       \n","04/06/2022 02:24:57 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","04/06/2022 02:24:57 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 02:25:09,491 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 02:25:09,491 >>   Num examples = 132357\n","[INFO|trainer.py:1292] 2022-04-06 02:25:09,491 >>   Num Epochs = 3\n","[INFO|trainer.py:1293] 2022-04-06 02:25:09,491 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1294] 2022-04-06 02:25:09,491 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1295] 2022-04-06 02:25:09,491 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 02:25:09,491 >>   Total optimization steps = 24819\n","{'loss': 2.7237, 'learning_rate': 3.919416576010315e-05, 'epoch': 0.06}\n","  2% 500/24819 [05:21<4:20:22,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 02:30:31,156 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 02:30:31,162 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:30:32,337 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:30:32,342 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:30:32,345 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 2.1103, 'learning_rate': 3.838833152020629e-05, 'epoch': 0.12}\n","  4% 1000/24819 [10:47<4:14:54,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 02:35:56,614 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 02:35:56,619 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:35:57,768 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:35:57,773 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:35:57,777 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.9367, 'learning_rate': 3.7582497280309447e-05, 'epoch': 0.18}\n","  6% 1500/24819 [16:12<4:09:23,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 02:41:22,172 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 02:41:22,178 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:41:23,343 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:41:23,348 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:41:23,352 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.8136, 'learning_rate': 3.677666304041259e-05, 'epoch': 0.24}\n","  8% 2000/24819 [21:37<4:04:36,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 02:46:47,308 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 02:46:47,314 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:46:48,488 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:46:48,492 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:46:48,496 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.7182, 'learning_rate': 3.5970828800515735e-05, 'epoch': 0.3}\n"," 10% 2500/24819 [27:03<3:59:12,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 02:52:12,676 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 02:52:12,682 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:52:13,838 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:52:13,843 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:52:13,848 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.6995, 'learning_rate': 3.516499456061888e-05, 'epoch': 0.36}\n"," 12% 3000/24819 [32:28<3:54:15,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 02:57:38,009 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 02:57:38,032 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:57:39,198 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:57:39,203 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:57:39,206 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.6203, 'learning_rate': 3.435916032072203e-05, 'epoch': 0.42}\n"," 14% 3500/24819 [37:53<3:48:12,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 03:03:03,247 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:03:03,254 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:03:04,399 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:03:04,422 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:03:04,427 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.6089, 'learning_rate': 3.355332608082518e-05, 'epoch': 0.48}\n"," 16% 4000/24819 [43:18<3:42:46,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 03:08:28,084 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:08:28,091 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:08:29,238 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:08:29,243 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:08:29,248 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.5545, 'learning_rate': 3.274749184092832e-05, 'epoch': 0.54}\n"," 18% 4500/24819 [48:43<3:36:39,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 03:13:52,886 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:13:52,892 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:13:54,043 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:13:54,048 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:13:54,051 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.5284, 'learning_rate': 3.194165760103147e-05, 'epoch': 0.6}\n"," 20% 5000/24819 [54:08<3:31:42,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 03:19:17,913 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:19:17,919 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:19:19,064 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:19:19,070 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:19:19,074 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.5152, 'learning_rate': 3.1135823361134615e-05, 'epoch': 0.66}\n"," 22% 5500/24819 [59:33<3:27:22,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:24:43,048 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:24:43,053 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:24:44,195 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:24:44,200 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:24:44,204 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.485, 'learning_rate': 3.0329989121237763e-05, 'epoch': 0.73}\n"," 24% 6000/24819 [1:04:58<3:21:55,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:30:08,109 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:30:08,115 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:30:09,254 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:30:09,260 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:30:09,264 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.4366, 'learning_rate': 2.952415488134091e-05, 'epoch': 0.79}\n"," 26% 6500/24819 [1:10:24<3:16:01,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 03:35:34,260 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:35:34,266 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:35:35,409 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:35:35,413 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:35:35,417 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.4484, 'learning_rate': 2.8718320641444055e-05, 'epoch': 0.85}\n"," 28% 7000/24819 [1:15:49<3:10:38,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 03:40:59,077 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:40:59,082 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:41:00,225 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:41:00,229 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:41:00,233 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.4159, 'learning_rate': 2.7912486401547206e-05, 'epoch': 0.91}\n"," 30% 7500/24819 [1:21:17<3:05:25,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 03:46:26,552 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:46:26,558 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:46:27,722 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:46:27,727 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:46:27,730 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 1.4255, 'learning_rate': 2.710665216165035e-05, 'epoch': 0.97}\n"," 32% 8000/24819 [1:26:45<3:00:03,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 03:51:54,643 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:51:54,650 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:51:55,802 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:51:55,808 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:51:55,811 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 1.2307, 'learning_rate': 2.6300817921753495e-05, 'epoch': 1.03}\n"," 34% 8500/24819 [1:32:09<2:54:50,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 03:57:19,171 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:57:19,176 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:57:20,389 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:57:20,394 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:57:20,398 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 1.0814, 'learning_rate': 2.5494983681856643e-05, 'epoch': 1.09}\n"," 36% 9000/24819 [1:37:34<2:49:19,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 04:02:44,461 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:02:44,468 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:02:45,728 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:02:45,733 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:02:45,737 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 1.0673, 'learning_rate': 2.468914944195979e-05, 'epoch': 1.15}\n"," 38% 9500/24819 [1:43:00<2:43:53,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 04:08:10,219 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:08:10,225 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:08:11,451 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:08:11,458 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:08:11,462 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 1.0877, 'learning_rate': 2.388331520206294e-05, 'epoch': 1.21}\n"," 40% 10000/24819 [1:48:25<2:38:56,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 04:13:35,411 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:13:35,417 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:13:36,616 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:13:36,621 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:13:36,624 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 1.0825, 'learning_rate': 2.3077480962166083e-05, 'epoch': 1.27}\n"," 42% 10500/24819 [1:53:51<2:33:00,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 04:19:00,564 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:19:00,570 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:19:01,762 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:19:01,768 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:19:01,772 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-10500/special_tokens_map.json\n","{'loss': 1.0823, 'learning_rate': 2.227164672226923e-05, 'epoch': 1.33}\n"," 44% 11000/24819 [1:59:16<2:27:48,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 04:24:25,554 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:24:25,565 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:24:26,764 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:24:26,769 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:24:26,772 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-11000/special_tokens_map.json\n","{'loss': 1.0882, 'learning_rate': 2.1465812482372378e-05, 'epoch': 1.39}\n"," 46% 11500/24819 [2:04:40<2:22:24,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 04:29:50,531 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-11500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:29:50,537 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:29:51,761 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:29:51,784 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:29:51,788 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-11500/special_tokens_map.json\n","{'loss': 1.073, 'learning_rate': 2.0659978242475526e-05, 'epoch': 1.45}\n"," 48% 12000/24819 [2:10:06<2:16:50,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 04:35:15,610 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-12000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:35:15,616 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:35:16,824 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:35:16,845 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:35:16,851 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-12000/special_tokens_map.json\n","{'loss': 1.058, 'learning_rate': 1.985414400257867e-05, 'epoch': 1.51}\n"," 50% 12500/24819 [2:15:31<2:11:43,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 04:40:40,612 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-12500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:40:40,618 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:40:41,813 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:40:41,817 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:40:41,821 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-12500/special_tokens_map.json\n","{'loss': 1.0551, 'learning_rate': 1.9048309762681818e-05, 'epoch': 1.57}\n"," 52% 13000/24819 [2:20:56<2:06:23,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 04:46:05,630 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-13000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:46:05,637 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:46:06,845 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:46:06,849 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:46:06,853 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-13000/special_tokens_map.json\n","{'loss': 1.0692, 'learning_rate': 1.8242475522784963e-05, 'epoch': 1.63}\n"," 54% 13500/24819 [2:26:21<2:01:14,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 04:51:30,841 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-13500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:51:30,847 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:51:32,036 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:51:32,040 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:51:32,043 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-13500/special_tokens_map.json\n","{'loss': 1.0392, 'learning_rate': 1.743664128288811e-05, 'epoch': 1.69}\n"," 56% 14000/24819 [2:31:46<1:55:29,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 04:56:56,002 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-14000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:56:56,008 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:56:57,224 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:56:57,229 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:56:57,233 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-14000/special_tokens_map.json\n","{'loss': 1.027, 'learning_rate': 1.6630807042991258e-05, 'epoch': 1.75}\n"," 58% 14500/24819 [2:37:11<1:50:19,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 05:02:21,157 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-14500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:02:21,162 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:02:22,371 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:02:22,375 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:02:22,379 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-14500/special_tokens_map.json\n","{'loss': 1.0498, 'learning_rate': 1.5824972803094406e-05, 'epoch': 1.81}\n"," 60% 15000/24819 [2:42:36<1:45:02,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 05:07:46,182 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-15000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:07:46,189 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:07:47,384 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:07:47,390 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:07:47,394 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-15000/special_tokens_map.json\n","{'loss': 1.0277, 'learning_rate': 1.501913856319755e-05, 'epoch': 1.87}\n"," 62% 15500/24819 [2:48:04<1:39:42,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 05:13:14,120 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-15500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:13:14,126 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:13:15,313 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:13:15,318 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:13:15,322 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-15500/special_tokens_map.json\n","{'loss': 1.0231, 'learning_rate': 1.4213304323300698e-05, 'epoch': 1.93}\n"," 64% 16000/24819 [2:53:32<1:34:08,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 05:18:42,126 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-16000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:18:42,132 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:18:43,331 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:18:43,336 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:18:43,340 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-16000/special_tokens_map.json\n","{'loss': 1.0331, 'learning_rate': 1.3407470083403844e-05, 'epoch': 1.99}\n"," 66% 16500/24819 [2:59:00<1:28:59,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 05:24:10,085 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-16500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:24:10,092 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-16500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:24:11,301 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-16500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:24:11,305 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-16500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:24:11,309 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-16500/special_tokens_map.json\n","{'loss': 0.7527, 'learning_rate': 1.2601635843506992e-05, 'epoch': 2.05}\n"," 68% 17000/24819 [3:04:28<1:23:33,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 05:29:37,583 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-17000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:29:37,589 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-17000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:29:38,795 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-17000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:29:38,800 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-17000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:29:38,803 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-17000/special_tokens_map.json\n","{'loss': 0.7034, 'learning_rate': 1.1795801603610138e-05, 'epoch': 2.12}\n"," 71% 17500/24819 [3:09:53<1:18:25,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 05:35:02,771 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-17500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:35:02,778 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-17500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:35:03,962 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-17500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:35:03,967 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-17500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:35:03,970 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-17500/special_tokens_map.json\n","{'loss': 0.7063, 'learning_rate': 1.0989967363713286e-05, 'epoch': 2.18}\n"," 73% 18000/24819 [3:15:18<1:12:53,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 05:40:28,306 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-18000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:40:28,311 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-18000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:40:29,528 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-18000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:40:29,534 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-18000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:40:29,538 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-18000/special_tokens_map.json\n","{'loss': 0.7083, 'learning_rate': 1.018413312381643e-05, 'epoch': 2.24}\n"," 75% 18500/24819 [3:20:45<1:07:26,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 05:45:54,790 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-18500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:45:54,796 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-18500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:45:56,011 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-18500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:45:56,017 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-18500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:45:56,021 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-18500/special_tokens_map.json\n","{'loss': 0.6968, 'learning_rate': 9.37829888391958e-06, 'epoch': 2.3}\n"," 77% 19000/24819 [3:26:09<1:02:10,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 05:51:19,456 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-19000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:51:19,462 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-19000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:51:20,650 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-19000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:51:20,656 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-19000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:51:20,660 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-19000/special_tokens_map.json\n","{'loss': 0.6888, 'learning_rate': 8.572464644022726e-06, 'epoch': 2.36}\n"," 79% 19500/24819 [3:31:34<56:52,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 05:56:44,269 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-19500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:56:44,276 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-19500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:56:45,489 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-19500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:56:45,494 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-19500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:56:45,497 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-19500/special_tokens_map.json\n","{'loss': 0.6941, 'learning_rate': 7.766630404125872e-06, 'epoch': 2.42}\n"," 81% 20000/24819 [3:37:00<51:36,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 06:02:09,563 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-20000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:02:09,569 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-20000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:02:10,786 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-20000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:02:10,791 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-20000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:02:10,795 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-20000/special_tokens_map.json\n","{'loss': 0.6829, 'learning_rate': 6.960796164229019e-06, 'epoch': 2.48}\n"," 83% 20500/24819 [3:42:25<46:15,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 06:07:34,566 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-20500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:07:34,572 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-20500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:07:35,777 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-20500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:07:35,782 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-20500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:07:35,785 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-20500/special_tokens_map.json\n","{'loss': 0.6963, 'learning_rate': 6.1549619243321655e-06, 'epoch': 2.54}\n"," 85% 21000/24819 [3:47:49<40:47,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 06:12:59,198 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-21000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:12:59,205 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-21000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:13:00,393 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-21000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:13:00,398 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-21000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:13:00,402 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-21000/special_tokens_map.json\n","{'loss': 0.6864, 'learning_rate': 5.3491276844353125e-06, 'epoch': 2.6}\n"," 87% 21500/24819 [3:53:14<35:24,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 06:18:23,727 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-21500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:18:23,732 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-21500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:18:24,954 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-21500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:18:24,959 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-21500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:18:24,963 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-21500/special_tokens_map.json\n","{'loss': 0.683, 'learning_rate': 4.5432934445384585e-06, 'epoch': 2.66}\n"," 89% 22000/24819 [3:58:39<30:07,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 06:23:48,598 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-22000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:23:48,605 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-22000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:23:49,808 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-22000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:23:49,812 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-22000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:23:49,816 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-22000/special_tokens_map.json\n","{'loss': 0.6787, 'learning_rate': 3.7374592046416055e-06, 'epoch': 2.72}\n"," 91% 22500/24819 [4:04:03<24:49,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 06:29:13,415 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-22500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:29:13,421 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-22500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:29:14,642 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-22500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:29:14,647 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-22500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:29:14,651 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-22500/special_tokens_map.json\n","{'loss': 0.6933, 'learning_rate': 2.931624964744752e-06, 'epoch': 2.78}\n"," 93% 23000/24819 [4:09:28<19:26,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 06:34:38,437 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-23000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:34:38,443 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-23000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:34:39,652 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-23000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:34:39,657 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-23000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:34:39,660 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-23000/special_tokens_map.json\n","{'loss': 0.6991, 'learning_rate': 2.125790724847899e-06, 'epoch': 2.84}\n"," 95% 23500/24819 [4:14:56<14:06,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 06:40:06,356 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-23500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:40:06,362 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-23500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:40:07,585 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-23500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:40:07,590 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-23500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:40:07,593 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-23500/special_tokens_map.json\n","{'loss': 0.697, 'learning_rate': 1.3199564849510458e-06, 'epoch': 2.9}\n"," 97% 24000/24819 [4:20:24<08:45,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 06:45:34,202 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-24000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:45:34,209 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-24000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:45:35,415 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-24000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:45:35,420 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-24000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:45:35,424 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-24000/special_tokens_map.json\n","{'loss': 0.6677, 'learning_rate': 5.141222450541924e-07, 'epoch': 2.96}\n"," 99% 24500/24819 [4:25:51<03:24,  1.56it/s][INFO|trainer.py:2166] 2022-04-06 06:51:01,310 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-24500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:51:01,316 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-24500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:51:02,533 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-24500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:51:02,538 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-24500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:51:02,541 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/checkpoint-24500/special_tokens_map.json\n","100% 24819/24819 [4:29:20<00:00,  1.92it/s][INFO|trainer.py:1530] 2022-04-06 06:54:30,067 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 16160.5757, 'train_samples_per_second': 24.57, 'train_steps_per_second': 1.536, 'train_loss': 1.1440450744493649, 'epoch': 3.0}\n","100% 24819/24819 [4:29:20<00:00,  1.54it/s]\n","[INFO|trainer.py:2166] 2022-04-06 06:54:30,072 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug\n","[INFO|configuration_utils.py:441] 2022-04-06 06:54:30,078 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:54:31,391 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:54:31,399 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:54:31,403 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =      1.144\n","  train_runtime            = 4:29:20.57\n","  train_samples            =     132357\n","  train_samples_per_second =      24.57\n","  train_steps_per_second   =      1.536\n","04/06/2022 06:54:33 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 06:54:33,028 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 06:54:33,050 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 06:54:33,050 >>   Num examples = 12199\n","[INFO|trainer.py:2421] 2022-04-06 06:54:33,051 >>   Batch size = 8\n","100% 1525/1525 [02:58<00:00,  8.73it/s]04/06/2022 06:57:45 - INFO - utils_qa - Post-processing 11873 example predictions split into 12199 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 31/11873 [00:00<00:39, 302.91it/s]\u001b[A\n","  1% 67/11873 [00:00<00:35, 329.61it/s]\u001b[A\n","  1% 104/11873 [00:00<00:34, 344.57it/s]\u001b[A\n","  1% 143/11873 [00:00<00:32, 359.42it/s]\u001b[A\n","  2% 185/11873 [00:00<00:30, 380.59it/s]\u001b[A\n","  2% 226/11873 [00:00<00:29, 388.35it/s]\u001b[A\n","  2% 266/11873 [00:00<00:29, 389.69it/s]\u001b[A\n","  3% 307/11873 [00:00<00:29, 395.97it/s]\u001b[A\n","  3% 347/11873 [00:00<00:29, 394.42it/s]\u001b[A\n","  3% 388/11873 [00:01<00:28, 398.49it/s]\u001b[A\n","  4% 428/11873 [00:01<00:28, 398.60it/s]\u001b[A\n","  4% 469/11873 [00:01<00:28, 399.90it/s]\u001b[A\n","  4% 509/11873 [00:01<00:28, 392.15it/s]\u001b[A\n","  5% 549/11873 [00:01<00:29, 382.77it/s]\u001b[A\n","  5% 588/11873 [00:01<00:29, 377.34it/s]\u001b[A\n","  5% 628/11873 [00:01<00:29, 381.60it/s]\u001b[A\n","  6% 669/11873 [00:01<00:28, 387.82it/s]\u001b[A\n","  6% 708/11873 [00:01<00:29, 381.24it/s]\u001b[A\n","  6% 747/11873 [00:01<00:29, 376.98it/s]\u001b[A\n","  7% 785/11873 [00:02<00:29, 374.92it/s]\u001b[A\n","  7% 824/11873 [00:02<00:29, 378.70it/s]\u001b[A\n","  7% 866/11873 [00:02<00:28, 389.17it/s]\u001b[A\n","  8% 907/11873 [00:02<00:27, 392.94it/s]\u001b[A\n","  8% 948/11873 [00:02<00:27, 395.99it/s]\u001b[A\n","  8% 988/11873 [00:02<00:28, 388.74it/s]\u001b[A\n","  9% 1027/11873 [00:02<00:29, 369.64it/s]\u001b[A\n","  9% 1065/11873 [00:02<00:30, 355.48it/s]\u001b[A\n","  9% 1101/11873 [00:02<00:31, 343.95it/s]\u001b[A\n"," 10% 1136/11873 [00:03<00:31, 340.30it/s]\u001b[A\n"," 10% 1171/11873 [00:03<00:32, 332.52it/s]\u001b[A\n"," 10% 1205/11873 [00:03<00:32, 331.87it/s]\u001b[A\n"," 10% 1239/11873 [00:03<00:32, 326.70it/s]\u001b[A\n"," 11% 1272/11873 [00:03<00:33, 319.58it/s]\u001b[A\n"," 11% 1304/11873 [00:03<00:33, 314.78it/s]\u001b[A\n"," 11% 1336/11873 [00:03<00:34, 306.40it/s]\u001b[A\n"," 12% 1369/11873 [00:03<00:33, 311.03it/s]\u001b[A\n"," 12% 1402/11873 [00:03<00:33, 314.44it/s]\u001b[A\n"," 12% 1435/11873 [00:03<00:32, 318.18it/s]\u001b[A\n"," 12% 1467/11873 [00:04<00:33, 314.62it/s]\u001b[A\n"," 13% 1499/11873 [00:04<00:32, 315.24it/s]\u001b[A\n"," 13% 1533/11873 [00:04<00:32, 320.35it/s]\u001b[A\n"," 13% 1566/11873 [00:04<00:32, 318.65it/s]\u001b[A\n"," 13% 1600/11873 [00:04<00:31, 323.76it/s]\u001b[A\n"," 14% 1633/11873 [00:04<00:32, 319.20it/s]\u001b[A\n"," 14% 1666/11873 [00:04<00:31, 321.24it/s]\u001b[A\n"," 14% 1699/11873 [00:04<00:32, 317.25it/s]\u001b[A\n"," 15% 1732/11873 [00:04<00:31, 318.52it/s]\u001b[A\n"," 15% 1764/11873 [00:05<00:31, 317.67it/s]\u001b[A\n"," 15% 1796/11873 [00:05<00:32, 314.02it/s]\u001b[A\n"," 15% 1829/11873 [00:05<00:31, 316.73it/s]\u001b[A\n"," 16% 1862/11873 [00:05<00:31, 319.08it/s]\u001b[A\n"," 16% 1894/11873 [00:05<00:31, 319.35it/s]\u001b[A\n"," 16% 1927/11873 [00:05<00:31, 319.65it/s]\u001b[A\n"," 17% 1960/11873 [00:05<00:30, 321.52it/s]\u001b[A\n"," 17% 1993/11873 [00:05<00:30, 320.86it/s]\u001b[A\n"," 17% 2026/11873 [00:05<00:30, 322.44it/s]\u001b[A\n"," 17% 2059/11873 [00:05<00:30, 320.20it/s]\u001b[A\n"," 18% 2092/11873 [00:06<00:30, 321.78it/s]\u001b[A\n"," 18% 2125/11873 [00:06<00:30, 320.14it/s]\u001b[A\n"," 18% 2159/11873 [00:06<00:30, 322.51it/s]\u001b[A\n"," 18% 2192/11873 [00:06<00:30, 318.86it/s]\u001b[A\n"," 19% 2224/11873 [00:06<00:30, 318.03it/s]\u001b[A\n"," 19% 2257/11873 [00:06<00:30, 318.74it/s]\u001b[A\n"," 19% 2289/11873 [00:06<00:30, 318.38it/s]\u001b[A\n"," 20% 2321/11873 [00:06<00:30, 315.28it/s]\u001b[A\n"," 20% 2353/11873 [00:06<00:30, 316.46it/s]\u001b[A\n"," 20% 2386/11873 [00:06<00:29, 319.69it/s]\u001b[A\n"," 20% 2418/11873 [00:07<00:30, 312.73it/s]\u001b[A\n"," 21% 2450/11873 [00:07<00:30, 312.30it/s]\u001b[A\n"," 21% 2482/11873 [00:07<00:29, 313.81it/s]\u001b[A\n"," 21% 2514/11873 [00:07<00:29, 312.31it/s]\u001b[A\n"," 21% 2546/11873 [00:07<00:30, 306.31it/s]\u001b[A\n"," 22% 2577/11873 [00:07<00:30, 304.10it/s]\u001b[A\n"," 22% 2608/11873 [00:07<00:30, 301.15it/s]\u001b[A\n"," 22% 2640/11873 [00:07<00:30, 304.72it/s]\u001b[A\n"," 23% 2673/11873 [00:07<00:29, 311.08it/s]\u001b[A\n"," 23% 2706/11873 [00:08<00:29, 315.01it/s]\u001b[A\n"," 23% 2738/11873 [00:08<00:28, 316.18it/s]\u001b[A\n"," 23% 2770/11873 [00:08<00:29, 313.87it/s]\u001b[A\n"," 24% 2803/11873 [00:08<00:28, 316.96it/s]\u001b[A\n"," 24% 2837/11873 [00:08<00:28, 321.24it/s]\u001b[A\n"," 24% 2870/11873 [00:08<00:27, 321.74it/s]\u001b[A\n"," 24% 2903/11873 [00:08<00:28, 320.29it/s]\u001b[A\n"," 25% 2936/11873 [00:08<00:28, 317.98it/s]\u001b[A\n"," 25% 2969/11873 [00:08<00:27, 320.35it/s]\u001b[A\n"," 25% 3002/11873 [00:08<00:31, 279.74it/s]\u001b[A\n"," 26% 3031/11873 [00:09<00:31, 280.92it/s]\u001b[A\n"," 26% 3060/11873 [00:09<00:31, 279.71it/s]\u001b[A\n"," 26% 3089/11873 [00:09<00:31, 276.77it/s]\u001b[A\n"," 26% 3117/11873 [00:09<00:36, 241.11it/s]\u001b[A\n"," 26% 3143/11873 [00:09<00:39, 221.47it/s]\u001b[A\n"," 27% 3166/11873 [00:09<00:40, 215.77it/s]\u001b[A\n"," 27% 3196/11873 [00:09<00:36, 236.29it/s]\u001b[A\n"," 27% 3226/11873 [00:09<00:34, 252.89it/s]\u001b[A\n"," 27% 3259/11873 [00:10<00:31, 272.91it/s]\u001b[A\n"," 28% 3287/11873 [00:10<00:40, 214.15it/s]\u001b[A\n"," 28% 3311/11873 [00:10<00:45, 189.32it/s]\u001b[A\n"," 28% 3332/11873 [00:10<00:48, 176.69it/s]\u001b[A\n"," 28% 3352/11873 [00:10<00:48, 175.29it/s]\u001b[A\n"," 28% 3371/11873 [00:10<00:49, 170.68it/s]\u001b[A\n"," 29% 3403/11873 [00:10<00:41, 206.52it/s]\u001b[A\n"," 29% 3435/11873 [00:10<00:35, 235.16it/s]\u001b[A\n"," 29% 3467/11873 [00:11<00:32, 256.68it/s]\u001b[A\n"," 29% 3499/11873 [00:11<00:30, 273.44it/s]\u001b[A\n"," 30% 3532/11873 [00:11<00:29, 287.31it/s]\u001b[A\n"," 30% 3565/11873 [00:11<00:27, 298.39it/s]\u001b[A\n"," 30% 3596/11873 [00:11<00:27, 299.66it/s]\u001b[A\n"," 31% 3627/11873 [00:11<00:39, 209.11it/s]\u001b[A\n"," 31% 3657/11873 [00:11<00:36, 227.99it/s]\u001b[A\n"," 31% 3687/11873 [00:11<00:33, 244.40it/s]\u001b[A\n"," 31% 3718/11873 [00:12<00:31, 260.99it/s]\u001b[A\n"," 32% 3750/11873 [00:12<00:29, 276.55it/s]\u001b[A\n"," 32% 3782/11873 [00:12<00:28, 288.02it/s]\u001b[A\n"," 32% 3814/11873 [00:12<00:27, 294.10it/s]\u001b[A\n"," 32% 3845/11873 [00:12<00:28, 278.43it/s]\u001b[A\n"," 33% 3876/11873 [00:12<00:27, 287.09it/s]\u001b[A\n"," 33% 3907/11873 [00:12<00:27, 292.76it/s]\u001b[A\n"," 33% 3937/11873 [00:12<00:29, 270.50it/s]\u001b[A\n"," 33% 3965/11873 [00:12<00:29, 264.00it/s]\u001b[A\n"," 34% 3995/11873 [00:12<00:28, 273.69it/s]\u001b[A\n"," 34% 4028/11873 [00:13<00:27, 288.36it/s]\u001b[A\n"," 34% 4059/11873 [00:13<00:26, 293.30it/s]\u001b[A\n"," 34% 4089/11873 [00:13<00:26, 294.43it/s]\u001b[A\n"," 35% 4121/11873 [00:13<00:25, 300.73it/s]\u001b[A\n"," 35% 4152/11873 [00:13<00:28, 273.24it/s]\u001b[A\n"," 35% 4181/11873 [00:13<00:27, 277.14it/s]\u001b[A\n"," 35% 4211/11873 [00:13<00:27, 281.02it/s]\u001b[A\n"," 36% 4245/11873 [00:13<00:25, 295.02it/s]\u001b[A\n"," 36% 4276/11873 [00:13<00:25, 297.78it/s]\u001b[A\n"," 36% 4307/11873 [00:14<00:25, 299.39it/s]\u001b[A\n"," 37% 4338/11873 [00:14<00:25, 300.76it/s]\u001b[A\n"," 37% 4370/11873 [00:14<00:24, 305.98it/s]\u001b[A\n"," 37% 4402/11873 [00:14<00:24, 309.02it/s]\u001b[A\n"," 37% 4433/11873 [00:14<00:29, 248.35it/s]\u001b[A\n"," 38% 4467/11873 [00:14<00:27, 269.67it/s]\u001b[A\n"," 38% 4498/11873 [00:14<00:26, 278.70it/s]\u001b[A\n"," 38% 4531/11873 [00:14<00:25, 290.19it/s]\u001b[A\n"," 38% 4562/11873 [00:14<00:24, 292.90it/s]\u001b[A\n"," 39% 4595/11873 [00:15<00:24, 301.64it/s]\u001b[A\n"," 39% 4626/11873 [00:15<00:23, 303.78it/s]\u001b[A\n"," 39% 4658/11873 [00:15<00:23, 308.13it/s]\u001b[A\n"," 40% 4691/11873 [00:15<00:22, 313.27it/s]\u001b[A\n"," 40% 4723/11873 [00:15<00:22, 313.01it/s]\u001b[A\n"," 40% 4755/11873 [00:15<00:23, 307.83it/s]\u001b[A\n"," 40% 4786/11873 [00:15<00:23, 302.37it/s]\u001b[A\n"," 41% 4817/11873 [00:15<00:23, 301.04it/s]\u001b[A\n"," 41% 4849/11873 [00:15<00:22, 305.95it/s]\u001b[A\n"," 41% 4880/11873 [00:15<00:23, 302.19it/s]\u001b[A\n"," 41% 4913/11873 [00:16<00:22, 307.68it/s]\u001b[A\n"," 42% 4947/11873 [00:16<00:21, 315.05it/s]\u001b[A\n"," 42% 4980/11873 [00:16<00:21, 317.25it/s]\u001b[A\n"," 42% 5012/11873 [00:16<00:21, 315.74it/s]\u001b[A\n"," 42% 5044/11873 [00:16<00:22, 310.04it/s]\u001b[A\n"," 43% 5076/11873 [00:16<00:22, 305.60it/s]\u001b[A\n"," 43% 5107/11873 [00:16<00:22, 302.32it/s]\u001b[A\n"," 43% 5140/11873 [00:16<00:21, 309.44it/s]\u001b[A\n"," 44% 5173/11873 [00:16<00:21, 312.77it/s]\u001b[A\n"," 44% 5205/11873 [00:17<00:21, 308.57it/s]\u001b[A\n"," 44% 5236/11873 [00:17<00:21, 306.83it/s]\u001b[A\n"," 44% 5267/11873 [00:17<00:24, 274.69it/s]\u001b[A\n"," 45% 5299/11873 [00:17<00:22, 286.55it/s]\u001b[A\n"," 45% 5331/11873 [00:17<00:22, 294.83it/s]\u001b[A\n"," 45% 5362/11873 [00:17<00:21, 297.87it/s]\u001b[A\n"," 45% 5393/11873 [00:17<00:21, 299.94it/s]\u001b[A\n"," 46% 5424/11873 [00:17<00:21, 295.60it/s]\u001b[A\n"," 46% 5454/11873 [00:17<00:21, 295.62it/s]\u001b[A\n"," 46% 5484/11873 [00:17<00:21, 294.52it/s]\u001b[A\n"," 46% 5515/11873 [00:18<00:21, 297.34it/s]\u001b[A\n"," 47% 5546/11873 [00:18<00:21, 298.81it/s]\u001b[A\n"," 47% 5579/11873 [00:18<00:20, 307.55it/s]\u001b[A\n"," 47% 5612/11873 [00:18<00:20, 312.70it/s]\u001b[A\n"," 48% 5644/11873 [00:18<00:20, 307.08it/s]\u001b[A\n"," 48% 5677/11873 [00:18<00:19, 311.84it/s]\u001b[A\n"," 48% 5709/11873 [00:18<00:19, 313.84it/s]\u001b[A\n"," 48% 5741/11873 [00:18<00:19, 314.56it/s]\u001b[A\n"," 49% 5773/11873 [00:18<00:19, 312.01it/s]\u001b[A\n"," 49% 5807/11873 [00:19<00:19, 317.70it/s]\u001b[A\n"," 49% 5839/11873 [00:19<00:18, 317.82it/s]\u001b[A\n"," 49% 5872/11873 [00:19<00:18, 318.72it/s]\u001b[A\n"," 50% 5904/11873 [00:19<00:18, 316.37it/s]\u001b[A\n"," 50% 5936/11873 [00:19<00:18, 314.75it/s]\u001b[A\n"," 50% 5968/11873 [00:19<00:18, 315.01it/s]\u001b[A\n"," 51% 6000/11873 [00:19<00:18, 309.29it/s]\u001b[A\n"," 51% 6031/11873 [00:19<00:19, 307.36it/s]\u001b[A\n"," 51% 6063/11873 [00:19<00:18, 309.54it/s]\u001b[A\n"," 51% 6094/11873 [00:19<00:18, 308.07it/s]\u001b[A\n"," 52% 6125/11873 [00:20<00:18, 307.00it/s]\u001b[A\n"," 52% 6156/11873 [00:20<00:18, 300.90it/s]\u001b[A\n"," 52% 6188/11873 [00:20<00:18, 304.44it/s]\u001b[A\n"," 52% 6219/11873 [00:20<00:18, 302.84it/s]\u001b[A\n"," 53% 6251/11873 [00:20<00:18, 307.19it/s]\u001b[A\n"," 53% 6282/11873 [00:20<00:18, 304.70it/s]\u001b[A\n"," 53% 6313/11873 [00:20<00:18, 303.99it/s]\u001b[A\n"," 53% 6344/11873 [00:20<00:18, 305.11it/s]\u001b[A\n"," 54% 6376/11873 [00:20<00:17, 308.68it/s]\u001b[A\n"," 54% 6408/11873 [00:20<00:17, 311.03it/s]\u001b[A\n"," 54% 6440/11873 [00:21<00:17, 307.66it/s]\u001b[A\n"," 55% 6472/11873 [00:21<00:17, 310.49it/s]\u001b[A\n"," 55% 6504/11873 [00:21<00:17, 307.95it/s]\u001b[A\n"," 55% 6537/11873 [00:21<00:17, 312.43it/s]\u001b[A\n"," 55% 6569/11873 [00:21<00:16, 312.97it/s]\u001b[A\n"," 56% 6602/11873 [00:21<00:16, 315.93it/s]\u001b[A\n"," 56% 6635/11873 [00:21<00:16, 318.29it/s]\u001b[A\n"," 56% 6668/11873 [00:21<00:16, 319.04it/s]\u001b[A\n"," 56% 6700/11873 [00:21<00:16, 317.23it/s]\u001b[A\n"," 57% 6732/11873 [00:22<00:18, 275.75it/s]\u001b[A\n"," 57% 6765/11873 [00:22<00:17, 288.37it/s]\u001b[A\n"," 57% 6797/11873 [00:22<00:17, 296.48it/s]\u001b[A\n"," 58% 6828/11873 [00:22<00:16, 298.85it/s]\u001b[A\n"," 58% 6861/11873 [00:22<00:16, 305.41it/s]\u001b[A\n"," 58% 6892/11873 [00:22<00:16, 304.34it/s]\u001b[A\n"," 58% 6923/11873 [00:22<00:16, 300.12it/s]\u001b[A\n"," 59% 6954/11873 [00:22<00:16, 295.43it/s]\u001b[A\n"," 59% 6987/11873 [00:22<00:16, 303.06it/s]\u001b[A\n"," 59% 7020/11873 [00:22<00:15, 310.68it/s]\u001b[A\n"," 59% 7052/11873 [00:23<00:15, 308.97it/s]\u001b[A\n"," 60% 7085/11873 [00:23<00:15, 313.05it/s]\u001b[A\n"," 60% 7117/11873 [00:23<00:15, 309.82it/s]\u001b[A\n"," 60% 7149/11873 [00:23<00:15, 310.67it/s]\u001b[A\n"," 60% 7182/11873 [00:23<00:14, 314.12it/s]\u001b[A\n"," 61% 7214/11873 [00:23<00:15, 309.29it/s]\u001b[A\n"," 61% 7246/11873 [00:23<00:14, 309.75it/s]\u001b[A\n"," 61% 7278/11873 [00:23<00:14, 310.09it/s]\u001b[A\n"," 62% 7310/11873 [00:23<00:14, 311.81it/s]\u001b[A\n"," 62% 7342/11873 [00:24<00:14, 311.39it/s]\u001b[A\n"," 62% 7374/11873 [00:24<00:14, 310.82it/s]\u001b[A\n"," 62% 7406/11873 [00:24<00:14, 307.79it/s]\u001b[A\n"," 63% 7437/11873 [00:24<00:15, 289.77it/s]\u001b[A\n"," 63% 7468/11873 [00:24<00:15, 292.93it/s]\u001b[A\n"," 63% 7501/11873 [00:24<00:14, 303.23it/s]\u001b[A\n"," 63% 7535/11873 [00:24<00:13, 311.97it/s]\u001b[A\n"," 64% 7568/11873 [00:24<00:13, 316.50it/s]\u001b[A\n"," 64% 7600/11873 [00:24<00:13, 315.29it/s]\u001b[A\n"," 64% 7632/11873 [00:24<00:13, 313.89it/s]\u001b[A\n"," 65% 7664/11873 [00:25<00:13, 310.54it/s]\u001b[A\n"," 65% 7696/11873 [00:25<00:13, 305.59it/s]\u001b[A\n"," 65% 7727/11873 [00:25<00:14, 281.33it/s]\u001b[A\n"," 65% 7758/11873 [00:25<00:14, 286.89it/s]\u001b[A\n"," 66% 7788/11873 [00:25<00:14, 290.09it/s]\u001b[A\n"," 66% 7819/11873 [00:25<00:13, 295.11it/s]\u001b[A\n"," 66% 7849/11873 [00:25<00:13, 295.11it/s]\u001b[A\n"," 66% 7879/11873 [00:25<00:14, 274.91it/s]\u001b[A\n"," 67% 7912/11873 [00:25<00:13, 289.31it/s]\u001b[A\n"," 67% 7944/11873 [00:26<00:13, 296.83it/s]\u001b[A\n"," 67% 7975/11873 [00:26<00:13, 298.12it/s]\u001b[A\n"," 67% 8009/11873 [00:26<00:12, 308.09it/s]\u001b[A\n"," 68% 8041/11873 [00:26<00:12, 310.73it/s]\u001b[A\n"," 68% 8074/11873 [00:26<00:12, 313.61it/s]\u001b[A\n"," 68% 8106/11873 [00:26<00:12, 313.03it/s]\u001b[A\n"," 69% 8138/11873 [00:26<00:11, 311.59it/s]\u001b[A\n"," 69% 8171/11873 [00:26<00:11, 313.60it/s]\u001b[A\n"," 69% 8203/11873 [00:26<00:11, 311.63it/s]\u001b[A\n"," 69% 8235/11873 [00:26<00:11, 313.39it/s]\u001b[A\n"," 70% 8268/11873 [00:27<00:11, 316.15it/s]\u001b[A\n"," 70% 8300/11873 [00:27<00:11, 316.28it/s]\u001b[A\n"," 70% 8333/11873 [00:27<00:11, 319.71it/s]\u001b[A\n"," 70% 8365/11873 [00:27<00:11, 318.13it/s]\u001b[A\n"," 71% 8398/11873 [00:27<00:10, 319.21it/s]\u001b[A\n"," 71% 8430/11873 [00:27<00:11, 312.96it/s]\u001b[A\n"," 71% 8462/11873 [00:27<00:11, 305.92it/s]\u001b[A\n"," 72% 8493/11873 [00:27<00:11, 301.48it/s]\u001b[A\n"," 72% 8526/11873 [00:27<00:10, 308.77it/s]\u001b[A\n"," 72% 8559/11873 [00:27<00:10, 313.17it/s]\u001b[A\n"," 72% 8591/11873 [00:28<00:10, 313.45it/s]\u001b[A\n"," 73% 8623/11873 [00:28<00:10, 307.48it/s]\u001b[A\n"," 73% 8655/11873 [00:28<00:10, 309.76it/s]\u001b[A\n"," 73% 8687/11873 [00:28<00:10, 311.16it/s]\u001b[A\n"," 73% 8719/11873 [00:28<00:10, 311.18it/s]\u001b[A\n"," 74% 8751/11873 [00:28<00:10, 310.83it/s]\u001b[A\n"," 74% 8784/11873 [00:28<00:09, 315.65it/s]\u001b[A\n"," 74% 8816/11873 [00:28<00:09, 313.23it/s]\u001b[A\n"," 75% 8848/11873 [00:28<00:09, 315.13it/s]\u001b[A\n"," 75% 8881/11873 [00:29<00:09, 316.90it/s]\u001b[A\n"," 75% 8913/11873 [00:29<00:09, 312.69it/s]\u001b[A\n"," 75% 8945/11873 [00:29<00:09, 300.60it/s]\u001b[A\n"," 76% 8976/11873 [00:29<00:09, 301.43it/s]\u001b[A\n"," 76% 9007/11873 [00:29<00:09, 301.12it/s]\u001b[A\n"," 76% 9038/11873 [00:29<00:09, 299.23it/s]\u001b[A\n"," 76% 9070/11873 [00:29<00:09, 305.18it/s]\u001b[A\n"," 77% 9102/11873 [00:29<00:08, 309.40it/s]\u001b[A\n"," 77% 9134/11873 [00:29<00:08, 311.75it/s]\u001b[A\n"," 77% 9167/11873 [00:29<00:08, 314.84it/s]\u001b[A\n"," 77% 9200/11873 [00:30<00:08, 316.88it/s]\u001b[A\n"," 78% 9232/11873 [00:30<00:08, 306.52it/s]\u001b[A\n"," 78% 9265/11873 [00:30<00:08, 310.97it/s]\u001b[A\n"," 78% 9297/11873 [00:30<00:08, 312.86it/s]\u001b[A\n"," 79% 9329/11873 [00:30<00:08, 311.99it/s]\u001b[A\n"," 79% 9362/11873 [00:30<00:07, 314.95it/s]\u001b[A\n"," 79% 9394/11873 [00:30<00:07, 310.31it/s]\u001b[A\n"," 79% 9426/11873 [00:30<00:07, 307.49it/s]\u001b[A\n"," 80% 9458/11873 [00:30<00:07, 309.21it/s]\u001b[A\n"," 80% 9490/11873 [00:30<00:07, 309.94it/s]\u001b[A\n"," 80% 9522/11873 [00:31<00:07, 309.51it/s]\u001b[A\n"," 80% 9554/11873 [00:31<00:07, 312.57it/s]\u001b[A\n"," 81% 9587/11873 [00:31<00:07, 315.48it/s]\u001b[A\n"," 81% 9619/11873 [00:31<00:07, 314.54it/s]\u001b[A\n"," 81% 9651/11873 [00:31<00:07, 316.05it/s]\u001b[A\n"," 82% 9684/11873 [00:31<00:06, 317.43it/s]\u001b[A\n"," 82% 9716/11873 [00:31<00:06, 314.79it/s]\u001b[A\n"," 82% 9750/11873 [00:31<00:06, 319.86it/s]\u001b[A\n"," 82% 9782/11873 [00:31<00:06, 318.79it/s]\u001b[A\n"," 83% 9814/11873 [00:32<00:06, 316.94it/s]\u001b[A\n"," 83% 9847/11873 [00:32<00:06, 318.82it/s]\u001b[A\n"," 83% 9880/11873 [00:32<00:06, 319.28it/s]\u001b[A\n"," 83% 9912/11873 [00:32<00:06, 317.55it/s]\u001b[A\n"," 84% 9945/11873 [00:32<00:06, 320.49it/s]\u001b[A\n"," 84% 9978/11873 [00:32<00:05, 320.33it/s]\u001b[A\n"," 84% 10011/11873 [00:32<00:05, 318.25it/s]\u001b[A\n"," 85% 10044/11873 [00:32<00:05, 319.43it/s]\u001b[A\n"," 85% 10077/11873 [00:32<00:05, 321.88it/s]\u001b[A\n"," 85% 10110/11873 [00:32<00:05, 318.70it/s]\u001b[A\n"," 85% 10142/11873 [00:33<00:05, 319.05it/s]\u001b[A\n"," 86% 10174/11873 [00:33<00:05, 316.77it/s]\u001b[A\n"," 86% 10206/11873 [00:33<00:05, 312.82it/s]\u001b[A\n"," 86% 10238/11873 [00:33<00:05, 314.71it/s]\u001b[A\n"," 86% 10270/11873 [00:33<00:05, 316.01it/s]\u001b[A\n"," 87% 10302/11873 [00:33<00:05, 309.90it/s]\u001b[A\n"," 87% 10336/11873 [00:33<00:04, 316.62it/s]\u001b[A\n"," 87% 10369/11873 [00:33<00:04, 317.88it/s]\u001b[A\n"," 88% 10403/11873 [00:33<00:04, 322.01it/s]\u001b[A\n"," 88% 10436/11873 [00:33<00:04, 298.91it/s]\u001b[A\n"," 88% 10469/11873 [00:34<00:04, 305.22it/s]\u001b[A\n"," 88% 10500/11873 [00:34<00:04, 303.44it/s]\u001b[A\n"," 89% 10531/11873 [00:34<00:04, 300.19it/s]\u001b[A\n"," 89% 10562/11873 [00:34<00:04, 283.29it/s]\u001b[A\n"," 89% 10594/11873 [00:34<00:04, 292.59it/s]\u001b[A\n"," 89% 10625/11873 [00:34<00:04, 297.27it/s]\u001b[A\n"," 90% 10657/11873 [00:34<00:04, 302.58it/s]\u001b[A\n"," 90% 10691/11873 [00:34<00:03, 311.18it/s]\u001b[A\n"," 90% 10723/11873 [00:34<00:03, 313.24it/s]\u001b[A\n"," 91% 10755/11873 [00:35<00:03, 313.97it/s]\u001b[A\n"," 91% 10787/11873 [00:35<00:03, 307.82it/s]\u001b[A\n"," 91% 10818/11873 [00:35<00:03, 306.56it/s]\u001b[A\n"," 91% 10849/11873 [00:35<00:03, 288.16it/s]\u001b[A\n"," 92% 10881/11873 [00:35<00:03, 295.67it/s]\u001b[A\n"," 92% 10913/11873 [00:35<00:03, 300.80it/s]\u001b[A\n"," 92% 10944/11873 [00:35<00:03, 295.51it/s]\u001b[A\n"," 92% 10976/11873 [00:35<00:02, 301.72it/s]\u001b[A\n"," 93% 11010/11873 [00:35<00:02, 310.09it/s]\u001b[A\n"," 93% 11044/11873 [00:35<00:02, 316.19it/s]\u001b[A\n"," 93% 11077/11873 [00:36<00:02, 318.31it/s]\u001b[A\n"," 94% 11109/11873 [00:36<00:02, 310.95it/s]\u001b[A\n"," 94% 11141/11873 [00:36<00:02, 307.01it/s]\u001b[A\n"," 94% 11174/11873 [00:36<00:02, 312.36it/s]\u001b[A\n"," 94% 11206/11873 [00:36<00:02, 311.11it/s]\u001b[A\n"," 95% 11238/11873 [00:36<00:02, 311.66it/s]\u001b[A\n"," 95% 11270/11873 [00:36<00:01, 312.31it/s]\u001b[A\n"," 95% 11302/11873 [00:36<00:01, 312.38it/s]\u001b[A\n"," 95% 11335/11873 [00:36<00:01, 316.23it/s]\u001b[A\n"," 96% 11367/11873 [00:37<00:01, 313.88it/s]\u001b[A\n"," 96% 11399/11873 [00:37<00:01, 312.38it/s]\u001b[A\n"," 96% 11431/11873 [00:37<00:01, 308.94it/s]\u001b[A\n"," 97% 11463/11873 [00:37<00:01, 310.27it/s]\u001b[A\n"," 97% 11495/11873 [00:37<00:01, 311.33it/s]\u001b[A\n"," 97% 11527/11873 [00:37<00:01, 310.80it/s]\u001b[A\n"," 97% 11559/11873 [00:37<00:01, 304.83it/s]\u001b[A\n"," 98% 11590/11873 [00:37<00:00, 304.69it/s]\u001b[A\n"," 98% 11621/11873 [00:37<00:00, 302.73it/s]\u001b[A\n"," 98% 11653/11873 [00:37<00:00, 306.63it/s]\u001b[A\n"," 98% 11684/11873 [00:38<00:00, 306.86it/s]\u001b[A\n"," 99% 11716/11873 [00:38<00:00, 309.43it/s]\u001b[A\n"," 99% 11748/11873 [00:38<00:00, 310.32it/s]\u001b[A\n"," 99% 11780/11873 [00:38<00:00, 311.44it/s]\u001b[A\n"," 99% 11813/11873 [00:38<00:00, 315.08it/s]\u001b[A\n","100% 11873/11873 [00:38<00:00, 307.13it/s]\n","04/06/2022 06:58:24 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/eval_predictions.json.\n","04/06/2022 06:58:24 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/eval_nbest_predictions.json.\n","04/06/2022 06:58:26 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/bert-base-cased/word2vec-aug/eval_null_odds.json.\n","04/06/2022 06:58:30 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1525/1525 [03:57<00:00,  6.42it/s]\n","***** eval metrics *****\n","  epoch                  =     3.0\n","  eval_HasAns_exact      = 72.6721\n","  eval_HasAns_f1         = 79.7647\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 47.2834\n","  eval_NoAns_f1          = 47.2834\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 60.3049\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 63.5329\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 59.9596\n","  eval_f1                = 63.5008\n","  eval_samples           =   12199\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 06:58:31,006 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_word2vec_aug', 'type': 'sichenzhong/squad_v2_word2vec_aug', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path albert-base-v2 \\\n","  --dataset_name sichenzhong/squad_v2_word2vec_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 512 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZte29JdR6bW","executionInfo":{"status":"ok","timestamp":1649243661087,"user_tz":240,"elapsed":15346023,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"f8f20fd8-c45d-4bbb-ce23-9ab967c17ad1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["04/06/2022 06:58:42 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 06:58:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/runs/Apr06_06-58-42_e998dce90490,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 06:58:43 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a\n","04/06/2022 06:58:43 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/06/2022 06:58:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","04/06/2022 06:58:43 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","04/06/2022 06:58:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","100% 2/2 [00:00<00:00, 13.20it/s]\n","[INFO|hub.py:583] 2022-04-06 06:58:43,867 >> https://huggingface.co/albert-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpj6oad1sw\n","Downloading: 100% 684/684 [00:00<00:00, 619kB/s]\n","[INFO|hub.py:587] 2022-04-06 06:58:44,088 >> storing https://huggingface.co/albert-base-v2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|hub.py:595] 2022-04-06 06:58:44,088 >> creating metadata file for /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:654] 2022-04-06 06:58:44,088 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-06 06:58:44,091 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-06 06:58:44,222 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-06 06:58:44,361 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-06 06:58:44,362 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 06:58:44,628 >> https://huggingface.co/albert-base-v2/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5_0rv5cs\n","Downloading: 100% 742k/742k [00:00<00:00, 5.06MB/s]\n","[INFO|hub.py:587] 2022-04-06 06:58:44,924 >> storing https://huggingface.co/albert-base-v2/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:595] 2022-04-06 06:58:44,924 >> creating metadata file for /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:583] 2022-04-06 06:58:45,056 >> https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi8wck0f6\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 7.56MB/s]\n","[INFO|hub.py:587] 2022-04-06 06:58:45,370 >> storing https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|hub.py:595] 2022-04-06 06:58:45,370 >> creating metadata file for /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:58:45,779 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:58:45,779 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:58:45,779 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:58:45,779 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:58:45,779 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-06 06:58:45,919 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-06 06:58:45,920 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 06:58:46,209 >> https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpcndjs7jx\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 49.6MB/s]\n","[INFO|hub.py:587] 2022-04-06 06:58:47,270 >> storing https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|hub.py:595] 2022-04-06 06:58:47,270 >> creating metadata file for /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1772] 2022-04-06 06:58:47,270 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[WARNING|modeling_utils.py:2050] 2022-04-06 06:58:47,404 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.LayerNorm.weight']\n","- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-06 06:58:47,404 >> Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/06/2022 06:58:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-3ceecc15687dd358.arrow\n","Running tokenizer on train dataset: 100% 131/131 [01:05<00:00,  2.00ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/06/2022 06:59:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-2f8fed45dbeff3c3.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:23<00:00,  6.96s/ba]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 07:01:20,458 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 07:01:20,458 >>   Num examples = 130597\n","[INFO|trainer.py:1292] 2022-04-06 07:01:20,458 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-06 07:01:20,458 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1294] 2022-04-06 07:01:20,458 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1295] 2022-04-06 07:01:20,458 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 07:01:20,458 >>   Total optimization steps = 16326\n","{'loss': 2.1754, 'learning_rate': 1.9387480093103028e-05, 'epoch': 0.06}\n","  3% 500/16326 [07:33<3:58:51,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:08:54,033 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:08:54,041 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:08:54,167 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:08:54,190 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:08:54,195 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 1.7157, 'learning_rate': 1.8774960186206054e-05, 'epoch': 0.12}\n","  6% 1000/16326 [15:07<3:51:37,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:16:28,361 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 07:16:28,368 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:16:28,507 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:16:28,513 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:16:28,517 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.5752, 'learning_rate': 1.816244027930908e-05, 'epoch': 0.18}\n","  9% 1500/16326 [22:42<3:44:27,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:24:02,762 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:24:02,768 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:24:02,902 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:24:02,908 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:24:02,911 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.5296, 'learning_rate': 1.7549920372412107e-05, 'epoch': 0.25}\n"," 12% 2000/16326 [30:16<3:36:47,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:31:37,414 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 07:31:37,420 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:31:37,547 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:31:37,570 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:31:37,573 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.4872, 'learning_rate': 1.693740046551513e-05, 'epoch': 0.31}\n"," 15% 2500/16326 [37:51<3:29:47,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:39:12,113 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:39:12,136 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:39:12,262 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:39:12,268 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:39:12,271 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.4447, 'learning_rate': 1.6324880558618156e-05, 'epoch': 0.37}\n"," 18% 3000/16326 [45:26<3:21:34,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:46:46,808 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 07:46:46,815 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:46:46,966 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:46:46,970 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:46:46,974 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.3996, 'learning_rate': 1.5712360651721183e-05, 'epoch': 0.43}\n"," 21% 3500/16326 [53:01<3:14:33,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:54:21,602 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:54:21,612 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:54:21,735 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:54:21,740 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:54:21,744 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.3775, 'learning_rate': 1.5099840744824207e-05, 'epoch': 0.49}\n"," 25% 4000/16326 [1:00:35<3:06:30,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:01:56,353 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:01:56,359 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:01:56,484 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:01:56,490 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:01:56,493 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.3496, 'learning_rate': 1.4487320837927234e-05, 'epoch': 0.55}\n"," 28% 4500/16326 [1:08:10<2:59:08,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:09:31,157 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:09:31,164 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:09:31,289 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:09:31,294 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:09:31,298 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.3183, 'learning_rate': 1.3874800931030258e-05, 'epoch': 0.61}\n"," 31% 5000/16326 [1:15:45<2:51:41,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:17:06,173 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:17:06,180 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:17:06,306 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:17:06,311 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:17:06,315 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.2886, 'learning_rate': 1.3262281024133286e-05, 'epoch': 0.67}\n"," 34% 5500/16326 [1:23:20<2:43:42,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:24:40,850 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:24:40,856 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:24:40,978 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:24:40,983 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:24:40,987 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.2795, 'learning_rate': 1.2649761117236313e-05, 'epoch': 0.74}\n"," 37% 6000/16326 [1:30:55<2:36:30,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:32:15,679 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:32:15,686 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:32:15,810 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:32:15,815 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:32:15,818 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.2701, 'learning_rate': 1.2037241210339337e-05, 'epoch': 0.8}\n"," 40% 6500/16326 [1:38:29<2:28:56,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:39:50,466 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:39:50,472 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:39:50,592 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:39:50,597 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:39:50,600 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.2626, 'learning_rate': 1.1424721303442364e-05, 'epoch': 0.86}\n"," 43% 7000/16326 [1:46:04<2:20:56,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:47:25,233 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:47:25,240 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:47:25,362 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:47:25,367 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:47:25,371 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.2339, 'learning_rate': 1.0812201396545388e-05, 'epoch': 0.92}\n"," 46% 7500/16326 [1:53:39<2:13:54,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:55:00,159 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:55:00,166 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:55:00,290 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:55:00,295 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:55:00,298 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 1.2307, 'learning_rate': 1.0199681489648415e-05, 'epoch': 0.98}\n"," 49% 8000/16326 [2:01:14<2:05:56,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:02:35,008 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:02:35,014 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:02:35,137 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:02:35,142 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:02:35,145 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 1.0765, 'learning_rate': 9.58716158275144e-06, 'epoch': 1.04}\n"," 52% 8500/16326 [2:08:48<1:58:24,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:10:09,299 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:10:09,306 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:10:09,439 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:10:09,444 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:10:09,449 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 1.0162, 'learning_rate': 8.974641675854466e-06, 'epoch': 1.1}\n"," 55% 9000/16326 [2:16:23<1:51:01,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:17:44,211 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:17:44,217 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:17:44,343 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:17:44,348 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:17:44,352 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 1.0007, 'learning_rate': 8.362121768957492e-06, 'epoch': 1.16}\n"," 58% 9500/16326 [2:23:58<1:43:17,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:25:19,229 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:25:19,235 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:25:19,358 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:25:19,363 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:25:19,368 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.999, 'learning_rate': 7.749601862060517e-06, 'epoch': 1.23}\n"," 61% 10000/16326 [2:31:33<1:35:52,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:32:53,938 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:32:53,944 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:32:54,069 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:32:54,075 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:32:54,078 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.9836, 'learning_rate': 7.137081955163543e-06, 'epoch': 1.29}\n"," 64% 10500/16326 [2:39:08<1:28:23,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:40:28,766 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:40:28,772 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:40:28,894 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:40:28,898 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:40:28,902 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-10500/special_tokens_map.json\n","{'loss': 1.0008, 'learning_rate': 6.5245620482665695e-06, 'epoch': 1.35}\n"," 67% 11000/16326 [2:46:43<1:20:34,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:48:03,550 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:48:03,556 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:48:03,679 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:48:03,685 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:48:03,689 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.9954, 'learning_rate': 5.912042141369595e-06, 'epoch': 1.41}\n"," 70% 11500/16326 [2:54:17<1:13:05,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:55:38,267 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-11500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:55:38,273 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:55:38,401 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:55:38,408 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:55:38,411 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.9646, 'learning_rate': 5.2995222344726205e-06, 'epoch': 1.47}\n"," 74% 12000/16326 [3:01:52<1:05:35,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:03:13,145 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-12000\n","[INFO|configuration_utils.py:441] 2022-04-06 10:03:13,151 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:03:13,280 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:03:13,286 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:03:13,315 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.9803, 'learning_rate': 4.687002327575647e-06, 'epoch': 1.53}\n"," 77% 12500/16326 [3:09:27<57:57,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:10:48,171 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-12500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:10:48,177 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:10:48,310 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:10:48,315 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:10:48,339 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.9711, 'learning_rate': 4.074482420678672e-06, 'epoch': 1.59}\n"," 80% 13000/16326 [3:17:02<50:25,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:18:23,163 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-13000\n","[INFO|configuration_utils.py:441] 2022-04-06 10:18:23,169 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:18:23,298 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:18:23,322 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:18:23,326 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.9289, 'learning_rate': 3.4619625137816982e-06, 'epoch': 1.65}\n"," 83% 13500/16326 [3:24:37<42:47,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:25:58,057 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-13500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:25:58,064 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:25:58,193 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:25:58,218 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:25:58,222 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.9618, 'learning_rate': 2.849442606884724e-06, 'epoch': 1.72}\n"," 86% 14000/16326 [3:32:12<35:16,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:33:33,016 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-14000\n","[INFO|configuration_utils.py:441] 2022-04-06 10:33:33,022 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:33:33,166 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:33:33,188 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:33:33,194 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.9602, 'learning_rate': 2.2369226999877497e-06, 'epoch': 1.78}\n"," 89% 14500/16326 [3:39:47<27:38,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:41:07,977 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-14500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:41:07,985 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:41:08,107 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:41:08,112 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:41:08,115 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.9266, 'learning_rate': 1.6244027930907754e-06, 'epoch': 1.84}\n"," 92% 15000/16326 [3:47:22<20:05,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:48:42,946 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-15000\n","[INFO|configuration_utils.py:441] 2022-04-06 10:48:42,970 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:48:43,094 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:48:43,099 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:48:43,103 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.952, 'learning_rate': 1.0118828861938013e-06, 'epoch': 1.9}\n"," 95% 15500/16326 [3:54:57<12:32,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:56:17,758 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-15500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:56:17,770 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:56:17,908 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:56:17,914 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:56:17,917 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.9051, 'learning_rate': 3.993629792968272e-07, 'epoch': 1.96}\n"," 98% 16000/16326 [4:02:31<04:55,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:03:52,416 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-16000\n","[INFO|configuration_utils.py:441] 2022-04-06 11:03:52,421 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:03:52,548 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:03:52,573 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:03:52,576 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/checkpoint-16000/special_tokens_map.json\n","100% 16326/16326 [4:07:28<00:00,  1.36it/s][INFO|trainer.py:1530] 2022-04-06 11:08:48,513 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 14848.055, 'train_samples_per_second': 17.591, 'train_steps_per_second': 1.1, 'train_loss': 1.1985976483093732, 'epoch': 2.0}\n","100% 16326/16326 [4:07:28<00:00,  1.10it/s]\n","[INFO|trainer.py:2166] 2022-04-06 11:08:48,518 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug\n","[INFO|configuration_utils.py:441] 2022-04-06 11:08:48,523 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:08:48,649 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:08:48,653 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:08:48,657 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     1.1986\n","  train_runtime            = 4:07:28.05\n","  train_samples            =     130597\n","  train_samples_per_second =     17.591\n","  train_steps_per_second   =        1.1\n","04/06/2022 11:08:48 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 11:08:48,701 >> The following columns in the evaluation set  don't have a corresponding argument in `AlbertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `AlbertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 11:08:48,704 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 11:08:48,704 >>   Num examples = 11968\n","[INFO|trainer.py:2421] 2022-04-06 11:08:48,704 >>   Batch size = 8\n","100% 1496/1496 [04:20<00:00,  5.75it/s]04/06/2022 11:13:26 - INFO - utils_qa - Post-processing 11873 example predictions split into 11968 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 32/11873 [00:00<00:37, 318.08it/s]\u001b[A\n","  1% 66/11873 [00:00<00:36, 326.49it/s]\u001b[A\n","  1% 99/11873 [00:00<00:36, 321.49it/s]\u001b[A\n","  1% 132/11873 [00:00<00:36, 323.52it/s]\u001b[A\n","  1% 165/11873 [00:00<00:36, 322.02it/s]\u001b[A\n","  2% 198/11873 [00:00<00:36, 316.68it/s]\u001b[A\n","  2% 231/11873 [00:00<00:36, 319.15it/s]\u001b[A\n","  2% 265/11873 [00:00<00:35, 323.12it/s]\u001b[A\n","  3% 300/11873 [00:00<00:35, 329.02it/s]\u001b[A\n","  3% 333/11873 [00:01<00:35, 326.41it/s]\u001b[A\n","  3% 369/11873 [00:01<00:34, 334.08it/s]\u001b[A\n","  3% 403/11873 [00:01<00:34, 332.81it/s]\u001b[A\n","  4% 439/11873 [00:01<00:33, 338.41it/s]\u001b[A\n","  4% 474/11873 [00:01<00:33, 340.71it/s]\u001b[A\n","  4% 509/11873 [00:01<00:33, 337.06it/s]\u001b[A\n","  5% 543/11873 [00:01<00:34, 328.23it/s]\u001b[A\n","  5% 576/11873 [00:01<00:34, 328.24it/s]\u001b[A\n","  5% 612/11873 [00:01<00:33, 336.64it/s]\u001b[A\n","  5% 648/11873 [00:01<00:32, 341.18it/s]\u001b[A\n","  6% 685/11873 [00:02<00:32, 349.32it/s]\u001b[A\n","  6% 720/11873 [00:02<00:32, 342.22it/s]\u001b[A\n","  6% 755/11873 [00:02<00:33, 336.60it/s]\u001b[A\n","  7% 790/11873 [00:02<00:32, 338.46it/s]\u001b[A\n","  7% 825/11873 [00:02<00:32, 341.68it/s]\u001b[A\n","  7% 862/11873 [00:02<00:31, 347.27it/s]\u001b[A\n","  8% 897/11873 [00:02<00:31, 346.53it/s]\u001b[A\n","  8% 932/11873 [00:02<00:31, 346.99it/s]\u001b[A\n","  8% 968/11873 [00:02<00:31, 349.09it/s]\u001b[A\n","  8% 1003/11873 [00:02<00:31, 341.42it/s]\u001b[A\n","  9% 1038/11873 [00:03<00:34, 318.40it/s]\u001b[A\n","  9% 1071/11873 [00:03<00:35, 301.72it/s]\u001b[A\n","  9% 1102/11873 [00:03<00:37, 288.79it/s]\u001b[A\n"," 10% 1132/11873 [00:03<00:38, 277.21it/s]\u001b[A\n"," 10% 1160/11873 [00:03<00:39, 271.48it/s]\u001b[A\n"," 10% 1188/11873 [00:03<00:40, 260.69it/s]\u001b[A\n"," 10% 1216/11873 [00:03<00:40, 264.01it/s]\u001b[A\n"," 10% 1243/11873 [00:03<00:40, 264.62it/s]\u001b[A\n"," 11% 1270/11873 [00:04<00:41, 257.90it/s]\u001b[A\n"," 11% 1296/11873 [00:04<00:41, 257.39it/s]\u001b[A\n"," 11% 1322/11873 [00:04<00:41, 254.18it/s]\u001b[A\n"," 11% 1348/11873 [00:04<00:41, 253.35it/s]\u001b[A\n"," 12% 1374/11873 [00:04<00:41, 252.58it/s]\u001b[A\n"," 12% 1400/11873 [00:04<00:41, 251.74it/s]\u001b[A\n"," 12% 1426/11873 [00:04<00:41, 249.47it/s]\u001b[A\n"," 12% 1452/11873 [00:04<00:41, 250.12it/s]\u001b[A\n"," 12% 1478/11873 [00:04<00:41, 249.96it/s]\u001b[A\n"," 13% 1505/11873 [00:04<00:40, 254.67it/s]\u001b[A\n"," 13% 1532/11873 [00:05<00:39, 258.53it/s]\u001b[A\n"," 13% 1559/11873 [00:05<00:39, 260.39it/s]\u001b[A\n"," 13% 1587/11873 [00:05<00:39, 263.62it/s]\u001b[A\n"," 14% 1614/11873 [00:05<00:39, 260.86it/s]\u001b[A\n"," 14% 1641/11873 [00:05<00:39, 261.27it/s]\u001b[A\n"," 14% 1668/11873 [00:05<00:38, 263.61it/s]\u001b[A\n"," 14% 1695/11873 [00:05<00:39, 260.60it/s]\u001b[A\n"," 15% 1722/11873 [00:05<00:38, 260.57it/s]\u001b[A\n"," 15% 1749/11873 [00:05<00:38, 260.88it/s]\u001b[A\n"," 15% 1776/11873 [00:05<00:38, 262.48it/s]\u001b[A\n"," 15% 1803/11873 [00:06<00:38, 260.70it/s]\u001b[A\n"," 15% 1830/11873 [00:06<00:39, 256.48it/s]\u001b[A\n"," 16% 1856/11873 [00:06<00:39, 255.84it/s]\u001b[A\n"," 16% 1882/11873 [00:06<00:39, 252.63it/s]\u001b[A\n"," 16% 1908/11873 [00:06<00:39, 251.70it/s]\u001b[A\n"," 16% 1934/11873 [00:06<00:39, 250.97it/s]\u001b[A\n"," 17% 1960/11873 [00:06<00:39, 250.16it/s]\u001b[A\n"," 17% 1987/11873 [00:06<00:38, 254.75it/s]\u001b[A\n"," 17% 2015/11873 [00:06<00:37, 260.74it/s]\u001b[A\n"," 17% 2042/11873 [00:07<00:37, 261.70it/s]\u001b[A\n"," 17% 2069/11873 [00:07<00:37, 261.23it/s]\u001b[A\n"," 18% 2096/11873 [00:07<00:37, 263.06it/s]\u001b[A\n"," 18% 2123/11873 [00:07<00:37, 262.91it/s]\u001b[A\n"," 18% 2150/11873 [00:07<00:36, 263.00it/s]\u001b[A\n"," 18% 2177/11873 [00:07<00:37, 260.06it/s]\u001b[A\n"," 19% 2204/11873 [00:07<00:37, 260.68it/s]\u001b[A\n"," 19% 2231/11873 [00:07<00:36, 262.20it/s]\u001b[A\n"," 19% 2258/11873 [00:07<00:36, 262.76it/s]\u001b[A\n"," 19% 2285/11873 [00:07<00:36, 261.44it/s]\u001b[A\n"," 19% 2312/11873 [00:08<00:36, 261.68it/s]\u001b[A\n"," 20% 2339/11873 [00:08<00:36, 263.75it/s]\u001b[A\n"," 20% 2366/11873 [00:08<00:35, 265.15it/s]\u001b[A\n"," 20% 2393/11873 [00:08<00:35, 264.36it/s]\u001b[A\n"," 20% 2420/11873 [00:08<00:35, 262.70it/s]\u001b[A\n"," 21% 2447/11873 [00:08<00:36, 257.24it/s]\u001b[A\n"," 21% 2473/11873 [00:08<00:37, 252.51it/s]\u001b[A\n"," 21% 2499/11873 [00:08<00:37, 251.74it/s]\u001b[A\n"," 21% 2525/11873 [00:08<00:37, 251.16it/s]\u001b[A\n"," 21% 2552/11873 [00:08<00:36, 253.97it/s]\u001b[A\n"," 22% 2579/11873 [00:09<00:36, 257.92it/s]\u001b[A\n"," 22% 2606/11873 [00:09<00:35, 259.16it/s]\u001b[A\n"," 22% 2633/11873 [00:09<00:35, 260.07it/s]\u001b[A\n"," 22% 2660/11873 [00:09<00:36, 254.61it/s]\u001b[A\n"," 23% 2687/11873 [00:09<00:35, 257.21it/s]\u001b[A\n"," 23% 2714/11873 [00:09<00:35, 259.88it/s]\u001b[A\n"," 23% 2741/11873 [00:09<00:35, 257.68it/s]\u001b[A\n"," 23% 2767/11873 [00:09<00:35, 257.95it/s]\u001b[A\n"," 24% 2793/11873 [00:09<00:35, 258.39it/s]\u001b[A\n"," 24% 2819/11873 [00:10<00:35, 258.47it/s]\u001b[A\n"," 24% 2845/11873 [00:10<00:35, 255.99it/s]\u001b[A\n"," 24% 2871/11873 [00:10<00:35, 256.21it/s]\u001b[A\n"," 24% 2897/11873 [00:10<00:36, 248.60it/s]\u001b[A\n"," 25% 2923/11873 [00:10<00:35, 249.14it/s]\u001b[A\n"," 25% 2950/11873 [00:10<00:35, 252.75it/s]\u001b[A\n"," 25% 2976/11873 [00:10<00:35, 251.87it/s]\u001b[A\n"," 25% 3002/11873 [00:10<00:36, 244.07it/s]\u001b[A\n"," 26% 3028/11873 [00:10<00:35, 246.26it/s]\u001b[A\n"," 26% 3053/11873 [00:10<00:36, 241.88it/s]\u001b[A\n"," 26% 3078/11873 [00:11<00:36, 243.22it/s]\u001b[A\n"," 26% 3103/11873 [00:11<00:36, 242.74it/s]\u001b[A\n"," 26% 3128/11873 [00:11<00:40, 215.70it/s]\u001b[A\n"," 27% 3151/11873 [00:11<00:40, 215.77it/s]\u001b[A\n"," 27% 3173/11873 [00:11<00:43, 202.10it/s]\u001b[A\n"," 27% 3198/11873 [00:11<00:40, 213.85it/s]\u001b[A\n"," 27% 3224/11873 [00:11<00:38, 224.37it/s]\u001b[A\n"," 27% 3251/11873 [00:11<00:36, 235.20it/s]\u001b[A\n"," 28% 3275/11873 [00:11<00:36, 234.62it/s]\u001b[A\n"," 28% 3299/11873 [00:12<00:44, 193.53it/s]\u001b[A\n"," 28% 3320/11873 [00:12<00:46, 184.57it/s]\u001b[A\n"," 28% 3344/11873 [00:12<00:43, 197.66it/s]\u001b[A\n"," 28% 3365/11873 [00:12<00:48, 174.36it/s]\u001b[A\n"," 29% 3388/11873 [00:12<00:45, 187.84it/s]\u001b[A\n"," 29% 3414/11873 [00:12<00:41, 205.35it/s]\u001b[A\n"," 29% 3440/11873 [00:12<00:38, 219.11it/s]\u001b[A\n"," 29% 3464/11873 [00:12<00:37, 223.67it/s]\u001b[A\n"," 29% 3490/11873 [00:13<00:35, 233.81it/s]\u001b[A\n"," 30% 3517/11873 [00:13<00:34, 242.88it/s]\u001b[A\n"," 30% 3544/11873 [00:13<00:33, 249.20it/s]\u001b[A\n"," 30% 3570/11873 [00:13<00:33, 248.66it/s]\u001b[A\n"," 30% 3597/11873 [00:13<00:32, 253.40it/s]\u001b[A\n"," 31% 3624/11873 [00:13<00:32, 256.47it/s]\u001b[A\n"," 31% 3650/11873 [00:13<00:31, 257.03it/s]\u001b[A\n"," 31% 3676/11873 [00:13<00:33, 245.17it/s]\u001b[A\n"," 31% 3702/11873 [00:13<00:32, 247.96it/s]\u001b[A\n"," 31% 3728/11873 [00:13<00:32, 249.34it/s]\u001b[A\n"," 32% 3755/11873 [00:14<00:31, 253.90it/s]\u001b[A\n"," 32% 3782/11873 [00:14<00:31, 256.87it/s]\u001b[A\n"," 32% 3809/11873 [00:14<00:31, 259.07it/s]\u001b[A\n"," 32% 3835/11873 [00:14<00:31, 258.24it/s]\u001b[A\n"," 33% 3861/11873 [00:14<00:31, 256.24it/s]\u001b[A\n"," 33% 3887/11873 [00:14<00:31, 253.24it/s]\u001b[A\n"," 33% 3913/11873 [00:14<00:31, 249.73it/s]\u001b[A\n"," 33% 3938/11873 [00:14<00:32, 242.19it/s]\u001b[A\n"," 33% 3963/11873 [00:14<00:32, 242.65it/s]\u001b[A\n"," 34% 3989/11873 [00:15<00:32, 245.28it/s]\u001b[A\n"," 34% 4016/11873 [00:15<00:31, 252.43it/s]\u001b[A\n"," 34% 4044/11873 [00:15<00:30, 258.12it/s]\u001b[A\n"," 34% 4071/11873 [00:15<00:29, 261.14it/s]\u001b[A\n"," 35% 4098/11873 [00:15<00:29, 259.25it/s]\u001b[A\n"," 35% 4124/11873 [00:15<00:29, 258.95it/s]\u001b[A\n"," 35% 4150/11873 [00:15<00:30, 254.00it/s]\u001b[A\n"," 35% 4176/11873 [00:15<00:31, 246.86it/s]\u001b[A\n"," 35% 4201/11873 [00:15<00:31, 244.80it/s]\u001b[A\n"," 36% 4227/11873 [00:15<00:30, 247.89it/s]\u001b[A\n"," 36% 4255/11873 [00:16<00:29, 255.36it/s]\u001b[A\n"," 36% 4282/11873 [00:16<00:29, 257.48it/s]\u001b[A\n"," 36% 4310/11873 [00:16<00:28, 261.18it/s]\u001b[A\n"," 37% 4338/11873 [00:16<00:28, 264.13it/s]\u001b[A\n"," 37% 4366/11873 [00:16<00:28, 266.97it/s]\u001b[A\n"," 37% 4393/11873 [00:16<00:28, 263.06it/s]\u001b[A\n"," 37% 4420/11873 [00:16<00:32, 228.73it/s]\u001b[A\n"," 37% 4444/11873 [00:16<00:33, 222.11it/s]\u001b[A\n"," 38% 4470/11873 [00:16<00:31, 232.17it/s]\u001b[A\n"," 38% 4495/11873 [00:17<00:31, 236.94it/s]\u001b[A\n"," 38% 4521/11873 [00:17<00:30, 243.38it/s]\u001b[A\n"," 38% 4548/11873 [00:17<00:29, 249.99it/s]\u001b[A\n"," 39% 4575/11873 [00:17<00:28, 253.61it/s]\u001b[A\n"," 39% 4602/11873 [00:17<00:28, 257.56it/s]\u001b[A\n"," 39% 4628/11873 [00:17<00:28, 255.84it/s]\u001b[A\n"," 39% 4654/11873 [00:17<00:28, 252.67it/s]\u001b[A\n"," 39% 4680/11873 [00:17<00:28, 254.55it/s]\u001b[A\n"," 40% 4706/11873 [00:17<00:28, 253.85it/s]\u001b[A\n"," 40% 4732/11873 [00:17<00:27, 255.46it/s]\u001b[A\n"," 40% 4758/11873 [00:18<00:28, 253.29it/s]\u001b[A\n"," 40% 4784/11873 [00:18<00:28, 253.17it/s]\u001b[A\n"," 41% 4810/11873 [00:18<00:27, 254.41it/s]\u001b[A\n"," 41% 4836/11873 [00:18<00:27, 254.24it/s]\u001b[A\n"," 41% 4862/11873 [00:18<00:27, 254.73it/s]\u001b[A\n"," 41% 4888/11873 [00:18<00:27, 255.04it/s]\u001b[A\n"," 41% 4914/11873 [00:18<00:27, 252.23it/s]\u001b[A\n"," 42% 4940/11873 [00:18<00:27, 251.62it/s]\u001b[A\n"," 42% 4966/11873 [00:18<00:27, 248.85it/s]\u001b[A\n"," 42% 4991/11873 [00:19<00:27, 248.54it/s]\u001b[A\n"," 42% 5017/11873 [00:19<00:27, 250.78it/s]\u001b[A\n"," 42% 5043/11873 [00:19<00:27, 252.39it/s]\u001b[A\n"," 43% 5070/11873 [00:19<00:26, 256.41it/s]\u001b[A\n"," 43% 5097/11873 [00:19<00:26, 258.74it/s]\u001b[A\n"," 43% 5124/11873 [00:19<00:25, 261.01it/s]\u001b[A\n"," 43% 5151/11873 [00:19<00:26, 258.46it/s]\u001b[A\n"," 44% 5177/11873 [00:19<00:26, 251.68it/s]\u001b[A\n"," 44% 5203/11873 [00:19<00:26, 250.79it/s]\u001b[A\n"," 44% 5229/11873 [00:19<00:26, 247.45it/s]\u001b[A\n"," 44% 5254/11873 [00:20<00:27, 241.03it/s]\u001b[A\n"," 44% 5279/11873 [00:20<00:29, 226.38it/s]\u001b[A\n"," 45% 5303/11873 [00:20<00:28, 227.71it/s]\u001b[A\n"," 45% 5329/11873 [00:20<00:27, 235.28it/s]\u001b[A\n"," 45% 5354/11873 [00:20<00:27, 236.92it/s]\u001b[A\n"," 45% 5380/11873 [00:20<00:26, 241.37it/s]\u001b[A\n"," 46% 5406/11873 [00:20<00:26, 244.39it/s]\u001b[A\n"," 46% 5432/11873 [00:20<00:25, 248.29it/s]\u001b[A\n"," 46% 5458/11873 [00:20<00:25, 250.20it/s]\u001b[A\n"," 46% 5484/11873 [00:20<00:25, 251.61it/s]\u001b[A\n"," 46% 5512/11873 [00:21<00:24, 257.18it/s]\u001b[A\n"," 47% 5539/11873 [00:21<00:24, 257.90it/s]\u001b[A\n"," 47% 5566/11873 [00:21<00:24, 260.05it/s]\u001b[A\n"," 47% 5593/11873 [00:21<00:24, 257.56it/s]\u001b[A\n"," 47% 5619/11873 [00:21<00:24, 255.79it/s]\u001b[A\n"," 48% 5645/11873 [00:21<00:24, 254.04it/s]\u001b[A\n"," 48% 5671/11873 [00:21<00:24, 253.64it/s]\u001b[A\n"," 48% 5697/11873 [00:21<00:24, 254.26it/s]\u001b[A\n"," 48% 5724/11873 [00:21<00:23, 256.34it/s]\u001b[A\n"," 48% 5751/11873 [00:22<00:23, 258.89it/s]\u001b[A\n"," 49% 5777/11873 [00:22<00:24, 246.14it/s]\u001b[A\n"," 49% 5805/11873 [00:22<00:23, 253.54it/s]\u001b[A\n"," 49% 5833/11873 [00:22<00:23, 258.11it/s]\u001b[A\n"," 49% 5861/11873 [00:22<00:22, 262.64it/s]\u001b[A\n"," 50% 5888/11873 [00:22<00:22, 263.74it/s]\u001b[A\n"," 50% 5915/11873 [00:22<00:22, 260.00it/s]\u001b[A\n"," 50% 5942/11873 [00:22<00:23, 256.64it/s]\u001b[A\n"," 50% 5968/11873 [00:22<00:23, 256.12it/s]\u001b[A\n"," 50% 5994/11873 [00:22<00:22, 257.24it/s]\u001b[A\n"," 51% 6021/11873 [00:23<00:22, 260.23it/s]\u001b[A\n"," 51% 6048/11873 [00:23<00:22, 259.57it/s]\u001b[A\n"," 51% 6074/11873 [00:23<00:22, 258.87it/s]\u001b[A\n"," 51% 6101/11873 [00:23<00:22, 261.31it/s]\u001b[A\n"," 52% 6128/11873 [00:23<00:22, 259.96it/s]\u001b[A\n"," 52% 6155/11873 [00:23<00:22, 259.14it/s]\u001b[A\n"," 52% 6182/11873 [00:23<00:21, 261.08it/s]\u001b[A\n"," 52% 6209/11873 [00:23<00:21, 263.06it/s]\u001b[A\n"," 53% 6236/11873 [00:23<00:21, 261.56it/s]\u001b[A\n"," 53% 6263/11873 [00:24<00:21, 260.40it/s]\u001b[A\n"," 53% 6290/11873 [00:24<00:21, 261.26it/s]\u001b[A\n"," 53% 6317/11873 [00:24<00:21, 255.65it/s]\u001b[A\n"," 53% 6343/11873 [00:24<00:21, 256.36it/s]\u001b[A\n"," 54% 6369/11873 [00:24<00:21, 255.51it/s]\u001b[A\n"," 54% 6395/11873 [00:24<00:21, 254.44it/s]\u001b[A\n"," 54% 6421/11873 [00:24<00:21, 254.20it/s]\u001b[A\n"," 54% 6447/11873 [00:24<00:21, 253.34it/s]\u001b[A\n"," 55% 6473/11873 [00:24<00:21, 251.20it/s]\u001b[A\n"," 55% 6500/11873 [00:24<00:21, 253.97it/s]\u001b[A\n"," 55% 6527/11873 [00:25<00:20, 257.07it/s]\u001b[A\n"," 55% 6553/11873 [00:25<00:20, 255.68it/s]\u001b[A\n"," 55% 6579/11873 [00:25<00:20, 255.90it/s]\u001b[A\n"," 56% 6606/11873 [00:25<00:20, 256.62it/s]\u001b[A\n"," 56% 6632/11873 [00:25<00:20, 256.50it/s]\u001b[A\n"," 56% 6658/11873 [00:25<00:20, 257.47it/s]\u001b[A\n"," 56% 6684/11873 [00:25<00:20, 255.15it/s]\u001b[A\n"," 57% 6710/11873 [00:25<00:20, 253.04it/s]\u001b[A\n"," 57% 6736/11873 [00:25<00:20, 249.18it/s]\u001b[A\n"," 57% 6763/11873 [00:25<00:20, 254.09it/s]\u001b[A\n"," 57% 6789/11873 [00:26<00:20, 251.02it/s]\u001b[A\n"," 57% 6815/11873 [00:26<00:20, 252.46it/s]\u001b[A\n"," 58% 6841/11873 [00:26<00:19, 254.38it/s]\u001b[A\n"," 58% 6867/11873 [00:26<00:19, 252.95it/s]\u001b[A\n"," 58% 6894/11873 [00:26<00:19, 255.80it/s]\u001b[A\n"," 58% 6921/11873 [00:26<00:19, 258.60it/s]\u001b[A\n"," 59% 6948/11873 [00:26<00:18, 259.90it/s]\u001b[A\n"," 59% 6975/11873 [00:26<00:18, 261.53it/s]\u001b[A\n"," 59% 7002/11873 [00:26<00:18, 261.24it/s]\u001b[A\n"," 59% 7029/11873 [00:27<00:18, 259.82it/s]\u001b[A\n"," 59% 7055/11873 [00:27<00:18, 259.76it/s]\u001b[A\n"," 60% 7082/11873 [00:27<00:18, 261.54it/s]\u001b[A\n"," 60% 7109/11873 [00:27<00:18, 259.69it/s]\u001b[A\n"," 60% 7135/11873 [00:27<00:18, 259.52it/s]\u001b[A\n"," 60% 7162/11873 [00:27<00:18, 260.16it/s]\u001b[A\n"," 61% 7189/11873 [00:27<00:18, 257.29it/s]\u001b[A\n"," 61% 7215/11873 [00:27<00:18, 251.19it/s]\u001b[A\n"," 61% 7241/11873 [00:27<00:18, 251.05it/s]\u001b[A\n"," 61% 7267/11873 [00:27<00:18, 252.99it/s]\u001b[A\n"," 61% 7293/11873 [00:28<00:17, 254.76it/s]\u001b[A\n"," 62% 7319/11873 [00:28<00:17, 255.62it/s]\u001b[A\n"," 62% 7345/11873 [00:28<00:17, 254.06it/s]\u001b[A\n"," 62% 7371/11873 [00:28<00:17, 255.22it/s]\u001b[A\n"," 62% 7397/11873 [00:28<00:17, 254.30it/s]\u001b[A\n"," 63% 7423/11873 [00:28<00:17, 254.36it/s]\u001b[A\n"," 63% 7449/11873 [00:28<00:17, 253.70it/s]\u001b[A\n"," 63% 7475/11873 [00:28<00:17, 251.40it/s]\u001b[A\n"," 63% 7501/11873 [00:28<00:17, 246.69it/s]\u001b[A\n"," 63% 7526/11873 [00:28<00:17, 246.73it/s]\u001b[A\n"," 64% 7551/11873 [00:29<00:17, 246.74it/s]\u001b[A\n"," 64% 7578/11873 [00:29<00:17, 252.08it/s]\u001b[A\n"," 64% 7604/11873 [00:29<00:16, 251.92it/s]\u001b[A\n"," 64% 7630/11873 [00:29<00:16, 252.98it/s]\u001b[A\n"," 64% 7656/11873 [00:29<00:16, 254.47it/s]\u001b[A\n"," 65% 7682/11873 [00:29<00:16, 254.53it/s]\u001b[A\n"," 65% 7708/11873 [00:29<00:16, 254.08it/s]\u001b[A\n"," 65% 7734/11873 [00:29<00:16, 255.38it/s]\u001b[A\n"," 65% 7760/11873 [00:29<00:16, 253.93it/s]\u001b[A\n"," 66% 7787/11873 [00:29<00:15, 255.87it/s]\u001b[A\n"," 66% 7813/11873 [00:30<00:15, 254.84it/s]\u001b[A\n"," 66% 7839/11873 [00:30<00:16, 249.98it/s]\u001b[A\n"," 66% 7865/11873 [00:30<00:16, 246.00it/s]\u001b[A\n"," 66% 7890/11873 [00:30<00:16, 243.70it/s]\u001b[A\n"," 67% 7915/11873 [00:30<00:16, 245.40it/s]\u001b[A\n"," 67% 7941/11873 [00:30<00:15, 247.10it/s]\u001b[A\n"," 67% 7966/11873 [00:30<00:16, 243.36it/s]\u001b[A\n"," 67% 7991/11873 [00:30<00:15, 243.26it/s]\u001b[A\n"," 68% 8017/11873 [00:30<00:15, 245.86it/s]\u001b[A\n"," 68% 8043/11873 [00:31<00:15, 249.02it/s]\u001b[A\n"," 68% 8069/11873 [00:31<00:15, 251.16it/s]\u001b[A\n"," 68% 8095/11873 [00:31<00:14, 252.98it/s]\u001b[A\n"," 68% 8121/11873 [00:31<00:14, 253.86it/s]\u001b[A\n"," 69% 8147/11873 [00:31<00:14, 255.13it/s]\u001b[A\n"," 69% 8173/11873 [00:31<00:14, 255.81it/s]\u001b[A\n"," 69% 8199/11873 [00:31<00:14, 254.70it/s]\u001b[A\n"," 69% 8225/11873 [00:31<00:14, 250.39it/s]\u001b[A\n"," 69% 8251/11873 [00:31<00:14, 251.33it/s]\u001b[A\n"," 70% 8277/11873 [00:31<00:14, 250.74it/s]\u001b[A\n"," 70% 8303/11873 [00:32<00:14, 251.48it/s]\u001b[A\n"," 70% 8329/11873 [00:32<00:14, 252.61it/s]\u001b[A\n"," 70% 8355/11873 [00:32<00:14, 247.78it/s]\u001b[A\n"," 71% 8380/11873 [00:32<00:14, 245.99it/s]\u001b[A\n"," 71% 8405/11873 [00:32<00:14, 245.81it/s]\u001b[A\n"," 71% 8430/11873 [00:32<00:14, 243.42it/s]\u001b[A\n"," 71% 8455/11873 [00:32<00:14, 241.13it/s]\u001b[A\n"," 71% 8481/11873 [00:32<00:13, 245.44it/s]\u001b[A\n"," 72% 8507/11873 [00:32<00:13, 247.53it/s]\u001b[A\n"," 72% 8534/11873 [00:32<00:13, 253.52it/s]\u001b[A\n"," 72% 8560/11873 [00:33<00:12, 255.07it/s]\u001b[A\n"," 72% 8587/11873 [00:33<00:12, 258.69it/s]\u001b[A\n"," 73% 8613/11873 [00:33<00:12, 258.90it/s]\u001b[A\n"," 73% 8640/11873 [00:33<00:12, 260.99it/s]\u001b[A\n"," 73% 8667/11873 [00:33<00:12, 257.34it/s]\u001b[A\n"," 73% 8693/11873 [00:33<00:12, 257.09it/s]\u001b[A\n"," 73% 8719/11873 [00:33<00:12, 256.71it/s]\u001b[A\n"," 74% 8746/11873 [00:33<00:12, 260.08it/s]\u001b[A\n"," 74% 8773/11873 [00:33<00:12, 255.75it/s]\u001b[A\n"," 74% 8799/11873 [00:34<00:12, 255.82it/s]\u001b[A\n"," 74% 8826/11873 [00:34<00:11, 258.39it/s]\u001b[A\n"," 75% 8852/11873 [00:34<00:11, 257.24it/s]\u001b[A\n"," 75% 8878/11873 [00:34<00:11, 253.47it/s]\u001b[A\n"," 75% 8904/11873 [00:34<00:11, 251.59it/s]\u001b[A\n"," 75% 8930/11873 [00:34<00:11, 249.88it/s]\u001b[A\n"," 75% 8955/11873 [00:34<00:11, 247.78it/s]\u001b[A\n"," 76% 8981/11873 [00:34<00:11, 249.73it/s]\u001b[A\n"," 76% 9008/11873 [00:34<00:11, 254.51it/s]\u001b[A\n"," 76% 9034/11873 [00:34<00:11, 255.49it/s]\u001b[A\n"," 76% 9062/11873 [00:35<00:10, 259.38it/s]\u001b[A\n"," 77% 9088/11873 [00:35<00:11, 251.05it/s]\u001b[A\n"," 77% 9115/11873 [00:35<00:10, 254.73it/s]\u001b[A\n"," 77% 9141/11873 [00:35<00:10, 256.17it/s]\u001b[A\n"," 77% 9168/11873 [00:35<00:10, 260.01it/s]\u001b[A\n"," 77% 9195/11873 [00:35<00:10, 261.94it/s]\u001b[A\n"," 78% 9222/11873 [00:35<00:10, 263.33it/s]\u001b[A\n"," 78% 9249/11873 [00:35<00:10, 259.69it/s]\u001b[A\n"," 78% 9275/11873 [00:35<00:10, 253.66it/s]\u001b[A\n"," 78% 9301/11873 [00:36<00:15, 171.29it/s]\u001b[A\n"," 79% 9328/11873 [00:36<00:13, 192.24it/s]\u001b[A\n"," 79% 9355/11873 [00:36<00:12, 208.94it/s]\u001b[A\n"," 79% 9381/11873 [00:36<00:11, 220.27it/s]\u001b[A\n"," 79% 9408/11873 [00:36<00:10, 231.46it/s]\u001b[A\n"," 79% 9434/11873 [00:36<00:10, 238.80it/s]\u001b[A\n"," 80% 9460/11873 [00:36<00:09, 242.92it/s]\u001b[A\n"," 80% 9486/11873 [00:36<00:09, 244.06it/s]\u001b[A\n"," 80% 9512/11873 [00:36<00:09, 247.34it/s]\u001b[A\n"," 80% 9539/11873 [00:37<00:09, 253.61it/s]\u001b[A\n"," 81% 9566/11873 [00:37<00:09, 256.03it/s]\u001b[A\n"," 81% 9593/11873 [00:37<00:08, 259.26it/s]\u001b[A\n"," 81% 9620/11873 [00:37<00:08, 259.89it/s]\u001b[A\n"," 81% 9647/11873 [00:37<00:08, 258.05it/s]\u001b[A\n"," 81% 9675/11873 [00:37<00:08, 261.84it/s]\u001b[A\n"," 82% 9702/11873 [00:37<00:08, 264.00it/s]\u001b[A\n"," 82% 9729/11873 [00:37<00:08, 256.22it/s]\u001b[A\n"," 82% 9756/11873 [00:37<00:08, 259.38it/s]\u001b[A\n"," 82% 9783/11873 [00:38<00:08, 255.47it/s]\u001b[A\n"," 83% 9809/11873 [00:38<00:08, 256.09it/s]\u001b[A\n"," 83% 9836/11873 [00:38<00:07, 258.38it/s]\u001b[A\n"," 83% 9863/11873 [00:38<00:07, 261.20it/s]\u001b[A\n"," 83% 9890/11873 [00:38<00:07, 261.74it/s]\u001b[A\n"," 84% 9917/11873 [00:38<00:07, 262.57it/s]\u001b[A\n"," 84% 9944/11873 [00:38<00:07, 260.57it/s]\u001b[A\n"," 84% 9971/11873 [00:38<00:07, 259.66it/s]\u001b[A\n"," 84% 9997/11873 [00:38<00:07, 256.13it/s]\u001b[A\n"," 84% 10023/11873 [00:38<00:07, 255.22it/s]\u001b[A\n"," 85% 10050/11873 [00:39<00:07, 257.12it/s]\u001b[A\n"," 85% 10077/11873 [00:39<00:06, 258.34it/s]\u001b[A\n"," 85% 10104/11873 [00:39<00:06, 259.06it/s]\u001b[A\n"," 85% 10130/11873 [00:39<00:06, 255.20it/s]\u001b[A\n"," 86% 10156/11873 [00:39<00:06, 254.27it/s]\u001b[A\n"," 86% 10182/11873 [00:39<00:06, 254.37it/s]\u001b[A\n"," 86% 10208/11873 [00:39<00:06, 252.82it/s]\u001b[A\n"," 86% 10235/11873 [00:39<00:06, 255.46it/s]\u001b[A\n"," 86% 10262/11873 [00:39<00:06, 259.23it/s]\u001b[A\n"," 87% 10288/11873 [00:39<00:06, 257.59it/s]\u001b[A\n"," 87% 10314/11873 [00:40<00:06, 256.67it/s]\u001b[A\n"," 87% 10341/11873 [00:40<00:05, 260.19it/s]\u001b[A\n"," 87% 10368/11873 [00:40<00:05, 256.18it/s]\u001b[A\n"," 88% 10394/11873 [00:40<00:05, 255.05it/s]\u001b[A\n"," 88% 10420/11873 [00:40<00:05, 255.59it/s]\u001b[A\n"," 88% 10446/11873 [00:40<00:05, 251.21it/s]\u001b[A\n"," 88% 10472/11873 [00:40<00:05, 252.22it/s]\u001b[A\n"," 88% 10499/11873 [00:40<00:05, 257.07it/s]\u001b[A\n"," 89% 10525/11873 [00:40<00:05, 256.29it/s]\u001b[A\n"," 89% 10551/11873 [00:41<00:05, 246.09it/s]\u001b[A\n"," 89% 10577/11873 [00:41<00:05, 249.56it/s]\u001b[A\n"," 89% 10603/11873 [00:41<00:05, 248.77it/s]\u001b[A\n"," 90% 10629/11873 [00:41<00:04, 251.80it/s]\u001b[A\n"," 90% 10655/11873 [00:41<00:04, 249.62it/s]\u001b[A\n"," 90% 10682/11873 [00:41<00:04, 252.87it/s]\u001b[A\n"," 90% 10709/11873 [00:41<00:04, 257.14it/s]\u001b[A\n"," 90% 10735/11873 [00:41<00:04, 251.24it/s]\u001b[A\n"," 91% 10761/11873 [00:41<00:04, 247.53it/s]\u001b[A\n"," 91% 10787/11873 [00:41<00:04, 248.59it/s]\u001b[A\n"," 91% 10812/11873 [00:42<00:04, 247.92it/s]\u001b[A\n"," 91% 10837/11873 [00:42<00:04, 246.04it/s]\u001b[A\n"," 91% 10862/11873 [00:42<00:04, 246.13it/s]\u001b[A\n"," 92% 10887/11873 [00:42<00:04, 239.26it/s]\u001b[A\n"," 92% 10912/11873 [00:42<00:03, 240.47it/s]\u001b[A\n"," 92% 10937/11873 [00:42<00:03, 235.60it/s]\u001b[A\n"," 92% 10962/11873 [00:42<00:03, 238.83it/s]\u001b[A\n"," 93% 10986/11873 [00:42<00:03, 237.91it/s]\u001b[A\n"," 93% 11011/11873 [00:42<00:03, 240.53it/s]\u001b[A\n"," 93% 11036/11873 [00:42<00:03, 242.37it/s]\u001b[A\n"," 93% 11063/11873 [00:43<00:03, 248.81it/s]\u001b[A\n"," 93% 11089/11873 [00:43<00:03, 250.23it/s]\u001b[A\n"," 94% 11115/11873 [00:43<00:03, 251.61it/s]\u001b[A\n"," 94% 11141/11873 [00:43<00:02, 249.60it/s]\u001b[A\n"," 94% 11167/11873 [00:43<00:02, 250.15it/s]\u001b[A\n"," 94% 11193/11873 [00:43<00:02, 246.39it/s]\u001b[A\n"," 94% 11218/11873 [00:43<00:02, 246.53it/s]\u001b[A\n"," 95% 11244/11873 [00:43<00:02, 248.52it/s]\u001b[A\n"," 95% 11269/11873 [00:43<00:02, 247.57it/s]\u001b[A\n"," 95% 11294/11873 [00:44<00:02, 247.91it/s]\u001b[A\n"," 95% 11321/11873 [00:44<00:02, 253.33it/s]\u001b[A\n"," 96% 11347/11873 [00:44<00:02, 253.28it/s]\u001b[A\n"," 96% 11373/11873 [00:44<00:01, 252.03it/s]\u001b[A\n"," 96% 11399/11873 [00:44<00:01, 248.76it/s]\u001b[A\n"," 96% 11424/11873 [00:44<00:01, 247.43it/s]\u001b[A\n"," 96% 11449/11873 [00:44<00:01, 247.86it/s]\u001b[A\n"," 97% 11474/11873 [00:44<00:01, 248.17it/s]\u001b[A\n"," 97% 11499/11873 [00:44<00:01, 240.95it/s]\u001b[A\n"," 97% 11524/11873 [00:44<00:01, 243.42it/s]\u001b[A\n"," 97% 11551/11873 [00:45<00:01, 249.92it/s]\u001b[A\n"," 98% 11577/11873 [00:45<00:01, 252.20it/s]\u001b[A\n"," 98% 11604/11873 [00:45<00:01, 254.60it/s]\u001b[A\n"," 98% 11630/11873 [00:45<00:00, 254.93it/s]\u001b[A\n"," 98% 11656/11873 [00:45<00:00, 255.04it/s]\u001b[A\n"," 98% 11682/11873 [00:45<00:00, 250.26it/s]\u001b[A\n"," 99% 11708/11873 [00:45<00:00, 246.94it/s]\u001b[A\n"," 99% 11735/11873 [00:45<00:00, 250.75it/s]\u001b[A\n"," 99% 11762/11873 [00:45<00:00, 254.94it/s]\u001b[A\n"," 99% 11788/11873 [00:45<00:00, 253.53it/s]\u001b[A\n","100% 11814/11873 [00:46<00:00, 249.33it/s]\u001b[A\n","100% 11839/11873 [00:46<00:00, 247.65it/s]\u001b[A\n","100% 11873/11873 [00:46<00:00, 256.15it/s]\n","04/06/2022 11:14:13 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/eval_predictions.json.\n","04/06/2022 11:14:13 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/eval_nbest_predictions.json.\n","04/06/2022 11:14:15 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/albert-base-v2/word2vec-aug/eval_null_odds.json.\n","04/06/2022 11:14:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1496/1496 [05:30<00:00,  4.52it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 76.3327\n","  eval_HasAns_f1         = 82.5969\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 64.5921\n","  eval_NoAns_f1          = 64.5921\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 70.4624\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           =   73.59\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             =  70.454\n","  eval_f1                = 73.5816\n","  eval_samples           =   11968\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 11:14:19,845 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_word2vec_aug', 'type': 'sichenzhong/squad_v2_word2vec_aug', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path roberta-base \\\n","  --dataset_name sichenzhong/squad_v2_word2vec_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"suoQe017fp_-","executionInfo":{"status":"ok","timestamp":1649254207430,"user_tz":240,"elapsed":10545476,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"8572f954-0463-4aee-d29a-815eb346cbd0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["04/06/2022 11:14:26 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 11:14:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/runs/Apr06_11-14-26_e998dce90490,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 11:14:27 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a\n","04/06/2022 11:14:27 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/06/2022 11:14:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","04/06/2022 11:14:27 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","04/06/2022 11:14:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","100% 2/2 [00:00<00:00, 494.55it/s]\n","[INFO|hub.py:583] 2022-04-06 11:14:27,143 >> https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3i6cyzhz\n","Downloading: 100% 481/481 [00:00<00:00, 616kB/s]\n","[INFO|hub.py:587] 2022-04-06 11:14:27,271 >> storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|hub.py:595] 2022-04-06 11:14:27,271 >> creating metadata file for /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:654] 2022-04-06 11:14:27,271 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-06 11:14:27,273 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-06 11:14:27,411 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-06 11:14:27,550 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-06 11:14:27,551 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 11:14:27,819 >> https://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp85yibu_1\n","Downloading: 100% 878k/878k [00:00<00:00, 5.89MB/s]\n","[INFO|hub.py:587] 2022-04-06 11:14:28,118 >> storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|hub.py:595] 2022-04-06 11:14:28,119 >> creating metadata file for /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|hub.py:583] 2022-04-06 11:14:28,248 >> https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp94a4h22k\n","Downloading: 100% 446k/446k [00:00<00:00, 3.33MB/s]\n","[INFO|hub.py:587] 2022-04-06 11:14:28,520 >> storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|hub.py:595] 2022-04-06 11:14:28,520 >> creating metadata file for /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|hub.py:583] 2022-04-06 11:14:28,661 >> https://huggingface.co/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp67ili12n\n","Downloading: 100% 1.29M/1.29M [00:00<00:00, 7.73MB/s]\n","[INFO|hub.py:587] 2022-04-06 11:14:28,977 >> storing https://huggingface.co/roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|hub.py:595] 2022-04-06 11:14:28,978 >> creating metadata file for /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 11:14:29,381 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 11:14:29,381 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 11:14:29,381 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 11:14:29,381 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 11:14:29,381 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 11:14:29,381 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-06 11:14:29,508 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-06 11:14:29,509 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 11:14:29,753 >> https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmtvm5ycb\n","Downloading: 100% 478M/478M [00:09<00:00, 50.5MB/s]\n","[INFO|hub.py:587] 2022-04-06 11:14:39,767 >> storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[INFO|hub.py:595] 2022-04-06 11:14:39,768 >> creating metadata file for /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[INFO|modeling_utils.py:1772] 2022-04-06 11:14:39,768 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[WARNING|modeling_utils.py:2050] 2022-04-06 11:14:41,204 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-06 11:14:41,204 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/06/2022 11:14:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-70a69cb9a5c13c44.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:46<00:00,  2.79ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/06/2022 11:15:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_word2vec_aug-ef952f6343331f2a/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-185811ce79f1dc5a.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:09<00:00,  5.82s/ba]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 11:16:42,321 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 11:16:42,321 >>   Num examples = 132057\n","[INFO|trainer.py:1292] 2022-04-06 11:16:42,321 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-06 11:16:42,321 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1294] 2022-04-06 11:16:42,321 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1295] 2022-04-06 11:16:42,321 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 11:16:42,321 >>   Total optimization steps = 11006\n","{'loss': 2.2097, 'learning_rate': 3.818280937670362e-05, 'epoch': 0.09}\n","  5% 500/11006 [07:36<2:39:40,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:24:18,714 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 11:24:18,720 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:24:20,586 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:24:20,592 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:24:20,597 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 1.6998, 'learning_rate': 3.636561875340724e-05, 'epoch': 0.18}\n","  9% 1000/11006 [15:18<2:31:56,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:32:00,364 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 11:32:00,372 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:32:01,731 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:32:01,738 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:32:01,743 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.578, 'learning_rate': 3.454842813011085e-05, 'epoch': 0.27}\n"," 14% 1500/11006 [22:59<2:23:58,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:39:41,654 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 11:39:41,660 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:39:42,992 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:39:42,999 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:39:43,003 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.53, 'learning_rate': 3.273123750681446e-05, 'epoch': 0.36}\n"," 18% 2000/11006 [30:40<2:17:20,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 11:47:23,054 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 11:47:23,061 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:47:24,431 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:47:24,436 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:47:24,439 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.485, 'learning_rate': 3.091404688351809e-05, 'epoch': 0.45}\n"," 23% 2500/11006 [38:21<2:09:19,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:55:04,095 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 11:55:04,100 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:55:05,463 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:55:05,469 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:55:05,474 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.4403, 'learning_rate': 2.90968562602217e-05, 'epoch': 0.55}\n"," 27% 3000/11006 [46:02<2:02:00,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 12:02:45,189 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 12:02:45,196 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:02:46,545 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:02:46,551 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:02:46,556 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.4132, 'learning_rate': 2.7279665636925313e-05, 'epoch': 0.64}\n"," 32% 3500/11006 [53:43<1:54:37,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 12:10:26,294 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 12:10:26,301 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:10:27,676 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:10:27,681 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:10:27,686 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.39, 'learning_rate': 2.5462475013628933e-05, 'epoch': 0.73}\n"," 36% 4000/11006 [1:01:28<1:46:34,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 12:18:10,843 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 12:18:10,849 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:18:12,217 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:18:12,222 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:18:12,226 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.3491, 'learning_rate': 2.3645284390332547e-05, 'epoch': 0.82}\n"," 41% 4500/11006 [1:09:11<1:39:05,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 12:25:53,564 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 12:25:53,570 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:25:54,924 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:25:54,929 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:25:55,876 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.3023, 'learning_rate': 2.1828093767036163e-05, 'epoch': 0.91}\n"," 45% 5000/11006 [1:16:53<1:31:14,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 12:33:35,757 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 12:33:35,779 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:33:37,137 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:33:37,142 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:33:37,146 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.2743, 'learning_rate': 2.001090314373978e-05, 'epoch': 1.0}\n"," 50% 5500/11006 [1:24:34<1:23:47,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 12:41:17,282 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 12:41:17,289 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:41:18,648 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:41:18,655 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:41:18,659 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.0416, 'learning_rate': 1.8193712520443397e-05, 'epoch': 1.09}\n"," 55% 6000/11006 [1:32:15<1:16:20,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 12:48:58,057 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 12:48:58,065 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:48:59,412 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:48:59,417 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:48:59,422 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.0259, 'learning_rate': 1.6376521897147014e-05, 'epoch': 1.18}\n"," 59% 6500/11006 [1:39:57<1:08:28,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 12:56:39,431 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 12:56:39,438 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:56:40,799 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:56:40,805 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:56:40,809 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.9896, 'learning_rate': 1.4559331273850627e-05, 'epoch': 1.27}\n"," 64% 7000/11006 [1:47:40<1:00:54,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:04:22,751 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 13:04:22,758 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:04:24,192 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:04:24,198 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:04:24,205 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.0021, 'learning_rate': 1.2742140650554245e-05, 'epoch': 1.36}\n"," 68% 7500/11006 [1:55:24<53:18,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:12:07,034 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 13:12:07,040 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:12:08,403 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:12:08,410 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:12:08,415 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.9983, 'learning_rate': 1.0924950027257862e-05, 'epoch': 1.45}\n"," 73% 8000/11006 [2:03:06<45:43,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:19:49,246 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 13:19:49,252 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:19:50,656 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:19:50,661 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:19:51,521 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.9768, 'learning_rate': 9.107759403961475e-06, 'epoch': 1.54}\n"," 77% 8500/11006 [2:10:49<38:02,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:27:31,460 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 13:27:31,466 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:27:32,761 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:27:32,767 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:27:32,771 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.98, 'learning_rate': 7.290568780665093e-06, 'epoch': 1.64}\n"," 82% 9000/11006 [2:18:31<30:27,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:35:13,730 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 13:35:13,737 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:35:15,057 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:35:15,063 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:35:15,067 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.9834, 'learning_rate': 5.473378157368709e-06, 'epoch': 1.73}\n"," 86% 9500/11006 [2:26:13<22:58,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 13:42:55,806 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 13:42:55,811 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:42:57,125 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:42:57,484 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:42:58,115 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.958, 'learning_rate': 3.6561875340723245e-06, 'epoch': 1.82}\n"," 91% 10000/11006 [2:33:55<15:17,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:50:37,818 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 13:50:37,824 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:50:39,142 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:50:39,148 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:50:39,152 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.9555, 'learning_rate': 1.8389969107759405e-06, 'epoch': 1.91}\n"," 95% 10500/11006 [2:41:36<07:41,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:58:18,486 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 13:58:18,493 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:58:19,811 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:58:19,817 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:58:19,822 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.9376, 'learning_rate': 2.1806287479556606e-08, 'epoch': 2.0}\n","100% 11000/11006 [2:49:17<00:05,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 14:05:59,447 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-06 14:05:59,454 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:06:00,768 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:06:00,774 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:06:00,779 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/checkpoint-11000/special_tokens_map.json\n","100% 11006/11006 [2:49:29<00:00,  1.16s/it][INFO|trainer.py:1530] 2022-04-06 14:06:12,222 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10169.9012, 'train_samples_per_second': 25.97, 'train_steps_per_second': 1.082, 'train_loss': 1.2509085931453883, 'epoch': 2.0}\n","100% 11006/11006 [2:49:29<00:00,  1.08it/s]\n","[INFO|trainer.py:2166] 2022-04-06 14:06:12,236 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug\n","[INFO|configuration_utils.py:441] 2022-04-06 14:06:12,242 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:06:13,690 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:06:13,697 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:06:13,702 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     1.2509\n","  train_runtime            = 2:49:29.90\n","  train_samples            =     132057\n","  train_samples_per_second =      25.97\n","  train_steps_per_second   =      1.082\n","04/06/2022 14:06:13 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 14:06:13,878 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 14:06:13,881 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 14:06:13,881 >>   Num examples = 12165\n","[INFO|trainer.py:2421] 2022-04-06 14:06:13,881 >>   Batch size = 8\n","100% 1520/1521 [02:56<00:00,  8.63it/s]04/06/2022 14:09:23 - INFO - utils_qa - Post-processing 11873 example predictions split into 12165 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 41/11873 [00:00<00:29, 404.72it/s]\u001b[A\n","  1% 82/11873 [00:00<00:32, 367.42it/s]\u001b[A\n","  1% 123/11873 [00:00<00:30, 381.87it/s]\u001b[A\n","  1% 165/11873 [00:00<00:29, 394.90it/s]\u001b[A\n","  2% 208/11873 [00:00<00:28, 404.46it/s]\u001b[A\n","  2% 252/11873 [00:00<00:28, 413.54it/s]\u001b[A\n","  3% 297/11873 [00:00<00:27, 424.25it/s]\u001b[A\n","  3% 340/11873 [00:00<00:27, 421.49it/s]\u001b[A\n","  3% 385/11873 [00:00<00:26, 428.26it/s]\u001b[A\n","  4% 429/11873 [00:01<00:26, 431.16it/s]\u001b[A\n","  4% 473/11873 [00:01<00:26, 432.55it/s]\u001b[A\n","  4% 517/11873 [00:01<00:26, 429.22it/s]\u001b[A\n","  5% 560/11873 [00:01<00:26, 424.56it/s]\u001b[A\n","  5% 605/11873 [00:01<00:26, 429.71it/s]\u001b[A\n","  5% 649/11873 [00:01<00:26, 429.76it/s]\u001b[A\n","  6% 694/11873 [00:01<00:25, 432.24it/s]\u001b[A\n","  6% 738/11873 [00:01<00:26, 426.35it/s]\u001b[A\n","  7% 781/11873 [00:01<00:26, 417.43it/s]\u001b[A\n","  7% 825/11873 [00:01<00:26, 421.31it/s]\u001b[A\n","  7% 873/11873 [00:02<00:25, 438.01it/s]\u001b[A\n","  8% 917/11873 [00:02<00:24, 438.44it/s]\u001b[A\n","  8% 962/11873 [00:02<00:24, 440.57it/s]\u001b[A\n","  8% 1007/11873 [00:02<00:25, 420.76it/s]\u001b[A\n","  9% 1050/11873 [00:02<00:27, 386.77it/s]\u001b[A\n","  9% 1090/11873 [00:02<00:28, 372.57it/s]\u001b[A\n"," 10% 1128/11873 [00:02<00:30, 352.45it/s]\u001b[A\n"," 10% 1164/11873 [00:02<00:31, 341.94it/s]\u001b[A\n"," 10% 1199/11873 [00:02<00:31, 339.82it/s]\u001b[A\n"," 10% 1235/11873 [00:03<00:30, 344.23it/s]\u001b[A\n"," 11% 1270/11873 [00:03<00:31, 340.34it/s]\u001b[A\n"," 11% 1305/11873 [00:03<00:31, 340.31it/s]\u001b[A\n"," 11% 1340/11873 [00:03<00:31, 339.22it/s]\u001b[A\n"," 12% 1374/11873 [00:03<00:31, 335.40it/s]\u001b[A\n"," 12% 1409/11873 [00:03<00:30, 338.56it/s]\u001b[A\n"," 12% 1443/11873 [00:03<00:30, 337.53it/s]\u001b[A\n"," 12% 1477/11873 [00:03<00:31, 327.75it/s]\u001b[A\n"," 13% 1511/11873 [00:03<00:31, 328.76it/s]\u001b[A\n"," 13% 1544/11873 [00:04<00:31, 328.00it/s]\u001b[A\n"," 13% 1578/11873 [00:04<00:31, 330.98it/s]\u001b[A\n"," 14% 1613/11873 [00:04<00:30, 333.66it/s]\u001b[A\n"," 14% 1648/11873 [00:04<00:30, 336.81it/s]\u001b[A\n"," 14% 1682/11873 [00:04<00:30, 337.56it/s]\u001b[A\n"," 14% 1716/11873 [00:04<00:30, 336.00it/s]\u001b[A\n"," 15% 1751/11873 [00:04<00:29, 337.90it/s]\u001b[A\n"," 15% 1785/11873 [00:04<00:29, 336.84it/s]\u001b[A\n"," 15% 1819/11873 [00:04<00:29, 335.72it/s]\u001b[A\n"," 16% 1854/11873 [00:04<00:29, 338.54it/s]\u001b[A\n"," 16% 1889/11873 [00:05<00:29, 340.89it/s]\u001b[A\n"," 16% 1924/11873 [00:05<00:29, 340.37it/s]\u001b[A\n"," 16% 1959/11873 [00:05<00:29, 339.82it/s]\u001b[A\n"," 17% 1993/11873 [00:05<00:29, 334.80it/s]\u001b[A\n"," 17% 2028/11873 [00:05<00:29, 338.07it/s]\u001b[A\n"," 17% 2062/11873 [00:05<00:29, 335.04it/s]\u001b[A\n"," 18% 2097/11873 [00:05<00:28, 338.51it/s]\u001b[A\n"," 18% 2131/11873 [00:05<00:28, 338.34it/s]\u001b[A\n"," 18% 2165/11873 [00:05<00:28, 336.70it/s]\u001b[A\n"," 19% 2200/11873 [00:05<00:28, 338.65it/s]\u001b[A\n"," 19% 2235/11873 [00:06<00:28, 340.52it/s]\u001b[A\n"," 19% 2270/11873 [00:06<00:28, 337.26it/s]\u001b[A\n"," 19% 2304/11873 [00:06<00:28, 335.87it/s]\u001b[A\n"," 20% 2338/11873 [00:06<00:29, 325.37it/s]\u001b[A\n"," 20% 2372/11873 [00:06<00:29, 327.43it/s]\u001b[A\n"," 20% 2405/11873 [00:06<00:28, 327.75it/s]\u001b[A\n"," 21% 2438/11873 [00:06<00:28, 327.76it/s]\u001b[A\n"," 21% 2473/11873 [00:06<00:28, 331.94it/s]\u001b[A\n"," 21% 2508/11873 [00:06<00:27, 335.76it/s]\u001b[A\n"," 21% 2542/11873 [00:06<00:27, 336.17it/s]\u001b[A\n"," 22% 2577/11873 [00:07<00:27, 339.44it/s]\u001b[A\n"," 22% 2611/11873 [00:07<00:27, 333.97it/s]\u001b[A\n"," 22% 2646/11873 [00:07<00:27, 336.71it/s]\u001b[A\n"," 23% 2680/11873 [00:07<00:27, 336.77it/s]\u001b[A\n"," 23% 2715/11873 [00:07<00:27, 338.10it/s]\u001b[A\n"," 23% 2749/11873 [00:07<00:26, 338.11it/s]\u001b[A\n"," 23% 2783/11873 [00:07<00:27, 334.56it/s]\u001b[A\n"," 24% 2817/11873 [00:07<00:27, 334.88it/s]\u001b[A\n"," 24% 2851/11873 [00:07<00:27, 332.64it/s]\u001b[A\n"," 24% 2885/11873 [00:08<00:27, 331.29it/s]\u001b[A\n"," 25% 2919/11873 [00:08<00:26, 333.29it/s]\u001b[A\n"," 25% 2953/11873 [00:08<00:26, 333.44it/s]\u001b[A\n"," 25% 2988/11873 [00:08<00:26, 336.54it/s]\u001b[A\n"," 25% 3022/11873 [00:08<00:26, 329.40it/s]\u001b[A\n"," 26% 3055/11873 [00:08<00:26, 326.89it/s]\u001b[A\n"," 26% 3088/11873 [00:08<00:27, 318.06it/s]\u001b[A\n"," 26% 3120/11873 [00:08<00:31, 280.07it/s]\u001b[A\n"," 27% 3149/11873 [00:08<00:32, 265.57it/s]\u001b[A\n"," 27% 3177/11873 [00:09<00:33, 259.58it/s]\u001b[A\n"," 27% 3211/11873 [00:09<00:31, 278.99it/s]\u001b[A\n"," 27% 3245/11873 [00:09<00:29, 294.68it/s]\u001b[A\n"," 28% 3275/11873 [00:09<00:31, 270.44it/s]\u001b[A\n"," 28% 3303/11873 [00:09<00:41, 204.38it/s]\u001b[A\n"," 28% 3327/11873 [00:09<00:45, 188.15it/s]\u001b[A\n"," 28% 3349/11873 [00:09<00:44, 192.24it/s]\u001b[A\n"," 28% 3370/11873 [00:09<00:47, 180.82it/s]\u001b[A\n"," 29% 3405/11873 [00:10<00:38, 219.58it/s]\u001b[A\n"," 29% 3439/11873 [00:10<00:33, 249.89it/s]\u001b[A\n"," 29% 3473/11873 [00:10<00:30, 272.67it/s]\u001b[A\n"," 30% 3507/11873 [00:10<00:28, 289.72it/s]\u001b[A\n"," 30% 3541/11873 [00:10<00:27, 301.49it/s]\u001b[A\n"," 30% 3574/11873 [00:10<00:26, 308.18it/s]\u001b[A\n"," 30% 3610/11873 [00:10<00:25, 321.48it/s]\u001b[A\n"," 31% 3644/11873 [00:10<00:25, 324.58it/s]\u001b[A\n"," 31% 3677/11873 [00:10<00:25, 323.49it/s]\u001b[A\n"," 31% 3710/11873 [00:11<00:25, 324.33it/s]\u001b[A\n"," 32% 3744/11873 [00:11<00:24, 328.45it/s]\u001b[A\n"," 32% 3778/11873 [00:11<00:24, 330.19it/s]\u001b[A\n"," 32% 3812/11873 [00:11<00:24, 331.06it/s]\u001b[A\n"," 32% 3846/11873 [00:11<00:26, 307.25it/s]\u001b[A\n"," 33% 3880/11873 [00:11<00:25, 315.20it/s]\u001b[A\n"," 33% 3912/11873 [00:11<00:25, 313.87it/s]\u001b[A\n"," 33% 3944/11873 [00:11<00:26, 301.11it/s]\u001b[A\n"," 33% 3977/11873 [00:11<00:25, 309.07it/s]\u001b[A\n"," 34% 4014/11873 [00:11<00:24, 324.37it/s]\u001b[A\n"," 34% 4048/11873 [00:12<00:23, 328.01it/s]\u001b[A\n"," 34% 4082/11873 [00:12<00:23, 331.21it/s]\u001b[A\n"," 35% 4117/11873 [00:12<00:23, 334.44it/s]\u001b[A\n"," 35% 4151/11873 [00:12<00:24, 311.68it/s]\u001b[A\n"," 35% 4183/11873 [00:12<00:24, 311.63it/s]\u001b[A\n"," 36% 4215/11873 [00:12<00:24, 313.78it/s]\u001b[A\n"," 36% 4250/11873 [00:12<00:23, 323.84it/s]\u001b[A\n"," 36% 4284/11873 [00:12<00:23, 326.19it/s]\u001b[A\n"," 36% 4317/11873 [00:12<00:23, 327.01it/s]\u001b[A\n"," 37% 4351/11873 [00:12<00:22, 329.44it/s]\u001b[A\n"," 37% 4386/11873 [00:13<00:22, 335.28it/s]\u001b[A\n"," 37% 4420/11873 [00:13<00:26, 281.70it/s]\u001b[A\n"," 37% 4450/11873 [00:13<00:26, 277.59it/s]\u001b[A\n"," 38% 4486/11873 [00:13<00:24, 297.12it/s]\u001b[A\n"," 38% 4520/11873 [00:13<00:23, 307.93it/s]\u001b[A\n"," 38% 4554/11873 [00:13<00:23, 315.82it/s]\u001b[A\n"," 39% 4587/11873 [00:13<00:22, 318.59it/s]\u001b[A\n"," 39% 4620/11873 [00:13<00:22, 321.55it/s]\u001b[A\n"," 39% 4654/11873 [00:13<00:22, 324.94it/s]\u001b[A\n"," 39% 4689/11873 [00:14<00:21, 331.75it/s]\u001b[A\n"," 40% 4723/11873 [00:14<00:21, 330.44it/s]\u001b[A\n"," 40% 4757/11873 [00:14<00:21, 331.08it/s]\u001b[A\n"," 40% 4791/11873 [00:14<00:21, 323.10it/s]\u001b[A\n"," 41% 4824/11873 [00:14<00:21, 324.99it/s]\u001b[A\n"," 41% 4859/11873 [00:14<00:21, 329.82it/s]\u001b[A\n"," 41% 4893/11873 [00:14<00:21, 331.50it/s]\u001b[A\n"," 42% 4929/11873 [00:14<00:20, 337.30it/s]\u001b[A\n"," 42% 4963/11873 [00:14<00:20, 334.84it/s]\u001b[A\n"," 42% 4997/11873 [00:15<00:20, 328.72it/s]\u001b[A\n"," 42% 5032/11873 [00:15<00:20, 332.36it/s]\u001b[A\n"," 43% 5067/11873 [00:15<00:20, 336.18it/s]\u001b[A\n"," 43% 5102/11873 [00:15<00:20, 338.01it/s]\u001b[A\n"," 43% 5136/11873 [00:15<00:19, 338.02it/s]\u001b[A\n"," 44% 5170/11873 [00:15<00:19, 336.90it/s]\u001b[A\n"," 44% 5204/11873 [00:15<00:19, 337.62it/s]\u001b[A\n"," 44% 5239/11873 [00:15<00:19, 340.31it/s]\u001b[A\n"," 44% 5274/11873 [00:15<00:21, 312.69it/s]\u001b[A\n"," 45% 5308/11873 [00:15<00:20, 320.02it/s]\u001b[A\n"," 45% 5344/11873 [00:16<00:19, 329.02it/s]\u001b[A\n"," 45% 5380/11873 [00:16<00:19, 336.32it/s]\u001b[A\n"," 46% 5417/11873 [00:16<00:18, 343.56it/s]\u001b[A\n"," 46% 5452/11873 [00:16<00:18, 338.29it/s]\u001b[A\n"," 46% 5487/11873 [00:16<00:18, 338.61it/s]\u001b[A\n"," 47% 5523/11873 [00:16<00:18, 343.66it/s]\u001b[A\n"," 47% 5558/11873 [00:16<00:18, 342.93it/s]\u001b[A\n"," 47% 5593/11873 [00:16<00:18, 343.64it/s]\u001b[A\n"," 47% 5628/11873 [00:16<00:18, 332.24it/s]\u001b[A\n"," 48% 5662/11873 [00:17<00:18, 332.14it/s]\u001b[A\n"," 48% 5696/11873 [00:17<00:18, 329.00it/s]\u001b[A\n"," 48% 5729/11873 [00:17<00:18, 324.47it/s]\u001b[A\n"," 49% 5762/11873 [00:17<00:18, 325.13it/s]\u001b[A\n"," 49% 5795/11873 [00:17<00:18, 321.11it/s]\u001b[A\n"," 49% 5828/11873 [00:17<00:18, 319.34it/s]\u001b[A\n"," 49% 5862/11873 [00:17<00:18, 323.92it/s]\u001b[A\n"," 50% 5896/11873 [00:17<00:18, 326.97it/s]\u001b[A\n"," 50% 5930/11873 [00:17<00:18, 329.79it/s]\u001b[A\n"," 50% 5964/11873 [00:17<00:17, 332.50it/s]\u001b[A\n"," 51% 5998/11873 [00:18<00:17, 333.40it/s]\u001b[A\n"," 51% 6033/11873 [00:18<00:17, 335.85it/s]\u001b[A\n"," 51% 6067/11873 [00:18<00:17, 336.57it/s]\u001b[A\n"," 51% 6101/11873 [00:18<00:17, 333.60it/s]\u001b[A\n"," 52% 6135/11873 [00:18<00:17, 331.25it/s]\u001b[A\n"," 52% 6169/11873 [00:18<00:17, 332.28it/s]\u001b[A\n"," 52% 6205/11873 [00:18<00:16, 337.78it/s]\u001b[A\n"," 53% 6240/11873 [00:18<00:16, 341.13it/s]\u001b[A\n"," 53% 6276/11873 [00:18<00:16, 344.27it/s]\u001b[A\n"," 53% 6312/11873 [00:18<00:16, 347.04it/s]\u001b[A\n"," 53% 6348/11873 [00:19<00:15, 349.00it/s]\u001b[A\n"," 54% 6383/11873 [00:19<00:16, 341.18it/s]\u001b[A\n"," 54% 6419/11873 [00:19<00:15, 343.71it/s]\u001b[A\n"," 54% 6454/11873 [00:19<00:15, 340.36it/s]\u001b[A\n"," 55% 6489/11873 [00:19<00:16, 333.60it/s]\u001b[A\n"," 55% 6523/11873 [00:19<00:16, 329.47it/s]\u001b[A\n"," 55% 6557/11873 [00:19<00:16, 331.12it/s]\u001b[A\n"," 56% 6591/11873 [00:19<00:16, 328.16it/s]\u001b[A\n"," 56% 6624/11873 [00:19<00:16, 322.80it/s]\u001b[A\n"," 56% 6657/11873 [00:20<00:16, 317.42it/s]\u001b[A\n"," 56% 6689/11873 [00:20<00:16, 315.48it/s]\u001b[A\n"," 57% 6721/11873 [00:20<00:18, 281.91it/s]\u001b[A\n"," 57% 6756/11873 [00:20<00:17, 298.14it/s]\u001b[A\n"," 57% 6789/11873 [00:20<00:16, 305.98it/s]\u001b[A\n"," 57% 6821/11873 [00:20<00:16, 306.05it/s]\u001b[A\n"," 58% 6855/11873 [00:20<00:15, 315.66it/s]\u001b[A\n"," 58% 6887/11873 [00:20<00:15, 313.07it/s]\u001b[A\n"," 58% 6920/11873 [00:20<00:15, 316.72it/s]\u001b[A\n"," 59% 6955/11873 [00:20<00:15, 324.06it/s]\u001b[A\n"," 59% 6990/11873 [00:21<00:14, 330.47it/s]\u001b[A\n"," 59% 7025/11873 [00:21<00:14, 334.24it/s]\u001b[A\n"," 59% 7060/11873 [00:21<00:14, 337.69it/s]\u001b[A\n"," 60% 7095/11873 [00:21<00:14, 340.36it/s]\u001b[A\n"," 60% 7130/11873 [00:21<00:13, 338.89it/s]\u001b[A\n"," 60% 7164/11873 [00:21<00:13, 338.70it/s]\u001b[A\n"," 61% 7199/11873 [00:21<00:13, 339.27it/s]\u001b[A\n"," 61% 7234/11873 [00:21<00:13, 340.00it/s]\u001b[A\n"," 61% 7269/11873 [00:21<00:13, 338.19it/s]\u001b[A\n"," 62% 7304/11873 [00:21<00:13, 339.19it/s]\u001b[A\n"," 62% 7338/11873 [00:22<00:13, 338.73it/s]\u001b[A\n"," 62% 7373/11873 [00:22<00:13, 340.65it/s]\u001b[A\n"," 62% 7408/11873 [00:22<00:13, 339.75it/s]\u001b[A\n"," 63% 7442/11873 [00:22<00:14, 316.31it/s]\u001b[A\n"," 63% 7477/11873 [00:22<00:13, 323.79it/s]\u001b[A\n"," 63% 7512/11873 [00:22<00:13, 329.31it/s]\u001b[A\n"," 64% 7546/11873 [00:22<00:13, 331.23it/s]\u001b[A\n"," 64% 7581/11873 [00:22<00:12, 334.02it/s]\u001b[A\n"," 64% 7615/11873 [00:22<00:13, 326.76it/s]\u001b[A\n"," 64% 7648/11873 [00:23<00:13, 323.81it/s]\u001b[A\n"," 65% 7681/11873 [00:23<00:13, 321.97it/s]\u001b[A\n"," 65% 7714/11873 [00:23<00:13, 298.55it/s]\u001b[A\n"," 65% 7747/11873 [00:23<00:13, 306.85it/s]\u001b[A\n"," 66% 7780/11873 [00:23<00:13, 310.86it/s]\u001b[A\n"," 66% 7814/11873 [00:23<00:12, 318.37it/s]\u001b[A\n"," 66% 7847/11873 [00:23<00:12, 317.54it/s]\u001b[A\n"," 66% 7879/11873 [00:23<00:13, 299.54it/s]\u001b[A\n"," 67% 7913/11873 [00:23<00:12, 307.92it/s]\u001b[A\n"," 67% 7946/11873 [00:24<00:12, 312.06it/s]\u001b[A\n"," 67% 7978/11873 [00:24<00:12, 308.09it/s]\u001b[A\n"," 67% 8011/11873 [00:24<00:12, 313.54it/s]\u001b[A\n"," 68% 8044/11873 [00:24<00:12, 317.01it/s]\u001b[A\n"," 68% 8076/11873 [00:24<00:12, 315.29it/s]\u001b[A\n"," 68% 8109/11873 [00:24<00:11, 318.54it/s]\u001b[A\n"," 69% 8144/11873 [00:24<00:11, 326.75it/s]\u001b[A\n"," 69% 8179/11873 [00:24<00:11, 331.50it/s]\u001b[A\n"," 69% 8213/11873 [00:24<00:11, 332.12it/s]\u001b[A\n"," 69% 8248/11873 [00:24<00:10, 335.72it/s]\u001b[A\n"," 70% 8283/11873 [00:25<00:10, 338.42it/s]\u001b[A\n"," 70% 8318/11873 [00:25<00:10, 340.80it/s]\u001b[A\n"," 70% 8353/11873 [00:25<00:10, 342.37it/s]\u001b[A\n"," 71% 8388/11873 [00:25<00:10, 339.84it/s]\u001b[A\n"," 71% 8422/11873 [00:25<00:10, 339.73it/s]\u001b[A\n"," 71% 8456/11873 [00:25<00:10, 333.47it/s]\u001b[A\n"," 72% 8490/11873 [00:25<00:10, 331.93it/s]\u001b[A\n"," 72% 8526/11873 [00:25<00:09, 340.13it/s]\u001b[A\n"," 72% 8561/11873 [00:25<00:09, 340.09it/s]\u001b[A\n"," 72% 8596/11873 [00:25<00:09, 340.27it/s]\u001b[A\n"," 73% 8631/11873 [00:26<00:09, 332.61it/s]\u001b[A\n"," 73% 8666/11873 [00:26<00:09, 336.58it/s]\u001b[A\n"," 73% 8701/11873 [00:26<00:09, 340.06it/s]\u001b[A\n"," 74% 8737/11873 [00:26<00:09, 345.89it/s]\u001b[A\n"," 74% 8772/11873 [00:26<00:08, 345.00it/s]\u001b[A\n"," 74% 8807/11873 [00:26<00:09, 340.36it/s]\u001b[A\n"," 74% 8843/11873 [00:26<00:08, 343.48it/s]\u001b[A\n"," 75% 8878/11873 [00:26<00:08, 342.25it/s]\u001b[A\n"," 75% 8914/11873 [00:26<00:08, 344.42it/s]\u001b[A\n"," 75% 8949/11873 [00:26<00:08, 341.65it/s]\u001b[A\n"," 76% 8984/11873 [00:27<00:08, 341.32it/s]\u001b[A\n"," 76% 9019/11873 [00:27<00:08, 341.33it/s]\u001b[A\n"," 76% 9054/11873 [00:27<00:08, 341.16it/s]\u001b[A\n"," 77% 9089/11873 [00:27<00:08, 340.07it/s]\u001b[A\n"," 77% 9124/11873 [00:27<00:08, 341.76it/s]\u001b[A\n"," 77% 9159/11873 [00:27<00:08, 338.43it/s]\u001b[A\n"," 77% 9193/11873 [00:27<00:07, 337.31it/s]\u001b[A\n"," 78% 9227/11873 [00:27<00:07, 336.37it/s]\u001b[A\n"," 78% 9261/11873 [00:27<00:07, 332.76it/s]\u001b[A\n"," 78% 9295/11873 [00:28<00:07, 331.11it/s]\u001b[A\n"," 79% 9329/11873 [00:28<00:07, 333.43it/s]\u001b[A\n"," 79% 9364/11873 [00:28<00:07, 335.23it/s]\u001b[A\n"," 79% 9400/11873 [00:28<00:07, 338.78it/s]\u001b[A\n"," 79% 9434/11873 [00:28<00:07, 335.89it/s]\u001b[A\n"," 80% 9468/11873 [00:28<00:07, 329.43it/s]\u001b[A\n"," 80% 9503/11873 [00:28<00:07, 333.57it/s]\u001b[A\n"," 80% 9537/11873 [00:28<00:06, 334.15it/s]\u001b[A\n"," 81% 9572/11873 [00:28<00:06, 337.14it/s]\u001b[A\n"," 81% 9606/11873 [00:28<00:06, 329.26it/s]\u001b[A\n"," 81% 9639/11873 [00:29<00:06, 327.93it/s]\u001b[A\n"," 81% 9674/11873 [00:29<00:06, 331.82it/s]\u001b[A\n"," 82% 9710/11873 [00:29<00:06, 338.51it/s]\u001b[A\n"," 82% 9745/11873 [00:29<00:06, 339.83it/s]\u001b[A\n"," 82% 9781/11873 [00:29<00:06, 343.57it/s]\u001b[A\n"," 83% 9816/11873 [00:29<00:06, 338.90it/s]\u001b[A\n"," 83% 9852/11873 [00:29<00:05, 342.91it/s]\u001b[A\n"," 83% 9887/11873 [00:29<00:05, 338.86it/s]\u001b[A\n"," 84% 9921/11873 [00:29<00:05, 337.36it/s]\u001b[A\n"," 84% 9955/11873 [00:29<00:05, 337.44it/s]\u001b[A\n"," 84% 9989/11873 [00:30<00:05, 328.51it/s]\u001b[A\n"," 84% 10022/11873 [00:30<00:05, 325.68it/s]\u001b[A\n"," 85% 10055/11873 [00:30<00:05, 323.54it/s]\u001b[A\n"," 85% 10088/11873 [00:30<00:05, 322.08it/s]\u001b[A\n"," 85% 10121/11873 [00:30<00:05, 314.31it/s]\u001b[A\n"," 86% 10153/11873 [00:30<00:05, 311.56it/s]\u001b[A\n"," 86% 10185/11873 [00:30<00:05, 311.77it/s]\u001b[A\n"," 86% 10220/11873 [00:30<00:05, 321.38it/s]\u001b[A\n"," 86% 10255/11873 [00:30<00:04, 327.43it/s]\u001b[A\n"," 87% 10288/11873 [00:31<00:04, 325.21it/s]\u001b[A\n"," 87% 10321/11873 [00:31<00:04, 325.82it/s]\u001b[A\n"," 87% 10354/11873 [00:31<00:04, 325.55it/s]\u001b[A\n"," 87% 10387/11873 [00:31<00:04, 325.90it/s]\u001b[A\n"," 88% 10420/11873 [00:31<00:04, 323.87it/s]\u001b[A\n"," 88% 10453/11873 [00:31<00:04, 307.55it/s]\u001b[A\n"," 88% 10486/11873 [00:31<00:04, 312.02it/s]\u001b[A\n"," 89% 10520/11873 [00:31<00:04, 318.27it/s]\u001b[A\n"," 89% 10553/11873 [00:31<00:04, 318.96it/s]\u001b[A\n"," 89% 10586/11873 [00:31<00:04, 320.19it/s]\u001b[A\n"," 89% 10620/11873 [00:32<00:03, 323.12it/s]\u001b[A\n"," 90% 10653/11873 [00:32<00:03, 318.95it/s]\u001b[A\n"," 90% 10685/11873 [00:32<00:03, 312.73it/s]\u001b[A\n"," 90% 10718/11873 [00:32<00:03, 316.87it/s]\u001b[A\n"," 91% 10751/11873 [00:32<00:03, 319.38it/s]\u001b[A\n"," 91% 10785/11873 [00:32<00:03, 323.65it/s]\u001b[A\n"," 91% 10818/11873 [00:32<00:03, 323.48it/s]\u001b[A\n"," 91% 10851/11873 [00:32<00:03, 302.72it/s]\u001b[A\n"," 92% 10884/11873 [00:32<00:03, 310.19it/s]\u001b[A\n"," 92% 10917/11873 [00:33<00:03, 314.88it/s]\u001b[A\n"," 92% 10949/11873 [00:33<00:02, 311.38it/s]\u001b[A\n"," 92% 10981/11873 [00:33<00:02, 307.87it/s]\u001b[A\n"," 93% 11013/11873 [00:33<00:02, 308.99it/s]\u001b[A\n"," 93% 11047/11873 [00:33<00:02, 317.01it/s]\u001b[A\n"," 93% 11079/11873 [00:33<00:02, 315.87it/s]\u001b[A\n"," 94% 11111/11873 [00:33<00:02, 310.81it/s]\u001b[A\n"," 94% 11143/11873 [00:33<00:02, 308.98it/s]\u001b[A\n"," 94% 11175/11873 [00:33<00:02, 311.42it/s]\u001b[A\n"," 94% 11207/11873 [00:33<00:02, 313.68it/s]\u001b[A\n"," 95% 11239/11873 [00:34<00:02, 314.00it/s]\u001b[A\n"," 95% 11271/11873 [00:34<00:01, 314.56it/s]\u001b[A\n"," 95% 11304/11873 [00:34<00:01, 318.85it/s]\u001b[A\n"," 95% 11338/11873 [00:34<00:01, 323.44it/s]\u001b[A\n"," 96% 11372/11873 [00:34<00:01, 325.39it/s]\u001b[A\n"," 96% 11406/11873 [00:34<00:01, 326.88it/s]\u001b[A\n"," 96% 11440/11873 [00:34<00:01, 328.51it/s]\u001b[A\n"," 97% 11474/11873 [00:34<00:01, 330.43it/s]\u001b[A\n"," 97% 11508/11873 [00:34<00:01, 331.38it/s]\u001b[A\n"," 97% 11542/11873 [00:34<00:01, 328.91it/s]\u001b[A\n"," 98% 11577/11873 [00:35<00:00, 331.99it/s]\u001b[A\n"," 98% 11611/11873 [00:35<00:00, 331.56it/s]\u001b[A\n"," 98% 11646/11873 [00:35<00:00, 334.68it/s]\u001b[A\n"," 98% 11680/11873 [00:35<00:00, 331.64it/s]\u001b[A\n"," 99% 11715/11873 [00:35<00:00, 335.34it/s]\u001b[A\n"," 99% 11749/11873 [00:35<00:00, 332.45it/s]\u001b[A\n"," 99% 11783/11873 [00:35<00:00, 327.04it/s]\u001b[A\n","100% 11816/11873 [00:35<00:00, 324.88it/s]\u001b[A\n","100% 11873/11873 [00:35<00:00, 329.93it/s]\n","04/06/2022 14:09:59 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/eval_predictions.json.\n","04/06/2022 14:09:59 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/eval_nbest_predictions.json.\n","04/06/2022 14:10:02 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/roberta-base/word2vec-aug/eval_null_odds.json.\n","04/06/2022 14:10:05 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1521/1521 [03:51<00:00,  6.56it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 79.7402\n","  eval_HasAns_f1         = 85.8517\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 60.8915\n","  eval_NoAns_f1          = 60.8915\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 70.3108\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 73.3622\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 70.3024\n","  eval_f1                = 73.3538\n","  eval_samples           =   12165\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 14:10:06,027 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_word2vec_aug', 'type': 'sichenzhong/squad_v2_word2vec_aug', 'args': 'squad_v2'}}\n"]}]}]}