{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"albert model.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70oLi9mZP6oK","executionInfo":{"status":"ok","timestamp":1649358401396,"user_tz":240,"elapsed":168,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"aa225baf-fe6a-4def-c935-83d76bb50e0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Apr  7 19:06:39 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGqcN-zXTvvo","executionInfo":{"status":"ok","timestamp":1649358412329,"user_tz":240,"elapsed":9840,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"efc7c06d-235c-4359-a124-80611a6c902f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n","\u001b[?25l\r\u001b[K     |█                               | 10 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 81 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 245 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 256 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 266 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 276 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 286 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 296 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 307 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 317 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 325 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 52.2 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 5.5 MB/s \n","\u001b[?25hCollecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 48.3 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 45.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 61.9 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.4 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 37.4 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 59.2 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.1 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tI_RBT1FSotu","outputId":"7c1b0ce3-3053-4365-9ba2-1c1bf6d06aeb","executionInfo":{"status":"ok","timestamp":1649358438556,"user_tz":240,"elapsed":26234,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-jigzkadi\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-jigzkadi\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.5.1)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 5.1 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 51.1 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (3.6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.11.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.63.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (1.21.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 51.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0.dev0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.0.dev0) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.0.dev0) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (1.25.11)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.1.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.19.0.dev0-py3-none-any.whl size=3966221 sha256=4b44a4d76a5fe64ce75be585cbd3b2ed2e44726ee7cc43ebf64fe44632b50f6f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-637amerb/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.19.0.dev0\n"]}]},{"cell_type":"code","source":["import torch\n","import datetime\n","import json\n","import os\n","import time\n","import datasets\n","import pprint\n","import random\n","import string\n","import sys\n","import transformers"],"metadata":{"id":"DZ3Ma-pCRJDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available()\n","                      else 'cpu')"],"metadata":{"id":"HNMUVyBpRGw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/huggingface/transformers.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUVkgX-IQIiR","executionInfo":{"status":"ok","timestamp":1649358454211,"user_tz":240,"elapsed":7169,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"dae96748-2e06-4aef-b159-38f55ed5dcd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 108832, done.\u001b[K\n","remote: Counting objects: 100% (6/6), done.\u001b[K\n","remote: Compressing objects: 100% (6/6), done.\u001b[K\n","remote: Total 108832 (delta 1), reused 1 (delta 0), pack-reused 108826\u001b[K\n","Receiving objects: 100% (108832/108832), 95.67 MiB | 31.13 MiB/s, done.\n","Resolving deltas: 100% (79317/79317), done.\n"]}]},{"cell_type":"code","source":["%cd /content/transformers/examples/pytorch/question-answering/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uLaizsXQrk9","executionInfo":{"status":"ok","timestamp":1649358454212,"user_tz":240,"elapsed":18,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"a3cb1ad9-8d8e-4559-8a94-0eb8716268fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/transformers/examples/pytorch/question-answering\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path albert-base-v2 \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 3e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOm5ck1RQqE_","executionInfo":{"status":"ok","timestamp":1648480799684,"user_tz":240,"elapsed":11080595,"user":{"displayName":"SICHEN ZHONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGziZ-Uu-PyjGuHsDy1aSyMZvJYEN6bO8qLBQyKw=s64","userId":"08427994781088390833"}},"outputId":"1fad596f-826c-49ce-cd28-4cffcc90ba39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["03/28/2022 12:15:23 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","03/28/2022 12:15:23 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Mar28_12-15-23_8abefa9ff02e,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","03/28/2022 12:15:24 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpkn73yog8\n","Downloading builder script: 5.28kB [00:00, 5.06MB/s]       \n","03/28/2022 12:15:24 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/28/2022 12:15:24 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/28/2022 12:15:24 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpng1v01lg\n","Downloading metadata: 2.40kB [00:00, 3.59MB/s]       \n","03/28/2022 12:15:24 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/28/2022 12:15:24 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/28/2022 12:15:24 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","03/28/2022 12:15:24 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/28/2022 12:15:24 - INFO - datasets.builder - Generating dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","03/28/2022 12:15:24 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]03/28/2022 12:15:24 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpif9kfqs2\n","\n","Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n","Downloading data:  78% 7.43M/9.55M [00:00<00:00, 74.3MB/s]\u001b[A\n","Downloading data: 15.0MB [00:00, 75.2MB/s]                \u001b[A\n","Downloading data: 22.5MB [00:00, 75.2MB/s]\u001b[A\n","Downloading data: 30.2MB [00:00, 75.5MB/s]\u001b[A\n","Downloading data: 42.1MB [00:00, 75.1MB/s]\n","03/28/2022 12:15:25 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","03/28/2022 12:15:25 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","Downloading data files:  50% 1/2 [00:00<00:00,  1.37it/s]03/28/2022 12:15:25 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp4emj5noo\n","\n","Downloading data: 4.37MB [00:00, 77.3MB/s]      \n","03/28/2022 12:15:25 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","03/28/2022 12:15:25 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","Downloading data files: 100% 2/2 [00:01<00:00,  1.96it/s]\n","03/28/2022 12:15:25 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","03/28/2022 12:15:25 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1137.59it/s]\n","03/28/2022 12:15:25 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n","03/28/2022 12:15:25 - INFO - datasets.builder - Generating train split\n","03/28/2022 12:15:40 - INFO - datasets.builder - Generating validation split\n","03/28/2022 12:15:41 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 273.10it/s]\n","[INFO|hub.py:583] 2022-03-28 12:15:41,718 >> https://huggingface.co/albert-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp52isn1sj\n","Downloading: 100% 684/684 [00:00<00:00, 483kB/s]\n","[INFO|hub.py:587] 2022-03-28 12:15:42,068 >> storing https://huggingface.co/albert-base-v2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|hub.py:595] 2022-03-28 12:15:42,068 >> creating metadata file for /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:653] 2022-03-28 12:15:42,069 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-28 12:15:42,070 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-03-28 12:15:42,419 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:653] 2022-03-28 12:15:42,765 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-28 12:15:42,765 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-03-28 12:15:43,460 >> https://huggingface.co/albert-base-v2/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqn8unrht\n","Downloading: 100% 742k/742k [00:00<00:00, 1.85MB/s]\n","[INFO|hub.py:587] 2022-03-28 12:15:44,223 >> storing https://huggingface.co/albert-base-v2/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:595] 2022-03-28 12:15:44,223 >> creating metadata file for /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:583] 2022-03-28 12:15:44,585 >> https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpcnf3qda9\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 3.15MB/s]\n","[INFO|hub.py:587] 2022-03-28 12:15:45,444 >> storing https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|hub.py:595] 2022-03-28 12:15:45,444 >> creating metadata file for /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-03-28 12:15:46,498 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1778] 2022-03-28 12:15:46,498 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-03-28 12:15:46,498 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-03-28 12:15:46,498 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-03-28 12:15:46,498 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:653] 2022-03-28 12:15:46,845 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-28 12:15:46,846 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-03-28 12:15:47,266 >> https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpd_w4o_o9\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 48.6MB/s]\n","[INFO|hub.py:587] 2022-03-28 12:15:48,303 >> storing https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|hub.py:595] 2022-03-28 12:15:48,303 >> creating metadata file for /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1771] 2022-03-28 12:15:48,304 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[WARNING|modeling_utils.py:2049] 2022-03-28 12:15:48,517 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.weight']\n","- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-03-28 12:15:48,517 >> Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]03/28/2022 12:15:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-ce78ca212e19d384.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:59<00:00,  2.22ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]03/28/2022 12:16:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-ecb8a4cb36e8b3a5.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:17<00:00,  6.48s/ba]\n","03/28/2022 12:18:05 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpqhmsenvu\n","Downloading builder script: 6.46kB [00:00, 6.20MB/s]       \n","03/28/2022 12:18:06 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/28/2022 12:18:06 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/28/2022 12:18:06 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmprtx1spt9\n","Downloading extra modules: 11.3kB [00:00, 9.55MB/s]       \n","03/28/2022 12:18:06 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","03/28/2022 12:18:06 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1288] 2022-03-28 12:18:17,762 >> ***** Running training *****\n","[INFO|trainer.py:1289] 2022-03-28 12:18:17,762 >>   Num examples = 131958\n","[INFO|trainer.py:1290] 2022-03-28 12:18:17,762 >>   Num Epochs = 2\n","[INFO|trainer.py:1291] 2022-03-28 12:18:17,763 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1292] 2022-03-28 12:18:17,763 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1293] 2022-03-28 12:18:17,763 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1294] 2022-03-28 12:18:17,763 >>   Total optimization steps = 10998\n","{'loss': 1.5502, 'learning_rate': 2.8636115657392255e-05, 'epoch': 0.09}\n","  5% 500/10998 [08:03<2:49:11,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 12:26:20,896 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:440] 2022-03-28 12:26:20,897 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 12:26:20,983 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 12:26:20,984 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 12:26:20,984 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.1489, 'learning_rate': 2.727223131478451e-05, 'epoch': 0.18}\n","  9% 1000/10998 [16:06<2:41:01,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 12:34:24,171 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:440] 2022-03-28 12:34:24,172 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 12:34:24,253 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 12:34:24,254 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 12:34:24,254 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.0462, 'learning_rate': 2.590834697217676e-05, 'epoch': 0.27}\n"," 14% 1500/10998 [24:09<2:33:09,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 12:42:27,501 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:440] 2022-03-28 12:42:27,502 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 12:42:27,582 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 12:42:27,583 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 12:42:27,583 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.02, 'learning_rate': 2.4544462629569013e-05, 'epoch': 0.36}\n"," 18% 2000/10998 [32:13<2:24:54,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 12:50:31,038 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:440] 2022-03-28 12:50:31,039 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 12:50:31,117 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 12:50:31,118 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 12:50:31,118 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 0.9927, 'learning_rate': 2.3180578286961267e-05, 'epoch': 0.45}\n"," 23% 2500/10998 [40:16<2:16:48,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 12:58:34,498 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:440] 2022-03-28 12:58:34,499 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 12:58:34,577 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 12:58:34,578 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 12:58:34,578 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 0.9482, 'learning_rate': 2.181669394435352e-05, 'epoch': 0.55}\n"," 27% 3000/10998 [48:20<2:08:59,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 13:06:38,145 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:440] 2022-03-28 13:06:38,146 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 13:06:38,226 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 13:06:38,227 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 13:06:38,227 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.9352, 'learning_rate': 2.0452809601745775e-05, 'epoch': 0.64}\n"," 32% 3500/10998 [56:24<2:00:43,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 13:14:41,884 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:440] 2022-03-28 13:14:41,885 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 13:14:41,962 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 13:14:41,963 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 13:14:41,963 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.9075, 'learning_rate': 1.9088925259138025e-05, 'epoch': 0.73}\n"," 36% 4000/10998 [1:04:27<1:52:36,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 13:22:45,655 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:440] 2022-03-28 13:22:45,656 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 13:22:45,734 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 13:22:45,735 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 13:22:45,735 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.87, 'learning_rate': 1.7725040916530276e-05, 'epoch': 0.82}\n"," 41% 4500/10998 [1:12:31<1:44:45,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 13:30:49,219 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:440] 2022-03-28 13:30:49,220 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 13:30:49,297 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 13:30:49,298 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 13:30:49,298 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.8516, 'learning_rate': 1.636115657392253e-05, 'epoch': 0.91}\n"," 45% 5000/10998 [1:20:35<1:36:46,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 13:38:52,823 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:440] 2022-03-28 13:38:52,825 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 13:38:52,904 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 13:38:52,905 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 13:38:52,905 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.8375, 'learning_rate': 1.4997272231314786e-05, 'epoch': 1.0}\n"," 50% 5500/10998 [1:28:37<1:15:08,  1.22it/s][INFO|trainer.py:2162] 2022-03-28 13:46:55,698 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:440] 2022-03-28 13:46:55,699 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 13:46:55,778 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 13:46:55,779 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 13:46:55,779 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.6505, 'learning_rate': 1.3633387888707038e-05, 'epoch': 1.09}\n"," 55% 6000/10998 [1:36:41<1:20:23,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 13:54:59,290 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:440] 2022-03-28 13:54:59,291 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 13:54:59,373 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 13:54:59,374 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 13:54:59,374 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.664, 'learning_rate': 1.226950354609929e-05, 'epoch': 1.18}\n"," 59% 6500/10998 [1:44:45<1:12:37,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 14:03:03,037 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:440] 2022-03-28 14:03:03,038 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 14:03:03,118 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 14:03:03,119 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 14:03:03,119 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.6333, 'learning_rate': 1.0905619203491544e-05, 'epoch': 1.27}\n"," 64% 7000/10998 [1:52:48<1:04:29,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 14:11:06,744 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:440] 2022-03-28 14:11:06,745 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 14:11:06,824 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 14:11:06,825 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 14:11:06,825 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.6327, 'learning_rate': 9.541734860883798e-06, 'epoch': 1.36}\n"," 68% 7500/10998 [2:00:52<56:22,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 14:19:10,704 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:440] 2022-03-28 14:19:10,705 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 14:19:10,784 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 14:19:10,785 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 14:19:10,786 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.6237, 'learning_rate': 8.17785051827605e-06, 'epoch': 1.45}\n"," 73% 8000/10998 [2:08:56<48:18,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 14:27:14,626 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:440] 2022-03-28 14:27:14,627 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 14:27:14,702 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 14:27:14,703 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 14:27:14,703 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.6282, 'learning_rate': 6.813966175668303e-06, 'epoch': 1.55}\n"," 77% 8500/10998 [2:17:00<40:16,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 14:35:18,554 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:440] 2022-03-28 14:35:18,555 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 14:35:18,636 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 14:35:18,637 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 14:35:18,637 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.6102, 'learning_rate': 5.450081833060556e-06, 'epoch': 1.64}\n"," 82% 9000/10998 [2:25:04<32:13,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 14:43:22,444 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:440] 2022-03-28 14:43:22,445 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 14:43:22,523 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 14:43:22,524 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 14:43:22,524 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.6172, 'learning_rate': 4.086197490452809e-06, 'epoch': 1.73}\n"," 86% 9500/10998 [2:33:08<24:09,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 14:51:26,347 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:440] 2022-03-28 14:51:26,349 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 14:51:26,427 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 14:51:26,428 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 14:51:26,428 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.596, 'learning_rate': 2.7223131478450624e-06, 'epoch': 1.82}\n"," 91% 10000/10998 [2:41:12<16:06,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 14:59:30,277 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:440] 2022-03-28 14:59:30,278 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 14:59:30,359 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 14:59:30,360 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 14:59:30,360 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.5912, 'learning_rate': 1.358428805237316e-06, 'epoch': 1.91}\n"," 95% 10500/10998 [2:49:16<08:02,  1.03it/s][INFO|trainer.py:2162] 2022-03-28 15:07:34,314 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:440] 2022-03-28 15:07:34,315 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 15:07:34,393 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 15:07:34,394 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 15:07:34,394 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","100% 10998/10998 [2:57:17<00:00,  1.32it/s][INFO|trainer.py:1526] 2022-03-28 15:15:35,749 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10637.9863, 'train_samples_per_second': 24.809, 'train_steps_per_second': 1.034, 'train_loss': 0.8154834054300711, 'epoch': 2.0}\n","100% 10998/10998 [2:57:17<00:00,  1.03it/s]\n","[INFO|trainer.py:2162] 2022-03-28 15:15:35,751 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:440] 2022-03-28 15:15:35,753 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 15:15:35,831 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 15:15:35,832 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 15:15:35,832 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.8155\n","  train_runtime            = 2:57:17.98\n","  train_samples            =     131958\n","  train_samples_per_second =     24.809\n","  train_steps_per_second   =      1.034\n","03/28/2022 15:15:35 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:571] 2022-03-28 15:15:35,842 >> The following columns in the evaluation set  don't have a corresponding argument in `AlbertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `AlbertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2412] 2022-03-28 15:15:35,844 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2414] 2022-03-28 15:15:35,844 >>   Num examples = 12171\n","[INFO|trainer.py:2417] 2022-03-28 15:15:35,844 >>   Batch size = 8\n","100% 1521/1522 [03:19<00:00,  7.62it/s]03/28/2022 15:19:10 - INFO - utils_qa - Post-processing 11873 example predictions split into 12171 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 36/11873 [00:00<00:32, 359.62it/s]\u001b[A\n","  1% 72/11873 [00:00<00:35, 334.89it/s]\u001b[A\n","  1% 109/11873 [00:00<00:33, 348.43it/s]\u001b[A\n","  1% 147/11873 [00:00<00:32, 360.06it/s]\u001b[A\n","  2% 186/11873 [00:00<00:31, 367.05it/s]\u001b[A\n","  2% 226/11873 [00:00<00:30, 376.96it/s]\u001b[A\n","  2% 265/11873 [00:00<00:30, 378.63it/s]\u001b[A\n","  3% 304/11873 [00:00<00:30, 380.06it/s]\u001b[A\n","  3% 343/11873 [00:00<00:30, 381.04it/s]\u001b[A\n","  3% 382/11873 [00:01<00:30, 379.82it/s]\u001b[A\n","  4% 420/11873 [00:01<00:30, 379.64it/s]\u001b[A\n","  4% 460/11873 [00:01<00:29, 383.76it/s]\u001b[A\n","  4% 499/11873 [00:01<00:30, 374.17it/s]\u001b[A\n","  5% 537/11873 [00:01<00:30, 370.52it/s]\u001b[A\n","  5% 575/11873 [00:01<00:30, 365.63it/s]\u001b[A\n","  5% 615/11873 [00:01<00:30, 374.85it/s]\u001b[A\n","  6% 654/11873 [00:01<00:29, 378.18it/s]\u001b[A\n","  6% 694/11873 [00:01<00:29, 382.52it/s]\u001b[A\n","  6% 733/11873 [00:01<00:29, 382.15it/s]\u001b[A\n","  7% 772/11873 [00:02<00:29, 378.28it/s]\u001b[A\n","  7% 812/11873 [00:02<00:28, 382.54it/s]\u001b[A\n","  7% 852/11873 [00:02<00:28, 387.06it/s]\u001b[A\n","  8% 894/11873 [00:02<00:27, 396.45it/s]\u001b[A\n","  8% 934/11873 [00:02<00:27, 396.79it/s]\u001b[A\n","  8% 974/11873 [00:02<00:27, 394.30it/s]\u001b[A\n","  9% 1014/11873 [00:02<00:28, 375.44it/s]\u001b[A\n","  9% 1052/11873 [00:02<00:31, 347.14it/s]\u001b[A\n","  9% 1088/11873 [00:02<00:32, 335.81it/s]\u001b[A\n","  9% 1122/11873 [00:03<00:32, 326.20it/s]\u001b[A\n"," 10% 1155/11873 [00:03<00:33, 316.97it/s]\u001b[A\n"," 10% 1187/11873 [00:03<00:34, 311.27it/s]\u001b[A\n"," 10% 1219/11873 [00:03<00:34, 307.96it/s]\u001b[A\n"," 11% 1250/11873 [00:03<00:34, 306.15it/s]\u001b[A\n"," 11% 1281/11873 [00:03<00:34, 305.01it/s]\u001b[A\n"," 11% 1312/11873 [00:03<00:34, 304.19it/s]\u001b[A\n"," 11% 1343/11873 [00:03<00:34, 303.95it/s]\u001b[A\n"," 12% 1374/11873 [00:03<00:34, 304.32it/s]\u001b[A\n"," 12% 1406/11873 [00:03<00:34, 306.35it/s]\u001b[A\n"," 12% 1437/11873 [00:04<00:34, 305.41it/s]\u001b[A\n"," 12% 1468/11873 [00:04<00:34, 303.90it/s]\u001b[A\n"," 13% 1499/11873 [00:04<00:34, 304.84it/s]\u001b[A\n"," 13% 1530/11873 [00:04<00:34, 303.50it/s]\u001b[A\n"," 13% 1561/11873 [00:04<00:34, 302.62it/s]\u001b[A\n"," 13% 1592/11873 [00:04<00:34, 302.01it/s]\u001b[A\n"," 14% 1623/11873 [00:04<00:33, 301.74it/s]\u001b[A\n"," 14% 1654/11873 [00:04<00:33, 302.76it/s]\u001b[A\n"," 14% 1685/11873 [00:04<00:33, 303.99it/s]\u001b[A\n"," 14% 1716/11873 [00:05<00:33, 303.87it/s]\u001b[A\n"," 15% 1748/11873 [00:05<00:32, 307.11it/s]\u001b[A\n"," 15% 1779/11873 [00:05<00:33, 303.37it/s]\u001b[A\n"," 15% 1811/11873 [00:05<00:32, 305.78it/s]\u001b[A\n"," 16% 1843/11873 [00:05<00:32, 307.82it/s]\u001b[A\n"," 16% 1875/11873 [00:05<00:32, 309.14it/s]\u001b[A\n"," 16% 1907/11873 [00:05<00:32, 310.45it/s]\u001b[A\n"," 16% 1939/11873 [00:05<00:32, 308.28it/s]\u001b[A\n"," 17% 1970/11873 [00:05<00:32, 308.66it/s]\u001b[A\n"," 17% 2001/11873 [00:05<00:32, 305.44it/s]\u001b[A\n"," 17% 2032/11873 [00:06<00:32, 305.93it/s]\u001b[A\n"," 17% 2063/11873 [00:06<00:32, 303.91it/s]\u001b[A\n"," 18% 2094/11873 [00:06<00:32, 305.36it/s]\u001b[A\n"," 18% 2125/11873 [00:06<00:31, 306.11it/s]\u001b[A\n"," 18% 2156/11873 [00:06<00:31, 304.96it/s]\u001b[A\n"," 18% 2187/11873 [00:06<00:31, 304.99it/s]\u001b[A\n"," 19% 2218/11873 [00:06<00:32, 300.62it/s]\u001b[A\n"," 19% 2249/11873 [00:06<00:32, 299.78it/s]\u001b[A\n"," 19% 2279/11873 [00:06<00:32, 296.38it/s]\u001b[A\n"," 19% 2309/11873 [00:06<00:32, 297.17it/s]\u001b[A\n"," 20% 2339/11873 [00:07<00:32, 297.63it/s]\u001b[A\n"," 20% 2370/11873 [00:07<00:31, 300.38it/s]\u001b[A\n"," 20% 2401/11873 [00:07<00:31, 302.71it/s]\u001b[A\n"," 20% 2432/11873 [00:07<00:31, 302.23it/s]\u001b[A\n"," 21% 2463/11873 [00:07<00:31, 300.61it/s]\u001b[A\n"," 21% 2494/11873 [00:07<00:31, 302.17it/s]\u001b[A\n"," 21% 2526/11873 [00:07<00:30, 304.62it/s]\u001b[A\n"," 22% 2557/11873 [00:07<00:30, 303.36it/s]\u001b[A\n"," 22% 2589/11873 [00:07<00:30, 306.67it/s]\u001b[A\n"," 22% 2622/11873 [00:07<00:29, 310.78it/s]\u001b[A\n"," 22% 2654/11873 [00:08<00:29, 310.66it/s]\u001b[A\n"," 23% 2686/11873 [00:08<00:29, 310.65it/s]\u001b[A\n"," 23% 2718/11873 [00:08<00:29, 310.02it/s]\u001b[A\n"," 23% 2750/11873 [00:08<00:29, 307.13it/s]\u001b[A\n"," 23% 2781/11873 [00:08<00:29, 305.80it/s]\u001b[A\n"," 24% 2812/11873 [00:08<00:29, 304.22it/s]\u001b[A\n"," 24% 2843/11873 [00:08<00:29, 302.58it/s]\u001b[A\n"," 24% 2874/11873 [00:08<00:29, 304.23it/s]\u001b[A\n"," 24% 2905/11873 [00:08<00:29, 300.84it/s]\u001b[A\n"," 25% 2936/11873 [00:09<00:29, 302.44it/s]\u001b[A\n"," 25% 2967/11873 [00:09<00:29, 303.65it/s]\u001b[A\n"," 25% 2998/11873 [00:09<00:29, 299.06it/s]\u001b[A\n"," 26% 3030/11873 [00:09<00:29, 302.29it/s]\u001b[A\n"," 26% 3061/11873 [00:09<00:30, 290.72it/s]\u001b[A\n"," 26% 3091/11873 [00:09<00:30, 290.25it/s]\u001b[A\n"," 26% 3121/11873 [00:09<00:36, 240.99it/s]\u001b[A\n"," 27% 3147/11873 [00:09<00:37, 233.18it/s]\u001b[A\n"," 27% 3172/11873 [00:09<00:38, 227.27it/s]\u001b[A\n"," 27% 3203/11873 [00:10<00:34, 248.30it/s]\u001b[A\n"," 27% 3234/11873 [00:10<00:32, 264.08it/s]\u001b[A\n"," 27% 3264/11873 [00:10<00:31, 273.52it/s]\u001b[A\n"," 28% 3292/11873 [00:10<00:43, 199.00it/s]\u001b[A\n"," 28% 3316/11873 [00:10<00:49, 174.34it/s]\u001b[A\n"," 28% 3337/11873 [00:10<00:49, 171.27it/s]\u001b[A\n"," 28% 3356/11873 [00:10<00:51, 164.34it/s]\u001b[A\n"," 28% 3376/11873 [00:11<00:49, 171.70it/s]\u001b[A\n"," 29% 3406/11873 [00:11<00:41, 202.19it/s]\u001b[A\n"," 29% 3436/11873 [00:11<00:37, 226.69it/s]\u001b[A\n"," 29% 3467/11873 [00:11<00:33, 247.54it/s]\u001b[A\n"," 29% 3496/11873 [00:11<00:32, 257.57it/s]\u001b[A\n"," 30% 3526/11873 [00:11<00:31, 268.50it/s]\u001b[A\n"," 30% 3555/11873 [00:11<00:30, 274.34it/s]\u001b[A\n"," 30% 3585/11873 [00:11<00:29, 280.58it/s]\u001b[A\n"," 30% 3614/11873 [00:11<00:29, 280.27it/s]\u001b[A\n"," 31% 3645/11873 [00:11<00:28, 286.48it/s]\u001b[A\n"," 31% 3675/11873 [00:12<00:28, 287.88it/s]\u001b[A\n"," 31% 3705/11873 [00:12<00:28, 289.60it/s]\u001b[A\n"," 31% 3735/11873 [00:12<00:28, 289.67it/s]\u001b[A\n"," 32% 3767/11873 [00:12<00:27, 296.64it/s]\u001b[A\n"," 32% 3797/11873 [00:12<00:27, 293.68it/s]\u001b[A\n"," 32% 3827/11873 [00:12<00:29, 269.33it/s]\u001b[A\n"," 32% 3855/11873 [00:12<00:29, 271.95it/s]\u001b[A\n"," 33% 3887/11873 [00:12<00:28, 283.49it/s]\u001b[A\n"," 33% 3916/11873 [00:12<00:28, 274.46it/s]\u001b[A\n"," 33% 3944/11873 [00:13<00:29, 269.43it/s]\u001b[A\n"," 33% 3974/11873 [00:13<00:28, 277.62it/s]\u001b[A\n"," 34% 4003/11873 [00:13<00:28, 279.21it/s]\u001b[A\n"," 34% 4036/11873 [00:13<00:26, 292.56it/s]\u001b[A\n"," 34% 4068/11873 [00:13<00:26, 297.92it/s]\u001b[A\n"," 35% 4099/11873 [00:13<00:25, 301.05it/s]\u001b[A\n"," 35% 4130/11873 [00:13<00:25, 302.01it/s]\u001b[A\n"," 35% 4161/11873 [00:13<00:28, 272.80it/s]\u001b[A\n"," 35% 4189/11873 [00:13<00:29, 258.38it/s]\u001b[A\n"," 36% 4219/11873 [00:14<00:28, 269.21it/s]\u001b[A\n"," 36% 4248/11873 [00:14<00:27, 274.50it/s]\u001b[A\n"," 36% 4278/11873 [00:14<00:27, 279.81it/s]\u001b[A\n"," 36% 4310/11873 [00:14<00:26, 289.27it/s]\u001b[A\n"," 37% 4340/11873 [00:14<00:26, 289.55it/s]\u001b[A\n"," 37% 4372/11873 [00:14<00:25, 296.31it/s]\u001b[A\n"," 37% 4403/11873 [00:14<00:25, 298.75it/s]\u001b[A\n"," 37% 4433/11873 [00:14<00:31, 233.71it/s]\u001b[A\n"," 38% 4463/11873 [00:14<00:29, 249.12it/s]\u001b[A\n"," 38% 4494/11873 [00:15<00:27, 264.12it/s]\u001b[A\n"," 38% 4525/11873 [00:15<00:26, 273.83it/s]\u001b[A\n"," 38% 4554/11873 [00:15<00:26, 276.66it/s]\u001b[A\n"," 39% 4585/11873 [00:15<00:25, 283.62it/s]\u001b[A\n"," 39% 4615/11873 [00:15<00:25, 286.68it/s]\u001b[A\n"," 39% 4645/11873 [00:15<00:25, 288.73it/s]\u001b[A\n"," 39% 4677/11873 [00:15<00:24, 296.18it/s]\u001b[A\n"," 40% 4708/11873 [00:15<00:24, 298.30it/s]\u001b[A\n"," 40% 4739/11873 [00:15<00:23, 300.27it/s]\u001b[A\n"," 40% 4771/11873 [00:15<00:23, 305.06it/s]\u001b[A\n"," 40% 4802/11873 [00:16<00:23, 298.71it/s]\u001b[A\n"," 41% 4832/11873 [00:16<00:23, 298.81it/s]\u001b[A\n"," 41% 4862/11873 [00:16<00:23, 296.57it/s]\u001b[A\n"," 41% 4892/11873 [00:16<00:23, 295.74it/s]\u001b[A\n"," 41% 4923/11873 [00:16<00:23, 297.24it/s]\u001b[A\n"," 42% 4954/11873 [00:16<00:23, 300.36it/s]\u001b[A\n"," 42% 4986/11873 [00:16<00:22, 304.72it/s]\u001b[A\n"," 42% 5017/11873 [00:16<00:22, 305.30it/s]\u001b[A\n"," 43% 5048/11873 [00:16<00:22, 304.64it/s]\u001b[A\n"," 43% 5080/11873 [00:16<00:22, 307.60it/s]\u001b[A\n"," 43% 5113/11873 [00:17<00:21, 311.65it/s]\u001b[A\n"," 43% 5145/11873 [00:17<00:21, 309.67it/s]\u001b[A\n"," 44% 5177/11873 [00:17<00:21, 311.19it/s]\u001b[A\n"," 44% 5209/11873 [00:17<00:21, 310.32it/s]\u001b[A\n"," 44% 5241/11873 [00:17<00:21, 312.69it/s]\u001b[A\n"," 44% 5273/11873 [00:17<00:23, 282.71it/s]\u001b[A\n"," 45% 5303/11873 [00:17<00:22, 287.10it/s]\u001b[A\n"," 45% 5333/11873 [00:17<00:22, 285.23it/s]\u001b[A\n"," 45% 5362/11873 [00:17<00:22, 283.83it/s]\u001b[A\n"," 45% 5394/11873 [00:18<00:22, 292.99it/s]\u001b[A\n"," 46% 5426/11873 [00:18<00:21, 298.65it/s]\u001b[A\n"," 46% 5457/11873 [00:18<00:21, 296.28it/s]\u001b[A\n"," 46% 5488/11873 [00:18<00:21, 298.78it/s]\u001b[A\n"," 46% 5519/11873 [00:18<00:21, 300.70it/s]\u001b[A\n"," 47% 5550/11873 [00:18<00:21, 297.89it/s]\u001b[A\n"," 47% 5580/11873 [00:18<00:21, 297.95it/s]\u001b[A\n"," 47% 5610/11873 [00:18<00:20, 298.37it/s]\u001b[A\n"," 48% 5640/11873 [00:18<00:21, 293.70it/s]\u001b[A\n"," 48% 5670/11873 [00:18<00:21, 294.52it/s]\u001b[A\n"," 48% 5700/11873 [00:19<00:21, 293.64it/s]\u001b[A\n"," 48% 5730/11873 [00:19<00:20, 293.99it/s]\u001b[A\n"," 49% 5760/11873 [00:19<00:20, 291.80it/s]\u001b[A\n"," 49% 5790/11873 [00:19<00:20, 294.02it/s]\u001b[A\n"," 49% 5822/11873 [00:19<00:20, 299.52it/s]\u001b[A\n"," 49% 5853/11873 [00:19<00:20, 300.74it/s]\u001b[A\n"," 50% 5884/11873 [00:19<00:19, 303.29it/s]\u001b[A\n"," 50% 5915/11873 [00:19<00:19, 299.96it/s]\u001b[A\n"," 50% 5946/11873 [00:19<00:19, 299.64it/s]\u001b[A\n"," 50% 5977/11873 [00:19<00:19, 300.53it/s]\u001b[A\n"," 51% 6008/11873 [00:20<00:19, 298.42it/s]\u001b[A\n"," 51% 6039/11873 [00:20<00:19, 299.17it/s]\u001b[A\n"," 51% 6069/11873 [00:20<00:19, 297.45it/s]\u001b[A\n"," 51% 6099/11873 [00:20<00:19, 297.75it/s]\u001b[A\n"," 52% 6129/11873 [00:20<00:19, 293.41it/s]\u001b[A\n"," 52% 6159/11873 [00:20<00:19, 293.24it/s]\u001b[A\n"," 52% 6189/11873 [00:20<00:19, 294.03it/s]\u001b[A\n"," 52% 6219/11873 [00:20<00:19, 294.10it/s]\u001b[A\n"," 53% 6250/11873 [00:20<00:18, 296.37it/s]\u001b[A\n"," 53% 6281/11873 [00:21<00:18, 300.12it/s]\u001b[A\n"," 53% 6313/11873 [00:21<00:18, 304.29it/s]\u001b[A\n"," 53% 6344/11873 [00:21<00:18, 303.91it/s]\u001b[A\n"," 54% 6376/11873 [00:21<00:17, 307.13it/s]\u001b[A\n"," 54% 6407/11873 [00:21<00:17, 305.10it/s]\u001b[A\n"," 54% 6438/11873 [00:21<00:17, 306.33it/s]\u001b[A\n"," 54% 6469/11873 [00:21<00:17, 303.14it/s]\u001b[A\n"," 55% 6500/11873 [00:21<00:17, 304.36it/s]\u001b[A\n"," 55% 6532/11873 [00:21<00:17, 307.46it/s]\u001b[A\n"," 55% 6563/11873 [00:21<00:17, 307.92it/s]\u001b[A\n"," 56% 6594/11873 [00:22<00:17, 302.45it/s]\u001b[A\n"," 56% 6626/11873 [00:22<00:17, 305.42it/s]\u001b[A\n"," 56% 6657/11873 [00:22<00:17, 303.79it/s]\u001b[A\n"," 56% 6688/11873 [00:22<00:17, 299.37it/s]\u001b[A\n"," 57% 6718/11873 [00:22<00:19, 263.86it/s]\u001b[A\n"," 57% 6750/11873 [00:22<00:18, 277.22it/s]\u001b[A\n"," 57% 6780/11873 [00:22<00:17, 283.37it/s]\u001b[A\n"," 57% 6809/11873 [00:22<00:17, 284.97it/s]\u001b[A\n"," 58% 6839/11873 [00:22<00:17, 288.69it/s]\u001b[A\n"," 58% 6869/11873 [00:23<00:17, 288.88it/s]\u001b[A\n"," 58% 6899/11873 [00:23<00:17, 288.18it/s]\u001b[A\n"," 58% 6929/11873 [00:23<00:17, 290.10it/s]\u001b[A\n"," 59% 6959/11873 [00:23<00:16, 289.89it/s]\u001b[A\n"," 59% 6989/11873 [00:23<00:16, 292.72it/s]\u001b[A\n"," 59% 7020/11873 [00:23<00:16, 295.62it/s]\u001b[A\n"," 59% 7050/11873 [00:23<00:16, 296.40it/s]\u001b[A\n"," 60% 7081/11873 [00:23<00:16, 297.88it/s]\u001b[A\n"," 60% 7112/11873 [00:23<00:15, 298.85it/s]\u001b[A\n"," 60% 7142/11873 [00:23<00:16, 294.95it/s]\u001b[A\n"," 60% 7174/11873 [00:24<00:15, 300.52it/s]\u001b[A\n"," 61% 7206/11873 [00:24<00:15, 303.89it/s]\u001b[A\n"," 61% 7237/11873 [00:24<00:15, 301.35it/s]\u001b[A\n"," 61% 7268/11873 [00:24<00:15, 299.96it/s]\u001b[A\n"," 61% 7299/11873 [00:24<00:15, 301.37it/s]\u001b[A\n"," 62% 7330/11873 [00:24<00:15, 297.31it/s]\u001b[A\n"," 62% 7360/11873 [00:24<00:15, 297.17it/s]\u001b[A\n"," 62% 7390/11873 [00:24<00:15, 295.50it/s]\u001b[A\n"," 62% 7420/11873 [00:24<00:16, 276.68it/s]\u001b[A\n"," 63% 7450/11873 [00:24<00:15, 283.09it/s]\u001b[A\n"," 63% 7481/11873 [00:25<00:15, 288.98it/s]\u001b[A\n"," 63% 7513/11873 [00:25<00:14, 296.56it/s]\u001b[A\n"," 64% 7544/11873 [00:25<00:14, 297.59it/s]\u001b[A\n"," 64% 7574/11873 [00:25<00:14, 296.90it/s]\u001b[A\n"," 64% 7604/11873 [00:25<00:14, 292.91it/s]\u001b[A\n"," 64% 7634/11873 [00:25<00:14, 291.14it/s]\u001b[A\n"," 65% 7665/11873 [00:25<00:14, 294.23it/s]\u001b[A\n"," 65% 7695/11873 [00:25<00:14, 291.81it/s]\u001b[A\n"," 65% 7725/11873 [00:25<00:14, 290.22it/s]\u001b[A\n"," 65% 7755/11873 [00:26<00:14, 291.21it/s]\u001b[A\n"," 66% 7786/11873 [00:26<00:13, 294.01it/s]\u001b[A\n"," 66% 7816/11873 [00:26<00:13, 293.68it/s]\u001b[A\n"," 66% 7846/11873 [00:26<00:13, 289.42it/s]\u001b[A\n"," 66% 7875/11873 [00:26<00:14, 267.41it/s]\u001b[A\n"," 67% 7906/11873 [00:26<00:14, 277.09it/s]\u001b[A\n"," 67% 7935/11873 [00:26<00:14, 280.16it/s]\u001b[A\n"," 67% 7965/11873 [00:26<00:13, 284.65it/s]\u001b[A\n"," 67% 7995/11873 [00:26<00:13, 288.68it/s]\u001b[A\n"," 68% 8025/11873 [00:26<00:13, 291.25it/s]\u001b[A\n"," 68% 8056/11873 [00:27<00:12, 296.69it/s]\u001b[A\n"," 68% 8088/11873 [00:27<00:12, 303.25it/s]\u001b[A\n"," 68% 8119/11873 [00:27<00:12, 300.63it/s]\u001b[A\n"," 69% 8151/11873 [00:27<00:12, 302.68it/s]\u001b[A\n"," 69% 8182/11873 [00:27<00:12, 297.17it/s]\u001b[A\n"," 69% 8212/11873 [00:27<00:12, 295.17it/s]\u001b[A\n"," 69% 8244/11873 [00:27<00:12, 299.88it/s]\u001b[A\n"," 70% 8275/11873 [00:27<00:11, 302.73it/s]\u001b[A\n"," 70% 8307/11873 [00:27<00:11, 306.04it/s]\u001b[A\n"," 70% 8338/11873 [00:27<00:11, 304.89it/s]\u001b[A\n"," 70% 8370/11873 [00:28<00:11, 306.97it/s]\u001b[A\n"," 71% 8401/11873 [00:28<00:11, 307.13it/s]\u001b[A\n"," 71% 8432/11873 [00:28<00:11, 305.15it/s]\u001b[A\n"," 71% 8463/11873 [00:28<00:11, 302.94it/s]\u001b[A\n"," 72% 8494/11873 [00:28<00:11, 304.09it/s]\u001b[A\n"," 72% 8525/11873 [00:28<00:11, 302.94it/s]\u001b[A\n"," 72% 8556/11873 [00:28<00:10, 301.63it/s]\u001b[A\n"," 72% 8587/11873 [00:28<00:10, 303.03it/s]\u001b[A\n"," 73% 8618/11873 [00:28<00:10, 300.92it/s]\u001b[A\n"," 73% 8650/11873 [00:29<00:10, 303.68it/s]\u001b[A\n"," 73% 8682/11873 [00:29<00:10, 306.00it/s]\u001b[A\n"," 73% 8714/11873 [00:29<00:10, 307.59it/s]\u001b[A\n"," 74% 8745/11873 [00:29<00:10, 302.36it/s]\u001b[A\n"," 74% 8776/11873 [00:29<00:10, 303.27it/s]\u001b[A\n"," 74% 8807/11873 [00:29<00:10, 299.94it/s]\u001b[A\n"," 74% 8838/11873 [00:29<00:10, 295.75it/s]\u001b[A\n"," 75% 8868/11873 [00:29<00:10, 295.56it/s]\u001b[A\n"," 75% 8899/11873 [00:29<00:09, 298.31it/s]\u001b[A\n"," 75% 8929/11873 [00:29<00:09, 295.64it/s]\u001b[A\n"," 75% 8960/11873 [00:30<00:09, 299.04it/s]\u001b[A\n"," 76% 8992/11873 [00:30<00:09, 300.89it/s]\u001b[A\n"," 76% 9023/11873 [00:30<00:09, 297.95it/s]\u001b[A\n"," 76% 9053/11873 [00:30<00:09, 296.40it/s]\u001b[A\n"," 77% 9084/11873 [00:30<00:09, 298.87it/s]\u001b[A\n"," 77% 9114/11873 [00:30<00:09, 298.12it/s]\u001b[A\n"," 77% 9145/11873 [00:30<00:09, 300.13it/s]\u001b[A\n"," 77% 9176/11873 [00:30<00:08, 301.99it/s]\u001b[A\n"," 78% 9207/11873 [00:30<00:08, 300.70it/s]\u001b[A\n"," 78% 9238/11873 [00:30<00:08, 300.98it/s]\u001b[A\n"," 78% 9270/11873 [00:31<00:08, 304.72it/s]\u001b[A\n"," 78% 9301/11873 [00:31<00:08, 301.79it/s]\u001b[A\n"," 79% 9332/11873 [00:31<00:08, 303.31it/s]\u001b[A\n"," 79% 9363/11873 [00:31<00:08, 304.03it/s]\u001b[A\n"," 79% 9395/11873 [00:31<00:08, 308.61it/s]\u001b[A\n"," 79% 9426/11873 [00:31<00:07, 308.29it/s]\u001b[A\n"," 80% 9457/11873 [00:31<00:07, 302.53it/s]\u001b[A\n"," 80% 9488/11873 [00:31<00:07, 300.37it/s]\u001b[A\n"," 80% 9519/11873 [00:31<00:07, 301.90it/s]\u001b[A\n"," 80% 9550/11873 [00:32<00:07, 304.14it/s]\u001b[A\n"," 81% 9582/11873 [00:32<00:07, 307.67it/s]\u001b[A\n"," 81% 9613/11873 [00:32<00:07, 304.77it/s]\u001b[A\n"," 81% 9644/11873 [00:32<00:07, 303.01it/s]\u001b[A\n"," 81% 9675/11873 [00:32<00:07, 302.87it/s]\u001b[A\n"," 82% 9707/11873 [00:32<00:07, 307.53it/s]\u001b[A\n"," 82% 9738/11873 [00:32<00:06, 308.01it/s]\u001b[A\n"," 82% 9770/11873 [00:32<00:06, 309.35it/s]\u001b[A\n"," 83% 9801/11873 [00:32<00:06, 303.56it/s]\u001b[A\n"," 83% 9832/11873 [00:32<00:06, 302.78it/s]\u001b[A\n"," 83% 9864/11873 [00:33<00:06, 306.50it/s]\u001b[A\n"," 83% 9895/11873 [00:33<00:06, 303.82it/s]\u001b[A\n"," 84% 9927/11873 [00:33<00:06, 306.14it/s]\u001b[A\n"," 84% 9959/11873 [00:33<00:06, 307.52it/s]\u001b[A\n"," 84% 9990/11873 [00:33<00:06, 301.46it/s]\u001b[A\n"," 84% 10021/11873 [00:33<00:06, 303.74it/s]\u001b[A\n"," 85% 10053/11873 [00:33<00:05, 307.99it/s]\u001b[A\n"," 85% 10086/11873 [00:33<00:05, 311.67it/s]\u001b[A\n"," 85% 10118/11873 [00:33<00:05, 310.27it/s]\u001b[A\n"," 85% 10151/11873 [00:33<00:05, 314.12it/s]\u001b[A\n"," 86% 10183/11873 [00:34<00:05, 311.67it/s]\u001b[A\n"," 86% 10215/11873 [00:34<00:05, 311.59it/s]\u001b[A\n"," 86% 10247/11873 [00:34<00:05, 313.35it/s]\u001b[A\n"," 87% 10279/11873 [00:34<00:05, 306.88it/s]\u001b[A\n"," 87% 10310/11873 [00:34<00:05, 298.86it/s]\u001b[A\n"," 87% 10341/11873 [00:34<00:05, 301.30it/s]\u001b[A\n"," 87% 10372/11873 [00:34<00:05, 298.54it/s]\u001b[A\n"," 88% 10402/11873 [00:34<00:04, 297.98it/s]\u001b[A\n"," 88% 10432/11873 [00:34<00:05, 274.75it/s]\u001b[A\n"," 88% 10463/11873 [00:35<00:04, 283.95it/s]\u001b[A\n"," 88% 10496/11873 [00:35<00:04, 295.00it/s]\u001b[A\n"," 89% 10526/11873 [00:35<00:04, 296.40it/s]\u001b[A\n"," 89% 10556/11873 [00:35<00:04, 297.23it/s]\u001b[A\n"," 89% 10587/11873 [00:35<00:04, 300.06it/s]\u001b[A\n"," 89% 10618/11873 [00:35<00:04, 301.05it/s]\u001b[A\n"," 90% 10649/11873 [00:35<00:04, 294.93it/s]\u001b[A\n"," 90% 10680/11873 [00:35<00:04, 297.91it/s]\u001b[A\n"," 90% 10710/11873 [00:35<00:03, 297.96it/s]\u001b[A\n"," 90% 10741/11873 [00:35<00:03, 299.10it/s]\u001b[A\n"," 91% 10772/11873 [00:36<00:03, 301.90it/s]\u001b[A\n"," 91% 10803/11873 [00:36<00:03, 302.67it/s]\u001b[A\n"," 91% 10834/11873 [00:36<00:03, 284.67it/s]\u001b[A\n"," 92% 10865/11873 [00:36<00:03, 290.02it/s]\u001b[A\n"," 92% 10896/11873 [00:36<00:03, 293.32it/s]\u001b[A\n"," 92% 10926/11873 [00:36<00:03, 290.23it/s]\u001b[A\n"," 92% 10958/11873 [00:36<00:03, 298.02it/s]\u001b[A\n"," 93% 10990/11873 [00:36<00:02, 304.09it/s]\u001b[A\n"," 93% 11022/11873 [00:36<00:02, 306.72it/s]\u001b[A\n"," 93% 11053/11873 [00:36<00:02, 307.35it/s]\u001b[A\n"," 93% 11084/11873 [00:37<00:02, 308.09it/s]\u001b[A\n"," 94% 11116/11873 [00:37<00:02, 310.01it/s]\u001b[A\n"," 94% 11148/11873 [00:37<00:02, 309.32it/s]\u001b[A\n"," 94% 11179/11873 [00:37<00:02, 308.47it/s]\u001b[A\n"," 94% 11210/11873 [00:37<00:02, 308.30it/s]\u001b[A\n"," 95% 11241/11873 [00:37<00:02, 307.78it/s]\u001b[A\n"," 95% 11272/11873 [00:37<00:01, 306.45it/s]\u001b[A\n"," 95% 11303/11873 [00:37<00:01, 303.18it/s]\u001b[A\n"," 95% 11334/11873 [00:37<00:01, 300.47it/s]\u001b[A\n"," 96% 11365/11873 [00:38<00:01, 302.92it/s]\u001b[A\n"," 96% 11396/11873 [00:38<00:01, 303.90it/s]\u001b[A\n"," 96% 11427/11873 [00:38<00:01, 300.10it/s]\u001b[A\n"," 97% 11460/11873 [00:38<00:01, 307.40it/s]\u001b[A\n"," 97% 11491/11873 [00:38<00:01, 300.65it/s]\u001b[A\n"," 97% 11522/11873 [00:38<00:01, 300.18it/s]\u001b[A\n"," 97% 11553/11873 [00:38<00:01, 301.62it/s]\u001b[A\n"," 98% 11584/11873 [00:38<00:00, 303.92it/s]\u001b[A\n"," 98% 11615/11873 [00:38<00:00, 304.92it/s]\u001b[A\n"," 98% 11646/11873 [00:38<00:00, 303.88it/s]\u001b[A\n"," 98% 11677/11873 [00:39<00:00, 300.35it/s]\u001b[A\n"," 99% 11708/11873 [00:39<00:00, 303.07it/s]\u001b[A\n"," 99% 11740/11873 [00:39<00:00, 307.02it/s]\u001b[A\n"," 99% 11772/11873 [00:39<00:00, 309.17it/s]\u001b[A\n"," 99% 11803/11873 [00:39<00:00, 307.69it/s]\u001b[A\n","100% 11835/11873 [00:39<00:00, 311.28it/s]\u001b[A\n","100% 11873/11873 [00:39<00:00, 299.34it/s]\n","03/28/2022 15:19:49 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","03/28/2022 15:19:49 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","03/28/2022 15:19:52 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","03/28/2022 15:19:56 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1522/1522 [04:20<00:00,  5.84it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 74.6457\n","  eval_HasAns_f1         =  81.194\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 82.9941\n","  eval_NoAns_f1          = 82.9941\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 78.8343\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 82.1038\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 78.8259\n","  eval_f1                = 82.0954\n","  eval_samples           =   12171\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-03-28 15:19:56,829 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path albert-base-v2 \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"733dVYZw6YsD","executionInfo":{"status":"ok","timestamp":1648505237525,"user_tz":240,"elapsed":10979003,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"42f3f72b-3af6-44d1-c0ce-ed9623925399"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["03/28/2022 19:04:21 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","03/28/2022 19:04:21 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Mar28_19-04-21_46f2d00cf248,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","03/28/2022 19:04:22 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp8qvl6q6c\n","Downloading builder script: 5.28kB [00:00, 6.88MB/s]       \n","03/28/2022 19:04:22 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/28/2022 19:04:22 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/28/2022 19:04:22 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpml5emtu4\n","Downloading metadata: 2.40kB [00:00, 4.42MB/s]       \n","03/28/2022 19:04:22 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/28/2022 19:04:22 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/28/2022 19:04:22 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","03/28/2022 19:04:22 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/28/2022 19:04:22 - INFO - datasets.builder - Generating dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","03/28/2022 19:04:22 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]03/28/2022 19:04:23 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpkjcd047x\n","\n","Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n","Downloading data: 10.1MB [00:00, 101MB/s]        \u001b[A\n","Downloading data: 20.5MB [00:00, 103MB/s]\u001b[A\n","Downloading data: 31.1MB [00:00, 104MB/s]\u001b[A\n","Downloading data: 42.1MB [00:00, 104MB/s]\n","03/28/2022 19:04:24 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","03/28/2022 19:04:24 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","Downloading data files:  50% 1/2 [00:01<00:01,  1.39s/it]03/28/2022 19:04:24 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpfb79v1fa\n","\n","Downloading data: 4.37MB [00:00, 113MB/s]       \n","03/28/2022 19:04:24 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","03/28/2022 19:04:24 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","Downloading data files: 100% 2/2 [00:01<00:00,  1.20it/s]\n","03/28/2022 19:04:24 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","03/28/2022 19:04:24 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1597.53it/s]\n","03/28/2022 19:04:24 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n","03/28/2022 19:04:24 - INFO - datasets.builder - Generating train split\n","03/28/2022 19:04:35 - INFO - datasets.builder - Generating validation split\n","03/28/2022 19:04:35 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 328.50it/s]\n","[INFO|hub.py:583] 2022-03-28 19:04:36,346 >> https://huggingface.co/albert-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp14a0lxxs\n","Downloading: 100% 684/684 [00:00<00:00, 832kB/s]\n","[INFO|hub.py:587] 2022-03-28 19:04:36,704 >> storing https://huggingface.co/albert-base-v2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|hub.py:595] 2022-03-28 19:04:36,704 >> creating metadata file for /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:653] 2022-03-28 19:04:36,704 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-28 19:04:36,706 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-03-28 19:04:37,067 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:653] 2022-03-28 19:04:37,426 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-28 19:04:37,426 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-03-28 19:04:38,148 >> https://huggingface.co/albert-base-v2/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmprpn__3wf\n","Downloading: 100% 742k/742k [00:00<00:00, 1.78MB/s]\n","[INFO|hub.py:587] 2022-03-28 19:04:38,941 >> storing https://huggingface.co/albert-base-v2/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:595] 2022-03-28 19:04:38,942 >> creating metadata file for /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:583] 2022-03-28 19:04:39,297 >> https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxg5mi8vo\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 2.57MB/s]\n","[INFO|hub.py:587] 2022-03-28 19:04:40,177 >> storing https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|hub.py:595] 2022-03-28 19:04:40,177 >> creating metadata file for /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-03-28 19:04:41,265 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1778] 2022-03-28 19:04:41,265 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-03-28 19:04:41,265 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-03-28 19:04:41,265 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-03-28 19:04:41,265 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:653] 2022-03-28 19:04:41,622 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-28 19:04:41,622 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-03-28 19:04:42,035 >> https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbcaub663\n","Downloading: 100% 45.2M/45.2M [00:01<00:00, 38.8MB/s]\n","[INFO|hub.py:587] 2022-03-28 19:04:43,295 >> storing https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|hub.py:595] 2022-03-28 19:04:43,295 >> creating metadata file for /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1771] 2022-03-28 19:04:43,295 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[WARNING|modeling_utils.py:2049] 2022-03-28 19:04:43,467 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias']\n","- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-03-28 19:04:43,468 >> Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]03/28/2022 19:04:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-ce78ca212e19d384.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:46<00:00,  2.82ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]03/28/2022 19:05:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-ecb8a4cb36e8b3a5.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:01<00:00,  5.14s/ba]\n","03/28/2022 19:06:32 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpsjeesrnz\n","Downloading builder script: 6.46kB [00:00, 7.50MB/s]       \n","03/28/2022 19:06:32 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/28/2022 19:06:32 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/28/2022 19:06:32 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpzowcc7_x\n","Downloading extra modules: 11.3kB [00:00, 11.0MB/s]       \n","03/28/2022 19:06:32 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","03/28/2022 19:06:32 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1288] 2022-03-28 19:06:41,831 >> ***** Running training *****\n","[INFO|trainer.py:1289] 2022-03-28 19:06:41,831 >>   Num examples = 131958\n","[INFO|trainer.py:1290] 2022-03-28 19:06:41,831 >>   Num Epochs = 2\n","[INFO|trainer.py:1291] 2022-03-28 19:06:41,831 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1292] 2022-03-28 19:06:41,831 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1293] 2022-03-28 19:06:41,831 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1294] 2022-03-28 19:06:41,831 >>   Total optimization steps = 10998\n","{'loss': 1.5927, 'learning_rate': 1.909074377159484e-05, 'epoch': 0.09}\n","  5% 500/10998 [08:00<2:48:11,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 19:14:42,727 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:440] 2022-03-28 19:14:42,728 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 19:14:42,803 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 19:14:42,804 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 19:14:42,804 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.1371, 'learning_rate': 1.8181487543189672e-05, 'epoch': 0.18}\n","  9% 1000/10998 [16:01<2:40:10,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 19:22:43,671 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:440] 2022-03-28 19:22:43,671 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 19:22:43,740 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 19:22:43,740 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 19:22:43,740 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.0318, 'learning_rate': 1.7272231314784506e-05, 'epoch': 0.27}\n"," 14% 1500/10998 [24:02<2:31:51,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 19:30:44,790 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:440] 2022-03-28 19:30:44,791 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 19:30:44,862 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 19:30:44,862 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 19:30:44,862 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.0065, 'learning_rate': 1.6362975086379343e-05, 'epoch': 0.36}\n"," 18% 2000/10998 [32:04<2:24:01,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 19:38:45,941 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:440] 2022-03-28 19:38:45,942 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 19:38:46,010 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 19:38:46,011 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 19:38:46,011 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 0.9814, 'learning_rate': 1.5453718857974177e-05, 'epoch': 0.45}\n"," 23% 2500/10998 [40:05<2:16:04,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 19:46:47,266 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:440] 2022-03-28 19:46:47,267 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 19:46:47,335 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 19:46:47,336 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 19:46:47,336 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 0.9334, 'learning_rate': 1.4544462629569014e-05, 'epoch': 0.55}\n"," 27% 3000/10998 [48:06<2:08:07,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 19:54:48,598 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:440] 2022-03-28 19:54:48,599 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 19:54:48,667 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 19:54:48,668 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 19:54:48,668 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.9226, 'learning_rate': 1.363520640116385e-05, 'epoch': 0.64}\n"," 32% 3500/10998 [56:07<2:00:12,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 20:02:49,756 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:440] 2022-03-28 20:02:49,757 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 20:02:49,828 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 20:02:49,829 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 20:02:49,829 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.9058, 'learning_rate': 1.2725950172758685e-05, 'epoch': 0.73}\n"," 36% 4000/10998 [1:04:09<1:52:04,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 20:10:50,973 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:440] 2022-03-28 20:10:50,974 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 20:10:51,043 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 20:10:51,044 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 20:10:51,044 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.8689, 'learning_rate': 1.1816693944353518e-05, 'epoch': 0.82}\n"," 41% 4500/10998 [1:12:10<1:44:12,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 20:18:52,333 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:440] 2022-03-28 20:18:52,334 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 20:18:52,403 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 20:18:52,403 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 20:18:52,403 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.8484, 'learning_rate': 1.0907437715948354e-05, 'epoch': 0.91}\n"," 45% 5000/10998 [1:20:11<1:36:09,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 20:26:53,669 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:440] 2022-03-28 20:26:53,670 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 20:26:53,738 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 20:26:53,739 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 20:26:53,739 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.8356, 'learning_rate': 9.998181487543191e-06, 'epoch': 1.0}\n"," 50% 5500/10998 [1:28:12<1:14:52,  1.22it/s][INFO|trainer.py:2162] 2022-03-28 20:34:54,266 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:440] 2022-03-28 20:34:54,266 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 20:34:54,335 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 20:34:54,336 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 20:34:54,336 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.6695, 'learning_rate': 9.088925259138026e-06, 'epoch': 1.09}\n"," 55% 6000/10998 [1:36:13<1:20:16,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 20:42:55,547 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:440] 2022-03-28 20:42:55,548 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 20:42:55,616 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 20:42:55,617 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 20:42:55,617 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.6739, 'learning_rate': 8.17966903073286e-06, 'epoch': 1.18}\n"," 59% 6500/10998 [1:44:15<1:12:08,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 20:50:56,856 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:440] 2022-03-28 20:50:56,857 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 20:50:56,925 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 20:50:56,926 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 20:50:56,926 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.6583, 'learning_rate': 7.270412802327696e-06, 'epoch': 1.27}\n"," 64% 7000/10998 [1:52:16<1:04:08,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 20:58:58,310 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:440] 2022-03-28 20:58:58,311 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 20:58:58,379 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 20:58:58,379 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 20:58:58,379 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.6525, 'learning_rate': 6.3611565739225325e-06, 'epoch': 1.36}\n"," 68% 7500/10998 [2:00:17<56:08,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 21:06:59,839 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:440] 2022-03-28 21:06:59,841 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 21:06:59,911 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 21:06:59,912 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 21:06:59,912 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.6418, 'learning_rate': 5.451900345517367e-06, 'epoch': 1.45}\n"," 73% 8000/10998 [2:08:19<48:09,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 21:15:01,493 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:440] 2022-03-28 21:15:01,494 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 21:15:01,564 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 21:15:01,564 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 21:15:01,564 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.6572, 'learning_rate': 4.542644117112202e-06, 'epoch': 1.55}\n"," 77% 8500/10998 [2:16:21<40:01,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 21:23:03,102 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:440] 2022-03-28 21:23:03,102 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 21:23:03,171 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 21:23:03,172 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 21:23:03,172 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.6302, 'learning_rate': 3.633387888707038e-06, 'epoch': 1.64}\n"," 82% 9000/10998 [2:24:22<32:00,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 21:31:04,696 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:440] 2022-03-28 21:31:04,697 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 21:31:04,765 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 21:31:04,766 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 21:31:04,766 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.6398, 'learning_rate': 2.724131660301873e-06, 'epoch': 1.73}\n"," 86% 9500/10998 [2:32:24<24:02,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 21:39:06,304 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:440] 2022-03-28 21:39:06,305 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 21:39:06,373 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 21:39:06,374 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 21:39:06,374 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.6195, 'learning_rate': 1.8148754318967086e-06, 'epoch': 1.82}\n"," 91% 10000/10998 [2:40:26<16:00,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 21:47:07,868 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:440] 2022-03-28 21:47:07,868 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 21:47:07,937 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 21:47:07,937 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 21:47:07,937 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6215, 'learning_rate': 9.05619203491544e-07, 'epoch': 1.91}\n"," 95% 10500/10998 [2:48:27<07:59,  1.04it/s][INFO|trainer.py:2162] 2022-03-28 21:55:09,521 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:440] 2022-03-28 21:55:09,521 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 21:55:09,590 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 21:55:09,590 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 21:55:09,591 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","100% 10998/10998 [2:56:26<00:00,  1.33it/s][INFO|trainer.py:1526] 2022-03-28 22:03:08,598 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10586.7666, 'train_samples_per_second': 24.929, 'train_steps_per_second': 1.039, 'train_loss': 0.8243509151433073, 'epoch': 2.0}\n","100% 10998/10998 [2:56:26<00:00,  1.04it/s]\n","[INFO|trainer.py:2162] 2022-03-28 22:03:08,600 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:440] 2022-03-28 22:03:08,601 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-28 22:03:08,668 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-28 22:03:08,669 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-28 22:03:08,669 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.8244\n","  train_runtime            = 2:56:26.76\n","  train_samples            =     131958\n","  train_samples_per_second =     24.929\n","  train_steps_per_second   =      1.039\n","03/28/2022 22:03:08 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:571] 2022-03-28 22:03:08,677 >> The following columns in the evaluation set  don't have a corresponding argument in `AlbertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `AlbertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2412] 2022-03-28 22:03:08,679 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2414] 2022-03-28 22:03:08,679 >>   Num examples = 12171\n","[INFO|trainer.py:2417] 2022-03-28 22:03:08,679 >>   Batch size = 8\n","100% 1521/1522 [03:16<00:00,  7.73it/s]03/28/2022 22:06:37 - INFO - utils_qa - Post-processing 11873 example predictions split into 12171 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 45/11873 [00:00<00:26, 447.10it/s]\u001b[A\n","  1% 90/11873 [00:00<00:28, 419.36it/s]\u001b[A\n","  1% 139/11873 [00:00<00:26, 446.84it/s]\u001b[A\n","  2% 188/11873 [00:00<00:25, 462.17it/s]\u001b[A\n","  2% 238/11873 [00:00<00:24, 472.78it/s]\u001b[A\n","  2% 288/11873 [00:00<00:24, 480.10it/s]\u001b[A\n","  3% 337/11873 [00:00<00:24, 473.58it/s]\u001b[A\n","  3% 388/11873 [00:00<00:23, 482.34it/s]\u001b[A\n","  4% 437/11873 [00:00<00:24, 468.30it/s]\u001b[A\n","  4% 484/11873 [00:01<00:25, 453.18it/s]\u001b[A\n","  4% 530/11873 [00:01<00:25, 441.56it/s]\u001b[A\n","  5% 575/11873 [00:01<00:25, 434.87it/s]\u001b[A\n","  5% 622/11873 [00:01<00:25, 443.94it/s]\u001b[A\n","  6% 671/11873 [00:01<00:24, 455.18it/s]\u001b[A\n","  6% 719/11873 [00:01<00:24, 461.23it/s]\u001b[A\n","  6% 766/11873 [00:01<00:24, 462.49it/s]\u001b[A\n","  7% 817/11873 [00:01<00:23, 475.38it/s]\u001b[A\n","  7% 869/11873 [00:01<00:22, 486.41it/s]\u001b[A\n","  8% 918/11873 [00:01<00:23, 475.07it/s]\u001b[A\n","  8% 966/11873 [00:02<00:23, 470.33it/s]\u001b[A\n","  9% 1014/11873 [00:02<00:23, 459.89it/s]\u001b[A\n","  9% 1061/11873 [00:02<00:25, 428.89it/s]\u001b[A\n","  9% 1105/11873 [00:02<00:26, 410.25it/s]\u001b[A\n"," 10% 1147/11873 [00:02<00:26, 401.26it/s]\u001b[A\n"," 10% 1188/11873 [00:02<00:27, 390.89it/s]\u001b[A\n"," 10% 1228/11873 [00:02<00:27, 386.18it/s]\u001b[A\n"," 11% 1267/11873 [00:02<00:27, 385.20it/s]\u001b[A\n"," 11% 1306/11873 [00:02<00:28, 374.94it/s]\u001b[A\n"," 11% 1344/11873 [00:03<00:28, 370.69it/s]\u001b[A\n"," 12% 1383/11873 [00:03<00:27, 375.37it/s]\u001b[A\n"," 12% 1421/11873 [00:03<00:27, 374.74it/s]\u001b[A\n"," 12% 1460/11873 [00:03<00:27, 377.27it/s]\u001b[A\n"," 13% 1498/11873 [00:03<00:28, 369.57it/s]\u001b[A\n"," 13% 1535/11873 [00:03<00:28, 367.40it/s]\u001b[A\n"," 13% 1573/11873 [00:03<00:27, 370.78it/s]\u001b[A\n"," 14% 1612/11873 [00:03<00:27, 373.67it/s]\u001b[A\n"," 14% 1650/11873 [00:03<00:27, 371.00it/s]\u001b[A\n"," 14% 1688/11873 [00:04<00:27, 368.01it/s]\u001b[A\n"," 15% 1726/11873 [00:04<00:27, 370.14it/s]\u001b[A\n"," 15% 1764/11873 [00:04<00:27, 371.82it/s]\u001b[A\n"," 15% 1802/11873 [00:04<00:27, 372.15it/s]\u001b[A\n"," 15% 1840/11873 [00:04<00:26, 372.92it/s]\u001b[A\n"," 16% 1878/11873 [00:04<00:27, 365.96it/s]\u001b[A\n"," 16% 1915/11873 [00:04<00:27, 361.20it/s]\u001b[A\n"," 16% 1952/11873 [00:04<00:27, 361.58it/s]\u001b[A\n"," 17% 1989/11873 [00:04<00:27, 356.77it/s]\u001b[A\n"," 17% 2026/11873 [00:04<00:27, 359.25it/s]\u001b[A\n"," 17% 2062/11873 [00:05<00:27, 350.74it/s]\u001b[A\n"," 18% 2098/11873 [00:05<00:27, 352.54it/s]\u001b[A\n"," 18% 2134/11873 [00:05<00:27, 353.15it/s]\u001b[A\n"," 18% 2170/11873 [00:05<00:27, 350.63it/s]\u001b[A\n"," 19% 2207/11873 [00:05<00:27, 355.58it/s]\u001b[A\n"," 19% 2246/11873 [00:05<00:26, 364.64it/s]\u001b[A\n"," 19% 2283/11873 [00:05<00:26, 365.89it/s]\u001b[A\n"," 20% 2321/11873 [00:05<00:25, 369.59it/s]\u001b[A\n"," 20% 2360/11873 [00:05<00:25, 374.19it/s]\u001b[A\n"," 20% 2398/11873 [00:05<00:25, 366.19it/s]\u001b[A\n"," 21% 2435/11873 [00:06<00:26, 358.20it/s]\u001b[A\n"," 21% 2472/11873 [00:06<00:26, 360.81it/s]\u001b[A\n"," 21% 2510/11873 [00:06<00:25, 361.31it/s]\u001b[A\n"," 21% 2548/11873 [00:06<00:25, 366.22it/s]\u001b[A\n"," 22% 2585/11873 [00:06<00:25, 359.90it/s]\u001b[A\n"," 22% 2622/11873 [00:06<00:25, 362.12it/s]\u001b[A\n"," 22% 2660/11873 [00:06<00:25, 365.35it/s]\u001b[A\n"," 23% 2698/11873 [00:06<00:24, 368.86it/s]\u001b[A\n"," 23% 2735/11873 [00:06<00:25, 361.38it/s]\u001b[A\n"," 23% 2772/11873 [00:07<00:25, 363.44it/s]\u001b[A\n"," 24% 2809/11873 [00:07<00:24, 364.87it/s]\u001b[A\n"," 24% 2847/11873 [00:07<00:24, 368.82it/s]\u001b[A\n"," 24% 2885/11873 [00:07<00:24, 369.72it/s]\u001b[A\n"," 25% 2923/11873 [00:07<00:24, 370.49it/s]\u001b[A\n"," 25% 2961/11873 [00:07<00:24, 370.00it/s]\u001b[A\n"," 25% 2999/11873 [00:07<00:24, 367.11it/s]\u001b[A\n"," 26% 3036/11873 [00:07<00:24, 361.50it/s]\u001b[A\n"," 26% 3073/11873 [00:07<00:24, 355.69it/s]\u001b[A\n"," 26% 3109/11873 [00:07<00:26, 333.19it/s]\u001b[A\n"," 26% 3143/11873 [00:08<00:30, 288.32it/s]\u001b[A\n"," 27% 3173/11873 [00:08<00:31, 279.80it/s]\u001b[A\n"," 27% 3210/11873 [00:08<00:28, 301.93it/s]\u001b[A\n"," 27% 3248/11873 [00:08<00:26, 322.20it/s]\u001b[A\n"," 28% 3282/11873 [00:08<00:31, 276.95it/s]\u001b[A\n"," 28% 3312/11873 [00:08<00:37, 228.46it/s]\u001b[A\n"," 28% 3338/11873 [00:08<00:39, 215.05it/s]\u001b[A\n"," 28% 3362/11873 [00:09<00:41, 203.47it/s]\u001b[A\n"," 29% 3392/11873 [00:09<00:37, 224.93it/s]\u001b[A\n"," 29% 3430/11873 [00:09<00:32, 262.01it/s]\u001b[A\n"," 29% 3468/11873 [00:09<00:28, 291.59it/s]\u001b[A\n"," 30% 3504/11873 [00:09<00:27, 309.67it/s]\u001b[A\n"," 30% 3540/11873 [00:09<00:25, 323.53it/s]\u001b[A\n"," 30% 3578/11873 [00:09<00:24, 336.99it/s]\u001b[A\n"," 30% 3616/11873 [00:09<00:23, 347.63it/s]\u001b[A\n"," 31% 3654/11873 [00:09<00:23, 355.44it/s]\u001b[A\n"," 31% 3690/11873 [00:09<00:23, 354.11it/s]\u001b[A\n"," 31% 3727/11873 [00:10<00:22, 356.22it/s]\u001b[A\n"," 32% 3764/11873 [00:10<00:22, 359.52it/s]\u001b[A\n"," 32% 3801/11873 [00:10<00:22, 359.68it/s]\u001b[A\n"," 32% 3838/11873 [00:10<00:23, 336.34it/s]\u001b[A\n"," 33% 3874/11873 [00:10<00:23, 342.24it/s]\u001b[A\n"," 33% 3911/11873 [00:10<00:22, 347.01it/s]\u001b[A\n"," 33% 3946/11873 [00:10<00:24, 329.18it/s]\u001b[A\n"," 34% 3983/11873 [00:10<00:23, 338.47it/s]\u001b[A\n"," 34% 4022/11873 [00:10<00:22, 351.83it/s]\u001b[A\n"," 34% 4061/11873 [00:11<00:21, 360.61it/s]\u001b[A\n"," 35% 4098/11873 [00:11<00:21, 362.91it/s]\u001b[A\n"," 35% 4135/11873 [00:11<00:21, 364.03it/s]\u001b[A\n"," 35% 4172/11873 [00:11<00:24, 317.28it/s]\u001b[A\n"," 35% 4208/11873 [00:11<00:23, 328.18it/s]\u001b[A\n"," 36% 4246/11873 [00:11<00:22, 342.39it/s]\u001b[A\n"," 36% 4284/11873 [00:11<00:21, 351.36it/s]\u001b[A\n"," 36% 4323/11873 [00:11<00:20, 360.33it/s]\u001b[A\n"," 37% 4361/11873 [00:11<00:20, 364.91it/s]\u001b[A\n"," 37% 4398/11873 [00:12<00:20, 366.26it/s]\u001b[A\n"," 37% 4435/11873 [00:12<00:24, 298.34it/s]\u001b[A\n"," 38% 4474/11873 [00:12<00:23, 320.25it/s]\u001b[A\n"," 38% 4511/11873 [00:12<00:22, 332.32it/s]\u001b[A\n"," 38% 4549/11873 [00:12<00:21, 343.62it/s]\u001b[A\n"," 39% 4587/11873 [00:12<00:20, 352.15it/s]\u001b[A\n"," 39% 4625/11873 [00:12<00:20, 358.17it/s]\u001b[A\n"," 39% 4663/11873 [00:12<00:19, 362.48it/s]\u001b[A\n"," 40% 4701/11873 [00:12<00:19, 365.91it/s]\u001b[A\n"," 40% 4738/11873 [00:13<00:19, 366.49it/s]\u001b[A\n"," 40% 4776/11873 [00:13<00:19, 367.53it/s]\u001b[A\n"," 41% 4813/11873 [00:13<00:19, 362.53it/s]\u001b[A\n"," 41% 4850/11873 [00:13<00:19, 363.17it/s]\u001b[A\n"," 41% 4887/11873 [00:13<00:19, 364.09it/s]\u001b[A\n"," 41% 4925/11873 [00:13<00:18, 366.89it/s]\u001b[A\n"," 42% 4963/11873 [00:13<00:18, 369.26it/s]\u001b[A\n"," 42% 5000/11873 [00:13<00:18, 367.85it/s]\u001b[A\n"," 42% 5037/11873 [00:13<00:18, 367.42it/s]\u001b[A\n"," 43% 5074/11873 [00:13<00:18, 367.51it/s]\u001b[A\n"," 43% 5113/11873 [00:14<00:18, 373.04it/s]\u001b[A\n"," 43% 5151/11873 [00:14<00:18, 366.07it/s]\u001b[A\n"," 44% 5188/11873 [00:14<00:18, 357.42it/s]\u001b[A\n"," 44% 5225/11873 [00:14<00:18, 360.30it/s]\u001b[A\n"," 44% 5262/11873 [00:14<00:19, 336.10it/s]\u001b[A\n"," 45% 5300/11873 [00:14<00:18, 346.72it/s]\u001b[A\n"," 45% 5338/11873 [00:14<00:18, 355.91it/s]\u001b[A\n"," 45% 5377/11873 [00:14<00:17, 363.10it/s]\u001b[A\n"," 46% 5415/11873 [00:14<00:17, 367.70it/s]\u001b[A\n"," 46% 5452/11873 [00:14<00:17, 367.87it/s]\u001b[A\n"," 46% 5489/11873 [00:15<00:17, 366.13it/s]\u001b[A\n"," 47% 5527/11873 [00:15<00:17, 367.90it/s]\u001b[A\n"," 47% 5565/11873 [00:15<00:17, 369.34it/s]\u001b[A\n"," 47% 5602/11873 [00:15<00:17, 362.64it/s]\u001b[A\n"," 47% 5639/11873 [00:15<00:17, 358.68it/s]\u001b[A\n"," 48% 5676/11873 [00:15<00:17, 360.46it/s]\u001b[A\n"," 48% 5713/11873 [00:15<00:17, 357.67it/s]\u001b[A\n"," 48% 5750/11873 [00:15<00:17, 359.80it/s]\u001b[A\n"," 49% 5787/11873 [00:15<00:16, 362.15it/s]\u001b[A\n"," 49% 5825/11873 [00:15<00:16, 366.92it/s]\u001b[A\n"," 49% 5862/11873 [00:16<00:16, 367.24it/s]\u001b[A\n"," 50% 5899/11873 [00:16<00:17, 346.53it/s]\u001b[A\n"," 50% 5937/11873 [00:16<00:16, 353.77it/s]\u001b[A\n"," 50% 5976/11873 [00:16<00:16, 362.29it/s]\u001b[A\n"," 51% 6014/11873 [00:16<00:15, 366.98it/s]\u001b[A\n"," 51% 6052/11873 [00:16<00:15, 370.73it/s]\u001b[A\n"," 51% 6090/11873 [00:16<00:15, 372.78it/s]\u001b[A\n"," 52% 6128/11873 [00:16<00:15, 371.72it/s]\u001b[A\n"," 52% 6166/11873 [00:16<00:15, 370.50it/s]\u001b[A\n"," 52% 6205/11873 [00:17<00:15, 375.05it/s]\u001b[A\n"," 53% 6243/11873 [00:17<00:15, 372.59it/s]\u001b[A\n"," 53% 6282/11873 [00:17<00:14, 375.69it/s]\u001b[A\n"," 53% 6321/11873 [00:17<00:14, 379.59it/s]\u001b[A\n"," 54% 6360/11873 [00:17<00:14, 381.32it/s]\u001b[A\n"," 54% 6399/11873 [00:17<00:14, 379.82it/s]\u001b[A\n"," 54% 6437/11873 [00:17<00:14, 363.59it/s]\u001b[A\n"," 55% 6474/11873 [00:17<00:15, 354.78it/s]\u001b[A\n"," 55% 6510/11873 [00:17<00:15, 353.41it/s]\u001b[A\n"," 55% 6549/11873 [00:17<00:14, 361.95it/s]\u001b[A\n"," 55% 6586/11873 [00:18<00:14, 364.06it/s]\u001b[A\n"," 56% 6624/11873 [00:18<00:14, 367.83it/s]\u001b[A\n"," 56% 6661/11873 [00:18<00:14, 366.52it/s]\u001b[A\n"," 56% 6699/11873 [00:18<00:14, 367.89it/s]\u001b[A\n"," 57% 6736/11873 [00:18<00:15, 326.40it/s]\u001b[A\n"," 57% 6773/11873 [00:18<00:15, 338.19it/s]\u001b[A\n"," 57% 6811/11873 [00:18<00:14, 347.33it/s]\u001b[A\n"," 58% 6849/11873 [00:18<00:14, 355.72it/s]\u001b[A\n"," 58% 6887/11873 [00:18<00:13, 362.19it/s]\u001b[A\n"," 58% 6924/11873 [00:19<00:13, 354.72it/s]\u001b[A\n"," 59% 6960/11873 [00:19<00:14, 344.80it/s]\u001b[A\n"," 59% 6996/11873 [00:19<00:14, 347.57it/s]\u001b[A\n"," 59% 7032/11873 [00:19<00:13, 348.51it/s]\u001b[A\n"," 60% 7068/11873 [00:19<00:13, 349.42it/s]\u001b[A\n"," 60% 7107/11873 [00:19<00:13, 359.56it/s]\u001b[A\n"," 60% 7144/11873 [00:19<00:13, 359.94it/s]\u001b[A\n"," 60% 7182/11873 [00:19<00:12, 363.54it/s]\u001b[A\n"," 61% 7220/11873 [00:19<00:12, 367.11it/s]\u001b[A\n"," 61% 7258/11873 [00:19<00:12, 370.25it/s]\u001b[A\n"," 61% 7296/11873 [00:20<00:12, 371.58it/s]\u001b[A\n"," 62% 7334/11873 [00:20<00:12, 367.54it/s]\u001b[A\n"," 62% 7372/11873 [00:20<00:12, 369.36it/s]\u001b[A\n"," 62% 7409/11873 [00:20<00:12, 364.91it/s]\u001b[A\n"," 63% 7446/11873 [00:20<00:12, 348.66it/s]\u001b[A\n"," 63% 7484/11873 [00:20<00:12, 356.40it/s]\u001b[A\n"," 63% 7520/11873 [00:20<00:12, 357.35it/s]\u001b[A\n"," 64% 7556/11873 [00:20<00:12, 345.63it/s]\u001b[A\n"," 64% 7591/11873 [00:20<00:12, 336.81it/s]\u001b[A\n"," 64% 7625/11873 [00:21<00:13, 325.57it/s]\u001b[A\n"," 64% 7658/11873 [00:21<00:13, 323.98it/s]\u001b[A\n"," 65% 7695/11873 [00:21<00:12, 336.80it/s]\u001b[A\n"," 65% 7732/11873 [00:21<00:11, 346.25it/s]\u001b[A\n"," 65% 7769/11873 [00:21<00:11, 351.67it/s]\u001b[A\n"," 66% 7805/11873 [00:21<00:11, 353.99it/s]\u001b[A\n"," 66% 7841/11873 [00:21<00:11, 354.91it/s]\u001b[A\n"," 66% 7877/11873 [00:21<00:11, 337.39it/s]\u001b[A\n"," 67% 7915/11873 [00:21<00:11, 349.16it/s]\u001b[A\n"," 67% 7953/11873 [00:21<00:11, 355.89it/s]\u001b[A\n"," 67% 7990/11873 [00:22<00:10, 358.29it/s]\u001b[A\n"," 68% 8028/11873 [00:22<00:10, 362.71it/s]\u001b[A\n"," 68% 8065/11873 [00:22<00:10, 364.77it/s]\u001b[A\n"," 68% 8104/11873 [00:22<00:10, 369.79it/s]\u001b[A\n"," 69% 8142/11873 [00:22<00:10, 370.35it/s]\u001b[A\n"," 69% 8180/11873 [00:22<00:10, 368.37it/s]\u001b[A\n"," 69% 8217/11873 [00:22<00:10, 364.01it/s]\u001b[A\n"," 70% 8256/11873 [00:22<00:09, 370.11it/s]\u001b[A\n"," 70% 8295/11873 [00:22<00:09, 373.91it/s]\u001b[A\n"," 70% 8334/11873 [00:22<00:09, 377.40it/s]\u001b[A\n"," 71% 8372/11873 [00:23<00:09, 376.22it/s]\u001b[A\n"," 71% 8410/11873 [00:23<00:09, 370.07it/s]\u001b[A\n"," 71% 8448/11873 [00:23<00:09, 368.05it/s]\u001b[A\n"," 71% 8485/11873 [00:23<00:09, 363.64it/s]\u001b[A\n"," 72% 8523/11873 [00:23<00:09, 366.83it/s]\u001b[A\n"," 72% 8560/11873 [00:23<00:09, 367.35it/s]\u001b[A\n"," 72% 8597/11873 [00:23<00:09, 353.93it/s]\u001b[A\n"," 73% 8633/11873 [00:23<00:09, 342.88it/s]\u001b[A\n"," 73% 8669/11873 [00:23<00:09, 345.83it/s]\u001b[A\n"," 73% 8704/11873 [00:24<00:09, 344.11it/s]\u001b[A\n"," 74% 8741/11873 [00:24<00:08, 350.41it/s]\u001b[A\n"," 74% 8779/11873 [00:24<00:08, 357.33it/s]\u001b[A\n"," 74% 8816/11873 [00:24<00:08, 358.68it/s]\u001b[A\n"," 75% 8854/11873 [00:24<00:08, 364.37it/s]\u001b[A\n"," 75% 8892/11873 [00:24<00:08, 367.66it/s]\u001b[A\n"," 75% 8929/11873 [00:24<00:08, 367.30it/s]\u001b[A\n"," 76% 8966/11873 [00:24<00:08, 360.49it/s]\u001b[A\n"," 76% 9003/11873 [00:24<00:08, 357.34it/s]\u001b[A\n"," 76% 9039/11873 [00:24<00:08, 352.53it/s]\u001b[A\n"," 76% 9075/11873 [00:25<00:07, 354.69it/s]\u001b[A\n"," 77% 9111/11873 [00:25<00:07, 354.33it/s]\u001b[A\n"," 77% 9149/11873 [00:25<00:07, 361.12it/s]\u001b[A\n"," 77% 9188/11873 [00:25<00:07, 368.34it/s]\u001b[A\n"," 78% 9226/11873 [00:25<00:07, 371.48it/s]\u001b[A\n"," 78% 9264/11873 [00:25<00:07, 371.24it/s]\u001b[A\n"," 78% 9302/11873 [00:25<00:06, 371.17it/s]\u001b[A\n"," 79% 9341/11873 [00:25<00:06, 374.85it/s]\u001b[A\n"," 79% 9379/11873 [00:25<00:06, 375.26it/s]\u001b[A\n"," 79% 9417/11873 [00:25<00:06, 375.80it/s]\u001b[A\n"," 80% 9455/11873 [00:26<00:06, 375.36it/s]\u001b[A\n"," 80% 9493/11873 [00:26<00:06, 374.85it/s]\u001b[A\n"," 80% 9532/11873 [00:26<00:06, 377.21it/s]\u001b[A\n"," 81% 9572/11873 [00:26<00:06, 381.40it/s]\u001b[A\n"," 81% 9611/11873 [00:26<00:05, 378.68it/s]\u001b[A\n"," 81% 9650/11873 [00:26<00:05, 380.55it/s]\u001b[A\n"," 82% 9689/11873 [00:26<00:05, 375.42it/s]\u001b[A\n"," 82% 9727/11873 [00:26<00:05, 361.10it/s]\u001b[A\n"," 82% 9764/11873 [00:26<00:05, 356.53it/s]\u001b[A\n"," 83% 9800/11873 [00:27<00:05, 353.67it/s]\u001b[A\n"," 83% 9836/11873 [00:27<00:05, 350.11it/s]\u001b[A\n"," 83% 9872/11873 [00:27<00:05, 348.37it/s]\u001b[A\n"," 83% 9908/11873 [00:27<00:05, 350.25it/s]\u001b[A\n"," 84% 9944/11873 [00:27<00:05, 353.05it/s]\u001b[A\n"," 84% 9980/11873 [00:27<00:05, 353.73it/s]\u001b[A\n"," 84% 10017/11873 [00:27<00:05, 355.62it/s]\u001b[A\n"," 85% 10055/11873 [00:27<00:05, 361.78it/s]\u001b[A\n"," 85% 10094/11873 [00:27<00:04, 369.73it/s]\u001b[A\n"," 85% 10132/11873 [00:27<00:04, 372.57it/s]\u001b[A\n"," 86% 10170/11873 [00:28<00:04, 374.22it/s]\u001b[A\n"," 86% 10208/11873 [00:28<00:04, 372.84it/s]\u001b[A\n"," 86% 10247/11873 [00:28<00:04, 375.69it/s]\u001b[A\n"," 87% 10285/11873 [00:28<00:04, 374.19it/s]\u001b[A\n"," 87% 10323/11873 [00:28<00:04, 372.83it/s]\u001b[A\n"," 87% 10361/11873 [00:28<00:04, 374.80it/s]\u001b[A\n"," 88% 10400/11873 [00:28<00:03, 376.46it/s]\u001b[A\n"," 88% 10438/11873 [00:28<00:04, 349.37it/s]\u001b[A\n"," 88% 10477/11873 [00:28<00:03, 358.31it/s]\u001b[A\n"," 89% 10515/11873 [00:28<00:03, 362.46it/s]\u001b[A\n"," 89% 10553/11873 [00:29<00:03, 364.95it/s]\u001b[A\n"," 89% 10591/11873 [00:29<00:03, 366.96it/s]\u001b[A\n"," 90% 10629/11873 [00:29<00:03, 370.04it/s]\u001b[A\n"," 90% 10667/11873 [00:29<00:03, 369.13it/s]\u001b[A\n"," 90% 10705/11873 [00:29<00:03, 370.71it/s]\u001b[A\n"," 90% 10744/11873 [00:29<00:03, 374.39it/s]\u001b[A\n"," 91% 10782/11873 [00:29<00:02, 373.43it/s]\u001b[A\n"," 91% 10820/11873 [00:29<00:02, 365.83it/s]\u001b[A\n"," 91% 10857/11873 [00:29<00:02, 352.29it/s]\u001b[A\n"," 92% 10895/11873 [00:30<00:02, 357.77it/s]\u001b[A\n"," 92% 10931/11873 [00:30<00:02, 353.24it/s]\u001b[A\n"," 92% 10970/11873 [00:30<00:02, 362.54it/s]\u001b[A\n"," 93% 11008/11873 [00:30<00:02, 365.12it/s]\u001b[A\n"," 93% 11046/11873 [00:30<00:02, 367.93it/s]\u001b[A\n"," 93% 11085/11873 [00:30<00:02, 372.86it/s]\u001b[A\n"," 94% 11124/11873 [00:30<00:01, 375.37it/s]\u001b[A\n"," 94% 11162/11873 [00:30<00:01, 376.12it/s]\u001b[A\n"," 94% 11200/11873 [00:30<00:01, 365.58it/s]\u001b[A\n"," 95% 11237/11873 [00:30<00:01, 359.44it/s]\u001b[A\n"," 95% 11274/11873 [00:31<00:01, 352.12it/s]\u001b[A\n"," 95% 11311/11873 [00:31<00:01, 355.51it/s]\u001b[A\n"," 96% 11347/11873 [00:31<00:01, 356.79it/s]\u001b[A\n"," 96% 11385/11873 [00:31<00:01, 360.82it/s]\u001b[A\n"," 96% 11423/11873 [00:31<00:01, 363.52it/s]\u001b[A\n"," 97% 11462/11873 [00:31<00:01, 368.65it/s]\u001b[A\n"," 97% 11500/11873 [00:31<00:01, 370.31it/s]\u001b[A\n"," 97% 11538/11873 [00:31<00:00, 371.97it/s]\u001b[A\n"," 97% 11576/11873 [00:31<00:00, 373.34it/s]\u001b[A\n"," 98% 11614/11873 [00:31<00:00, 374.67it/s]\u001b[A\n"," 98% 11653/11873 [00:32<00:00, 376.34it/s]\u001b[A\n"," 98% 11691/11873 [00:32<00:00, 369.57it/s]\u001b[A\n"," 99% 11730/11873 [00:32<00:00, 372.89it/s]\u001b[A\n"," 99% 11769/11873 [00:32<00:00, 375.84it/s]\u001b[A\n"," 99% 11807/11873 [00:32<00:00, 376.74it/s]\u001b[A\n","100% 11873/11873 [00:32<00:00, 363.53it/s]\n","03/28/2022 22:07:10 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","03/28/2022 22:07:10 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","03/28/2022 22:07:12 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","03/28/2022 22:07:15 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1522/1522 [04:06<00:00,  6.17it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 75.6073\n","  eval_HasAns_f1         = 81.9183\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 82.4222\n","  eval_NoAns_f1          = 82.4222\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        =  79.028\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           =  82.179\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 79.0196\n","  eval_f1                = 82.1706\n","  eval_samples           =   12171\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-03-28 22:07:15,708 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path albert-base-v2 \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 1e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjv8xa9YazUB","executionInfo":{"status":"ok","timestamp":1648530613508,"user_tz":240,"elapsed":11089707,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"7cafe50a-3a65-433a-bafd-3920857250f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["03/29/2022 02:05:26 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","03/29/2022 02:05:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=1e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Mar29_02-05-26_856171be3f3f,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","03/29/2022 02:05:27 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpz19uzlsr\n","Downloading builder script: 5.28kB [00:00, 4.43MB/s]       \n","03/29/2022 02:05:27 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/29/2022 02:05:27 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/29/2022 02:05:27 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmphonyqxoo\n","Downloading metadata: 2.40kB [00:00, 2.73MB/s]       \n","03/29/2022 02:05:27 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/29/2022 02:05:27 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/29/2022 02:05:27 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","03/29/2022 02:05:27 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/29/2022 02:05:27 - INFO - datasets.builder - Generating dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","03/29/2022 02:05:27 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]03/29/2022 02:05:28 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpoof389er\n","\n","Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n","Downloading data:  75% 7.12M/9.55M [00:00<00:00, 71.1MB/s]\u001b[A\n","Downloading data: 14.4MB [00:00, 72.4MB/s]                \u001b[A\n","Downloading data: 21.7MB [00:00, 71.5MB/s]\u001b[A\n","Downloading data: 29.1MB [00:00, 72.4MB/s]\u001b[A\n","Downloading data: 42.1MB [00:00, 72.5MB/s]\n","03/29/2022 02:05:28 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","03/29/2022 02:05:28 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","Downloading data files:  50% 1/2 [00:01<00:01,  1.55s/it]03/29/2022 02:05:29 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpwtsxbg6h\n","\n","Downloading data: 4.37MB [00:00, 76.2MB/s]      \n","03/29/2022 02:05:29 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","03/29/2022 02:05:29 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","Downloading data files: 100% 2/2 [00:01<00:00,  1.08it/s]\n","03/29/2022 02:05:29 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","03/29/2022 02:05:29 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1203.70it/s]\n","03/29/2022 02:05:29 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n","03/29/2022 02:05:29 - INFO - datasets.builder - Generating train split\n","03/29/2022 02:05:42 - INFO - datasets.builder - Generating validation split\n","03/29/2022 02:05:44 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 258.62it/s]\n","[INFO|hub.py:583] 2022-03-29 02:05:44,485 >> https://huggingface.co/albert-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwt1n4cwu\n","Downloading: 100% 684/684 [00:00<00:00, 588kB/s]\n","[INFO|hub.py:587] 2022-03-29 02:05:44,834 >> storing https://huggingface.co/albert-base-v2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|hub.py:595] 2022-03-29 02:05:44,834 >> creating metadata file for /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:653] 2022-03-29 02:05:44,834 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-29 02:05:44,836 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-03-29 02:05:45,181 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:653] 2022-03-29 02:05:45,527 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-29 02:05:45,528 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-03-29 02:05:46,223 >> https://huggingface.co/albert-base-v2/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf7yz7k1e\n","Downloading: 100% 742k/742k [00:00<00:00, 1.84MB/s]\n","[INFO|hub.py:587] 2022-03-29 02:05:46,990 >> storing https://huggingface.co/albert-base-v2/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:595] 2022-03-29 02:05:46,990 >> creating metadata file for /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:583] 2022-03-29 02:05:47,335 >> https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8jfoaskn\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 2.65MB/s]\n","[INFO|hub.py:587] 2022-03-29 02:05:48,186 >> storing https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|hub.py:595] 2022-03-29 02:05:48,187 >> creating metadata file for /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 02:05:49,231 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 02:05:49,231 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 02:05:49,231 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 02:05:49,231 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 02:05:49,231 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:653] 2022-03-29 02:05:49,579 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-29 02:05:49,580 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-03-29 02:05:50,007 >> https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi1o7o5vz\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 47.5MB/s]\n","[INFO|hub.py:587] 2022-03-29 02:05:51,053 >> storing https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|hub.py:595] 2022-03-29 02:05:51,053 >> creating metadata file for /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1771] 2022-03-29 02:05:51,053 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[WARNING|modeling_utils.py:2049] 2022-03-29 02:05:51,266 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.bias']\n","- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-03-29 02:05:51,267 >> Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]03/29/2022 02:05:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-ce78ca212e19d384.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:56<00:00,  2.30ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]03/29/2022 02:06:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-ecb8a4cb36e8b3a5.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:13<00:00,  6.15s/ba]\n","03/29/2022 02:08:02 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpjs4rap3b\n","Downloading builder script: 6.46kB [00:00, 7.57MB/s]       \n","03/29/2022 02:08:02 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/29/2022 02:08:02 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/29/2022 02:08:03 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpb3xwcg_7\n","Downloading extra modules: 11.3kB [00:00, 10.5MB/s]       \n","03/29/2022 02:08:03 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","03/29/2022 02:08:03 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1288] 2022-03-29 02:08:14,445 >> ***** Running training *****\n","[INFO|trainer.py:1289] 2022-03-29 02:08:14,445 >>   Num examples = 131958\n","[INFO|trainer.py:1290] 2022-03-29 02:08:14,445 >>   Num Epochs = 2\n","[INFO|trainer.py:1291] 2022-03-29 02:08:14,445 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1292] 2022-03-29 02:08:14,445 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1293] 2022-03-29 02:08:14,445 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1294] 2022-03-29 02:08:14,445 >>   Total optimization steps = 10998\n","{'loss': 1.7541, 'learning_rate': 9.54537188579742e-06, 'epoch': 0.09}\n","  5% 500/10998 [08:04<2:49:43,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 02:16:18,492 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:440] 2022-03-29 02:16:18,493 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 02:16:18,575 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 02:16:18,575 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 02:16:18,575 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.1817, 'learning_rate': 9.090743771594836e-06, 'epoch': 0.18}\n","  9% 1000/10998 [16:08<2:41:17,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 02:24:22,660 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:440] 2022-03-29 02:24:22,661 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 02:24:22,737 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 02:24:22,738 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 02:24:22,738 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.0581, 'learning_rate': 8.636115657392253e-06, 'epoch': 0.27}\n"," 14% 1500/10998 [24:12<2:33:05,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 02:32:26,776 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:440] 2022-03-29 02:32:26,777 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 02:32:26,851 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 02:32:26,852 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 02:32:26,852 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.0278, 'learning_rate': 8.181487543189672e-06, 'epoch': 0.36}\n"," 18% 2000/10998 [32:16<2:25:07,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 02:40:31,066 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:440] 2022-03-29 02:40:31,067 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 02:40:31,145 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 02:40:31,146 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 02:40:31,146 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.0021, 'learning_rate': 7.726859428987088e-06, 'epoch': 0.45}\n"," 23% 2500/10998 [40:21<2:17:04,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 02:48:35,609 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:440] 2022-03-29 02:48:35,610 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 02:48:35,687 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 02:48:35,688 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 02:48:35,688 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 0.954, 'learning_rate': 7.272231314784507e-06, 'epoch': 0.55}\n"," 27% 3000/10998 [48:25<2:09:09,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 02:56:40,099 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:440] 2022-03-29 02:56:40,100 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 02:56:40,177 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 02:56:40,178 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 02:56:40,178 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.9416, 'learning_rate': 6.817603200581925e-06, 'epoch': 0.64}\n"," 32% 3500/10998 [56:30<2:01:00,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 03:04:44,878 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:440] 2022-03-29 03:04:44,879 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 03:04:44,958 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 03:04:44,959 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 03:04:44,959 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.9167, 'learning_rate': 6.362975086379342e-06, 'epoch': 0.73}\n"," 36% 4000/10998 [1:04:34<1:53:18,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 03:12:49,333 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:440] 2022-03-29 03:12:49,334 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 03:12:49,412 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 03:12:49,413 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 03:12:49,413 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.892, 'learning_rate': 5.908346972176759e-06, 'epoch': 0.82}\n"," 41% 4500/10998 [1:12:39<1:44:50,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 03:20:53,785 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:440] 2022-03-29 03:20:53,786 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 03:20:53,865 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 03:20:53,865 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 03:20:53,866 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.8749, 'learning_rate': 5.453718857974177e-06, 'epoch': 0.91}\n"," 45% 5000/10998 [1:20:43<1:36:48,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 03:28:58,162 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:440] 2022-03-29 03:28:58,163 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 03:28:58,239 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 03:28:58,240 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 03:28:58,240 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.8595, 'learning_rate': 4.9990907437715955e-06, 'epoch': 1.0}\n"," 50% 5500/10998 [1:28:47<1:15:18,  1.22it/s][INFO|trainer.py:2162] 2022-03-29 03:37:01,924 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:440] 2022-03-29 03:37:01,925 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 03:37:02,005 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 03:37:02,006 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 03:37:02,006 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.7351, 'learning_rate': 4.544462629569013e-06, 'epoch': 1.09}\n"," 55% 6000/10998 [1:36:51<1:20:36,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 03:45:06,453 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:440] 2022-03-29 03:45:06,454 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 03:45:06,531 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 03:45:06,532 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 03:45:06,532 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.7386, 'learning_rate': 4.08983451536643e-06, 'epoch': 1.18}\n"," 59% 6500/10998 [1:44:56<1:12:35,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 03:53:11,068 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:440] 2022-03-29 03:53:11,069 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 03:53:11,145 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 03:53:11,146 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 03:53:11,146 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.72, 'learning_rate': 3.635206401163848e-06, 'epoch': 1.27}\n"," 64% 7000/10998 [1:53:01<1:04:30,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 04:01:15,605 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:440] 2022-03-29 04:01:15,606 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 04:01:15,686 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 04:01:15,687 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 04:01:15,687 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.7163, 'learning_rate': 3.1805782869612662e-06, 'epoch': 1.36}\n"," 68% 7500/10998 [2:01:05<56:26,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 04:09:19,912 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:440] 2022-03-29 04:09:19,913 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 04:09:19,990 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 04:09:19,991 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 04:09:19,991 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.7007, 'learning_rate': 2.7259501727586835e-06, 'epoch': 1.45}\n"," 73% 8000/10998 [2:09:09<48:16,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 04:17:24,390 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:440] 2022-03-29 04:17:24,391 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 04:17:24,467 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 04:17:24,468 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 04:17:24,468 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.7292, 'learning_rate': 2.271322058556101e-06, 'epoch': 1.55}\n"," 77% 8500/10998 [2:17:14<40:13,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 04:25:28,639 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:440] 2022-03-29 04:25:28,640 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 04:25:28,716 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 04:25:28,717 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 04:25:28,717 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.7032, 'learning_rate': 1.816693944353519e-06, 'epoch': 1.64}\n"," 82% 9000/10998 [2:25:18<32:13,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 04:33:32,933 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:440] 2022-03-29 04:33:32,935 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 04:33:33,012 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 04:33:33,012 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 04:33:33,012 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.712, 'learning_rate': 1.3620658301509366e-06, 'epoch': 1.73}\n"," 86% 9500/10998 [2:33:22<24:09,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 04:41:37,122 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:440] 2022-03-29 04:41:37,123 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 04:41:37,201 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 04:41:37,202 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 04:41:37,203 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.6978, 'learning_rate': 9.074377159483543e-07, 'epoch': 1.82}\n"," 91% 10000/10998 [2:41:27<16:05,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 04:49:41,594 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:440] 2022-03-29 04:49:41,595 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 04:49:41,672 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 04:49:41,673 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 04:49:41,673 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6973, 'learning_rate': 4.52809601745772e-07, 'epoch': 1.91}\n"," 95% 10500/10998 [2:49:31<08:02,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 04:57:46,077 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:440] 2022-03-29 04:57:46,078 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 04:57:46,152 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 04:57:46,153 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 04:57:46,153 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","100% 10998/10998 [2:57:33<00:00,  1.32it/s][INFO|trainer.py:1526] 2022-03-29 05:05:47,812 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10653.3673, 'train_samples_per_second': 24.773, 'train_steps_per_second': 1.032, 'train_loss': 0.8770301847983369, 'epoch': 2.0}\n","100% 10998/10998 [2:57:33<00:00,  1.03it/s]\n","[INFO|trainer.py:2162] 2022-03-29 05:05:47,814 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:440] 2022-03-29 05:05:47,815 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 05:05:47,891 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 05:05:47,892 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 05:05:47,892 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =      0.877\n","  train_runtime            = 2:57:33.36\n","  train_samples            =     131958\n","  train_samples_per_second =     24.773\n","  train_steps_per_second   =      1.032\n","03/29/2022 05:05:47 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:571] 2022-03-29 05:05:47,902 >> The following columns in the evaluation set  don't have a corresponding argument in `AlbertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `AlbertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2412] 2022-03-29 05:05:47,904 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2414] 2022-03-29 05:05:47,904 >>   Num examples = 12171\n","[INFO|trainer.py:2417] 2022-03-29 05:05:47,904 >>   Batch size = 8\n","100% 1521/1522 [03:19<00:00,  7.61it/s]03/29/2022 05:09:22 - INFO - utils_qa - Post-processing 11873 example predictions split into 12171 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 35/11873 [00:00<00:34, 342.93it/s]\u001b[A\n","  1% 70/11873 [00:00<00:36, 324.35it/s]\u001b[A\n","  1% 109/11873 [00:00<00:33, 349.21it/s]\u001b[A\n","  1% 149/11873 [00:00<00:31, 367.45it/s]\u001b[A\n","  2% 186/11873 [00:00<00:32, 364.03it/s]\u001b[A\n","  2% 226/11873 [00:00<00:31, 374.44it/s]\u001b[A\n","  2% 264/11873 [00:00<00:30, 374.99it/s]\u001b[A\n","  3% 303/11873 [00:00<00:30, 379.62it/s]\u001b[A\n","  3% 342/11873 [00:00<00:30, 381.45it/s]\u001b[A\n","  3% 382/11873 [00:01<00:29, 385.02it/s]\u001b[A\n","  4% 421/11873 [00:01<00:29, 384.86it/s]\u001b[A\n","  4% 462/11873 [00:01<00:29, 389.62it/s]\u001b[A\n","  4% 501/11873 [00:01<00:29, 387.98it/s]\u001b[A\n","  5% 540/11873 [00:01<00:29, 386.33it/s]\u001b[A\n","  5% 579/11873 [00:01<00:29, 383.58it/s]\u001b[A\n","  5% 621/11873 [00:01<00:28, 391.84it/s]\u001b[A\n","  6% 661/11873 [00:01<00:28, 394.24it/s]\u001b[A\n","  6% 701/11873 [00:01<00:28, 394.51it/s]\u001b[A\n","  6% 741/11873 [00:01<00:28, 393.16it/s]\u001b[A\n","  7% 781/11873 [00:02<00:28, 387.60it/s]\u001b[A\n","  7% 820/11873 [00:02<00:28, 386.27it/s]\u001b[A\n","  7% 861/11873 [00:02<00:28, 391.95it/s]\u001b[A\n","  8% 902/11873 [00:02<00:27, 396.93it/s]\u001b[A\n","  8% 942/11873 [00:02<00:27, 397.76it/s]\u001b[A\n","  8% 982/11873 [00:02<00:27, 393.59it/s]\u001b[A\n","  9% 1022/11873 [00:02<00:28, 374.35it/s]\u001b[A\n","  9% 1060/11873 [00:02<00:30, 355.50it/s]\u001b[A\n","  9% 1096/11873 [00:02<00:31, 339.89it/s]\u001b[A\n"," 10% 1131/11873 [00:03<00:33, 324.56it/s]\u001b[A\n"," 10% 1164/11873 [00:03<00:33, 319.29it/s]\u001b[A\n"," 10% 1197/11873 [00:03<00:34, 311.49it/s]\u001b[A\n"," 10% 1229/11873 [00:03<00:34, 308.33it/s]\u001b[A\n"," 11% 1260/11873 [00:03<00:34, 305.97it/s]\u001b[A\n"," 11% 1291/11873 [00:03<00:35, 298.79it/s]\u001b[A\n"," 11% 1322/11873 [00:03<00:35, 299.54it/s]\u001b[A\n"," 11% 1352/11873 [00:03<00:35, 296.20it/s]\u001b[A\n"," 12% 1383/11873 [00:03<00:35, 299.27it/s]\u001b[A\n"," 12% 1415/11873 [00:03<00:34, 302.90it/s]\u001b[A\n"," 12% 1446/11873 [00:04<00:34, 304.19it/s]\u001b[A\n"," 12% 1477/11873 [00:04<00:34, 304.55it/s]\u001b[A\n"," 13% 1510/11873 [00:04<00:33, 309.26it/s]\u001b[A\n"," 13% 1541/11873 [00:04<00:33, 309.21it/s]\u001b[A\n"," 13% 1574/11873 [00:04<00:32, 313.57it/s]\u001b[A\n"," 14% 1606/11873 [00:04<00:32, 313.14it/s]\u001b[A\n"," 14% 1638/11873 [00:04<00:32, 311.42it/s]\u001b[A\n"," 14% 1670/11873 [00:04<00:32, 313.10it/s]\u001b[A\n"," 14% 1702/11873 [00:04<00:33, 306.14it/s]\u001b[A\n"," 15% 1734/11873 [00:05<00:32, 308.40it/s]\u001b[A\n"," 15% 1765/11873 [00:05<00:32, 308.16it/s]\u001b[A\n"," 15% 1796/11873 [00:05<00:32, 307.75it/s]\u001b[A\n"," 15% 1827/11873 [00:05<00:32, 304.47it/s]\u001b[A\n"," 16% 1858/11873 [00:05<00:33, 300.05it/s]\u001b[A\n"," 16% 1889/11873 [00:05<00:33, 295.30it/s]\u001b[A\n"," 16% 1919/11873 [00:05<00:33, 292.88it/s]\u001b[A\n"," 16% 1949/11873 [00:05<00:34, 290.97it/s]\u001b[A\n"," 17% 1979/11873 [00:05<00:33, 291.55it/s]\u001b[A\n"," 17% 2009/11873 [00:05<00:33, 293.03it/s]\u001b[A\n"," 17% 2039/11873 [00:06<00:34, 287.45it/s]\u001b[A\n"," 17% 2068/11873 [00:06<00:34, 281.99it/s]\u001b[A\n"," 18% 2098/11873 [00:06<00:34, 287.11it/s]\u001b[A\n"," 18% 2128/11873 [00:06<00:33, 290.11it/s]\u001b[A\n"," 18% 2160/11873 [00:06<00:32, 298.88it/s]\u001b[A\n"," 18% 2190/11873 [00:06<00:32, 297.39it/s]\u001b[A\n"," 19% 2222/11873 [00:06<00:31, 301.93it/s]\u001b[A\n"," 19% 2254/11873 [00:06<00:31, 305.58it/s]\u001b[A\n"," 19% 2285/11873 [00:06<00:31, 305.09it/s]\u001b[A\n"," 20% 2316/11873 [00:06<00:31, 304.05it/s]\u001b[A\n"," 20% 2347/11873 [00:07<00:31, 302.61it/s]\u001b[A\n"," 20% 2378/11873 [00:07<00:31, 297.75it/s]\u001b[A\n"," 20% 2409/11873 [00:07<00:31, 300.60it/s]\u001b[A\n"," 21% 2440/11873 [00:07<00:31, 302.82it/s]\u001b[A\n"," 21% 2471/11873 [00:07<00:31, 301.76it/s]\u001b[A\n"," 21% 2502/11873 [00:07<00:31, 296.19it/s]\u001b[A\n"," 21% 2532/11873 [00:07<00:32, 288.86it/s]\u001b[A\n"," 22% 2562/11873 [00:07<00:31, 290.98it/s]\u001b[A\n"," 22% 2592/11873 [00:07<00:31, 292.65it/s]\u001b[A\n"," 22% 2623/11873 [00:08<00:31, 296.92it/s]\u001b[A\n"," 22% 2653/11873 [00:08<00:31, 297.11it/s]\u001b[A\n"," 23% 2684/11873 [00:08<00:30, 299.78it/s]\u001b[A\n"," 23% 2714/11873 [00:08<00:30, 296.08it/s]\u001b[A\n"," 23% 2745/11873 [00:08<00:30, 298.76it/s]\u001b[A\n"," 23% 2775/11873 [00:08<00:30, 296.37it/s]\u001b[A\n"," 24% 2806/11873 [00:08<00:30, 299.89it/s]\u001b[A\n"," 24% 2838/11873 [00:08<00:29, 304.52it/s]\u001b[A\n"," 24% 2870/11873 [00:08<00:29, 307.50it/s]\u001b[A\n"," 24% 2901/11873 [00:08<00:29, 299.53it/s]\u001b[A\n"," 25% 2931/11873 [00:09<00:30, 296.64it/s]\u001b[A\n"," 25% 2961/11873 [00:09<00:30, 292.30it/s]\u001b[A\n"," 25% 2991/11873 [00:09<00:30, 291.26it/s]\u001b[A\n"," 25% 3021/11873 [00:09<00:30, 288.17it/s]\u001b[A\n"," 26% 3050/11873 [00:09<00:31, 281.26it/s]\u001b[A\n"," 26% 3079/11873 [00:09<00:31, 280.38it/s]\u001b[A\n"," 26% 3108/11873 [00:09<00:32, 267.06it/s]\u001b[A\n"," 26% 3135/11873 [00:09<00:37, 231.66it/s]\u001b[A\n"," 27% 3159/11873 [00:09<00:41, 209.97it/s]\u001b[A\n"," 27% 3188/11873 [00:10<00:37, 228.85it/s]\u001b[A\n"," 27% 3217/11873 [00:10<00:35, 244.77it/s]\u001b[A\n"," 27% 3248/11873 [00:10<00:33, 260.80it/s]\u001b[A\n"," 28% 3275/11873 [00:10<00:35, 241.70it/s]\u001b[A\n"," 28% 3300/11873 [00:10<00:46, 182.62it/s]\u001b[A\n"," 28% 3321/11873 [00:10<00:52, 163.38it/s]\u001b[A\n"," 28% 3341/11873 [00:10<00:50, 170.09it/s]\u001b[A\n"," 28% 3360/11873 [00:11<00:54, 157.52it/s]\u001b[A\n"," 28% 3382/11873 [00:11<00:49, 171.39it/s]\u001b[A\n"," 29% 3411/11873 [00:11<00:42, 200.31it/s]\u001b[A\n"," 29% 3438/11873 [00:11<00:38, 217.78it/s]\u001b[A\n"," 29% 3465/11873 [00:11<00:36, 230.36it/s]\u001b[A\n"," 29% 3492/11873 [00:11<00:34, 240.83it/s]\u001b[A\n"," 30% 3519/11873 [00:11<00:33, 248.69it/s]\u001b[A\n"," 30% 3549/11873 [00:11<00:31, 263.02it/s]\u001b[A\n"," 30% 3578/11873 [00:11<00:30, 270.38it/s]\u001b[A\n"," 30% 3609/11873 [00:11<00:29, 280.71it/s]\u001b[A\n"," 31% 3639/11873 [00:12<00:28, 285.88it/s]\u001b[A\n"," 31% 3670/11873 [00:12<00:28, 290.15it/s]\u001b[A\n"," 31% 3700/11873 [00:12<00:28, 287.91it/s]\u001b[A\n"," 31% 3729/11873 [00:12<00:28, 287.42it/s]\u001b[A\n"," 32% 3759/11873 [00:12<00:27, 290.78it/s]\u001b[A\n"," 32% 3789/11873 [00:12<00:27, 289.18it/s]\u001b[A\n"," 32% 3818/11873 [00:12<00:28, 279.71it/s]\u001b[A\n"," 32% 3847/11873 [00:12<00:29, 272.72it/s]\u001b[A\n"," 33% 3878/11873 [00:12<00:28, 281.03it/s]\u001b[A\n"," 33% 3910/11873 [00:13<00:27, 286.61it/s]\u001b[A\n"," 33% 3939/11873 [00:13<00:29, 264.58it/s]\u001b[A\n"," 33% 3969/11873 [00:13<00:28, 273.03it/s]\u001b[A\n"," 34% 4000/11873 [00:13<00:27, 282.05it/s]\u001b[A\n"," 34% 4031/11873 [00:13<00:27, 289.96it/s]\u001b[A\n"," 34% 4063/11873 [00:13<00:26, 296.60it/s]\u001b[A\n"," 34% 4094/11873 [00:13<00:25, 299.89it/s]\u001b[A\n"," 35% 4125/11873 [00:13<00:25, 299.69it/s]\u001b[A\n"," 35% 4156/11873 [00:13<00:28, 268.87it/s]\u001b[A\n"," 35% 4184/11873 [00:14<00:29, 256.37it/s]\u001b[A\n"," 35% 4213/11873 [00:14<00:28, 264.99it/s]\u001b[A\n"," 36% 4245/11873 [00:14<00:27, 279.18it/s]\u001b[A\n"," 36% 4277/11873 [00:14<00:26, 288.71it/s]\u001b[A\n"," 36% 4308/11873 [00:14<00:25, 293.53it/s]\u001b[A\n"," 37% 4340/11873 [00:14<00:25, 299.13it/s]\u001b[A\n"," 37% 4372/11873 [00:14<00:24, 304.18it/s]\u001b[A\n"," 37% 4403/11873 [00:14<00:24, 301.64it/s]\u001b[A\n"," 37% 4434/11873 [00:14<00:31, 238.77it/s]\u001b[A\n"," 38% 4463/11873 [00:15<00:29, 251.03it/s]\u001b[A\n"," 38% 4493/11873 [00:15<00:28, 262.32it/s]\u001b[A\n"," 38% 4521/11873 [00:15<00:28, 259.73it/s]\u001b[A\n"," 38% 4549/11873 [00:15<00:27, 265.00it/s]\u001b[A\n"," 39% 4579/11873 [00:15<00:26, 273.22it/s]\u001b[A\n"," 39% 4609/11873 [00:15<00:26, 278.63it/s]\u001b[A\n"," 39% 4638/11873 [00:15<00:25, 280.50it/s]\u001b[A\n"," 39% 4667/11873 [00:15<00:25, 283.05it/s]\u001b[A\n"," 40% 4697/11873 [00:15<00:24, 287.68it/s]\u001b[A\n"," 40% 4728/11873 [00:15<00:24, 294.12it/s]\u001b[A\n"," 40% 4759/11873 [00:16<00:23, 297.99it/s]\u001b[A\n"," 40% 4789/11873 [00:16<00:23, 296.18it/s]\u001b[A\n"," 41% 4819/11873 [00:16<00:23, 297.23it/s]\u001b[A\n"," 41% 4850/11873 [00:16<00:23, 300.43it/s]\u001b[A\n"," 41% 4881/11873 [00:16<00:23, 301.48it/s]\u001b[A\n"," 41% 4913/11873 [00:16<00:22, 304.23it/s]\u001b[A\n"," 42% 4944/11873 [00:16<00:22, 305.74it/s]\u001b[A\n"," 42% 4975/11873 [00:16<00:22, 306.15it/s]\u001b[A\n"," 42% 5006/11873 [00:16<00:22, 304.06it/s]\u001b[A\n"," 42% 5037/11873 [00:16<00:22, 302.81it/s]\u001b[A\n"," 43% 5069/11873 [00:17<00:22, 304.91it/s]\u001b[A\n"," 43% 5100/11873 [00:17<00:22, 300.95it/s]\u001b[A\n"," 43% 5131/11873 [00:17<00:22, 301.76it/s]\u001b[A\n"," 43% 5162/11873 [00:17<00:22, 302.00it/s]\u001b[A\n"," 44% 5193/11873 [00:17<00:22, 302.06it/s]\u001b[A\n"," 44% 5224/11873 [00:17<00:22, 301.04it/s]\u001b[A\n"," 44% 5255/11873 [00:17<00:23, 285.23it/s]\u001b[A\n"," 45% 5284/11873 [00:17<00:24, 272.42it/s]\u001b[A\n"," 45% 5312/11873 [00:17<00:23, 274.34it/s]\u001b[A\n"," 45% 5341/11873 [00:18<00:23, 277.91it/s]\u001b[A\n"," 45% 5371/11873 [00:18<00:22, 283.21it/s]\u001b[A\n"," 45% 5401/11873 [00:18<00:22, 288.06it/s]\u001b[A\n"," 46% 5432/11873 [00:18<00:22, 292.46it/s]\u001b[A\n"," 46% 5462/11873 [00:18<00:22, 289.65it/s]\u001b[A\n"," 46% 5492/11873 [00:18<00:21, 292.26it/s]\u001b[A\n"," 47% 5522/11873 [00:18<00:21, 294.43it/s]\u001b[A\n"," 47% 5553/11873 [00:18<00:21, 297.44it/s]\u001b[A\n"," 47% 5583/11873 [00:18<00:21, 297.27it/s]\u001b[A\n"," 47% 5613/11873 [00:18<00:21, 296.86it/s]\u001b[A\n"," 48% 5643/11873 [00:19<00:21, 293.46it/s]\u001b[A\n"," 48% 5673/11873 [00:19<00:21, 288.02it/s]\u001b[A\n"," 48% 5702/11873 [00:19<00:21, 284.92it/s]\u001b[A\n"," 48% 5733/11873 [00:19<00:21, 289.46it/s]\u001b[A\n"," 49% 5763/11873 [00:19<00:20, 292.08it/s]\u001b[A\n"," 49% 5794/11873 [00:19<00:20, 296.74it/s]\u001b[A\n"," 49% 5825/11873 [00:19<00:20, 298.97it/s]\u001b[A\n"," 49% 5856/11873 [00:19<00:20, 300.50it/s]\u001b[A\n"," 50% 5887/11873 [00:19<00:20, 294.61it/s]\u001b[A\n"," 50% 5917/11873 [00:20<00:20, 295.43it/s]\u001b[A\n"," 50% 5947/11873 [00:20<00:20, 294.35it/s]\u001b[A\n"," 50% 5977/11873 [00:20<00:20, 292.19it/s]\u001b[A\n"," 51% 6007/11873 [00:20<00:20, 291.03it/s]\u001b[A\n"," 51% 6038/11873 [00:20<00:19, 295.67it/s]\u001b[A\n"," 51% 6068/11873 [00:20<00:19, 296.16it/s]\u001b[A\n"," 51% 6098/11873 [00:20<00:19, 293.59it/s]\u001b[A\n"," 52% 6128/11873 [00:20<00:19, 292.60it/s]\u001b[A\n"," 52% 6158/11873 [00:20<00:19, 293.20it/s]\u001b[A\n"," 52% 6189/11873 [00:20<00:19, 296.98it/s]\u001b[A\n"," 52% 6219/11873 [00:21<00:19, 293.59it/s]\u001b[A\n"," 53% 6250/11873 [00:21<00:18, 296.79it/s]\u001b[A\n"," 53% 6281/11873 [00:21<00:18, 298.17it/s]\u001b[A\n"," 53% 6311/11873 [00:21<00:18, 298.49it/s]\u001b[A\n"," 53% 6342/11873 [00:21<00:18, 301.24it/s]\u001b[A\n"," 54% 6373/11873 [00:21<00:18, 298.04it/s]\u001b[A\n"," 54% 6404/11873 [00:21<00:18, 299.88it/s]\u001b[A\n"," 54% 6434/11873 [00:21<00:18, 297.81it/s]\u001b[A\n"," 54% 6464/11873 [00:21<00:18, 296.44it/s]\u001b[A\n"," 55% 6494/11873 [00:21<00:18, 293.24it/s]\u001b[A\n"," 55% 6524/11873 [00:22<00:18, 283.68it/s]\u001b[A\n"," 55% 6553/11873 [00:22<00:18, 281.44it/s]\u001b[A\n"," 55% 6582/11873 [00:22<00:18, 278.99it/s]\u001b[A\n"," 56% 6610/11873 [00:22<00:19, 276.65it/s]\u001b[A\n"," 56% 6639/11873 [00:22<00:18, 278.46it/s]\u001b[A\n"," 56% 6667/11873 [00:22<00:18, 278.40it/s]\u001b[A\n"," 56% 6695/11873 [00:22<00:18, 277.27it/s]\u001b[A\n"," 57% 6723/11873 [00:22<00:21, 238.72it/s]\u001b[A\n"," 57% 6751/11873 [00:22<00:20, 249.27it/s]\u001b[A\n"," 57% 6780/11873 [00:23<00:19, 259.34it/s]\u001b[A\n"," 57% 6809/11873 [00:23<00:19, 265.69it/s]\u001b[A\n"," 58% 6839/11873 [00:23<00:18, 275.04it/s]\u001b[A\n"," 58% 6868/11873 [00:23<00:17, 278.58it/s]\u001b[A\n"," 58% 6898/11873 [00:23<00:17, 283.60it/s]\u001b[A\n"," 58% 6930/11873 [00:23<00:16, 291.73it/s]\u001b[A\n"," 59% 6960/11873 [00:23<00:16, 292.23it/s]\u001b[A\n"," 59% 6991/11873 [00:23<00:16, 294.71it/s]\u001b[A\n"," 59% 7021/11873 [00:23<00:16, 295.75it/s]\u001b[A\n"," 59% 7051/11873 [00:23<00:16, 295.42it/s]\u001b[A\n"," 60% 7082/11873 [00:24<00:16, 297.13it/s]\u001b[A\n"," 60% 7112/11873 [00:24<00:16, 295.05it/s]\u001b[A\n"," 60% 7142/11873 [00:24<00:16, 295.19it/s]\u001b[A\n"," 60% 7172/11873 [00:24<00:15, 295.62it/s]\u001b[A\n"," 61% 7203/11873 [00:24<00:15, 299.66it/s]\u001b[A\n"," 61% 7234/11873 [00:24<00:15, 301.23it/s]\u001b[A\n"," 61% 7265/11873 [00:24<00:15, 300.43it/s]\u001b[A\n"," 61% 7296/11873 [00:24<00:15, 302.00it/s]\u001b[A\n"," 62% 7327/11873 [00:24<00:15, 301.56it/s]\u001b[A\n"," 62% 7359/11873 [00:24<00:14, 305.87it/s]\u001b[A\n"," 62% 7390/11873 [00:25<00:14, 300.38it/s]\u001b[A\n"," 63% 7421/11873 [00:25<00:15, 279.29it/s]\u001b[A\n"," 63% 7450/11873 [00:25<00:15, 282.19it/s]\u001b[A\n"," 63% 7479/11873 [00:25<00:15, 283.71it/s]\u001b[A\n"," 63% 7508/11873 [00:25<00:15, 283.39it/s]\u001b[A\n"," 63% 7537/11873 [00:25<00:15, 284.66it/s]\u001b[A\n"," 64% 7566/11873 [00:25<00:15, 284.71it/s]\u001b[A\n"," 64% 7595/11873 [00:25<00:15, 281.47it/s]\u001b[A\n"," 64% 7625/11873 [00:25<00:14, 285.06it/s]\u001b[A\n"," 64% 7654/11873 [00:26<00:14, 285.66it/s]\u001b[A\n"," 65% 7685/11873 [00:26<00:14, 290.71it/s]\u001b[A\n"," 65% 7715/11873 [00:26<00:14, 292.32it/s]\u001b[A\n"," 65% 7745/11873 [00:26<00:14, 294.22it/s]\u001b[A\n"," 65% 7776/11873 [00:26<00:13, 297.04it/s]\u001b[A\n"," 66% 7806/11873 [00:26<00:13, 296.19it/s]\u001b[A\n"," 66% 7836/11873 [00:26<00:13, 292.60it/s]\u001b[A\n"," 66% 7866/11873 [00:26<00:13, 291.52it/s]\u001b[A\n"," 67% 7896/11873 [00:26<00:14, 269.55it/s]\u001b[A\n"," 67% 7925/11873 [00:26<00:14, 273.37it/s]\u001b[A\n"," 67% 7953/11873 [00:27<00:14, 272.77it/s]\u001b[A\n"," 67% 7981/11873 [00:27<00:14, 270.76it/s]\u001b[A\n"," 67% 8012/11873 [00:27<00:13, 280.63it/s]\u001b[A\n"," 68% 8043/11873 [00:27<00:13, 287.09it/s]\u001b[A\n"," 68% 8074/11873 [00:27<00:13, 291.32it/s]\u001b[A\n"," 68% 8105/11873 [00:27<00:12, 294.41it/s]\u001b[A\n"," 69% 8135/11873 [00:27<00:12, 292.63it/s]\u001b[A\n"," 69% 8165/11873 [00:27<00:12, 287.09it/s]\u001b[A\n"," 69% 8194/11873 [00:27<00:12, 287.81it/s]\u001b[A\n"," 69% 8223/11873 [00:28<00:12, 286.44it/s]\u001b[A\n"," 70% 8254/11873 [00:28<00:12, 291.86it/s]\u001b[A\n"," 70% 8286/11873 [00:28<00:12, 297.99it/s]\u001b[A\n"," 70% 8318/11873 [00:28<00:11, 302.83it/s]\u001b[A\n"," 70% 8350/11873 [00:28<00:11, 306.87it/s]\u001b[A\n"," 71% 8382/11873 [00:28<00:11, 307.84it/s]\u001b[A\n"," 71% 8413/11873 [00:28<00:11, 308.11it/s]\u001b[A\n"," 71% 8444/11873 [00:28<00:11, 304.47it/s]\u001b[A\n"," 71% 8475/11873 [00:28<00:11, 299.02it/s]\u001b[A\n"," 72% 8505/11873 [00:28<00:11, 297.50it/s]\u001b[A\n"," 72% 8535/11873 [00:29<00:11, 294.56it/s]\u001b[A\n"," 72% 8565/11873 [00:29<00:11, 295.27it/s]\u001b[A\n"," 72% 8596/11873 [00:29<00:10, 299.22it/s]\u001b[A\n"," 73% 8626/11873 [00:29<00:11, 290.97it/s]\u001b[A\n"," 73% 8658/11873 [00:29<00:10, 296.80it/s]\u001b[A\n"," 73% 8690/11873 [00:29<00:10, 301.21it/s]\u001b[A\n"," 73% 8721/11873 [00:29<00:10, 301.41it/s]\u001b[A\n"," 74% 8752/11873 [00:29<00:10, 303.36it/s]\u001b[A\n"," 74% 8783/11873 [00:29<00:10, 304.37it/s]\u001b[A\n"," 74% 8814/11873 [00:29<00:10, 297.38it/s]\u001b[A\n"," 75% 8846/11873 [00:30<00:10, 302.43it/s]\u001b[A\n"," 75% 8878/11873 [00:30<00:09, 305.07it/s]\u001b[A\n"," 75% 8910/11873 [00:30<00:09, 304.37it/s]\u001b[A\n"," 75% 8941/11873 [00:30<00:09, 300.79it/s]\u001b[A\n"," 76% 8972/11873 [00:30<00:09, 297.23it/s]\u001b[A\n"," 76% 9003/11873 [00:30<00:09, 300.38it/s]\u001b[A\n"," 76% 9034/11873 [00:30<00:09, 301.57it/s]\u001b[A\n"," 76% 9065/11873 [00:30<00:09, 302.70it/s]\u001b[A\n"," 77% 9096/11873 [00:30<00:09, 304.72it/s]\u001b[A\n"," 77% 9127/11873 [00:31<00:09, 301.85it/s]\u001b[A\n"," 77% 9158/11873 [00:31<00:08, 302.88it/s]\u001b[A\n"," 77% 9189/11873 [00:31<00:08, 300.39it/s]\u001b[A\n"," 78% 9220/11873 [00:31<00:08, 300.37it/s]\u001b[A\n"," 78% 9251/11873 [00:31<00:08, 299.83it/s]\u001b[A\n"," 78% 9283/11873 [00:31<00:08, 303.00it/s]\u001b[A\n"," 78% 9314/11873 [00:31<00:08, 301.90it/s]\u001b[A\n"," 79% 9345/11873 [00:31<00:08, 299.42it/s]\u001b[A\n"," 79% 9377/11873 [00:31<00:08, 305.16it/s]\u001b[A\n"," 79% 9408/11873 [00:31<00:08, 304.47it/s]\u001b[A\n"," 80% 9440/11873 [00:32<00:07, 306.34it/s]\u001b[A\n"," 80% 9472/11873 [00:32<00:07, 308.01it/s]\u001b[A\n"," 80% 9504/11873 [00:32<00:07, 309.09it/s]\u001b[A\n"," 80% 9536/11873 [00:32<00:07, 311.53it/s]\u001b[A\n"," 81% 9568/11873 [00:32<00:07, 311.80it/s]\u001b[A\n"," 81% 9600/11873 [00:32<00:07, 306.71it/s]\u001b[A\n"," 81% 9631/11873 [00:32<00:07, 306.94it/s]\u001b[A\n"," 81% 9662/11873 [00:32<00:07, 306.52it/s]\u001b[A\n"," 82% 9694/11873 [00:32<00:07, 310.31it/s]\u001b[A\n"," 82% 9726/11873 [00:32<00:06, 312.49it/s]\u001b[A\n"," 82% 9759/11873 [00:33<00:06, 315.53it/s]\u001b[A\n"," 82% 9791/11873 [00:33<00:06, 311.99it/s]\u001b[A\n"," 83% 9823/11873 [00:33<00:06, 308.71it/s]\u001b[A\n"," 83% 9854/11873 [00:33<00:06, 307.86it/s]\u001b[A\n"," 83% 9885/11873 [00:33<00:06, 303.28it/s]\u001b[A\n"," 84% 9916/11873 [00:33<00:06, 300.72it/s]\u001b[A\n"," 84% 9947/11873 [00:33<00:06, 298.59it/s]\u001b[A\n"," 84% 9978/11873 [00:33<00:06, 299.87it/s]\u001b[A\n"," 84% 10009/11873 [00:33<00:06, 302.78it/s]\u001b[A\n"," 85% 10041/11873 [00:34<00:06, 305.12it/s]\u001b[A\n"," 85% 10072/11873 [00:34<00:05, 305.18it/s]\u001b[A\n"," 85% 10104/11873 [00:34<00:05, 309.07it/s]\u001b[A\n"," 85% 10135/11873 [00:34<00:05, 308.41it/s]\u001b[A\n"," 86% 10166/11873 [00:34<00:05, 303.39it/s]\u001b[A\n"," 86% 10197/11873 [00:34<00:05, 304.74it/s]\u001b[A\n"," 86% 10228/11873 [00:34<00:05, 303.33it/s]\u001b[A\n"," 86% 10260/11873 [00:34<00:05, 303.94it/s]\u001b[A\n"," 87% 10291/11873 [00:34<00:05, 299.76it/s]\u001b[A\n"," 87% 10322/11873 [00:34<00:05, 302.26it/s]\u001b[A\n"," 87% 10353/11873 [00:35<00:05, 299.50it/s]\u001b[A\n"," 87% 10383/11873 [00:35<00:05, 297.93it/s]\u001b[A\n"," 88% 10413/11873 [00:35<00:04, 295.53it/s]\u001b[A\n"," 88% 10443/11873 [00:35<00:05, 267.89it/s]\u001b[A\n"," 88% 10473/11873 [00:35<00:05, 274.62it/s]\u001b[A\n"," 88% 10502/11873 [00:35<00:04, 277.43it/s]\u001b[A\n"," 89% 10532/11873 [00:35<00:04, 282.70it/s]\u001b[A\n"," 89% 10562/11873 [00:35<00:04, 285.58it/s]\u001b[A\n"," 89% 10591/11873 [00:35<00:04, 280.26it/s]\u001b[A\n"," 89% 10622/11873 [00:35<00:04, 286.97it/s]\u001b[A\n"," 90% 10652/11873 [00:36<00:04, 289.91it/s]\u001b[A\n"," 90% 10683/11873 [00:36<00:04, 293.49it/s]\u001b[A\n"," 90% 10715/11873 [00:36<00:03, 298.75it/s]\u001b[A\n"," 91% 10747/11873 [00:36<00:03, 303.30it/s]\u001b[A\n"," 91% 10778/11873 [00:36<00:03, 300.66it/s]\u001b[A\n"," 91% 10809/11873 [00:36<00:03, 301.69it/s]\u001b[A\n"," 91% 10840/11873 [00:36<00:03, 283.48it/s]\u001b[A\n"," 92% 10871/11873 [00:36<00:03, 289.62it/s]\u001b[A\n"," 92% 10902/11873 [00:36<00:03, 293.54it/s]\u001b[A\n"," 92% 10932/11873 [00:37<00:03, 286.18it/s]\u001b[A\n"," 92% 10962/11873 [00:37<00:03, 288.41it/s]\u001b[A\n"," 93% 10992/11873 [00:37<00:03, 291.59it/s]\u001b[A\n"," 93% 11022/11873 [00:37<00:02, 290.98it/s]\u001b[A\n"," 93% 11052/11873 [00:37<00:02, 292.54it/s]\u001b[A\n"," 93% 11082/11873 [00:37<00:02, 294.36it/s]\u001b[A\n"," 94% 11112/11873 [00:37<00:02, 291.69it/s]\u001b[A\n"," 94% 11143/11873 [00:37<00:02, 296.39it/s]\u001b[A\n"," 94% 11174/11873 [00:37<00:02, 298.03it/s]\u001b[A\n"," 94% 11204/11873 [00:37<00:02, 292.65it/s]\u001b[A\n"," 95% 11236/11873 [00:38<00:02, 298.16it/s]\u001b[A\n"," 95% 11266/11873 [00:38<00:02, 295.46it/s]\u001b[A\n"," 95% 11298/11873 [00:38<00:01, 300.15it/s]\u001b[A\n"," 95% 11330/11873 [00:38<00:01, 303.55it/s]\u001b[A\n"," 96% 11361/11873 [00:38<00:01, 299.78it/s]\u001b[A\n"," 96% 11393/11873 [00:38<00:01, 303.21it/s]\u001b[A\n"," 96% 11425/11873 [00:38<00:01, 306.53it/s]\u001b[A\n"," 96% 11456/11873 [00:38<00:01, 305.86it/s]\u001b[A\n"," 97% 11487/11873 [00:38<00:01, 296.72it/s]\u001b[A\n"," 97% 11519/11873 [00:39<00:01, 301.95it/s]\u001b[A\n"," 97% 11550/11873 [00:39<00:01, 302.24it/s]\u001b[A\n"," 98% 11582/11873 [00:39<00:00, 306.93it/s]\u001b[A\n"," 98% 11613/11873 [00:39<00:00, 305.17it/s]\u001b[A\n"," 98% 11644/11873 [00:39<00:00, 306.16it/s]\u001b[A\n"," 98% 11675/11873 [00:39<00:00, 302.34it/s]\u001b[A\n"," 99% 11706/11873 [00:39<00:00, 300.56it/s]\u001b[A\n"," 99% 11737/11873 [00:39<00:00, 303.09it/s]\u001b[A\n"," 99% 11769/11873 [00:39<00:00, 306.37it/s]\u001b[A\n"," 99% 11800/11873 [00:39<00:00, 304.69it/s]\u001b[A\n","100% 11832/11873 [00:40<00:00, 307.05it/s]\u001b[A\n","100% 11873/11873 [00:40<00:00, 295.61it/s]\n","03/29/2022 05:10:02 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","03/29/2022 05:10:02 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","03/29/2022 05:10:05 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","03/29/2022 05:10:09 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1522/1522 [04:21<00:00,  5.83it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 74.6289\n","  eval_HasAns_f1         = 81.0495\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 81.8671\n","  eval_NoAns_f1          = 81.8671\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 78.2616\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 81.4673\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 78.2532\n","  eval_f1                = 81.4589\n","  eval_samples           =   12171\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-03-29 05:10:09,560 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path albert-base-v2 \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 3 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HeCDg8DlFS0h","executionInfo":{"status":"ok","timestamp":1648547139715,"user_tz":240,"elapsed":16483105,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"77d8d36d-0bd9-43c7-b714-0e51289dc32d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["03/29/2022 05:10:59 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","03/29/2022 05:10:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Mar29_05-10-59_856171be3f3f,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","03/29/2022 05:10:59 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","03/29/2022 05:10:59 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/29/2022 05:10:59 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","03/29/2022 05:10:59 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/29/2022 05:10:59 - WARNING - datasets.builder - Reusing dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","03/29/2022 05:10:59 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","100% 2/2 [00:00<00:00, 511.88it/s]\n","[INFO|configuration_utils.py:653] 2022-03-29 05:11:00,271 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-29 05:11:00,273 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-03-29 05:11:00,618 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:653] 2022-03-29 05:11:00,965 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-29 05:11:00,966 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 05:11:03,041 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 05:11:03,041 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 05:11:03,041 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 05:11:03,041 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 05:11:03,041 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:653] 2022-03-29 05:11:03,388 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-29 05:11:03,389 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|modeling_utils.py:1771] 2022-03-29 05:11:03,811 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[WARNING|modeling_utils.py:2049] 2022-03-29 05:11:03,949 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.dense.bias']\n","- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-03-29 05:11:03,949 >> Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/29/2022 05:11:03 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-ce78ca212e19d384.arrow\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]03/29/2022 05:11:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-9d9e03ecb8cf8de7.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:12<00:00,  6.01s/ba]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1288] 2022-03-29 05:12:20,581 >> ***** Running training *****\n","[INFO|trainer.py:1289] 2022-03-29 05:12:20,581 >>   Num examples = 131958\n","[INFO|trainer.py:1290] 2022-03-29 05:12:20,581 >>   Num Epochs = 3\n","[INFO|trainer.py:1291] 2022-03-29 05:12:20,581 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1292] 2022-03-29 05:12:20,581 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1293] 2022-03-29 05:12:20,581 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1294] 2022-03-29 05:12:20,581 >>   Total optimization steps = 16497\n","{'loss': 1.5899, 'learning_rate': 1.9393829181063226e-05, 'epoch': 0.09}\n","  3% 500/16497 [08:07<4:20:03,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 05:20:28,343 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:440] 2022-03-29 05:20:28,344 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 05:20:28,424 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 05:20:28,425 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 05:20:28,425 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.1417, 'learning_rate': 1.8787658362126448e-05, 'epoch': 0.18}\n","  6% 1000/16497 [16:16<4:11:52,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 05:28:36,753 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:440] 2022-03-29 05:28:36,754 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 05:28:36,836 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 05:28:36,837 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 05:28:36,837 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.0375, 'learning_rate': 1.8181487543189672e-05, 'epoch': 0.27}\n","  9% 1500/16497 [24:24<4:03:58,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 05:36:45,159 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:440] 2022-03-29 05:36:45,160 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 05:36:45,242 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 05:36:45,243 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 05:36:45,243 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.0151, 'learning_rate': 1.7575316724252897e-05, 'epoch': 0.36}\n"," 12% 2000/16497 [32:32<3:55:33,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 05:44:53,555 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:440] 2022-03-29 05:44:53,557 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 05:44:53,639 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 05:44:53,640 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 05:44:53,640 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 0.9867, 'learning_rate': 1.696914590531612e-05, 'epoch': 0.45}\n"," 15% 2500/16497 [40:41<3:47:28,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 05:53:02,092 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:440] 2022-03-29 05:53:02,093 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 05:53:02,177 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 05:53:02,178 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 05:53:02,178 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 0.9399, 'learning_rate': 1.6362975086379343e-05, 'epoch': 0.55}\n"," 18% 3000/16497 [48:50<3:39:40,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 06:01:10,641 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:440] 2022-03-29 06:01:10,642 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 06:01:10,726 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 06:01:10,727 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 06:01:10,727 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.9325, 'learning_rate': 1.5756804267442568e-05, 'epoch': 0.64}\n"," 21% 3500/16497 [56:58<3:31:39,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 06:09:19,255 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:440] 2022-03-29 06:09:19,256 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 06:09:19,339 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 06:09:19,340 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 06:09:19,340 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.9156, 'learning_rate': 1.515063344850579e-05, 'epoch': 0.73}\n"," 24% 4000/16497 [1:05:07<3:23:07,  1.03it/s][INFO|trainer.py:2162] 2022-03-29 06:17:27,889 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:440] 2022-03-29 06:17:27,890 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 06:17:27,971 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 06:17:27,972 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 06:17:27,972 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.8795, 'learning_rate': 1.4544462629569014e-05, 'epoch': 0.82}\n"," 27% 4500/16497 [1:13:15<3:15:24,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 06:25:36,514 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:440] 2022-03-29 06:25:36,515 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 06:25:36,599 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 06:25:36,600 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 06:25:36,600 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.857, 'learning_rate': 1.3938291810632237e-05, 'epoch': 0.91}\n"," 30% 5000/16497 [1:21:25<3:07:29,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 06:33:45,641 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:440] 2022-03-29 06:33:45,642 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 06:33:45,725 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 06:33:45,726 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 06:33:45,726 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.8511, 'learning_rate': 1.333212099169546e-05, 'epoch': 1.0}\n"," 33% 5500/16497 [1:29:33<2:32:12,  1.20it/s][INFO|trainer.py:2162] 2022-03-29 06:41:53,900 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:440] 2022-03-29 06:41:53,901 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 06:41:53,985 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 06:41:53,985 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 06:41:53,986 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.6838, 'learning_rate': 1.2725950172758685e-05, 'epoch': 1.09}\n"," 36% 6000/16497 [1:37:42<2:50:48,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 06:50:02,848 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:440] 2022-03-29 06:50:02,849 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 06:50:02,928 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 06:50:02,929 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 06:50:02,929 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.6886, 'learning_rate': 1.2119779353821908e-05, 'epoch': 1.18}\n"," 39% 6500/16497 [1:45:50<2:42:46,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 06:58:11,586 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:440] 2022-03-29 06:58:11,587 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 06:58:11,668 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 06:58:11,669 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 06:58:11,669 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.6747, 'learning_rate': 1.1513608534885133e-05, 'epoch': 1.27}\n"," 42% 7000/16497 [1:54:00<2:34:50,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 07:06:20,735 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:440] 2022-03-29 07:06:20,736 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 07:06:20,818 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 07:06:20,819 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 07:06:20,819 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.6777, 'learning_rate': 1.0907437715948354e-05, 'epoch': 1.36}\n"," 45% 7500/16497 [2:02:09<2:26:30,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 07:14:29,855 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:440] 2022-03-29 07:14:29,856 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 07:14:29,940 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 07:14:29,941 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 07:14:29,941 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.6669, 'learning_rate': 1.0301266897011579e-05, 'epoch': 1.45}\n"," 48% 8000/16497 [2:10:18<2:18:19,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 07:22:39,008 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:440] 2022-03-29 07:22:39,009 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 07:22:39,091 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 07:22:39,092 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 07:22:39,092 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.677, 'learning_rate': 9.695096078074803e-06, 'epoch': 1.55}\n"," 52% 8500/16497 [2:18:27<2:10:29,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 07:30:47,910 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:440] 2022-03-29 07:30:47,911 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 07:30:47,995 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 07:30:47,995 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 07:30:47,996 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.657, 'learning_rate': 9.088925259138026e-06, 'epoch': 1.64}\n"," 55% 9000/16497 [2:26:36<2:02:07,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 07:38:56,858 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:440] 2022-03-29 07:38:56,859 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 07:38:56,943 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 07:38:56,944 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 07:38:56,945 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.6641, 'learning_rate': 8.48275444020125e-06, 'epoch': 1.73}\n"," 58% 9500/16497 [2:34:45<1:54:00,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 07:47:05,774 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:440] 2022-03-29 07:47:05,775 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 07:47:05,856 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 07:47:05,857 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 07:47:05,857 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.6525, 'learning_rate': 7.876583621264472e-06, 'epoch': 1.82}\n"," 61% 10000/16497 [2:42:54<1:45:44,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 07:55:14,868 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:440] 2022-03-29 07:55:14,869 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 07:55:14,955 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 07:55:14,957 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 07:55:14,957 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6525, 'learning_rate': 7.270412802327696e-06, 'epoch': 1.91}\n"," 64% 10500/16497 [2:51:03<1:37:44,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 08:03:24,156 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:440] 2022-03-29 08:03:24,157 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 08:03:24,239 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 08:03:24,240 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 08:03:24,240 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.633, 'learning_rate': 6.66424198339092e-06, 'epoch': 2.0}\n"," 67% 11000/16497 [2:59:12<1:20:05,  1.14it/s][INFO|trainer.py:2162] 2022-03-29 08:11:32,701 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11000\n","[INFO|configuration_utils.py:440] 2022-03-29 08:11:32,702 >> Configuration saved in /tmp/debug_squad/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 08:11:32,779 >> Model weights saved in /tmp/debug_squad/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 08:11:32,780 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 08:11:32,780 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.466, 'learning_rate': 6.058071164454143e-06, 'epoch': 2.09}\n"," 70% 11500/16497 [3:07:21<1:21:29,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 08:19:41,879 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11500\n","[INFO|configuration_utils.py:440] 2022-03-29 08:19:41,881 >> Configuration saved in /tmp/debug_squad/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 08:19:41,963 >> Model weights saved in /tmp/debug_squad/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 08:19:41,964 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 08:19:41,964 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.4846, 'learning_rate': 5.451900345517367e-06, 'epoch': 2.18}\n"," 73% 12000/16497 [3:15:30<1:13:13,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 08:27:51,012 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12000\n","[INFO|configuration_utils.py:440] 2022-03-29 08:27:51,013 >> Configuration saved in /tmp/debug_squad/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 08:27:51,091 >> Model weights saved in /tmp/debug_squad/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 08:27:51,092 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 08:27:51,093 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.4755, 'learning_rate': 4.845729526580591e-06, 'epoch': 2.27}\n"," 76% 12500/16497 [3:23:39<1:05:10,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 08:36:00,266 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12500\n","[INFO|configuration_utils.py:440] 2022-03-29 08:36:00,268 >> Configuration saved in /tmp/debug_squad/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 08:36:00,344 >> Model weights saved in /tmp/debug_squad/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 08:36:00,344 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 08:36:00,345 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.4677, 'learning_rate': 4.239558707643815e-06, 'epoch': 2.36}\n"," 79% 13000/16497 [3:31:48<56:57,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 08:44:09,450 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13000\n","[INFO|configuration_utils.py:440] 2022-03-29 08:44:09,451 >> Configuration saved in /tmp/debug_squad/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 08:44:09,528 >> Model weights saved in /tmp/debug_squad/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 08:44:09,529 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 08:44:09,529 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.4776, 'learning_rate': 3.633387888707038e-06, 'epoch': 2.45}\n"," 82% 13500/16497 [3:39:58<48:47,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 08:52:18,663 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13500\n","[INFO|configuration_utils.py:440] 2022-03-29 08:52:18,664 >> Configuration saved in /tmp/debug_squad/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 08:52:18,741 >> Model weights saved in /tmp/debug_squad/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 08:52:18,742 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 08:52:18,742 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.4659, 'learning_rate': 3.0272170697702612e-06, 'epoch': 2.55}\n"," 85% 14000/16497 [3:48:07<40:42,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 09:00:27,818 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14000\n","[INFO|configuration_utils.py:440] 2022-03-29 09:00:27,819 >> Configuration saved in /tmp/debug_squad/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 09:00:27,896 >> Model weights saved in /tmp/debug_squad/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 09:00:27,897 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 09:00:27,897 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.4723, 'learning_rate': 2.421046250833485e-06, 'epoch': 2.64}\n"," 88% 14500/16497 [3:56:16<32:31,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 09:08:36,992 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14500\n","[INFO|configuration_utils.py:440] 2022-03-29 09:08:36,993 >> Configuration saved in /tmp/debug_squad/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 09:08:37,071 >> Model weights saved in /tmp/debug_squad/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 09:08:37,071 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 09:08:37,072 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.4552, 'learning_rate': 1.8148754318967086e-06, 'epoch': 2.73}\n"," 91% 15000/16497 [4:04:25<24:24,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 09:16:46,058 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15000\n","[INFO|configuration_utils.py:440] 2022-03-29 09:16:46,059 >> Configuration saved in /tmp/debug_squad/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 09:16:46,137 >> Model weights saved in /tmp/debug_squad/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 09:16:46,138 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 09:16:46,139 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.4461, 'learning_rate': 1.2087046129599322e-06, 'epoch': 2.82}\n"," 94% 15500/16497 [4:12:34<16:16,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 09:24:55,248 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15500\n","[INFO|configuration_utils.py:440] 2022-03-29 09:24:55,249 >> Configuration saved in /tmp/debug_squad/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 09:24:55,328 >> Model weights saved in /tmp/debug_squad/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 09:24:55,328 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 09:24:55,329 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.4544, 'learning_rate': 6.025337940231558e-07, 'epoch': 2.91}\n"," 97% 16000/16497 [4:20:43<08:05,  1.02it/s][INFO|trainer.py:2162] 2022-03-29 09:33:04,321 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-16000\n","[INFO|configuration_utils.py:440] 2022-03-29 09:33:04,323 >> Configuration saved in /tmp/debug_squad/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 09:33:04,403 >> Model weights saved in /tmp/debug_squad/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 09:33:04,404 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 09:33:04,404 >> Special tokens file saved in /tmp/debug_squad/checkpoint-16000/special_tokens_map.json\n","100% 16497/16497 [4:28:49<00:00,  1.31it/s][INFO|trainer.py:1526] 2022-03-29 09:41:09,797 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 16129.216, 'train_samples_per_second': 24.544, 'train_steps_per_second': 1.023, 'train_loss': 0.7149748033615013, 'epoch': 3.0}\n","100% 16497/16497 [4:28:49<00:00,  1.02it/s]\n","[INFO|trainer.py:2162] 2022-03-29 09:41:09,800 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:440] 2022-03-29 09:41:09,801 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 09:41:09,887 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 09:41:09,887 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 09:41:09,887 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =      0.715\n","  train_runtime            = 4:28:49.21\n","  train_samples            =     131958\n","  train_samples_per_second =     24.544\n","  train_steps_per_second   =      1.023\n","03/29/2022 09:41:09 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:571] 2022-03-29 09:41:09,898 >> The following columns in the evaluation set  don't have a corresponding argument in `AlbertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `AlbertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2412] 2022-03-29 09:41:09,900 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2414] 2022-03-29 09:41:09,900 >>   Num examples = 12171\n","[INFO|trainer.py:2417] 2022-03-29 09:41:09,900 >>   Batch size = 8\n","100% 1521/1522 [03:20<00:00,  7.60it/s]03/29/2022 09:44:46 - INFO - utils_qa - Post-processing 11873 example predictions split into 12171 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 34/11873 [00:00<00:35, 332.66it/s]\u001b[A\n","  1% 68/11873 [00:00<00:36, 319.23it/s]\u001b[A\n","  1% 102/11873 [00:00<00:35, 327.64it/s]\u001b[A\n","  1% 138/11873 [00:00<00:34, 338.80it/s]\u001b[A\n","  1% 173/11873 [00:00<00:34, 342.49it/s]\u001b[A\n","  2% 210/11873 [00:00<00:33, 350.17it/s]\u001b[A\n","  2% 247/11873 [00:00<00:32, 353.70it/s]\u001b[A\n","  2% 284/11873 [00:00<00:32, 357.89it/s]\u001b[A\n","  3% 320/11873 [00:00<00:32, 356.04it/s]\u001b[A\n","  3% 356/11873 [00:01<00:32, 350.32it/s]\u001b[A\n","  3% 394/11873 [00:01<00:32, 356.15it/s]\u001b[A\n","  4% 432/11873 [00:01<00:31, 360.43it/s]\u001b[A\n","  4% 469/11873 [00:01<00:31, 362.81it/s]\u001b[A\n","  4% 506/11873 [00:01<00:31, 363.38it/s]\u001b[A\n","  5% 543/11873 [00:01<00:31, 357.29it/s]\u001b[A\n","  5% 579/11873 [00:01<00:31, 354.44it/s]\u001b[A\n","  5% 617/11873 [00:01<00:31, 361.89it/s]\u001b[A\n","  6% 654/11873 [00:01<00:31, 358.90it/s]\u001b[A\n","  6% 691/11873 [00:01<00:30, 361.57it/s]\u001b[A\n","  6% 728/11873 [00:02<00:31, 357.51it/s]\u001b[A\n","  6% 764/11873 [00:02<00:31, 355.05it/s]\u001b[A\n","  7% 803/11873 [00:02<00:30, 362.80it/s]\u001b[A\n","  7% 840/11873 [00:02<00:30, 364.65it/s]\u001b[A\n","  7% 880/11873 [00:02<00:29, 374.49it/s]\u001b[A\n","  8% 918/11873 [00:02<00:29, 374.15it/s]\u001b[A\n","  8% 958/11873 [00:02<00:28, 379.45it/s]\u001b[A\n","  8% 996/11873 [00:02<00:29, 369.23it/s]\u001b[A\n","  9% 1033/11873 [00:02<00:31, 345.94it/s]\u001b[A\n","  9% 1068/11873 [00:03<00:32, 327.90it/s]\u001b[A\n","  9% 1102/11873 [00:03<00:34, 314.67it/s]\u001b[A\n"," 10% 1134/11873 [00:03<00:35, 304.41it/s]\u001b[A\n"," 10% 1165/11873 [00:03<00:35, 300.67it/s]\u001b[A\n"," 10% 1196/11873 [00:03<00:35, 300.05it/s]\u001b[A\n"," 10% 1227/11873 [00:03<00:36, 292.60it/s]\u001b[A\n"," 11% 1257/11873 [00:03<00:37, 282.64it/s]\u001b[A\n"," 11% 1286/11873 [00:03<00:38, 275.30it/s]\u001b[A\n"," 11% 1315/11873 [00:03<00:38, 276.85it/s]\u001b[A\n"," 11% 1345/11873 [00:04<00:37, 281.99it/s]\u001b[A\n"," 12% 1375/11873 [00:04<00:36, 285.20it/s]\u001b[A\n"," 12% 1405/11873 [00:04<00:36, 288.75it/s]\u001b[A\n"," 12% 1435/11873 [00:04<00:35, 291.14it/s]\u001b[A\n"," 12% 1465/11873 [00:04<00:35, 292.77it/s]\u001b[A\n"," 13% 1495/11873 [00:04<00:35, 294.70it/s]\u001b[A\n"," 13% 1526/11873 [00:04<00:34, 297.41it/s]\u001b[A\n"," 13% 1557/11873 [00:04<00:34, 297.67it/s]\u001b[A\n"," 13% 1587/11873 [00:04<00:34, 297.20it/s]\u001b[A\n"," 14% 1617/11873 [00:04<00:34, 294.72it/s]\u001b[A\n"," 14% 1647/11873 [00:05<00:35, 291.99it/s]\u001b[A\n"," 14% 1677/11873 [00:05<00:34, 292.75it/s]\u001b[A\n"," 14% 1707/11873 [00:05<00:35, 290.27it/s]\u001b[A\n"," 15% 1737/11873 [00:05<00:34, 291.57it/s]\u001b[A\n"," 15% 1767/11873 [00:05<00:34, 289.52it/s]\u001b[A\n"," 15% 1796/11873 [00:05<00:34, 288.99it/s]\u001b[A\n"," 15% 1825/11873 [00:05<00:35, 287.06it/s]\u001b[A\n"," 16% 1855/11873 [00:05<00:34, 289.58it/s]\u001b[A\n"," 16% 1884/11873 [00:05<00:34, 289.45it/s]\u001b[A\n"," 16% 1914/11873 [00:05<00:34, 290.35it/s]\u001b[A\n"," 16% 1944/11873 [00:06<00:34, 290.79it/s]\u001b[A\n"," 17% 1974/11873 [00:06<00:33, 291.25it/s]\u001b[A\n"," 17% 2004/11873 [00:06<00:34, 290.22it/s]\u001b[A\n"," 17% 2034/11873 [00:06<00:33, 292.77it/s]\u001b[A\n"," 17% 2064/11873 [00:06<00:33, 293.35it/s]\u001b[A\n"," 18% 2094/11873 [00:06<00:33, 291.08it/s]\u001b[A\n"," 18% 2124/11873 [00:06<00:33, 291.83it/s]\u001b[A\n"," 18% 2154/11873 [00:06<00:33, 291.02it/s]\u001b[A\n"," 18% 2184/11873 [00:06<00:33, 287.33it/s]\u001b[A\n"," 19% 2213/11873 [00:06<00:33, 287.38it/s]\u001b[A\n"," 19% 2242/11873 [00:07<00:33, 287.19it/s]\u001b[A\n"," 19% 2272/11873 [00:07<00:33, 288.37it/s]\u001b[A\n"," 19% 2301/11873 [00:07<00:33, 287.93it/s]\u001b[A\n"," 20% 2330/11873 [00:07<00:33, 284.34it/s]\u001b[A\n"," 20% 2359/11873 [00:07<00:33, 285.64it/s]\u001b[A\n"," 20% 2388/11873 [00:07<00:33, 286.90it/s]\u001b[A\n"," 20% 2417/11873 [00:07<00:33, 284.53it/s]\u001b[A\n"," 21% 2446/11873 [00:07<00:33, 283.76it/s]\u001b[A\n"," 21% 2475/11873 [00:07<00:32, 284.97it/s]\u001b[A\n"," 21% 2504/11873 [00:08<00:32, 286.38it/s]\u001b[A\n"," 21% 2534/11873 [00:08<00:32, 289.03it/s]\u001b[A\n"," 22% 2563/11873 [00:08<00:32, 288.57it/s]\u001b[A\n"," 22% 2592/11873 [00:08<00:32, 287.09it/s]\u001b[A\n"," 22% 2622/11873 [00:08<00:31, 289.12it/s]\u001b[A\n"," 22% 2651/11873 [00:08<00:32, 286.91it/s]\u001b[A\n"," 23% 2681/11873 [00:08<00:31, 288.15it/s]\u001b[A\n"," 23% 2711/11873 [00:08<00:31, 288.98it/s]\u001b[A\n"," 23% 2740/11873 [00:08<00:31, 287.06it/s]\u001b[A\n"," 23% 2769/11873 [00:08<00:32, 283.62it/s]\u001b[A\n"," 24% 2798/11873 [00:09<00:32, 275.74it/s]\u001b[A\n"," 24% 2826/11873 [00:09<00:33, 271.56it/s]\u001b[A\n"," 24% 2854/11873 [00:09<00:33, 268.31it/s]\u001b[A\n"," 24% 2881/11873 [00:09<00:34, 264.21it/s]\u001b[A\n"," 24% 2908/11873 [00:09<00:34, 259.51it/s]\u001b[A\n"," 25% 2934/11873 [00:09<00:34, 255.94it/s]\u001b[A\n"," 25% 2961/11873 [00:09<00:34, 258.69it/s]\u001b[A\n"," 25% 2987/11873 [00:09<00:34, 258.71it/s]\u001b[A\n"," 25% 3013/11873 [00:09<00:35, 251.75it/s]\u001b[A\n"," 26% 3039/11873 [00:09<00:35, 248.75it/s]\u001b[A\n"," 26% 3064/11873 [00:10<00:35, 244.96it/s]\u001b[A\n"," 26% 3089/11873 [00:10<00:36, 241.59it/s]\u001b[A\n"," 26% 3114/11873 [00:10<00:41, 212.55it/s]\u001b[A\n"," 26% 3136/11873 [00:10<00:43, 203.09it/s]\u001b[A\n"," 27% 3157/11873 [00:10<00:45, 193.44it/s]\u001b[A\n"," 27% 3184/11873 [00:10<00:40, 212.41it/s]\u001b[A\n"," 27% 3212/11873 [00:10<00:37, 230.43it/s]\u001b[A\n"," 27% 3242/11873 [00:10<00:34, 248.22it/s]\u001b[A\n"," 28% 3268/11873 [00:11<00:35, 242.58it/s]\u001b[A\n"," 28% 3293/11873 [00:11<00:48, 176.39it/s]\u001b[A\n"," 28% 3314/11873 [00:11<00:52, 163.13it/s]\u001b[A\n"," 28% 3333/11873 [00:11<00:53, 159.57it/s]\u001b[A\n"," 28% 3351/11873 [00:11<00:53, 159.22it/s]\u001b[A\n"," 28% 3368/11873 [00:11<00:56, 149.72it/s]\u001b[A\n"," 29% 3395/11873 [00:11<00:47, 178.05it/s]\u001b[A\n"," 29% 3423/11873 [00:11<00:41, 204.00it/s]\u001b[A\n"," 29% 3451/11873 [00:12<00:37, 224.04it/s]\u001b[A\n"," 29% 3481/11873 [00:12<00:34, 243.68it/s]\u001b[A\n"," 30% 3511/11873 [00:12<00:32, 258.16it/s]\u001b[A\n"," 30% 3540/11873 [00:12<00:31, 266.68it/s]\u001b[A\n"," 30% 3569/11873 [00:12<00:30, 271.81it/s]\u001b[A\n"," 30% 3599/11873 [00:12<00:29, 279.08it/s]\u001b[A\n"," 31% 3628/11873 [00:12<00:29, 282.02it/s]\u001b[A\n"," 31% 3657/11873 [00:12<00:28, 283.50it/s]\u001b[A\n"," 31% 3686/11873 [00:12<00:29, 278.95it/s]\u001b[A\n"," 31% 3715/11873 [00:13<00:29, 276.43it/s]\u001b[A\n"," 32% 3743/11873 [00:13<00:29, 276.87it/s]\u001b[A\n"," 32% 3771/11873 [00:13<00:29, 277.28it/s]\u001b[A\n"," 32% 3799/11873 [00:13<00:29, 274.94it/s]\u001b[A\n"," 32% 3827/11873 [00:13<00:32, 248.42it/s]\u001b[A\n"," 32% 3855/11873 [00:13<00:31, 256.40it/s]\u001b[A\n"," 33% 3882/11873 [00:13<00:30, 259.07it/s]\u001b[A\n"," 33% 3910/11873 [00:13<00:30, 263.73it/s]\u001b[A\n"," 33% 3937/11873 [00:13<00:32, 244.97it/s]\u001b[A\n"," 33% 3965/11873 [00:13<00:31, 254.39it/s]\u001b[A\n"," 34% 3992/11873 [00:14<00:30, 258.78it/s]\u001b[A\n"," 34% 4019/11873 [00:14<00:30, 258.64it/s]\u001b[A\n"," 34% 4046/11873 [00:14<00:30, 256.32it/s]\u001b[A\n"," 34% 4072/11873 [00:14<00:30, 254.32it/s]\u001b[A\n"," 35% 4098/11873 [00:14<00:30, 253.95it/s]\u001b[A\n"," 35% 4124/11873 [00:14<00:30, 252.92it/s]\u001b[A\n"," 35% 4150/11873 [00:14<00:33, 233.13it/s]\u001b[A\n"," 35% 4174/11873 [00:14<00:34, 221.40it/s]\u001b[A\n"," 35% 4202/11873 [00:14<00:32, 236.26it/s]\u001b[A\n"," 36% 4230/11873 [00:15<00:30, 247.12it/s]\u001b[A\n"," 36% 4256/11873 [00:15<00:30, 250.50it/s]\u001b[A\n"," 36% 4282/11873 [00:15<00:30, 249.31it/s]\u001b[A\n"," 36% 4309/11873 [00:15<00:29, 253.98it/s]\u001b[A\n"," 37% 4336/11873 [00:15<00:29, 256.49it/s]\u001b[A\n"," 37% 4362/11873 [00:15<00:29, 256.75it/s]\u001b[A\n"," 37% 4389/11873 [00:15<00:28, 258.60it/s]\u001b[A\n"," 37% 4415/11873 [00:15<00:32, 227.03it/s]\u001b[A\n"," 37% 4439/11873 [00:15<00:36, 203.02it/s]\u001b[A\n"," 38% 4469/11873 [00:16<00:32, 226.30it/s]\u001b[A\n"," 38% 4498/11873 [00:16<00:30, 241.45it/s]\u001b[A\n"," 38% 4528/11873 [00:16<00:28, 255.30it/s]\u001b[A\n"," 38% 4557/11873 [00:16<00:27, 264.86it/s]\u001b[A\n"," 39% 4587/11873 [00:16<00:26, 273.70it/s]\u001b[A\n"," 39% 4616/11873 [00:16<00:26, 278.35it/s]\u001b[A\n"," 39% 4645/11873 [00:16<00:25, 279.83it/s]\u001b[A\n"," 39% 4674/11873 [00:16<00:25, 282.07it/s]\u001b[A\n"," 40% 4703/11873 [00:16<00:25, 281.09it/s]\u001b[A\n"," 40% 4732/11873 [00:17<00:25, 278.70it/s]\u001b[A\n"," 40% 4761/11873 [00:17<00:25, 280.53it/s]\u001b[A\n"," 40% 4790/11873 [00:17<00:25, 279.30it/s]\u001b[A\n"," 41% 4818/11873 [00:17<00:25, 277.70it/s]\u001b[A\n"," 41% 4847/11873 [00:17<00:25, 279.96it/s]\u001b[A\n"," 41% 4876/11873 [00:17<00:25, 278.43it/s]\u001b[A\n"," 41% 4905/11873 [00:17<00:24, 280.59it/s]\u001b[A\n"," 42% 4935/11873 [00:17<00:24, 284.44it/s]\u001b[A\n"," 42% 4965/11873 [00:17<00:24, 286.71it/s]\u001b[A\n"," 42% 4994/11873 [00:17<00:23, 287.23it/s]\u001b[A\n"," 42% 5023/11873 [00:18<00:23, 286.17it/s]\u001b[A\n"," 43% 5052/11873 [00:18<00:24, 282.44it/s]\u001b[A\n"," 43% 5082/11873 [00:18<00:23, 285.47it/s]\u001b[A\n"," 43% 5112/11873 [00:18<00:23, 288.21it/s]\u001b[A\n"," 43% 5142/11873 [00:18<00:23, 290.06it/s]\u001b[A\n"," 44% 5172/11873 [00:18<00:23, 288.06it/s]\u001b[A\n"," 44% 5202/11873 [00:18<00:22, 290.24it/s]\u001b[A\n"," 44% 5232/11873 [00:18<00:22, 290.69it/s]\u001b[A\n"," 44% 5262/11873 [00:18<00:25, 262.88it/s]\u001b[A\n"," 45% 5291/11873 [00:18<00:24, 270.17it/s]\u001b[A\n"," 45% 5321/11873 [00:19<00:23, 278.20it/s]\u001b[A\n"," 45% 5351/11873 [00:19<00:23, 282.32it/s]\u001b[A\n"," 45% 5381/11873 [00:19<00:22, 285.72it/s]\u001b[A\n"," 46% 5411/11873 [00:19<00:22, 289.44it/s]\u001b[A\n"," 46% 5441/11873 [00:19<00:22, 287.16it/s]\u001b[A\n"," 46% 5470/11873 [00:19<00:22, 286.40it/s]\u001b[A\n"," 46% 5499/11873 [00:19<00:22, 286.99it/s]\u001b[A\n"," 47% 5529/11873 [00:19<00:21, 288.82it/s]\u001b[A\n"," 47% 5558/11873 [00:19<00:22, 286.70it/s]\u001b[A\n"," 47% 5587/11873 [00:20<00:22, 276.28it/s]\u001b[A\n"," 47% 5615/11873 [00:20<00:23, 267.46it/s]\u001b[A\n"," 48% 5642/11873 [00:20<00:23, 261.02it/s]\u001b[A\n"," 48% 5669/11873 [00:20<00:24, 257.39it/s]\u001b[A\n"," 48% 5695/11873 [00:20<00:24, 255.55it/s]\u001b[A\n"," 48% 5721/11873 [00:20<00:24, 254.86it/s]\u001b[A\n"," 48% 5747/11873 [00:20<00:24, 248.97it/s]\u001b[A\n"," 49% 5773/11873 [00:20<00:24, 250.14it/s]\u001b[A\n"," 49% 5801/11873 [00:20<00:23, 256.29it/s]\u001b[A\n"," 49% 5831/11873 [00:20<00:22, 268.41it/s]\u001b[A\n"," 49% 5861/11873 [00:21<00:21, 275.98it/s]\u001b[A\n"," 50% 5890/11873 [00:21<00:21, 278.38it/s]\u001b[A\n"," 50% 5919/11873 [00:21<00:21, 281.26it/s]\u001b[A\n"," 50% 5949/11873 [00:21<00:20, 284.49it/s]\u001b[A\n"," 50% 5979/11873 [00:21<00:20, 286.12it/s]\u001b[A\n"," 51% 6008/11873 [00:21<00:20, 284.93it/s]\u001b[A\n"," 51% 6038/11873 [00:21<00:20, 287.56it/s]\u001b[A\n"," 51% 6067/11873 [00:21<00:20, 286.36it/s]\u001b[A\n"," 51% 6096/11873 [00:21<00:20, 282.84it/s]\u001b[A\n"," 52% 6125/11873 [00:22<00:20, 277.96it/s]\u001b[A\n"," 52% 6153/11873 [00:22<00:20, 277.12it/s]\u001b[A\n"," 52% 6183/11873 [00:22<00:20, 281.06it/s]\u001b[A\n"," 52% 6212/11873 [00:22<00:20, 282.40it/s]\u001b[A\n"," 53% 6241/11873 [00:22<00:19, 284.05it/s]\u001b[A\n"," 53% 6271/11873 [00:22<00:19, 287.15it/s]\u001b[A\n"," 53% 6300/11873 [00:22<00:19, 285.38it/s]\u001b[A\n"," 53% 6330/11873 [00:22<00:19, 287.28it/s]\u001b[A\n"," 54% 6360/11873 [00:22<00:19, 289.24it/s]\u001b[A\n"," 54% 6389/11873 [00:22<00:19, 280.74it/s]\u001b[A\n"," 54% 6418/11873 [00:23<00:19, 277.12it/s]\u001b[A\n"," 54% 6446/11873 [00:23<00:19, 276.40it/s]\u001b[A\n"," 55% 6475/11873 [00:23<00:19, 278.53it/s]\u001b[A\n"," 55% 6504/11873 [00:23<00:19, 278.20it/s]\u001b[A\n"," 55% 6532/11873 [00:23<00:19, 277.79it/s]\u001b[A\n"," 55% 6561/11873 [00:23<00:19, 278.36it/s]\u001b[A\n"," 56% 6590/11873 [00:23<00:18, 279.33it/s]\u001b[A\n"," 56% 6620/11873 [00:23<00:18, 283.87it/s]\u001b[A\n"," 56% 6649/11873 [00:23<00:18, 281.64it/s]\u001b[A\n"," 56% 6678/11873 [00:23<00:18, 276.89it/s]\u001b[A\n"," 56% 6706/11873 [00:24<00:20, 255.04it/s]\u001b[A\n"," 57% 6732/11873 [00:24<00:22, 231.87it/s]\u001b[A\n"," 57% 6759/11873 [00:24<00:21, 240.09it/s]\u001b[A\n"," 57% 6784/11873 [00:24<00:21, 241.09it/s]\u001b[A\n"," 57% 6809/11873 [00:24<00:20, 243.00it/s]\u001b[A\n"," 58% 6836/11873 [00:24<00:20, 249.70it/s]\u001b[A\n"," 58% 6863/11873 [00:24<00:19, 252.77it/s]\u001b[A\n"," 58% 6891/11873 [00:24<00:19, 258.45it/s]\u001b[A\n"," 58% 6919/11873 [00:24<00:18, 263.42it/s]\u001b[A\n"," 59% 6949/11873 [00:25<00:18, 272.68it/s]\u001b[A\n"," 59% 6978/11873 [00:25<00:17, 277.24it/s]\u001b[A\n"," 59% 7007/11873 [00:25<00:17, 280.50it/s]\u001b[A\n"," 59% 7037/11873 [00:25<00:17, 284.47it/s]\u001b[A\n"," 60% 7066/11873 [00:25<00:16, 284.43it/s]\u001b[A\n"," 60% 7095/11873 [00:25<00:16, 281.47it/s]\u001b[A\n"," 60% 7125/11873 [00:25<00:16, 284.20it/s]\u001b[A\n"," 60% 7156/11873 [00:25<00:16, 289.30it/s]\u001b[A\n"," 61% 7185/11873 [00:25<00:16, 286.85it/s]\u001b[A\n"," 61% 7214/11873 [00:26<00:16, 276.86it/s]\u001b[A\n"," 61% 7242/11873 [00:26<00:17, 271.42it/s]\u001b[A\n"," 61% 7270/11873 [00:26<00:17, 265.00it/s]\u001b[A\n"," 61% 7297/11873 [00:26<00:17, 266.39it/s]\u001b[A\n"," 62% 7324/11873 [00:26<00:17, 264.16it/s]\u001b[A\n"," 62% 7351/11873 [00:26<00:17, 264.92it/s]\u001b[A\n"," 62% 7378/11873 [00:26<00:17, 263.36it/s]\u001b[A\n"," 62% 7405/11873 [00:26<00:16, 263.19it/s]\u001b[A\n"," 63% 7432/11873 [00:26<00:17, 250.55it/s]\u001b[A\n"," 63% 7462/11873 [00:26<00:16, 264.17it/s]\u001b[A\n"," 63% 7493/11873 [00:27<00:15, 275.73it/s]\u001b[A\n"," 63% 7521/11873 [00:27<00:15, 275.86it/s]\u001b[A\n"," 64% 7551/11873 [00:27<00:15, 282.13it/s]\u001b[A\n"," 64% 7581/11873 [00:27<00:15, 285.35it/s]\u001b[A\n"," 64% 7610/11873 [00:27<00:15, 283.66it/s]\u001b[A\n"," 64% 7639/11873 [00:27<00:14, 284.10it/s]\u001b[A\n"," 65% 7668/11873 [00:27<00:14, 285.75it/s]\u001b[A\n"," 65% 7698/11873 [00:27<00:14, 288.64it/s]\u001b[A\n"," 65% 7727/11873 [00:27<00:14, 284.91it/s]\u001b[A\n"," 65% 7756/11873 [00:27<00:14, 279.64it/s]\u001b[A\n"," 66% 7784/11873 [00:28<00:15, 269.56it/s]\u001b[A\n"," 66% 7812/11873 [00:28<00:15, 265.51it/s]\u001b[A\n"," 66% 7839/11873 [00:28<00:15, 259.72it/s]\u001b[A\n"," 66% 7867/11873 [00:28<00:15, 264.89it/s]\u001b[A\n"," 66% 7894/11873 [00:28<00:15, 249.75it/s]\u001b[A\n"," 67% 7924/11873 [00:28<00:15, 261.68it/s]\u001b[A\n"," 67% 7954/11873 [00:28<00:14, 271.33it/s]\u001b[A\n"," 67% 7982/11873 [00:28<00:14, 272.20it/s]\u001b[A\n"," 67% 8010/11873 [00:28<00:14, 272.12it/s]\u001b[A\n"," 68% 8040/11873 [00:29<00:13, 279.04it/s]\u001b[A\n"," 68% 8069/11873 [00:29<00:13, 281.16it/s]\u001b[A\n"," 68% 8099/11873 [00:29<00:13, 284.89it/s]\u001b[A\n"," 68% 8129/11873 [00:29<00:13, 287.24it/s]\u001b[A\n"," 69% 8158/11873 [00:29<00:13, 281.12it/s]\u001b[A\n"," 69% 8187/11873 [00:29<00:13, 281.54it/s]\u001b[A\n"," 69% 8216/11873 [00:29<00:13, 281.17it/s]\u001b[A\n"," 69% 8246/11873 [00:29<00:12, 285.12it/s]\u001b[A\n"," 70% 8277/11873 [00:29<00:12, 289.91it/s]\u001b[A\n"," 70% 8307/11873 [00:29<00:12, 291.21it/s]\u001b[A\n"," 70% 8337/11873 [00:30<00:12, 292.56it/s]\u001b[A\n"," 70% 8367/11873 [00:30<00:12, 291.40it/s]\u001b[A\n"," 71% 8397/11873 [00:30<00:11, 292.46it/s]\u001b[A\n"," 71% 8427/11873 [00:30<00:11, 287.66it/s]\u001b[A\n"," 71% 8456/11873 [00:30<00:12, 284.51it/s]\u001b[A\n"," 71% 8486/11873 [00:30<00:11, 286.78it/s]\u001b[A\n"," 72% 8516/11873 [00:30<00:11, 289.15it/s]\u001b[A\n"," 72% 8546/11873 [00:30<00:11, 291.98it/s]\u001b[A\n"," 72% 8576/11873 [00:30<00:11, 283.86it/s]\u001b[A\n"," 72% 8605/11873 [00:31<00:11, 282.86it/s]\u001b[A\n"," 73% 8634/11873 [00:31<00:11, 281.82it/s]\u001b[A\n"," 73% 8663/11873 [00:31<00:11, 283.94it/s]\u001b[A\n"," 73% 8693/11873 [00:31<00:11, 286.16it/s]\u001b[A\n"," 73% 8723/11873 [00:31<00:10, 289.30it/s]\u001b[A\n"," 74% 8753/11873 [00:31<00:10, 290.40it/s]\u001b[A\n"," 74% 8783/11873 [00:31<00:10, 291.17it/s]\u001b[A\n"," 74% 8813/11873 [00:31<00:10, 286.78it/s]\u001b[A\n"," 74% 8843/11873 [00:31<00:10, 289.26it/s]\u001b[A\n"," 75% 8872/11873 [00:31<00:10, 286.62it/s]\u001b[A\n"," 75% 8901/11873 [00:32<00:10, 279.77it/s]\u001b[A\n"," 75% 8930/11873 [00:32<00:10, 279.92it/s]\u001b[A\n"," 75% 8959/11873 [00:32<00:10, 279.35it/s]\u001b[A\n"," 76% 8988/11873 [00:32<00:10, 280.95it/s]\u001b[A\n"," 76% 9017/11873 [00:32<00:10, 283.59it/s]\u001b[A\n"," 76% 9046/11873 [00:32<00:09, 284.10it/s]\u001b[A\n"," 76% 9075/11873 [00:32<00:09, 285.62it/s]\u001b[A\n"," 77% 9104/11873 [00:32<00:09, 285.00it/s]\u001b[A\n"," 77% 9133/11873 [00:32<00:09, 276.30it/s]\u001b[A\n"," 77% 9161/11873 [00:32<00:09, 274.61it/s]\u001b[A\n"," 77% 9191/11873 [00:33<00:09, 281.36it/s]\u001b[A\n"," 78% 9221/11873 [00:33<00:09, 285.63it/s]\u001b[A\n"," 78% 9250/11873 [00:33<00:09, 285.24it/s]\u001b[A\n"," 78% 9281/11873 [00:33<00:08, 289.70it/s]\u001b[A\n"," 78% 9310/11873 [00:33<00:08, 288.35it/s]\u001b[A\n"," 79% 9339/11873 [00:33<00:08, 285.87it/s]\u001b[A\n"," 79% 9368/11873 [00:33<00:08, 284.42it/s]\u001b[A\n"," 79% 9397/11873 [00:33<00:08, 284.46it/s]\u001b[A\n"," 79% 9426/11873 [00:33<00:08, 281.02it/s]\u001b[A\n"," 80% 9455/11873 [00:34<00:08, 281.69it/s]\u001b[A\n"," 80% 9485/11873 [00:34<00:08, 285.98it/s]\u001b[A\n"," 80% 9514/11873 [00:34<00:08, 285.66it/s]\u001b[A\n"," 80% 9544/11873 [00:34<00:08, 289.47it/s]\u001b[A\n"," 81% 9575/11873 [00:34<00:07, 293.20it/s]\u001b[A\n"," 81% 9605/11873 [00:34<00:07, 291.48it/s]\u001b[A\n"," 81% 9635/11873 [00:34<00:07, 289.20it/s]\u001b[A\n"," 81% 9664/11873 [00:34<00:07, 286.82it/s]\u001b[A\n"," 82% 9694/11873 [00:34<00:07, 290.63it/s]\u001b[A\n"," 82% 9724/11873 [00:34<00:07, 288.42it/s]\u001b[A\n"," 82% 9753/11873 [00:35<00:07, 285.29it/s]\u001b[A\n"," 82% 9782/11873 [00:35<00:07, 285.53it/s]\u001b[A\n"," 83% 9811/11873 [00:35<00:07, 286.46it/s]\u001b[A\n"," 83% 9841/11873 [00:35<00:07, 289.54it/s]\u001b[A\n"," 83% 9872/11873 [00:35<00:06, 293.23it/s]\u001b[A\n"," 83% 9902/11873 [00:35<00:06, 291.74it/s]\u001b[A\n"," 84% 9933/11873 [00:35<00:06, 294.90it/s]\u001b[A\n"," 84% 9963/11873 [00:35<00:06, 293.52it/s]\u001b[A\n"," 84% 9993/11873 [00:35<00:06, 295.07it/s]\u001b[A\n"," 84% 10023/11873 [00:35<00:06, 293.94it/s]\u001b[A\n"," 85% 10053/11873 [00:36<00:06, 294.65it/s]\u001b[A\n"," 85% 10084/11873 [00:36<00:06, 297.79it/s]\u001b[A\n"," 85% 10115/11873 [00:36<00:05, 298.52it/s]\u001b[A\n"," 85% 10146/11873 [00:36<00:05, 299.64it/s]\u001b[A\n"," 86% 10176/11873 [00:36<00:05, 299.22it/s]\u001b[A\n"," 86% 10206/11873 [00:36<00:05, 294.37it/s]\u001b[A\n"," 86% 10236/11873 [00:36<00:05, 294.60it/s]\u001b[A\n"," 86% 10267/11873 [00:36<00:05, 296.34it/s]\u001b[A\n"," 87% 10297/11873 [00:36<00:05, 288.25it/s]\u001b[A\n"," 87% 10326/11873 [00:36<00:05, 286.60it/s]\u001b[A\n"," 87% 10356/11873 [00:37<00:05, 289.01it/s]\u001b[A\n"," 87% 10385/11873 [00:37<00:05, 288.85it/s]\u001b[A\n"," 88% 10415/11873 [00:37<00:05, 291.27it/s]\u001b[A\n"," 88% 10445/11873 [00:37<00:05, 270.31it/s]\u001b[A\n"," 88% 10475/11873 [00:37<00:05, 277.00it/s]\u001b[A\n"," 88% 10505/11873 [00:37<00:04, 282.84it/s]\u001b[A\n"," 89% 10535/11873 [00:37<00:04, 287.77it/s]\u001b[A\n"," 89% 10564/11873 [00:37<00:04, 287.37it/s]\u001b[A\n"," 89% 10593/11873 [00:37<00:04, 285.43it/s]\u001b[A\n"," 89% 10622/11873 [00:38<00:04, 282.11it/s]\u001b[A\n"," 90% 10651/11873 [00:38<00:04, 281.18it/s]\u001b[A\n"," 90% 10681/11873 [00:38<00:04, 285.59it/s]\u001b[A\n"," 90% 10711/11873 [00:38<00:04, 287.71it/s]\u001b[A\n"," 90% 10741/11873 [00:38<00:03, 289.73it/s]\u001b[A\n"," 91% 10771/11873 [00:38<00:03, 290.21it/s]\u001b[A\n"," 91% 10801/11873 [00:38<00:03, 283.96it/s]\u001b[A\n"," 91% 10830/11873 [00:38<00:03, 265.23it/s]\u001b[A\n"," 91% 10859/11873 [00:38<00:03, 271.84it/s]\u001b[A\n"," 92% 10889/11873 [00:38<00:03, 276.82it/s]\u001b[A\n"," 92% 10917/11873 [00:39<00:03, 277.43it/s]\u001b[A\n"," 92% 10945/11873 [00:39<00:03, 276.46it/s]\u001b[A\n"," 92% 10976/11873 [00:39<00:03, 284.55it/s]\u001b[A\n"," 93% 11006/11873 [00:39<00:03, 287.72it/s]\u001b[A\n"," 93% 11037/11873 [00:39<00:02, 293.43it/s]\u001b[A\n"," 93% 11068/11873 [00:39<00:02, 296.17it/s]\u001b[A\n"," 93% 11098/11873 [00:39<00:02, 292.85it/s]\u001b[A\n"," 94% 11128/11873 [00:39<00:02, 294.08it/s]\u001b[A\n"," 94% 11158/11873 [00:39<00:02, 293.82it/s]\u001b[A\n"," 94% 11188/11873 [00:40<00:02, 293.44it/s]\u001b[A\n"," 94% 11218/11873 [00:40<00:02, 294.01it/s]\u001b[A\n"," 95% 11248/11873 [00:40<00:02, 293.01it/s]\u001b[A\n"," 95% 11278/11873 [00:40<00:02, 286.91it/s]\u001b[A\n"," 95% 11307/11873 [00:40<00:01, 285.97it/s]\u001b[A\n"," 95% 11337/11873 [00:40<00:01, 287.75it/s]\u001b[A\n"," 96% 11366/11873 [00:40<00:01, 287.76it/s]\u001b[A\n"," 96% 11396/11873 [00:40<00:01, 290.14it/s]\u001b[A\n"," 96% 11426/11873 [00:40<00:01, 282.53it/s]\u001b[A\n"," 96% 11455/11873 [00:40<00:01, 274.69it/s]\u001b[A\n"," 97% 11483/11873 [00:41<00:01, 266.41it/s]\u001b[A\n"," 97% 11510/11873 [00:41<00:01, 265.02it/s]\u001b[A\n"," 97% 11537/11873 [00:41<00:01, 263.04it/s]\u001b[A\n"," 97% 11565/11873 [00:41<00:01, 266.80it/s]\u001b[A\n"," 98% 11594/11873 [00:41<00:01, 273.12it/s]\u001b[A\n"," 98% 11623/11873 [00:41<00:00, 277.15it/s]\u001b[A\n"," 98% 11653/11873 [00:41<00:00, 282.33it/s]\u001b[A\n"," 98% 11682/11873 [00:41<00:00, 278.16it/s]\u001b[A\n"," 99% 11711/11873 [00:41<00:00, 281.47it/s]\u001b[A\n"," 99% 11740/11873 [00:42<00:00, 277.23it/s]\u001b[A\n"," 99% 11769/11873 [00:42<00:00, 279.42it/s]\u001b[A\n"," 99% 11800/11873 [00:42<00:00, 285.63it/s]\u001b[A\n","100% 11831/11873 [00:42<00:00, 291.13it/s]\u001b[A\n","100% 11873/11873 [00:42<00:00, 279.70it/s]\n","03/29/2022 09:45:28 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","03/29/2022 09:45:28 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","03/29/2022 09:45:31 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","03/29/2022 09:45:35 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1522/1522 [04:25<00:00,  5.73it/s]\n","***** eval metrics *****\n","  epoch                  =     3.0\n","  eval_HasAns_exact      = 74.9157\n","  eval_HasAns_f1         = 81.4667\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 82.8259\n","  eval_NoAns_f1          = 82.8259\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 78.8849\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 82.1557\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 78.8764\n","  eval_f1                = 82.1473\n","  eval_samples           =   12171\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-03-29 09:45:36,214 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path albert-base-v2 \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r43pSZh0M5sy","executionInfo":{"status":"ok","timestamp":1648577687522,"user_tz":240,"elapsed":11467106,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"1c75c54c-5c55-4c33-e56e-5019e7f18929"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["03/29/2022 15:03:44 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","03/29/2022 15:03:44 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Mar29_15-03-44_eda83b95527c,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","03/29/2022 15:03:45 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmppwtlm178\n","Downloading builder script: 5.28kB [00:00, 5.47MB/s]       \n","03/29/2022 15:03:45 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/29/2022 15:03:45 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/29/2022 15:03:45 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpbj8qztmz\n","Downloading metadata: 2.40kB [00:00, 2.45MB/s]       \n","03/29/2022 15:03:45 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/29/2022 15:03:45 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/29/2022 15:03:45 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","03/29/2022 15:03:45 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/29/2022 15:03:45 - INFO - datasets.builder - Generating dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","03/29/2022 15:03:45 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]03/29/2022 15:03:46 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp7mf91iat\n","\n","Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n","Downloading data:  75% 7.20M/9.55M [00:00<00:00, 72.0MB/s]\u001b[A\n","Downloading data: 14.8MB [00:00, 74.4MB/s]                \u001b[A\n","Downloading data: 22.2MB [00:00, 74.4MB/s]\u001b[A\n","Downloading data: 29.8MB [00:00, 74.8MB/s]\u001b[A\n","Downloading data: 42.1MB [00:00, 74.8MB/s]\n","03/29/2022 15:03:47 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","03/29/2022 15:03:47 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","Downloading data files:  50% 1/2 [00:01<00:01,  1.60s/it]03/29/2022 15:03:47 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpldexuok_\n","\n","Downloading data: 4.37MB [00:00, 81.9MB/s]      \n","03/29/2022 15:03:47 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","03/29/2022 15:03:47 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","Downloading data files: 100% 2/2 [00:01<00:00,  1.03it/s]\n","03/29/2022 15:03:47 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","03/29/2022 15:03:47 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1147.87it/s]\n","03/29/2022 15:03:47 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n","03/29/2022 15:03:47 - INFO - datasets.builder - Generating train split\n","03/29/2022 15:04:01 - INFO - datasets.builder - Generating validation split\n","03/29/2022 15:04:02 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 270.43it/s]\n","[INFO|hub.py:583] 2022-03-29 15:04:03,018 >> https://huggingface.co/albert-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp21ckpfna\n","Downloading: 100% 684/684 [00:00<00:00, 532kB/s]\n","[INFO|hub.py:587] 2022-03-29 15:04:03,364 >> storing https://huggingface.co/albert-base-v2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|hub.py:595] 2022-03-29 15:04:03,365 >> creating metadata file for /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:653] 2022-03-29 15:04:03,365 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-29 15:04:03,366 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-03-29 15:04:03,714 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:653] 2022-03-29 15:04:04,060 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-29 15:04:04,061 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-03-29 15:04:04,765 >> https://huggingface.co/albert-base-v2/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_zokpb9e\n","Downloading: 100% 742k/742k [00:00<00:00, 1.57MB/s]\n","[INFO|hub.py:587] 2022-03-29 15:04:05,600 >> storing https://huggingface.co/albert-base-v2/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:595] 2022-03-29 15:04:05,600 >> creating metadata file for /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:583] 2022-03-29 15:04:05,945 >> https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp58zl4o97\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 2.67MB/s]\n","[INFO|hub.py:587] 2022-03-29 15:04:06,791 >> storing https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|hub.py:595] 2022-03-29 15:04:06,792 >> creating metadata file for /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 15:04:07,839 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 15:04:07,839 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 15:04:07,839 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 15:04:07,839 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 15:04:07,839 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:653] 2022-03-29 15:04:08,186 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-03-29 15:04:08,187 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-03-29 15:04:08,604 >> https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphjxyr6mz\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 49.7MB/s]\n","[INFO|hub.py:587] 2022-03-29 15:04:09,642 >> storing https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|hub.py:595] 2022-03-29 15:04:09,642 >> creating metadata file for /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1771] 2022-03-29 15:04:09,643 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[WARNING|modeling_utils.py:2049] 2022-03-29 15:04:09,863 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias', 'predictions.decoder.weight']\n","- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-03-29 15:04:09,863 >> Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]03/29/2022 15:04:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-ce78ca212e19d384.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:57<00:00,  2.27ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]03/29/2022 15:05:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-ecb8a4cb36e8b3a5.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:14<00:00,  6.21s/ba]\n","03/29/2022 15:06:22 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp1uwxznu9\n","Downloading builder script: 6.46kB [00:00, 4.01MB/s]       \n","03/29/2022 15:06:22 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/29/2022 15:06:22 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/29/2022 15:06:23 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmptuoghr2k\n","Downloading extra modules: 11.3kB [00:00, 11.5MB/s]       \n","03/29/2022 15:06:23 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","03/29/2022 15:06:23 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1288] 2022-03-29 15:06:35,281 >> ***** Running training *****\n","[INFO|trainer.py:1289] 2022-03-29 15:06:35,281 >>   Num examples = 131958\n","[INFO|trainer.py:1290] 2022-03-29 15:06:35,281 >>   Num Epochs = 2\n","[INFO|trainer.py:1291] 2022-03-29 15:06:35,281 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1292] 2022-03-29 15:06:35,281 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1293] 2022-03-29 15:06:35,281 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1294] 2022-03-29 15:06:35,281 >>   Total optimization steps = 16496\n","{'loss': 1.6761, 'learning_rate': 1.9393792434529584e-05, 'epoch': 0.06}\n","  3% 500/16496 [05:34<2:58:18,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 15:12:09,700 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:440] 2022-03-29 15:12:09,701 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:12:09,780 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:12:09,781 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:12:09,781 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.2235, 'learning_rate': 1.8787584869059167e-05, 'epoch': 0.12}\n","  6% 1000/16496 [11:08<2:52:34,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 15:17:44,237 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:440] 2022-03-29 15:17:44,238 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:17:44,313 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:17:44,313 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:17:44,314 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.1294, 'learning_rate': 1.818137730358875e-05, 'epoch': 0.18}\n","  9% 1500/16496 [16:43<2:46:57,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 15:23:18,648 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:440] 2022-03-29 15:23:18,649 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:23:18,728 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:23:18,729 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:23:18,729 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.0556, 'learning_rate': 1.7575169738118333e-05, 'epoch': 0.24}\n"," 12% 2000/16496 [22:17<2:41:29,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 15:28:53,071 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:440] 2022-03-29 15:28:53,072 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:28:53,150 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:28:53,150 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:28:53,151 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.0363, 'learning_rate': 1.6968962172647916e-05, 'epoch': 0.3}\n"," 15% 2500/16496 [27:52<2:36:00,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 15:34:27,446 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:440] 2022-03-29 15:34:27,447 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:34:27,527 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:34:27,528 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:34:27,528 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 0.9971, 'learning_rate': 1.63627546071775e-05, 'epoch': 0.36}\n"," 18% 3000/16496 [33:26<2:30:03,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 15:40:01,721 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:440] 2022-03-29 15:40:01,722 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:40:01,800 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:40:01,800 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:40:01,800 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.9857, 'learning_rate': 1.575654704170708e-05, 'epoch': 0.42}\n"," 21% 3500/16496 [39:00<2:24:29,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 15:45:36,008 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:440] 2022-03-29 15:45:36,009 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:45:36,090 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:45:36,091 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:45:36,091 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.9766, 'learning_rate': 1.5150339476236664e-05, 'epoch': 0.48}\n"," 24% 4000/16496 [44:34<2:19:02,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 15:51:10,239 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:440] 2022-03-29 15:51:10,240 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:51:10,320 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:51:10,321 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:51:10,321 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.937, 'learning_rate': 1.4544131910766249e-05, 'epoch': 0.55}\n"," 27% 4500/16496 [50:09<2:13:34,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 15:56:44,583 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:440] 2022-03-29 15:56:44,584 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:56:44,666 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:56:44,666 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:56:44,667 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.9428, 'learning_rate': 1.393792434529583e-05, 'epoch': 0.61}\n"," 30% 5000/16496 [55:43<2:08:08,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 16:02:19,028 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:440] 2022-03-29 16:02:19,029 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:02:19,108 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:02:19,109 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:02:19,109 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.8887, 'learning_rate': 1.3331716779825415e-05, 'epoch': 0.67}\n"," 33% 5500/16496 [1:01:18<2:02:26,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 16:07:53,483 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:440] 2022-03-29 16:07:53,484 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:07:53,567 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:07:53,568 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:07:53,568 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.9181, 'learning_rate': 1.2725509214354996e-05, 'epoch': 0.73}\n"," 36% 6000/16496 [1:06:52<1:56:58,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 16:13:27,940 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:440] 2022-03-29 16:13:27,941 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:13:28,021 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:13:28,022 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:13:28,022 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.8693, 'learning_rate': 1.2119301648884579e-05, 'epoch': 0.79}\n"," 39% 6500/16496 [1:12:27<1:51:19,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 16:19:02,337 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:440] 2022-03-29 16:19:02,338 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:19:02,422 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:19:02,423 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:19:02,423 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.8692, 'learning_rate': 1.1513094083414163e-05, 'epoch': 0.85}\n"," 42% 7000/16496 [1:18:01<1:45:42,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 16:24:36,688 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:440] 2022-03-29 16:24:36,689 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:24:36,771 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:24:36,772 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:24:36,772 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.845, 'learning_rate': 1.0906886517943744e-05, 'epoch': 0.91}\n"," 45% 7500/16496 [1:23:35<1:40:17,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 16:30:11,031 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:440] 2022-03-29 16:30:11,032 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:30:11,112 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:30:11,112 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:30:11,112 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.844, 'learning_rate': 1.0300678952473329e-05, 'epoch': 0.97}\n"," 48% 8000/16496 [1:29:10<1:34:34,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 16:35:45,351 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:440] 2022-03-29 16:35:45,352 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:35:45,436 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:35:45,437 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:35:45,437 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.7422, 'learning_rate': 9.69447138700291e-06, 'epoch': 1.03}\n"," 52% 8500/16496 [1:34:43<1:28:55,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 16:41:19,215 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:440] 2022-03-29 16:41:19,216 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:41:19,296 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:41:19,297 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:41:19,297 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.6689, 'learning_rate': 9.088263821532493e-06, 'epoch': 1.09}\n"," 55% 9000/16496 [1:40:18<1:23:24,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 16:46:53,507 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:440] 2022-03-29 16:46:53,508 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:46:53,590 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:46:53,591 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:46:53,591 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.6753, 'learning_rate': 8.482056256062078e-06, 'epoch': 1.15}\n"," 58% 9500/16496 [1:45:52<1:17:55,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 16:52:27,827 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:440] 2022-03-29 16:52:27,828 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:52:27,914 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:52:27,915 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:52:27,915 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.6434, 'learning_rate': 7.875848690591659e-06, 'epoch': 1.21}\n"," 61% 10000/16496 [1:51:26<1:12:19,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 16:58:02,146 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:440] 2022-03-29 16:58:02,147 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:58:02,226 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:58:02,227 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:58:02,227 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6573, 'learning_rate': 7.2696411251212416e-06, 'epoch': 1.27}\n"," 64% 10500/16496 [1:57:01<1:06:45,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 17:03:36,460 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:440] 2022-03-29 17:03:36,461 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:03:36,543 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:03:36,544 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:03:36,544 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.6584, 'learning_rate': 6.6634335596508244e-06, 'epoch': 1.33}\n"," 67% 11000/16496 [2:02:35<1:01:11,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 17:09:10,757 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11000\n","[INFO|configuration_utils.py:440] 2022-03-29 17:09:10,758 >> Configuration saved in /tmp/debug_squad/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:09:10,841 >> Model weights saved in /tmp/debug_squad/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:09:10,841 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:09:10,842 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.627, 'learning_rate': 6.057225994180408e-06, 'epoch': 1.39}\n"," 70% 11500/16496 [2:08:09<55:39,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 17:14:45,056 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11500\n","[INFO|configuration_utils.py:440] 2022-03-29 17:14:45,057 >> Configuration saved in /tmp/debug_squad/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:14:45,139 >> Model weights saved in /tmp/debug_squad/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:14:45,139 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:14:45,140 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.6345, 'learning_rate': 5.451018428709991e-06, 'epoch': 1.45}\n"," 73% 12000/16496 [2:13:44<50:01,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 17:20:19,311 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12000\n","[INFO|configuration_utils.py:440] 2022-03-29 17:20:19,312 >> Configuration saved in /tmp/debug_squad/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:20:19,389 >> Model weights saved in /tmp/debug_squad/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:20:19,389 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:20:19,389 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.6471, 'learning_rate': 4.844810863239574e-06, 'epoch': 1.52}\n"," 76% 12500/16496 [2:19:18<44:27,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 17:25:53,413 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12500\n","[INFO|configuration_utils.py:440] 2022-03-29 17:25:53,414 >> Configuration saved in /tmp/debug_squad/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:25:53,491 >> Model weights saved in /tmp/debug_squad/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:25:53,492 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:25:53,492 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.6357, 'learning_rate': 4.238603297769157e-06, 'epoch': 1.58}\n"," 79% 13000/16496 [2:24:52<38:56,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 17:31:27,452 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13000\n","[INFO|configuration_utils.py:440] 2022-03-29 17:31:27,453 >> Configuration saved in /tmp/debug_squad/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:31:27,529 >> Model weights saved in /tmp/debug_squad/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:31:27,530 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:31:27,530 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.618, 'learning_rate': 3.6323957322987395e-06, 'epoch': 1.64}\n"," 82% 13500/16496 [2:30:26<33:20,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 17:37:01,568 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13500\n","[INFO|configuration_utils.py:440] 2022-03-29 17:37:01,569 >> Configuration saved in /tmp/debug_squad/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:37:01,646 >> Model weights saved in /tmp/debug_squad/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:37:01,647 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:37:01,647 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.6285, 'learning_rate': 3.026188166828322e-06, 'epoch': 1.7}\n"," 85% 14000/16496 [2:36:00<27:49,  1.49it/s][INFO|trainer.py:2162] 2022-03-29 17:42:35,677 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14000\n","[INFO|configuration_utils.py:440] 2022-03-29 17:42:35,678 >> Configuration saved in /tmp/debug_squad/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:42:35,756 >> Model weights saved in /tmp/debug_squad/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:42:35,756 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:42:35,757 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.6278, 'learning_rate': 2.4199806013579053e-06, 'epoch': 1.76}\n"," 88% 14500/16496 [2:41:34<22:14,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 17:48:09,718 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14500\n","[INFO|configuration_utils.py:440] 2022-03-29 17:48:09,719 >> Configuration saved in /tmp/debug_squad/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:48:09,794 >> Model weights saved in /tmp/debug_squad/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:48:09,794 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:48:09,795 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.6103, 'learning_rate': 1.8137730358874881e-06, 'epoch': 1.82}\n"," 91% 15000/16496 [2:47:08<16:36,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 17:53:43,660 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15000\n","[INFO|configuration_utils.py:440] 2022-03-29 17:53:43,661 >> Configuration saved in /tmp/debug_squad/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:53:43,730 >> Model weights saved in /tmp/debug_squad/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:53:43,731 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:53:43,731 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.5981, 'learning_rate': 1.2075654704170707e-06, 'epoch': 1.88}\n"," 94% 15500/16496 [2:52:42<11:04,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 17:59:17,444 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15500\n","[INFO|configuration_utils.py:440] 2022-03-29 17:59:17,445 >> Configuration saved in /tmp/debug_squad/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:59:17,518 >> Model weights saved in /tmp/debug_squad/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:59:17,519 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:59:17,519 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.6003, 'learning_rate': 6.013579049466538e-07, 'epoch': 1.94}\n"," 97% 16000/16496 [2:58:16<05:30,  1.50it/s][INFO|trainer.py:2162] 2022-03-29 18:04:51,486 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-16000\n","[INFO|configuration_utils.py:440] 2022-03-29 18:04:51,487 >> Configuration saved in /tmp/debug_squad/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 18:04:51,562 >> Model weights saved in /tmp/debug_squad/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 18:04:51,563 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 18:04:51,563 >> Special tokens file saved in /tmp/debug_squad/checkpoint-16000/special_tokens_map.json\n","100% 16496/16496 [3:03:47<00:00,  1.83it/s][INFO|trainer.py:1526] 2022-03-29 18:10:22,416 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 11027.135, 'train_samples_per_second': 23.933, 'train_steps_per_second': 1.496, 'train_loss': 0.8203062507044795, 'epoch': 2.0}\n","100% 16496/16496 [3:03:47<00:00,  1.50it/s]\n","[INFO|trainer.py:2162] 2022-03-29 18:10:22,419 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:440] 2022-03-29 18:10:22,420 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 18:10:22,499 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 18:10:22,500 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 18:10:22,500 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.8203\n","  train_runtime            = 3:03:47.13\n","  train_samples            =     131958\n","  train_samples_per_second =     23.933\n","  train_steps_per_second   =      1.496\n","03/29/2022 18:10:22 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:571] 2022-03-29 18:10:22,511 >> The following columns in the evaluation set  don't have a corresponding argument in `AlbertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `AlbertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2412] 2022-03-29 18:10:22,513 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2414] 2022-03-29 18:10:22,514 >>   Num examples = 12171\n","[INFO|trainer.py:2417] 2022-03-29 18:10:22,514 >>   Batch size = 8\n","100% 1521/1522 [03:19<00:00,  7.60it/s]03/29/2022 18:13:57 - INFO - utils_qa - Post-processing 11873 example predictions split into 12171 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 36/11873 [00:00<00:33, 351.95it/s]\u001b[A\n","  1% 72/11873 [00:00<00:36, 320.56it/s]\u001b[A\n","  1% 110/11873 [00:00<00:34, 343.26it/s]\u001b[A\n","  1% 149/11873 [00:00<00:32, 359.48it/s]\u001b[A\n","  2% 188/11873 [00:00<00:31, 369.75it/s]\u001b[A\n","  2% 227/11873 [00:00<00:31, 374.72it/s]\u001b[A\n","  2% 265/11873 [00:00<00:31, 371.55it/s]\u001b[A\n","  3% 303/11873 [00:00<00:30, 373.43it/s]\u001b[A\n","  3% 341/11873 [00:00<00:31, 367.48it/s]\u001b[A\n","  3% 379/11873 [00:01<00:31, 369.42it/s]\u001b[A\n","  4% 417/11873 [00:01<00:30, 371.09it/s]\u001b[A\n","  4% 455/11873 [00:01<00:30, 370.47it/s]\u001b[A\n","  4% 493/11873 [00:01<00:30, 367.98it/s]\u001b[A\n","  4% 530/11873 [00:01<00:30, 366.81it/s]\u001b[A\n","  5% 567/11873 [00:01<00:30, 365.98it/s]\u001b[A\n","  5% 604/11873 [00:01<00:31, 362.44it/s]\u001b[A\n","  5% 642/11873 [00:01<00:30, 366.88it/s]\u001b[A\n","  6% 680/11873 [00:01<00:30, 370.61it/s]\u001b[A\n","  6% 718/11873 [00:01<00:30, 366.44it/s]\u001b[A\n","  6% 755/11873 [00:02<00:30, 366.32it/s]\u001b[A\n","  7% 793/11873 [00:02<00:30, 368.03it/s]\u001b[A\n","  7% 831/11873 [00:02<00:29, 370.69it/s]\u001b[A\n","  7% 870/11873 [00:02<00:29, 375.79it/s]\u001b[A\n","  8% 910/11873 [00:02<00:28, 382.26it/s]\u001b[A\n","  8% 949/11873 [00:02<00:28, 378.63it/s]\u001b[A\n","  8% 987/11873 [00:02<00:29, 371.42it/s]\u001b[A\n","  9% 1025/11873 [00:02<00:30, 351.40it/s]\u001b[A\n","  9% 1061/11873 [00:02<00:32, 335.39it/s]\u001b[A\n","  9% 1095/11873 [00:03<00:33, 319.37it/s]\u001b[A\n"," 10% 1128/11873 [00:03<00:34, 309.84it/s]\u001b[A\n"," 10% 1160/11873 [00:03<00:35, 305.50it/s]\u001b[A\n"," 10% 1191/11873 [00:03<00:35, 302.11it/s]\u001b[A\n"," 10% 1222/11873 [00:03<00:35, 299.74it/s]\u001b[A\n"," 11% 1253/11873 [00:03<00:35, 296.61it/s]\u001b[A\n"," 11% 1283/11873 [00:03<00:35, 297.15it/s]\u001b[A\n"," 11% 1313/11873 [00:03<00:35, 295.37it/s]\u001b[A\n"," 11% 1344/11873 [00:03<00:35, 298.55it/s]\u001b[A\n"," 12% 1374/11873 [00:03<00:35, 298.86it/s]\u001b[A\n"," 12% 1405/11873 [00:04<00:34, 301.00it/s]\u001b[A\n"," 12% 1436/11873 [00:04<00:34, 300.69it/s]\u001b[A\n"," 12% 1467/11873 [00:04<00:34, 299.66it/s]\u001b[A\n"," 13% 1498/11873 [00:04<00:34, 301.21it/s]\u001b[A\n"," 13% 1529/11873 [00:04<00:34, 296.77it/s]\u001b[A\n"," 13% 1559/11873 [00:04<00:34, 297.33it/s]\u001b[A\n"," 13% 1589/11873 [00:04<00:34, 295.30it/s]\u001b[A\n"," 14% 1619/11873 [00:04<00:35, 287.03it/s]\u001b[A\n"," 14% 1649/11873 [00:04<00:35, 288.23it/s]\u001b[A\n"," 14% 1679/11873 [00:05<00:35, 289.81it/s]\u001b[A\n"," 14% 1709/11873 [00:05<00:35, 290.29it/s]\u001b[A\n"," 15% 1739/11873 [00:05<00:34, 291.87it/s]\u001b[A\n"," 15% 1769/11873 [00:05<00:35, 288.61it/s]\u001b[A\n"," 15% 1798/11873 [00:05<00:35, 283.74it/s]\u001b[A\n"," 15% 1828/11873 [00:05<00:34, 287.19it/s]\u001b[A\n"," 16% 1858/11873 [00:05<00:34, 290.71it/s]\u001b[A\n"," 16% 1888/11873 [00:05<00:35, 284.85it/s]\u001b[A\n"," 16% 1917/11873 [00:05<00:34, 285.30it/s]\u001b[A\n"," 16% 1947/11873 [00:05<00:34, 287.79it/s]\u001b[A\n"," 17% 1976/11873 [00:06<00:34, 286.55it/s]\u001b[A\n"," 17% 2006/11873 [00:06<00:34, 289.17it/s]\u001b[A\n"," 17% 2037/11873 [00:06<00:33, 293.53it/s]\u001b[A\n"," 17% 2067/11873 [00:06<00:33, 292.51it/s]\u001b[A\n"," 18% 2097/11873 [00:06<00:33, 293.27it/s]\u001b[A\n"," 18% 2127/11873 [00:06<00:33, 293.57it/s]\u001b[A\n"," 18% 2157/11873 [00:06<00:33, 290.90it/s]\u001b[A\n"," 18% 2187/11873 [00:06<00:34, 284.64it/s]\u001b[A\n"," 19% 2217/11873 [00:06<00:33, 287.77it/s]\u001b[A\n"," 19% 2246/11873 [00:06<00:33, 288.21it/s]\u001b[A\n"," 19% 2275/11873 [00:07<00:33, 288.60it/s]\u001b[A\n"," 19% 2305/11873 [00:07<00:32, 290.26it/s]\u001b[A\n"," 20% 2335/11873 [00:07<00:32, 290.77it/s]\u001b[A\n"," 20% 2366/11873 [00:07<00:32, 295.87it/s]\u001b[A\n"," 20% 2396/11873 [00:07<00:31, 297.07it/s]\u001b[A\n"," 20% 2426/11873 [00:07<00:32, 295.01it/s]\u001b[A\n"," 21% 2456/11873 [00:07<00:31, 294.91it/s]\u001b[A\n"," 21% 2486/11873 [00:07<00:32, 291.22it/s]\u001b[A\n"," 21% 2516/11873 [00:07<00:32, 290.81it/s]\u001b[A\n"," 21% 2546/11873 [00:08<00:31, 292.65it/s]\u001b[A\n"," 22% 2576/11873 [00:08<00:31, 291.21it/s]\u001b[A\n"," 22% 2606/11873 [00:08<00:31, 290.64it/s]\u001b[A\n"," 22% 2636/11873 [00:08<00:31, 290.14it/s]\u001b[A\n"," 22% 2666/11873 [00:08<00:31, 288.22it/s]\u001b[A\n"," 23% 2696/11873 [00:08<00:31, 289.52it/s]\u001b[A\n"," 23% 2725/11873 [00:08<00:31, 289.17it/s]\u001b[A\n"," 23% 2754/11873 [00:08<00:32, 283.99it/s]\u001b[A\n"," 23% 2783/11873 [00:08<00:32, 283.52it/s]\u001b[A\n"," 24% 2813/11873 [00:08<00:31, 288.03it/s]\u001b[A\n"," 24% 2842/11873 [00:09<00:31, 287.18it/s]\u001b[A\n"," 24% 2873/11873 [00:09<00:30, 291.27it/s]\u001b[A\n"," 24% 2903/11873 [00:09<00:31, 287.14it/s]\u001b[A\n"," 25% 2932/11873 [00:09<00:31, 286.13it/s]\u001b[A\n"," 25% 2963/11873 [00:09<00:30, 289.89it/s]\u001b[A\n"," 25% 2992/11873 [00:09<00:30, 289.07it/s]\u001b[A\n"," 25% 3021/11873 [00:09<00:31, 282.88it/s]\u001b[A\n"," 26% 3050/11873 [00:09<00:31, 277.58it/s]\u001b[A\n"," 26% 3078/11873 [00:09<00:31, 276.75it/s]\u001b[A\n"," 26% 3106/11873 [00:09<00:32, 267.20it/s]\u001b[A\n"," 26% 3133/11873 [00:10<00:38, 224.56it/s]\u001b[A\n"," 27% 3157/11873 [00:10<00:41, 210.34it/s]\u001b[A\n"," 27% 3185/11873 [00:10<00:38, 226.22it/s]\u001b[A\n"," 27% 3215/11873 [00:10<00:35, 245.17it/s]\u001b[A\n"," 27% 3246/11873 [00:10<00:33, 261.26it/s]\u001b[A\n"," 28% 3273/11873 [00:10<00:34, 247.35it/s]\u001b[A\n"," 28% 3299/11873 [00:10<00:47, 182.17it/s]\u001b[A\n"," 28% 3321/11873 [00:11<00:53, 159.98it/s]\u001b[A\n"," 28% 3341/11873 [00:11<00:50, 167.45it/s]\u001b[A\n"," 28% 3360/11873 [00:11<00:54, 157.44it/s]\u001b[A\n"," 28% 3383/11873 [00:11<00:48, 173.35it/s]\u001b[A\n"," 29% 3412/11873 [00:11<00:41, 201.87it/s]\u001b[A\n"," 29% 3441/11873 [00:11<00:37, 224.66it/s]\u001b[A\n"," 29% 3471/11873 [00:11<00:34, 243.00it/s]\u001b[A\n"," 29% 3500/11873 [00:11<00:32, 254.76it/s]\u001b[A\n"," 30% 3530/11873 [00:12<00:31, 266.64it/s]\u001b[A\n"," 30% 3561/11873 [00:12<00:29, 277.48it/s]\u001b[A\n"," 30% 3591/11873 [00:12<00:29, 283.18it/s]\u001b[A\n"," 30% 3621/11873 [00:12<00:28, 286.67it/s]\u001b[A\n"," 31% 3650/11873 [00:12<00:28, 287.39it/s]\u001b[A\n"," 31% 3679/11873 [00:12<00:28, 283.58it/s]\u001b[A\n"," 31% 3708/11873 [00:12<00:28, 282.97it/s]\u001b[A\n"," 31% 3738/11873 [00:12<00:28, 286.69it/s]\u001b[A\n"," 32% 3768/11873 [00:12<00:28, 288.33it/s]\u001b[A\n"," 32% 3797/11873 [00:12<00:28, 287.41it/s]\u001b[A\n"," 32% 3826/11873 [00:13<00:30, 263.71it/s]\u001b[A\n"," 32% 3855/11873 [00:13<00:29, 269.00it/s]\u001b[A\n"," 33% 3884/11873 [00:13<00:29, 274.45it/s]\u001b[A\n"," 33% 3912/11873 [00:13<00:29, 271.10it/s]\u001b[A\n"," 33% 3940/11873 [00:13<00:31, 253.86it/s]\u001b[A\n"," 33% 3969/11873 [00:13<00:30, 261.14it/s]\u001b[A\n"," 34% 3997/11873 [00:13<00:29, 264.56it/s]\u001b[A\n"," 34% 4027/11873 [00:13<00:28, 272.77it/s]\u001b[A\n"," 34% 4057/11873 [00:13<00:28, 278.44it/s]\u001b[A\n"," 34% 4088/11873 [00:14<00:27, 285.77it/s]\u001b[A\n"," 35% 4119/11873 [00:14<00:26, 290.67it/s]\u001b[A\n"," 35% 4149/11873 [00:14<00:28, 271.76it/s]\u001b[A\n"," 35% 4177/11873 [00:14<00:31, 247.74it/s]\u001b[A\n"," 35% 4205/11873 [00:14<00:30, 255.41it/s]\u001b[A\n"," 36% 4236/11873 [00:14<00:28, 268.29it/s]\u001b[A\n"," 36% 4266/11873 [00:14<00:27, 275.86it/s]\u001b[A\n"," 36% 4296/11873 [00:14<00:26, 281.04it/s]\u001b[A\n"," 36% 4327/11873 [00:14<00:26, 287.17it/s]\u001b[A\n"," 37% 4357/11873 [00:14<00:25, 289.63it/s]\u001b[A\n"," 37% 4387/11873 [00:15<00:25, 289.83it/s]\u001b[A\n"," 37% 4417/11873 [00:15<00:29, 253.48it/s]\u001b[A\n"," 37% 4444/11873 [00:15<00:31, 235.73it/s]\u001b[A\n"," 38% 4475/11873 [00:15<00:29, 252.95it/s]\u001b[A\n"," 38% 4505/11873 [00:15<00:27, 263.41it/s]\u001b[A\n"," 38% 4535/11873 [00:15<00:27, 271.22it/s]\u001b[A\n"," 38% 4565/11873 [00:15<00:26, 278.40it/s]\u001b[A\n"," 39% 4595/11873 [00:15<00:25, 283.99it/s]\u001b[A\n"," 39% 4625/11873 [00:15<00:25, 286.04it/s]\u001b[A\n"," 39% 4655/11873 [00:16<00:24, 289.49it/s]\u001b[A\n"," 39% 4686/11873 [00:16<00:24, 292.72it/s]\u001b[A\n"," 40% 4716/11873 [00:16<00:24, 288.51it/s]\u001b[A\n"," 40% 4745/11873 [00:16<00:24, 288.16it/s]\u001b[A\n"," 40% 4775/11873 [00:16<00:24, 290.22it/s]\u001b[A\n"," 40% 4805/11873 [00:16<00:25, 281.30it/s]\u001b[A\n"," 41% 4834/11873 [00:16<00:25, 281.43it/s]\u001b[A\n"," 41% 4864/11873 [00:16<00:24, 284.16it/s]\u001b[A\n"," 41% 4893/11873 [00:16<00:24, 284.46it/s]\u001b[A\n"," 41% 4922/11873 [00:17<00:24, 285.78it/s]\u001b[A\n"," 42% 4952/11873 [00:17<00:24, 287.57it/s]\u001b[A\n"," 42% 4982/11873 [00:17<00:23, 288.34it/s]\u001b[A\n"," 42% 5011/11873 [00:17<00:23, 287.14it/s]\u001b[A\n"," 42% 5040/11873 [00:17<00:23, 287.42it/s]\u001b[A\n"," 43% 5070/11873 [00:17<00:23, 290.42it/s]\u001b[A\n"," 43% 5102/11873 [00:17<00:22, 296.50it/s]\u001b[A\n"," 43% 5132/11873 [00:17<00:22, 297.44it/s]\u001b[A\n"," 43% 5162/11873 [00:17<00:22, 294.65it/s]\u001b[A\n"," 44% 5192/11873 [00:17<00:22, 295.53it/s]\u001b[A\n"," 44% 5223/11873 [00:18<00:22, 297.90it/s]\u001b[A\n"," 44% 5253/11873 [00:18<00:22, 292.06it/s]\u001b[A\n"," 44% 5283/11873 [00:18<00:24, 270.29it/s]\u001b[A\n"," 45% 5313/11873 [00:18<00:23, 277.05it/s]\u001b[A\n"," 45% 5343/11873 [00:18<00:23, 281.60it/s]\u001b[A\n"," 45% 5372/11873 [00:18<00:23, 282.57it/s]\u001b[A\n"," 46% 5403/11873 [00:18<00:22, 288.32it/s]\u001b[A\n"," 46% 5432/11873 [00:18<00:22, 288.38it/s]\u001b[A\n"," 46% 5461/11873 [00:18<00:22, 288.74it/s]\u001b[A\n"," 46% 5490/11873 [00:18<00:22, 288.73it/s]\u001b[A\n"," 46% 5520/11873 [00:19<00:21, 291.66it/s]\u001b[A\n"," 47% 5550/11873 [00:19<00:21, 291.42it/s]\u001b[A\n"," 47% 5580/11873 [00:19<00:21, 291.65it/s]\u001b[A\n"," 47% 5610/11873 [00:19<00:21, 289.27it/s]\u001b[A\n"," 48% 5640/11873 [00:19<00:21, 290.16it/s]\u001b[A\n"," 48% 5671/11873 [00:19<00:21, 293.53it/s]\u001b[A\n"," 48% 5701/11873 [00:19<00:21, 289.25it/s]\u001b[A\n"," 48% 5732/11873 [00:19<00:20, 292.51it/s]\u001b[A\n"," 49% 5762/11873 [00:19<00:21, 290.67it/s]\u001b[A\n"," 49% 5792/11873 [00:20<00:20, 293.15it/s]\u001b[A\n"," 49% 5823/11873 [00:20<00:20, 297.14it/s]\u001b[A\n"," 49% 5853/11873 [00:20<00:20, 297.96it/s]\u001b[A\n"," 50% 5883/11873 [00:20<00:20, 298.22it/s]\u001b[A\n"," 50% 5913/11873 [00:20<00:20, 296.40it/s]\u001b[A\n"," 50% 5943/11873 [00:20<00:20, 294.77it/s]\u001b[A\n"," 50% 5973/11873 [00:20<00:19, 295.53it/s]\u001b[A\n"," 51% 6003/11873 [00:20<00:20, 293.44it/s]\u001b[A\n"," 51% 6033/11873 [00:20<00:19, 293.66it/s]\u001b[A\n"," 51% 6063/11873 [00:20<00:19, 292.29it/s]\u001b[A\n"," 51% 6093/11873 [00:21<00:19, 292.28it/s]\u001b[A\n"," 52% 6123/11873 [00:21<00:19, 293.76it/s]\u001b[A\n"," 52% 6153/11873 [00:21<00:19, 293.39it/s]\u001b[A\n"," 52% 6184/11873 [00:21<00:19, 296.43it/s]\u001b[A\n"," 52% 6214/11873 [00:21<00:19, 297.26it/s]\u001b[A\n"," 53% 6245/11873 [00:21<00:18, 298.58it/s]\u001b[A\n"," 53% 6276/11873 [00:21<00:18, 300.98it/s]\u001b[A\n"," 53% 6307/11873 [00:21<00:18, 300.49it/s]\u001b[A\n"," 53% 6338/11873 [00:21<00:18, 300.93it/s]\u001b[A\n"," 54% 6369/11873 [00:21<00:18, 296.69it/s]\u001b[A\n"," 54% 6399/11873 [00:22<00:18, 291.36it/s]\u001b[A\n"," 54% 6429/11873 [00:22<00:18, 288.33it/s]\u001b[A\n"," 54% 6458/11873 [00:22<00:18, 287.65it/s]\u001b[A\n"," 55% 6488/11873 [00:22<00:18, 289.96it/s]\u001b[A\n"," 55% 6518/11873 [00:22<00:18, 290.53it/s]\u001b[A\n"," 55% 6548/11873 [00:22<00:18, 288.71it/s]\u001b[A\n"," 55% 6578/11873 [00:22<00:18, 290.01it/s]\u001b[A\n"," 56% 6608/11873 [00:22<00:18, 290.29it/s]\u001b[A\n"," 56% 6638/11873 [00:22<00:17, 291.01it/s]\u001b[A\n"," 56% 6668/11873 [00:23<00:17, 291.56it/s]\u001b[A\n"," 56% 6698/11873 [00:23<00:17, 289.49it/s]\u001b[A\n"," 57% 6727/11873 [00:23<00:20, 250.40it/s]\u001b[A\n"," 57% 6758/11873 [00:23<00:19, 265.83it/s]\u001b[A\n"," 57% 6787/11873 [00:23<00:18, 271.40it/s]\u001b[A\n"," 57% 6817/11873 [00:23<00:18, 279.12it/s]\u001b[A\n"," 58% 6846/11873 [00:23<00:17, 279.88it/s]\u001b[A\n"," 58% 6875/11873 [00:23<00:17, 281.06it/s]\u001b[A\n"," 58% 6904/11873 [00:23<00:17, 278.09it/s]\u001b[A\n"," 58% 6934/11873 [00:23<00:17, 283.71it/s]\u001b[A\n"," 59% 6964/11873 [00:24<00:17, 286.66it/s]\u001b[A\n"," 59% 6994/11873 [00:24<00:16, 290.00it/s]\u001b[A\n"," 59% 7024/11873 [00:24<00:16, 287.73it/s]\u001b[A\n"," 59% 7054/11873 [00:24<00:16, 290.50it/s]\u001b[A\n"," 60% 7084/11873 [00:24<00:16, 286.87it/s]\u001b[A\n"," 60% 7113/11873 [00:24<00:16, 287.36it/s]\u001b[A\n"," 60% 7144/11873 [00:24<00:16, 291.98it/s]\u001b[A\n"," 60% 7174/11873 [00:24<00:16, 291.89it/s]\u001b[A\n"," 61% 7204/11873 [00:24<00:15, 292.96it/s]\u001b[A\n"," 61% 7234/11873 [00:25<00:15, 292.57it/s]\u001b[A\n"," 61% 7264/11873 [00:25<00:16, 287.15it/s]\u001b[A\n"," 61% 7294/11873 [00:25<00:15, 288.41it/s]\u001b[A\n"," 62% 7323/11873 [00:25<00:15, 285.55it/s]\u001b[A\n"," 62% 7354/11873 [00:25<00:15, 290.33it/s]\u001b[A\n"," 62% 7384/11873 [00:25<00:15, 292.38it/s]\u001b[A\n"," 62% 7414/11873 [00:25<00:15, 281.25it/s]\u001b[A\n"," 63% 7443/11873 [00:25<00:16, 271.49it/s]\u001b[A\n"," 63% 7474/11873 [00:25<00:15, 281.80it/s]\u001b[A\n"," 63% 7505/11873 [00:25<00:15, 287.79it/s]\u001b[A\n"," 63% 7535/11873 [00:26<00:14, 290.53it/s]\u001b[A\n"," 64% 7566/11873 [00:26<00:14, 294.91it/s]\u001b[A\n"," 64% 7596/11873 [00:26<00:14, 292.77it/s]\u001b[A\n"," 64% 7626/11873 [00:26<00:14, 292.37it/s]\u001b[A\n"," 64% 7656/11873 [00:26<00:14, 282.47it/s]\u001b[A\n"," 65% 7685/11873 [00:26<00:14, 284.17it/s]\u001b[A\n"," 65% 7714/11873 [00:26<00:14, 283.12it/s]\u001b[A\n"," 65% 7743/11873 [00:26<00:14, 283.17it/s]\u001b[A\n"," 65% 7772/11873 [00:26<00:14, 284.67it/s]\u001b[A\n"," 66% 7802/11873 [00:26<00:14, 287.57it/s]\u001b[A\n"," 66% 7831/11873 [00:27<00:14, 287.96it/s]\u001b[A\n"," 66% 7860/11873 [00:27<00:13, 286.73it/s]\u001b[A\n"," 66% 7889/11873 [00:27<00:15, 265.59it/s]\u001b[A\n"," 67% 7919/11873 [00:27<00:14, 273.58it/s]\u001b[A\n"," 67% 7950/11873 [00:27<00:13, 282.44it/s]\u001b[A\n"," 67% 7979/11873 [00:27<00:14, 277.66it/s]\u001b[A\n"," 67% 8008/11873 [00:27<00:13, 281.14it/s]\u001b[A\n"," 68% 8038/11873 [00:27<00:13, 285.48it/s]\u001b[A\n"," 68% 8068/11873 [00:27<00:13, 288.07it/s]\u001b[A\n"," 68% 8098/11873 [00:28<00:13, 289.03it/s]\u001b[A\n"," 68% 8127/11873 [00:28<00:13, 286.68it/s]\u001b[A\n"," 69% 8156/11873 [00:28<00:12, 286.69it/s]\u001b[A\n"," 69% 8185/11873 [00:28<00:12, 285.57it/s]\u001b[A\n"," 69% 8214/11873 [00:28<00:12, 284.33it/s]\u001b[A\n"," 69% 8244/11873 [00:28<00:12, 288.10it/s]\u001b[A\n"," 70% 8274/11873 [00:28<00:12, 290.93it/s]\u001b[A\n"," 70% 8305/11873 [00:28<00:12, 294.57it/s]\u001b[A\n"," 70% 8335/11873 [00:28<00:12, 292.70it/s]\u001b[A\n"," 70% 8365/11873 [00:28<00:11, 293.05it/s]\u001b[A\n"," 71% 8395/11873 [00:29<00:11, 293.66it/s]\u001b[A\n"," 71% 8425/11873 [00:29<00:12, 284.87it/s]\u001b[A\n"," 71% 8454/11873 [00:29<00:12, 284.12it/s]\u001b[A\n"," 71% 8484/11873 [00:29<00:11, 286.07it/s]\u001b[A\n"," 72% 8514/11873 [00:29<00:11, 289.34it/s]\u001b[A\n"," 72% 8544/11873 [00:29<00:11, 291.76it/s]\u001b[A\n"," 72% 8574/11873 [00:29<00:11, 294.02it/s]\u001b[A\n"," 72% 8604/11873 [00:29<00:11, 293.49it/s]\u001b[A\n"," 73% 8634/11873 [00:29<00:11, 291.27it/s]\u001b[A\n"," 73% 8665/11873 [00:30<00:10, 294.06it/s]\u001b[A\n"," 73% 8695/11873 [00:30<00:10, 294.08it/s]\u001b[A\n"," 73% 8725/11873 [00:30<00:10, 294.82it/s]\u001b[A\n"," 74% 8755/11873 [00:30<00:10, 294.25it/s]\u001b[A\n"," 74% 8785/11873 [00:30<00:10, 295.55it/s]\u001b[A\n"," 74% 8815/11873 [00:30<00:10, 291.22it/s]\u001b[A\n"," 74% 8845/11873 [00:30<00:10, 292.44it/s]\u001b[A\n"," 75% 8875/11873 [00:30<00:10, 294.43it/s]\u001b[A\n"," 75% 8905/11873 [00:30<00:10, 294.44it/s]\u001b[A\n"," 75% 8935/11873 [00:30<00:10, 293.18it/s]\u001b[A\n"," 76% 8965/11873 [00:31<00:09, 292.07it/s]\u001b[A\n"," 76% 8995/11873 [00:31<00:09, 293.28it/s]\u001b[A\n"," 76% 9025/11873 [00:31<00:09, 291.62it/s]\u001b[A\n"," 76% 9055/11873 [00:31<00:09, 291.04it/s]\u001b[A\n"," 77% 9085/11873 [00:31<00:09, 291.75it/s]\u001b[A\n"," 77% 9115/11873 [00:31<00:09, 292.12it/s]\u001b[A\n"," 77% 9145/11873 [00:31<00:09, 291.10it/s]\u001b[A\n"," 77% 9175/11873 [00:31<00:09, 293.20it/s]\u001b[A\n"," 78% 9206/11873 [00:31<00:09, 295.55it/s]\u001b[A\n"," 78% 9236/11873 [00:31<00:08, 296.45it/s]\u001b[A\n"," 78% 9266/11873 [00:32<00:08, 293.62it/s]\u001b[A\n"," 78% 9296/11873 [00:32<00:08, 293.07it/s]\u001b[A\n"," 79% 9326/11873 [00:32<00:08, 290.95it/s]\u001b[A\n"," 79% 9356/11873 [00:32<00:08, 290.71it/s]\u001b[A\n"," 79% 9386/11873 [00:32<00:08, 291.92it/s]\u001b[A\n"," 79% 9416/11873 [00:32<00:08, 293.50it/s]\u001b[A\n"," 80% 9446/11873 [00:32<00:08, 290.70it/s]\u001b[A\n"," 80% 9476/11873 [00:32<00:08, 292.84it/s]\u001b[A\n"," 80% 9506/11873 [00:32<00:08, 291.60it/s]\u001b[A\n"," 80% 9537/11873 [00:32<00:07, 295.24it/s]\u001b[A\n"," 81% 9568/11873 [00:33<00:07, 299.05it/s]\u001b[A\n"," 81% 9598/11873 [00:33<00:07, 295.43it/s]\u001b[A\n"," 81% 9628/11873 [00:33<00:07, 293.60it/s]\u001b[A\n"," 81% 9658/11873 [00:33<00:07, 291.00it/s]\u001b[A\n"," 82% 9688/11873 [00:33<00:07, 293.41it/s]\u001b[A\n"," 82% 9719/11873 [00:33<00:07, 296.81it/s]\u001b[A\n"," 82% 9750/11873 [00:33<00:07, 299.07it/s]\u001b[A\n"," 82% 9780/11873 [00:33<00:07, 298.64it/s]\u001b[A\n"," 83% 9810/11873 [00:33<00:06, 297.03it/s]\u001b[A\n"," 83% 9840/11873 [00:33<00:06, 295.98it/s]\u001b[A\n"," 83% 9871/11873 [00:34<00:06, 297.31it/s]\u001b[A\n"," 83% 9901/11873 [00:34<00:06, 291.89it/s]\u001b[A\n"," 84% 9931/11873 [00:34<00:06, 291.80it/s]\u001b[A\n"," 84% 9962/11873 [00:34<00:06, 295.05it/s]\u001b[A\n"," 84% 9992/11873 [00:34<00:06, 295.18it/s]\u001b[A\n"," 84% 10022/11873 [00:34<00:06, 292.06it/s]\u001b[A\n"," 85% 10053/11873 [00:34<00:06, 295.75it/s]\u001b[A\n"," 85% 10084/11873 [00:34<00:06, 297.32it/s]\u001b[A\n"," 85% 10115/11873 [00:34<00:05, 298.18it/s]\u001b[A\n"," 85% 10146/11873 [00:35<00:05, 299.50it/s]\u001b[A\n"," 86% 10176/11873 [00:35<00:05, 294.49it/s]\u001b[A\n"," 86% 10206/11873 [00:35<00:05, 291.68it/s]\u001b[A\n"," 86% 10237/11873 [00:35<00:05, 295.60it/s]\u001b[A\n"," 86% 10267/11873 [00:35<00:05, 295.64it/s]\u001b[A\n"," 87% 10297/11873 [00:35<00:05, 288.61it/s]\u001b[A\n"," 87% 10327/11873 [00:35<00:05, 291.10it/s]\u001b[A\n"," 87% 10357/11873 [00:35<00:05, 288.18it/s]\u001b[A\n"," 87% 10388/11873 [00:35<00:05, 292.35it/s]\u001b[A\n"," 88% 10419/11873 [00:35<00:04, 295.97it/s]\u001b[A\n"," 88% 10449/11873 [00:36<00:05, 269.86it/s]\u001b[A\n"," 88% 10480/11873 [00:36<00:04, 279.14it/s]\u001b[A\n"," 89% 10510/11873 [00:36<00:04, 282.68it/s]\u001b[A\n"," 89% 10540/11873 [00:36<00:04, 286.78it/s]\u001b[A\n"," 89% 10569/11873 [00:36<00:04, 286.04it/s]\u001b[A\n"," 89% 10598/11873 [00:36<00:04, 286.10it/s]\u001b[A\n"," 90% 10627/11873 [00:36<00:04, 285.64it/s]\u001b[A\n"," 90% 10656/11873 [00:36<00:04, 283.01it/s]\u001b[A\n"," 90% 10685/11873 [00:36<00:04, 284.08it/s]\u001b[A\n"," 90% 10715/11873 [00:37<00:04, 286.07it/s]\u001b[A\n"," 90% 10745/11873 [00:37<00:03, 288.49it/s]\u001b[A\n"," 91% 10774/11873 [00:37<00:03, 288.46it/s]\u001b[A\n"," 91% 10803/11873 [00:37<00:03, 284.23it/s]\u001b[A\n"," 91% 10832/11873 [00:37<00:03, 266.86it/s]\u001b[A\n"," 91% 10862/11873 [00:37<00:03, 272.97it/s]\u001b[A\n"," 92% 10891/11873 [00:37<00:03, 275.73it/s]\u001b[A\n"," 92% 10920/11873 [00:37<00:03, 277.19it/s]\u001b[A\n"," 92% 10948/11873 [00:37<00:03, 275.08it/s]\u001b[A\n"," 92% 10978/11873 [00:37<00:03, 282.08it/s]\u001b[A\n"," 93% 11007/11873 [00:38<00:03, 278.16it/s]\u001b[A\n"," 93% 11036/11873 [00:38<00:02, 279.07it/s]\u001b[A\n"," 93% 11065/11873 [00:38<00:02, 282.00it/s]\u001b[A\n"," 93% 11095/11873 [00:38<00:02, 286.43it/s]\u001b[A\n"," 94% 11124/11873 [00:38<00:02, 286.96it/s]\u001b[A\n"," 94% 11154/11873 [00:38<00:02, 289.40it/s]\u001b[A\n"," 94% 11184/11873 [00:38<00:02, 291.85it/s]\u001b[A\n"," 94% 11214/11873 [00:38<00:02, 290.16it/s]\u001b[A\n"," 95% 11244/11873 [00:38<00:02, 290.20it/s]\u001b[A\n"," 95% 11274/11873 [00:39<00:02, 287.13it/s]\u001b[A\n"," 95% 11303/11873 [00:39<00:01, 287.94it/s]\u001b[A\n"," 95% 11334/11873 [00:39<00:01, 293.15it/s]\u001b[A\n"," 96% 11364/11873 [00:39<00:01, 293.46it/s]\u001b[A\n"," 96% 11394/11873 [00:39<00:01, 293.70it/s]\u001b[A\n"," 96% 11424/11873 [00:39<00:01, 295.06it/s]\u001b[A\n"," 96% 11455/11873 [00:39<00:01, 297.78it/s]\u001b[A\n"," 97% 11485/11873 [00:39<00:01, 296.47it/s]\u001b[A\n"," 97% 11515/11873 [00:39<00:01, 296.87it/s]\u001b[A\n"," 97% 11545/11873 [00:39<00:01, 293.81it/s]\u001b[A\n"," 97% 11575/11873 [00:40<00:01, 288.85it/s]\u001b[A\n"," 98% 11606/11873 [00:40<00:00, 293.70it/s]\u001b[A\n"," 98% 11636/11873 [00:40<00:00, 294.93it/s]\u001b[A\n"," 98% 11666/11873 [00:40<00:00, 295.10it/s]\u001b[A\n"," 99% 11696/11873 [00:40<00:00, 289.85it/s]\u001b[A\n"," 99% 11726/11873 [00:40<00:00, 288.71it/s]\u001b[A\n"," 99% 11756/11873 [00:40<00:00, 289.95it/s]\u001b[A\n"," 99% 11786/11873 [00:40<00:00, 290.26it/s]\u001b[A\n","100% 11816/11873 [00:40<00:00, 291.30it/s]\u001b[A\n","100% 11873/11873 [00:41<00:00, 289.32it/s]\n","03/29/2022 18:14:38 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","03/29/2022 18:14:38 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","03/29/2022 18:14:40 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","03/29/2022 18:14:45 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1522/1522 [04:22<00:00,  5.80it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 75.6242\n","  eval_HasAns_f1         = 82.0158\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 81.9849\n","  eval_NoAns_f1          = 81.9849\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 78.8175\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 82.0087\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 78.8091\n","  eval_f1                = 82.0003\n","  eval_samples           =   12171\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-03-29 18:14:45,426 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DGFZLvcDbMeT","executionInfo":{"status":"ok","timestamp":1649358483051,"user_tz":240,"elapsed":13019,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"b998887e-f787-4740-d552-53f6f9061e99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path albert-base-v2 \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 512 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MmvqA-7Z9K_2","executionInfo":{"status":"ok","timestamp":1649374278456,"user_tz":240,"elapsed":15406048,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"74ec5bf6-2592-4074-efa1-c8601308189f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["04/07/2022 19:14:35 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/07/2022 19:14:35 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/runs/Apr07_19-14-35_bd97afe510bf,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/07/2022 19:14:35 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","04/07/2022 19:14:35 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/07/2022 19:14:35 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/07/2022 19:14:35 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/07/2022 19:14:35 - WARNING - datasets.builder - Reusing dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","04/07/2022 19:14:35 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","100% 2/2 [00:00<00:00, 557.72it/s]\n","[INFO|configuration_utils.py:654] 2022-04-07 19:14:36,100 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-07 19:14:36,102 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-07 19:14:36,210 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-07 19:14:36,314 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-07 19:14:36,315 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 19:14:36,956 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 19:14:36,956 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 19:14:36,956 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 19:14:36,956 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 19:14:36,956 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-07 19:14:37,062 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-07 19:14:37,063 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|modeling_utils.py:1772] 2022-04-07 19:14:37,248 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[WARNING|modeling_utils.py:2049] 2022-04-07 19:14:37,388 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n","- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-04-07 19:14:37,388 >> Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/07/2022 19:14:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-fff616d93416ff6c.arrow\n","Running tokenizer on train dataset: 100% 131/131 [01:03<00:00,  2.06ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/07/2022 19:15:41 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-dae936c418e44485.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:22<00:00,  6.83s/ba]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-07 19:17:07,087 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-07 19:17:07,088 >>   Num examples = 130550\n","[INFO|trainer.py:1292] 2022-04-07 19:17:07,088 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-07 19:17:07,088 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1294] 2022-04-07 19:17:07,088 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1295] 2022-04-07 19:17:07,088 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-07 19:17:07,088 >>   Total optimization steps = 16320\n","{'loss': 1.7207, 'learning_rate': 1.9387254901960785e-05, 'epoch': 0.06}\n","  3% 500/16320 [07:35<3:59:29,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 19:24:42,157 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-07 19:24:42,164 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 19:24:42,291 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 19:24:42,297 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 19:24:42,301 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-500/special_tokens_map.json\n","{'loss': 1.2182, 'learning_rate': 1.877450980392157e-05, 'epoch': 0.12}\n","  6% 1000/16320 [15:10<3:51:55,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 19:32:17,634 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-07 19:32:17,640 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 19:32:17,764 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 19:32:17,770 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 19:32:17,775 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.1241, 'learning_rate': 1.8161764705882355e-05, 'epoch': 0.18}\n","  9% 1500/16320 [22:46<3:45:04,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 19:39:53,546 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-07 19:39:53,553 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 19:39:53,687 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 19:39:53,695 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 19:39:53,699 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.0518, 'learning_rate': 1.7549019607843138e-05, 'epoch': 0.25}\n"," 12% 2000/16320 [30:22<3:36:58,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 19:47:29,766 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-07 19:47:29,772 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 19:47:29,896 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 19:47:29,901 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 19:47:29,904 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.0484, 'learning_rate': 1.693627450980392e-05, 'epoch': 0.31}\n"," 15% 2500/16320 [37:58<3:30:13,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 19:55:06,119 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-07 19:55:06,125 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 19:55:06,248 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 19:55:06,253 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 19:55:06,257 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.0243, 'learning_rate': 1.6323529411764708e-05, 'epoch': 0.37}\n"," 18% 3000/16320 [45:35<3:22:34,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 20:02:42,466 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-07 20:02:42,472 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 20:02:42,595 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 20:02:42,601 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 20:02:42,604 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.9696, 'learning_rate': 1.571078431372549e-05, 'epoch': 0.43}\n"," 21% 3500/16320 [53:12<3:14:45,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 20:10:19,364 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-07 20:10:19,369 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 20:10:19,492 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 20:10:19,497 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 20:10:19,501 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.9362, 'learning_rate': 1.5098039215686276e-05, 'epoch': 0.49}\n"," 25% 4000/16320 [1:00:49<3:07:40,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 20:17:56,660 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-07 20:17:56,683 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 20:17:56,812 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 20:17:56,818 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 20:17:56,822 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.9654, 'learning_rate': 1.448529411764706e-05, 'epoch': 0.55}\n"," 28% 4500/16320 [1:08:26<3:00:00,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 20:25:33,685 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-07 20:25:33,691 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 20:25:33,819 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 20:25:33,825 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 20:25:33,846 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.9197, 'learning_rate': 1.3872549019607844e-05, 'epoch': 0.61}\n"," 31% 5000/16320 [1:16:03<2:52:11,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 20:33:10,567 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-07 20:33:10,573 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 20:33:10,699 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 20:33:10,704 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 20:33:10,708 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.9154, 'learning_rate': 1.3259803921568627e-05, 'epoch': 0.67}\n"," 34% 5500/16320 [1:23:40<2:44:56,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 20:40:47,558 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-07 20:40:47,564 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 20:40:47,686 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 20:40:47,692 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 20:40:47,696 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.9, 'learning_rate': 1.2647058823529412e-05, 'epoch': 0.74}\n"," 37% 6000/16320 [1:31:17<2:36:58,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 20:48:24,793 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-07 20:48:24,799 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 20:48:24,923 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 20:48:24,927 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 20:48:24,931 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.886, 'learning_rate': 1.2034313725490197e-05, 'epoch': 0.8}\n"," 40% 6500/16320 [1:38:54<2:29:13,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 20:56:01,868 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-07 20:56:01,874 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 20:56:01,997 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 20:56:02,004 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 20:56:02,008 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.8734, 'learning_rate': 1.142156862745098e-05, 'epoch': 0.86}\n"," 43% 7000/16320 [1:46:32<2:22:10,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 21:03:39,535 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-07 21:03:39,541 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 21:03:39,666 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 21:03:39,672 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 21:03:39,676 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.8314, 'learning_rate': 1.0808823529411765e-05, 'epoch': 0.92}\n"," 46% 7500/16320 [1:54:10<2:14:31,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 21:11:17,356 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-07 21:11:17,362 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 21:11:17,490 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 21:11:17,496 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 21:11:17,500 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.8688, 'learning_rate': 1.0196078431372549e-05, 'epoch': 0.98}\n"," 49% 8000/16320 [2:01:47<2:06:45,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 21:18:54,835 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-07 21:18:54,842 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 21:18:54,970 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 21:18:54,976 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 21:18:54,979 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.7314, 'learning_rate': 9.583333333333335e-06, 'epoch': 1.04}\n"," 52% 8500/16320 [2:09:24<1:59:11,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 21:26:31,721 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-07 21:26:31,728 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 21:26:31,861 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 21:26:31,882 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 21:26:31,886 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.6735, 'learning_rate': 8.970588235294119e-06, 'epoch': 1.1}\n"," 55% 9000/16320 [2:17:02<1:51:33,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 21:34:09,264 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-07 21:34:09,271 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 21:34:09,396 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 21:34:09,402 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 21:34:09,406 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.6519, 'learning_rate': 8.357843137254903e-06, 'epoch': 1.16}\n"," 58% 9500/16320 [2:24:39<1:44:01,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 21:41:46,694 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-07 21:41:46,701 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 21:41:46,827 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 21:41:46,833 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 21:41:46,837 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.67, 'learning_rate': 7.745098039215687e-06, 'epoch': 1.23}\n"," 61% 10000/16320 [2:32:17<1:36:24,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 21:49:24,528 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-07 21:49:24,535 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 21:49:24,660 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 21:49:24,666 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 21:49:24,670 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6556, 'learning_rate': 7.132352941176472e-06, 'epoch': 1.29}\n"," 64% 10500/16320 [2:39:54<1:28:45,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 21:57:01,981 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-07 21:57:01,987 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 21:57:02,116 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 21:57:02,122 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 21:57:02,125 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.6401, 'learning_rate': 6.519607843137256e-06, 'epoch': 1.35}\n"," 67% 11000/16320 [2:47:32<1:20:46,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 22:04:39,321 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-07 22:04:39,327 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 22:04:39,455 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 22:04:39,459 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 22:04:39,463 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.6451, 'learning_rate': 5.90686274509804e-06, 'epoch': 1.41}\n"," 70% 11500/16320 [2:55:09<1:13:36,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 22:12:16,712 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-11500\n","[INFO|configuration_utils.py:441] 2022-04-07 22:12:16,719 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 22:12:16,842 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 22:12:16,848 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 22:12:16,852 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.6257, 'learning_rate': 5.294117647058824e-06, 'epoch': 1.47}\n"," 74% 12000/16320 [3:02:46<1:05:43,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 22:19:53,833 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-12000\n","[INFO|configuration_utils.py:441] 2022-04-07 22:19:53,840 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 22:19:53,969 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 22:19:53,974 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 22:19:53,978 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.6459, 'learning_rate': 4.681372549019608e-06, 'epoch': 1.53}\n"," 77% 12500/16320 [3:10:23<58:07,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 22:27:30,684 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-12500\n","[INFO|configuration_utils.py:441] 2022-04-07 22:27:30,690 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 22:27:30,815 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 22:27:30,839 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 22:27:30,842 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.6381, 'learning_rate': 4.068627450980392e-06, 'epoch': 1.59}\n"," 80% 13000/16320 [3:18:00<50:35,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 22:35:07,944 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-13000\n","[INFO|configuration_utils.py:441] 2022-04-07 22:35:07,950 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 22:35:08,074 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 22:35:08,080 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 22:35:08,084 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.6396, 'learning_rate': 3.4558823529411766e-06, 'epoch': 1.65}\n"," 83% 13500/16320 [3:25:38<42:49,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 22:42:45,171 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-13500\n","[INFO|configuration_utils.py:441] 2022-04-07 22:42:45,177 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 22:42:45,302 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 22:42:45,307 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 22:42:45,311 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.6223, 'learning_rate': 2.843137254901961e-06, 'epoch': 1.72}\n"," 86% 14000/16320 [3:33:15<35:16,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 22:50:22,128 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-14000\n","[INFO|configuration_utils.py:441] 2022-04-07 22:50:22,134 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 22:50:22,257 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 22:50:22,262 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 22:50:22,265 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.6114, 'learning_rate': 2.2303921568627456e-06, 'epoch': 1.78}\n"," 89% 14500/16320 [3:40:51<27:34,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 22:57:58,773 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-14500\n","[INFO|configuration_utils.py:441] 2022-04-07 22:57:58,780 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 22:57:58,897 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 22:57:58,901 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 22:57:58,905 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.5957, 'learning_rate': 1.6176470588235297e-06, 'epoch': 1.84}\n"," 92% 15000/16320 [3:48:28<20:01,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 23:05:35,364 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-15000\n","[INFO|configuration_utils.py:441] 2022-04-07 23:05:35,371 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 23:05:35,490 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 23:05:35,495 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 23:05:35,498 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.6002, 'learning_rate': 1.0049019607843138e-06, 'epoch': 1.9}\n"," 95% 15500/16320 [3:56:04<12:29,  1.09it/s][INFO|trainer.py:2166] 2022-04-07 23:13:11,909 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-15500\n","[INFO|configuration_utils.py:441] 2022-04-07 23:13:11,915 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 23:13:12,039 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 23:13:12,044 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 23:13:12,049 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.6088, 'learning_rate': 3.921568627450981e-07, 'epoch': 1.96}\n"," 98% 16000/16320 [4:03:41<04:51,  1.10it/s][INFO|trainer.py:2166] 2022-04-07 23:20:48,818 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-16000\n","[INFO|configuration_utils.py:441] 2022-04-07 23:20:48,824 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 23:20:48,947 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 23:20:48,952 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 23:20:48,956 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/checkpoint-16000/special_tokens_map.json\n","100% 16320/16320 [4:08:33<00:00,  1.34it/s][INFO|trainer.py:1530] 2022-04-07 23:25:40,930 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 14913.8428, 'train_samples_per_second': 17.507, 'train_steps_per_second': 1.094, 'train_loss': 0.8245296702665441, 'epoch': 2.0}\n","100% 16320/16320 [4:08:33<00:00,  1.09it/s]\n","[INFO|trainer.py:2166] 2022-04-07 23:25:40,936 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2\n","[INFO|configuration_utils.py:441] 2022-04-07 23:25:40,941 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 23:25:41,072 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 23:25:41,077 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 23:25:41,081 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.8245\n","  train_runtime            = 4:08:33.84\n","  train_samples            =     130550\n","  train_samples_per_second =     17.507\n","  train_steps_per_second   =      1.094\n","04/07/2022 23:25:41 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-07 23:25:41,110 >> The following columns in the evaluation set  don't have a corresponding argument in `AlbertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `AlbertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-07 23:25:41,113 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-07 23:25:41,113 >>   Num examples = 11968\n","[INFO|trainer.py:2421] 2022-04-07 23:25:41,113 >>   Batch size = 8\n","100% 1496/1496 [04:20<00:00,  5.73it/s]04/07/2022 23:30:20 - INFO - utils_qa - Post-processing 11873 example predictions split into 11968 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 31/11873 [00:00<00:39, 303.27it/s]\u001b[A\n","  1% 63/11873 [00:00<00:38, 308.55it/s]\u001b[A\n","  1% 94/11873 [00:00<00:39, 295.25it/s]\u001b[A\n","  1% 124/11873 [00:00<00:39, 296.12it/s]\u001b[A\n","  1% 161/11873 [00:00<00:36, 321.51it/s]\u001b[A\n","  2% 197/11873 [00:00<00:35, 332.39it/s]\u001b[A\n","  2% 233/11873 [00:00<00:34, 340.68it/s]\u001b[A\n","  2% 268/11873 [00:00<00:35, 327.22it/s]\u001b[A\n","  3% 301/11873 [00:00<00:35, 327.98it/s]\u001b[A\n","  3% 334/11873 [00:01<00:36, 319.23it/s]\u001b[A\n","  3% 367/11873 [00:01<00:35, 320.56it/s]\u001b[A\n","  3% 402/11873 [00:01<00:34, 329.13it/s]\u001b[A\n","  4% 441/11873 [00:01<00:33, 345.36it/s]\u001b[A\n","  4% 477/11873 [00:01<00:32, 346.81it/s]\u001b[A\n","  4% 512/11873 [00:01<00:32, 347.14it/s]\u001b[A\n","  5% 547/11873 [00:01<00:32, 344.20it/s]\u001b[A\n","  5% 582/11873 [00:01<00:32, 343.10it/s]\u001b[A\n","  5% 619/11873 [00:01<00:32, 350.74it/s]\u001b[A\n","  6% 655/11873 [00:01<00:31, 351.25it/s]\u001b[A\n","  6% 692/11873 [00:02<00:31, 354.81it/s]\u001b[A\n","  6% 728/11873 [00:02<00:32, 347.09it/s]\u001b[A\n","  6% 763/11873 [00:02<00:32, 337.92it/s]\u001b[A\n","  7% 798/11873 [00:02<00:32, 339.19it/s]\u001b[A\n","  7% 832/11873 [00:02<00:33, 331.67it/s]\u001b[A\n","  7% 866/11873 [00:02<00:33, 328.45it/s]\u001b[A\n","  8% 899/11873 [00:02<00:33, 323.33it/s]\u001b[A\n","  8% 932/11873 [00:02<00:34, 317.16it/s]\u001b[A\n","  8% 965/11873 [00:02<00:34, 319.04it/s]\u001b[A\n","  8% 997/11873 [00:03<00:34, 318.26it/s]\u001b[A\n","  9% 1029/11873 [00:03<00:36, 294.52it/s]\u001b[A\n","  9% 1059/11873 [00:03<00:39, 275.83it/s]\u001b[A\n","  9% 1087/11873 [00:03<00:40, 266.20it/s]\u001b[A\n","  9% 1114/11873 [00:03<00:41, 257.97it/s]\u001b[A\n"," 10% 1140/11873 [00:03<00:42, 252.57it/s]\u001b[A\n"," 10% 1166/11873 [00:03<00:43, 246.51it/s]\u001b[A\n"," 10% 1191/11873 [00:03<00:43, 242.92it/s]\u001b[A\n"," 10% 1216/11873 [00:03<00:44, 242.06it/s]\u001b[A\n"," 10% 1241/11873 [00:04<00:45, 235.12it/s]\u001b[A\n"," 11% 1265/11873 [00:04<00:45, 234.52it/s]\u001b[A\n"," 11% 1289/11873 [00:04<00:45, 231.21it/s]\u001b[A\n"," 11% 1313/11873 [00:04<00:45, 231.19it/s]\u001b[A\n"," 11% 1337/11873 [00:04<00:45, 229.06it/s]\u001b[A\n"," 11% 1365/11873 [00:04<00:43, 241.53it/s]\u001b[A\n"," 12% 1393/11873 [00:04<00:41, 251.72it/s]\u001b[A\n"," 12% 1420/11873 [00:04<00:41, 254.00it/s]\u001b[A\n"," 12% 1447/11873 [00:04<00:40, 257.17it/s]\u001b[A\n"," 12% 1473/11873 [00:04<00:40, 257.79it/s]\u001b[A\n"," 13% 1501/11873 [00:05<00:39, 261.94it/s]\u001b[A\n"," 13% 1529/11873 [00:05<00:38, 265.91it/s]\u001b[A\n"," 13% 1557/11873 [00:05<00:38, 267.39it/s]\u001b[A\n"," 13% 1584/11873 [00:05<00:38, 264.04it/s]\u001b[A\n"," 14% 1611/11873 [00:05<00:38, 264.46it/s]\u001b[A\n"," 14% 1638/11873 [00:05<00:38, 264.86it/s]\u001b[A\n"," 14% 1665/11873 [00:05<00:38, 263.89it/s]\u001b[A\n"," 14% 1692/11873 [00:05<00:39, 260.15it/s]\u001b[A\n"," 14% 1719/11873 [00:05<00:39, 258.07it/s]\u001b[A\n"," 15% 1746/11873 [00:06<00:39, 259.21it/s]\u001b[A\n"," 15% 1772/11873 [00:06<00:39, 258.15it/s]\u001b[A\n"," 15% 1798/11873 [00:06<00:39, 256.72it/s]\u001b[A\n"," 15% 1824/11873 [00:06<00:39, 257.48it/s]\u001b[A\n"," 16% 1851/11873 [00:06<00:38, 258.82it/s]\u001b[A\n"," 16% 1878/11873 [00:06<00:38, 259.52it/s]\u001b[A\n"," 16% 1905/11873 [00:06<00:38, 260.94it/s]\u001b[A\n"," 16% 1932/11873 [00:06<00:38, 260.66it/s]\u001b[A\n"," 16% 1959/11873 [00:06<00:37, 262.90it/s]\u001b[A\n"," 17% 1986/11873 [00:06<00:38, 259.84it/s]\u001b[A\n"," 17% 2013/11873 [00:07<00:37, 261.52it/s]\u001b[A\n"," 17% 2041/11873 [00:07<00:37, 265.38it/s]\u001b[A\n"," 17% 2068/11873 [00:07<00:36, 266.18it/s]\u001b[A\n"," 18% 2095/11873 [00:07<00:36, 265.54it/s]\u001b[A\n"," 18% 2122/11873 [00:07<00:36, 264.27it/s]\u001b[A\n"," 18% 2149/11873 [00:07<00:37, 260.75it/s]\u001b[A\n"," 18% 2176/11873 [00:07<00:38, 248.77it/s]\u001b[A\n"," 19% 2202/11873 [00:07<00:39, 243.16it/s]\u001b[A\n"," 19% 2227/11873 [00:07<00:40, 240.08it/s]\u001b[A\n"," 19% 2252/11873 [00:08<00:40, 235.38it/s]\u001b[A\n"," 19% 2276/11873 [00:08<00:41, 231.20it/s]\u001b[A\n"," 19% 2300/11873 [00:08<00:41, 230.66it/s]\u001b[A\n"," 20% 2324/11873 [00:08<00:41, 230.66it/s]\u001b[A\n"," 20% 2348/11873 [00:08<00:41, 230.85it/s]\u001b[A\n"," 20% 2375/11873 [00:08<00:39, 240.58it/s]\u001b[A\n"," 20% 2401/11873 [00:08<00:38, 245.88it/s]\u001b[A\n"," 20% 2427/11873 [00:08<00:38, 247.92it/s]\u001b[A\n"," 21% 2453/11873 [00:08<00:37, 250.27it/s]\u001b[A\n"," 21% 2480/11873 [00:08<00:36, 254.06it/s]\u001b[A\n"," 21% 2508/11873 [00:09<00:36, 259.07it/s]\u001b[A\n"," 21% 2535/11873 [00:09<00:35, 261.56it/s]\u001b[A\n"," 22% 2562/11873 [00:09<00:35, 263.56it/s]\u001b[A\n"," 22% 2589/11873 [00:09<00:35, 264.22it/s]\u001b[A\n"," 22% 2616/11873 [00:09<00:35, 264.04it/s]\u001b[A\n"," 22% 2644/11873 [00:09<00:34, 267.25it/s]\u001b[A\n"," 22% 2671/11873 [00:09<00:34, 265.79it/s]\u001b[A\n"," 23% 2698/11873 [00:09<00:34, 265.57it/s]\u001b[A\n"," 23% 2725/11873 [00:09<00:34, 266.75it/s]\u001b[A\n"," 23% 2752/11873 [00:09<00:34, 265.14it/s]\u001b[A\n"," 23% 2779/11873 [00:10<00:34, 264.76it/s]\u001b[A\n"," 24% 2806/11873 [00:10<00:34, 265.26it/s]\u001b[A\n"," 24% 2833/11873 [00:10<00:35, 256.95it/s]\u001b[A\n"," 24% 2859/11873 [00:10<00:36, 248.11it/s]\u001b[A\n"," 24% 2884/11873 [00:10<00:36, 247.26it/s]\u001b[A\n"," 25% 2909/11873 [00:10<00:36, 244.59it/s]\u001b[A\n"," 25% 2934/11873 [00:10<00:37, 240.12it/s]\u001b[A\n"," 25% 2959/11873 [00:10<00:37, 238.77it/s]\u001b[A\n"," 25% 2983/11873 [00:10<00:37, 235.26it/s]\u001b[A\n"," 25% 3007/11873 [00:11<00:39, 227.01it/s]\u001b[A\n"," 26% 3030/11873 [00:11<00:38, 227.55it/s]\u001b[A\n"," 26% 3053/11873 [00:11<00:38, 227.75it/s]\u001b[A\n"," 26% 3076/11873 [00:11<00:39, 220.65it/s]\u001b[A\n"," 26% 3099/11873 [00:11<00:39, 220.22it/s]\u001b[A\n"," 26% 3122/11873 [00:11<00:42, 205.51it/s]\u001b[A\n"," 26% 3146/11873 [00:11<00:40, 214.18it/s]\u001b[A\n"," 27% 3168/11873 [00:11<00:43, 200.60it/s]\u001b[A\n"," 27% 3194/11873 [00:11<00:40, 215.32it/s]\u001b[A\n"," 27% 3221/11873 [00:12<00:37, 228.15it/s]\u001b[A\n"," 27% 3247/11873 [00:12<00:36, 236.99it/s]\u001b[A\n"," 28% 3272/11873 [00:12<00:35, 240.69it/s]\u001b[A\n"," 28% 3297/11873 [00:12<00:44, 192.69it/s]\u001b[A\n"," 28% 3318/11873 [00:12<00:46, 185.39it/s]\u001b[A\n"," 28% 3340/11873 [00:12<00:44, 193.08it/s]\u001b[A\n"," 28% 3361/11873 [00:12<00:48, 175.17it/s]\u001b[A\n"," 28% 3380/11873 [00:12<00:49, 172.98it/s]\u001b[A\n"," 29% 3401/11873 [00:12<00:46, 180.74it/s]\u001b[A\n"," 29% 3425/11873 [00:13<00:43, 194.77it/s]\u001b[A\n"," 29% 3448/11873 [00:13<00:41, 204.36it/s]\u001b[A\n"," 29% 3472/11873 [00:13<00:39, 213.23it/s]\u001b[A\n"," 29% 3498/11873 [00:13<00:37, 226.11it/s]\u001b[A\n"," 30% 3526/11873 [00:13<00:34, 239.51it/s]\u001b[A\n"," 30% 3553/11873 [00:13<00:33, 246.73it/s]\u001b[A\n"," 30% 3581/11873 [00:13<00:32, 254.01it/s]\u001b[A\n"," 30% 3609/11873 [00:13<00:31, 259.33it/s]\u001b[A\n"," 31% 3636/11873 [00:13<00:31, 258.27it/s]\u001b[A\n"," 31% 3662/11873 [00:14<00:32, 253.01it/s]\u001b[A\n"," 31% 3688/11873 [00:14<00:32, 248.15it/s]\u001b[A\n"," 31% 3713/11873 [00:14<00:33, 242.84it/s]\u001b[A\n"," 31% 3738/11873 [00:14<00:33, 244.74it/s]\u001b[A\n"," 32% 3763/11873 [00:14<00:33, 244.95it/s]\u001b[A\n"," 32% 3788/11873 [00:14<00:33, 237.98it/s]\u001b[A\n"," 32% 3813/11873 [00:14<00:33, 239.72it/s]\u001b[A\n"," 32% 3838/11873 [00:14<00:33, 238.34it/s]\u001b[A\n"," 33% 3863/11873 [00:14<00:33, 239.94it/s]\u001b[A\n"," 33% 3889/11873 [00:14<00:32, 245.24it/s]\u001b[A\n"," 33% 3914/11873 [00:15<00:32, 241.46it/s]\u001b[A\n"," 33% 3939/11873 [00:15<00:34, 232.87it/s]\u001b[A\n"," 33% 3963/11873 [00:15<00:34, 227.63it/s]\u001b[A\n"," 34% 3986/11873 [00:15<00:35, 222.29it/s]\u001b[A\n"," 34% 4011/11873 [00:15<00:34, 229.15it/s]\u001b[A\n"," 34% 4036/11873 [00:15<00:33, 232.88it/s]\u001b[A\n"," 34% 4063/11873 [00:15<00:32, 242.38it/s]\u001b[A\n"," 34% 4088/11873 [00:15<00:32, 241.50it/s]\u001b[A\n"," 35% 4115/11873 [00:15<00:31, 247.50it/s]\u001b[A\n"," 35% 4142/11873 [00:16<00:30, 253.25it/s]\u001b[A\n"," 35% 4168/11873 [00:16<00:30, 253.23it/s]\u001b[A\n"," 35% 4194/11873 [00:16<00:30, 254.86it/s]\u001b[A\n"," 36% 4221/11873 [00:16<00:29, 257.58it/s]\u001b[A\n"," 36% 4248/11873 [00:16<00:29, 261.08it/s]\u001b[A\n"," 36% 4275/11873 [00:16<00:29, 259.24it/s]\u001b[A\n"," 36% 4301/11873 [00:16<00:29, 259.42it/s]\u001b[A\n"," 36% 4327/11873 [00:16<00:30, 249.32it/s]\u001b[A\n"," 37% 4353/11873 [00:16<00:30, 243.09it/s]\u001b[A\n"," 37% 4378/11873 [00:16<00:31, 239.16it/s]\u001b[A\n"," 37% 4402/11873 [00:17<00:31, 236.17it/s]\u001b[A\n"," 37% 4426/11873 [00:17<00:38, 195.42it/s]\u001b[A\n"," 37% 4450/11873 [00:17<00:36, 205.01it/s]\u001b[A\n"," 38% 4475/11873 [00:17<00:34, 216.48it/s]\u001b[A\n"," 38% 4498/11873 [00:17<00:33, 219.81it/s]\u001b[A\n"," 38% 4521/11873 [00:17<00:33, 216.88it/s]\u001b[A\n"," 38% 4544/11873 [00:17<00:33, 219.05it/s]\u001b[A\n"," 38% 4567/11873 [00:17<00:32, 221.88it/s]\u001b[A\n"," 39% 4591/11873 [00:17<00:32, 225.09it/s]\u001b[A\n"," 39% 4615/11873 [00:18<00:31, 227.21it/s]\u001b[A\n"," 39% 4638/11873 [00:18<00:31, 227.94it/s]\u001b[A\n"," 39% 4666/11873 [00:18<00:30, 240.22it/s]\u001b[A\n"," 40% 4691/11873 [00:18<00:30, 238.12it/s]\u001b[A\n"," 40% 4716/11873 [00:18<00:29, 240.18it/s]\u001b[A\n"," 40% 4742/11873 [00:18<00:29, 245.50it/s]\u001b[A\n"," 40% 4767/11873 [00:18<00:29, 244.90it/s]\u001b[A\n"," 40% 4792/11873 [00:18<00:28, 246.32it/s]\u001b[A\n"," 41% 4818/11873 [00:18<00:28, 249.70it/s]\u001b[A\n"," 41% 4845/11873 [00:19<00:27, 253.49it/s]\u001b[A\n"," 41% 4872/11873 [00:19<00:27, 258.29it/s]\u001b[A\n"," 41% 4898/11873 [00:19<00:27, 255.39it/s]\u001b[A\n"," 41% 4925/11873 [00:19<00:26, 259.29it/s]\u001b[A\n"," 42% 4952/11873 [00:19<00:26, 260.82it/s]\u001b[A\n"," 42% 4979/11873 [00:19<00:26, 260.27it/s]\u001b[A\n"," 42% 5006/11873 [00:19<00:26, 258.96it/s]\u001b[A\n"," 42% 5033/11873 [00:19<00:26, 259.45it/s]\u001b[A\n"," 43% 5060/11873 [00:19<00:26, 260.64it/s]\u001b[A\n"," 43% 5088/11873 [00:19<00:25, 263.68it/s]\u001b[A\n"," 43% 5115/11873 [00:20<00:25, 260.59it/s]\u001b[A\n"," 43% 5142/11873 [00:20<00:26, 252.71it/s]\u001b[A\n"," 44% 5168/11873 [00:20<00:27, 242.87it/s]\u001b[A\n"," 44% 5193/11873 [00:20<00:27, 241.64it/s]\u001b[A\n"," 44% 5218/11873 [00:20<00:28, 229.79it/s]\u001b[A\n"," 44% 5242/11873 [00:20<00:28, 230.09it/s]\u001b[A\n"," 44% 5266/11873 [00:20<00:31, 209.05it/s]\u001b[A\n"," 45% 5289/11873 [00:20<00:30, 212.88it/s]\u001b[A\n"," 45% 5313/11873 [00:20<00:30, 218.65it/s]\u001b[A\n"," 45% 5336/11873 [00:21<00:29, 221.05it/s]\u001b[A\n"," 45% 5359/11873 [00:21<00:29, 222.38it/s]\u001b[A\n"," 45% 5383/11873 [00:21<00:28, 226.30it/s]\u001b[A\n"," 46% 5406/11873 [00:21<00:28, 224.38it/s]\u001b[A\n"," 46% 5429/11873 [00:21<00:28, 225.81it/s]\u001b[A\n"," 46% 5452/11873 [00:21<00:28, 221.98it/s]\u001b[A\n"," 46% 5475/11873 [00:21<00:29, 219.61it/s]\u001b[A\n"," 46% 5498/11873 [00:21<00:28, 221.46it/s]\u001b[A\n"," 47% 5522/11873 [00:21<00:28, 226.28it/s]\u001b[A\n"," 47% 5545/11873 [00:21<00:28, 225.90it/s]\u001b[A\n"," 47% 5568/11873 [00:22<00:27, 225.74it/s]\u001b[A\n"," 47% 5592/11873 [00:22<00:27, 229.56it/s]\u001b[A\n"," 47% 5617/11873 [00:22<00:26, 235.10it/s]\u001b[A\n"," 48% 5641/11873 [00:22<00:26, 234.06it/s]\u001b[A\n"," 48% 5666/11873 [00:22<00:26, 236.83it/s]\u001b[A\n"," 48% 5691/11873 [00:22<00:25, 238.66it/s]\u001b[A\n"," 48% 5716/11873 [00:22<00:25, 241.71it/s]\u001b[A\n"," 48% 5741/11873 [00:22<00:25, 243.64it/s]\u001b[A\n"," 49% 5767/11873 [00:22<00:24, 246.41it/s]\u001b[A\n"," 49% 5794/11873 [00:22<00:24, 252.54it/s]\u001b[A\n"," 49% 5821/11873 [00:23<00:23, 255.64it/s]\u001b[A\n"," 49% 5847/11873 [00:23<00:23, 256.31it/s]\u001b[A\n"," 49% 5874/11873 [00:23<00:23, 259.07it/s]\u001b[A\n"," 50% 5900/11873 [00:23<00:23, 257.42it/s]\u001b[A\n"," 50% 5926/11873 [00:23<00:23, 255.39it/s]\u001b[A\n"," 50% 5952/11873 [00:23<00:23, 256.50it/s]\u001b[A\n"," 50% 5979/11873 [00:23<00:22, 258.54it/s]\u001b[A\n"," 51% 6006/11873 [00:23<00:22, 259.39it/s]\u001b[A\n"," 51% 6032/11873 [00:23<00:22, 255.98it/s]\u001b[A\n"," 51% 6058/11873 [00:24<00:22, 255.99it/s]\u001b[A\n"," 51% 6084/11873 [00:24<00:22, 257.14it/s]\u001b[A\n"," 51% 6110/11873 [00:24<00:22, 257.72it/s]\u001b[A\n"," 52% 6137/11873 [00:24<00:22, 258.79it/s]\u001b[A\n"," 52% 6163/11873 [00:24<00:22, 258.56it/s]\u001b[A\n"," 52% 6190/11873 [00:24<00:21, 261.22it/s]\u001b[A\n"," 52% 6217/11873 [00:24<00:21, 261.58it/s]\u001b[A\n"," 53% 6244/11873 [00:24<00:21, 263.82it/s]\u001b[A\n"," 53% 6271/11873 [00:24<00:21, 263.91it/s]\u001b[A\n"," 53% 6300/11873 [00:24<00:20, 269.04it/s]\u001b[A\n"," 53% 6327/11873 [00:25<00:20, 266.54it/s]\u001b[A\n"," 54% 6354/11873 [00:25<00:21, 258.29it/s]\u001b[A\n"," 54% 6380/11873 [00:25<00:21, 253.61it/s]\u001b[A\n"," 54% 6406/11873 [00:25<00:21, 250.87it/s]\u001b[A\n"," 54% 6432/11873 [00:25<00:21, 247.77it/s]\u001b[A\n"," 54% 6457/11873 [00:25<00:21, 246.52it/s]\u001b[A\n"," 55% 6484/11873 [00:25<00:21, 251.34it/s]\u001b[A\n"," 55% 6510/11873 [00:25<00:21, 250.36it/s]\u001b[A\n"," 55% 6537/11873 [00:25<00:20, 254.15it/s]\u001b[A\n"," 55% 6563/11873 [00:25<00:20, 254.51it/s]\u001b[A\n"," 55% 6589/11873 [00:26<00:20, 255.20it/s]\u001b[A\n"," 56% 6615/11873 [00:26<00:20, 253.57it/s]\u001b[A\n"," 56% 6641/11873 [00:26<00:20, 254.03it/s]\u001b[A\n"," 56% 6668/11873 [00:26<00:20, 257.26it/s]\u001b[A\n"," 56% 6694/11873 [00:26<00:20, 248.32it/s]\u001b[A\n"," 57% 6719/11873 [00:26<00:21, 236.75it/s]\u001b[A\n"," 57% 6743/11873 [00:26<00:21, 235.64it/s]\u001b[A\n"," 57% 6767/11873 [00:26<00:22, 231.38it/s]\u001b[A\n"," 57% 6791/11873 [00:26<00:22, 229.93it/s]\u001b[A\n"," 57% 6815/11873 [00:27<00:21, 230.54it/s]\u001b[A\n"," 58% 6840/11873 [00:27<00:21, 234.44it/s]\u001b[A\n"," 58% 6866/11873 [00:27<00:20, 239.77it/s]\u001b[A\n"," 58% 6893/11873 [00:27<00:20, 246.31it/s]\u001b[A\n"," 58% 6919/11873 [00:27<00:19, 248.38it/s]\u001b[A\n"," 58% 6944/11873 [00:27<00:19, 246.88it/s]\u001b[A\n"," 59% 6969/11873 [00:27<00:20, 241.91it/s]\u001b[A\n"," 59% 6994/11873 [00:27<00:20, 237.21it/s]\u001b[A\n"," 59% 7018/11873 [00:27<00:20, 236.27it/s]\u001b[A\n"," 59% 7042/11873 [00:27<00:20, 233.90it/s]\u001b[A\n"," 60% 7066/11873 [00:28<00:20, 233.25it/s]\u001b[A\n"," 60% 7090/11873 [00:28<00:20, 231.77it/s]\u001b[A\n"," 60% 7114/11873 [00:28<00:20, 231.25it/s]\u001b[A\n"," 60% 7138/11873 [00:28<00:20, 230.35it/s]\u001b[A\n"," 60% 7164/11873 [00:28<00:19, 236.29it/s]\u001b[A\n"," 61% 7190/11873 [00:28<00:19, 242.36it/s]\u001b[A\n"," 61% 7215/11873 [00:28<00:19, 238.27it/s]\u001b[A\n"," 61% 7239/11873 [00:28<00:19, 234.76it/s]\u001b[A\n"," 61% 7263/11873 [00:28<00:19, 233.16it/s]\u001b[A\n"," 61% 7287/11873 [00:29<00:19, 230.97it/s]\u001b[A\n"," 62% 7312/11873 [00:29<00:19, 234.20it/s]\u001b[A\n"," 62% 7339/11873 [00:29<00:18, 242.31it/s]\u001b[A\n"," 62% 7364/11873 [00:29<00:18, 242.31it/s]\u001b[A\n"," 62% 7389/11873 [00:29<00:18, 240.88it/s]\u001b[A\n"," 62% 7415/11873 [00:29<00:18, 245.29it/s]\u001b[A\n"," 63% 7442/11873 [00:29<00:17, 250.29it/s]\u001b[A\n"," 63% 7468/11873 [00:29<00:17, 249.54it/s]\u001b[A\n"," 63% 7493/11873 [00:29<00:18, 242.72it/s]\u001b[A\n"," 63% 7518/11873 [00:29<00:18, 239.62it/s]\u001b[A\n"," 64% 7542/11873 [00:30<00:18, 235.17it/s]\u001b[A\n"," 64% 7567/11873 [00:30<00:18, 239.02it/s]\u001b[A\n"," 64% 7594/11873 [00:30<00:17, 246.91it/s]\u001b[A\n"," 64% 7619/11873 [00:30<00:17, 242.88it/s]\u001b[A\n"," 64% 7644/11873 [00:30<00:17, 236.12it/s]\u001b[A\n"," 65% 7668/11873 [00:30<00:17, 234.35it/s]\u001b[A\n"," 65% 7692/11873 [00:30<00:18, 231.82it/s]\u001b[A\n"," 65% 7716/11873 [00:30<00:18, 227.12it/s]\u001b[A\n"," 65% 7739/11873 [00:30<00:18, 226.72it/s]\u001b[A\n"," 65% 7762/11873 [00:31<00:18, 222.00it/s]\u001b[A\n"," 66% 7785/11873 [00:31<00:18, 222.18it/s]\u001b[A\n"," 66% 7808/11873 [00:31<00:18, 221.67it/s]\u001b[A\n"," 66% 7831/11873 [00:31<00:18, 223.27it/s]\u001b[A\n"," 66% 7856/11873 [00:31<00:17, 229.60it/s]\u001b[A\n"," 66% 7879/11873 [00:31<00:17, 228.05it/s]\u001b[A\n"," 67% 7906/11873 [00:31<00:16, 239.39it/s]\u001b[A\n"," 67% 7930/11873 [00:31<00:16, 237.73it/s]\u001b[A\n"," 67% 7957/11873 [00:31<00:15, 245.12it/s]\u001b[A\n"," 67% 7983/11873 [00:31<00:15, 247.40it/s]\u001b[A\n"," 67% 8011/11873 [00:32<00:15, 254.62it/s]\u001b[A\n"," 68% 8039/11873 [00:32<00:14, 259.31it/s]\u001b[A\n"," 68% 8065/11873 [00:32<00:14, 255.03it/s]\u001b[A\n"," 68% 8092/11873 [00:32<00:14, 257.41it/s]\u001b[A\n"," 68% 8118/11873 [00:32<00:15, 248.60it/s]\u001b[A\n"," 69% 8143/11873 [00:32<00:15, 240.92it/s]\u001b[A\n"," 69% 8168/11873 [00:32<00:16, 231.07it/s]\u001b[A\n"," 69% 8192/11873 [00:32<00:16, 227.47it/s]\u001b[A\n"," 69% 8215/11873 [00:32<00:16, 225.83it/s]\u001b[A\n"," 69% 8239/11873 [00:33<00:15, 228.21it/s]\u001b[A\n"," 70% 8263/11873 [00:33<00:15, 230.78it/s]\u001b[A\n"," 70% 8287/11873 [00:33<00:15, 229.97it/s]\u001b[A\n"," 70% 8311/11873 [00:33<00:15, 230.76it/s]\u001b[A\n"," 70% 8338/11873 [00:33<00:14, 240.16it/s]\u001b[A\n"," 70% 8365/11873 [00:33<00:14, 246.68it/s]\u001b[A\n"," 71% 8390/11873 [00:33<00:14, 245.71it/s]\u001b[A\n"," 71% 8416/11873 [00:33<00:14, 246.68it/s]\u001b[A\n"," 71% 8442/11873 [00:33<00:13, 249.63it/s]\u001b[A\n"," 71% 8468/11873 [00:33<00:13, 251.61it/s]\u001b[A\n"," 72% 8494/11873 [00:34<00:13, 253.87it/s]\u001b[A\n"," 72% 8521/11873 [00:34<00:13, 257.57it/s]\u001b[A\n"," 72% 8547/11873 [00:34<00:13, 255.14it/s]\u001b[A\n"," 72% 8574/11873 [00:34<00:12, 258.31it/s]\u001b[A\n"," 72% 8601/11873 [00:34<00:12, 260.00it/s]\u001b[A\n"," 73% 8628/11873 [00:34<00:13, 245.67it/s]\u001b[A\n"," 73% 8653/11873 [00:34<00:13, 240.74it/s]\u001b[A\n"," 73% 8678/11873 [00:34<00:13, 230.78it/s]\u001b[A\n"," 73% 8702/11873 [00:34<00:13, 230.67it/s]\u001b[A\n"," 73% 8726/11873 [00:35<00:13, 229.82it/s]\u001b[A\n"," 74% 8753/11873 [00:35<00:13, 239.39it/s]\u001b[A\n"," 74% 8779/11873 [00:35<00:12, 245.22it/s]\u001b[A\n"," 74% 8805/11873 [00:35<00:12, 249.09it/s]\u001b[A\n"," 74% 8830/11873 [00:35<00:12, 243.75it/s]\u001b[A\n"," 75% 8855/11873 [00:35<00:12, 242.97it/s]\u001b[A\n"," 75% 8880/11873 [00:35<00:12, 237.29it/s]\u001b[A\n"," 75% 8904/11873 [00:35<00:12, 234.24it/s]\u001b[A\n"," 75% 8928/11873 [00:35<00:12, 232.59it/s]\u001b[A\n"," 75% 8952/11873 [00:35<00:12, 229.90it/s]\u001b[A\n"," 76% 8977/11873 [00:36<00:12, 234.58it/s]\u001b[A\n"," 76% 9003/11873 [00:36<00:11, 241.60it/s]\u001b[A\n"," 76% 9028/11873 [00:36<00:11, 239.03it/s]\u001b[A\n"," 76% 9052/11873 [00:36<00:12, 234.23it/s]\u001b[A\n"," 76% 9076/11873 [00:36<00:12, 232.86it/s]\u001b[A\n"," 77% 9101/11873 [00:36<00:11, 236.23it/s]\u001b[A\n"," 77% 9125/11873 [00:36<00:11, 233.09it/s]\u001b[A\n"," 77% 9149/11873 [00:36<00:11, 227.31it/s]\u001b[A\n"," 77% 9173/11873 [00:36<00:11, 228.48it/s]\u001b[A\n"," 77% 9196/11873 [00:37<00:17, 153.95it/s]\u001b[A\n"," 78% 9219/11873 [00:37<00:15, 170.05it/s]\u001b[A\n"," 78% 9242/11873 [00:37<00:14, 184.12it/s]\u001b[A\n"," 78% 9264/11873 [00:37<00:13, 192.51it/s]\u001b[A\n"," 78% 9286/11873 [00:37<00:13, 195.62it/s]\u001b[A\n"," 78% 9309/11873 [00:37<00:12, 203.77it/s]\u001b[A\n"," 79% 9331/11873 [00:37<00:12, 208.15it/s]\u001b[A\n"," 79% 9354/11873 [00:37<00:11, 213.05it/s]\u001b[A\n"," 79% 9376/11873 [00:38<00:11, 213.73it/s]\u001b[A\n"," 79% 9402/11873 [00:38<00:10, 225.86it/s]\u001b[A\n"," 79% 9428/11873 [00:38<00:10, 235.20it/s]\u001b[A\n"," 80% 9455/11873 [00:38<00:09, 243.24it/s]\u001b[A\n"," 80% 9481/11873 [00:38<00:09, 246.68it/s]\u001b[A\n"," 80% 9506/11873 [00:38<00:09, 241.96it/s]\u001b[A\n"," 80% 9531/11873 [00:38<00:09, 237.43it/s]\u001b[A\n"," 80% 9555/11873 [00:38<00:09, 234.34it/s]\u001b[A\n"," 81% 9579/11873 [00:38<00:09, 232.87it/s]\u001b[A\n"," 81% 9603/11873 [00:38<00:09, 229.50it/s]\u001b[A\n"," 81% 9627/11873 [00:39<00:09, 230.83it/s]\u001b[A\n"," 81% 9654/11873 [00:39<00:09, 241.30it/s]\u001b[A\n"," 82% 9680/11873 [00:39<00:08, 244.01it/s]\u001b[A\n"," 82% 9707/11873 [00:39<00:08, 249.67it/s]\u001b[A\n"," 82% 9732/11873 [00:39<00:08, 248.63it/s]\u001b[A\n"," 82% 9758/11873 [00:39<00:08, 251.36it/s]\u001b[A\n"," 82% 9784/11873 [00:39<00:08, 242.06it/s]\u001b[A\n"," 83% 9809/11873 [00:39<00:08, 234.78it/s]\u001b[A\n"," 83% 9833/11873 [00:39<00:08, 232.56it/s]\u001b[A\n"," 83% 9857/11873 [00:40<00:08, 227.98it/s]\u001b[A\n"," 83% 9883/11873 [00:40<00:08, 236.21it/s]\u001b[A\n"," 83% 9910/11873 [00:40<00:08, 244.01it/s]\u001b[A\n"," 84% 9937/11873 [00:40<00:07, 251.25it/s]\u001b[A\n"," 84% 9963/11873 [00:40<00:07, 251.30it/s]\u001b[A\n"," 84% 9989/11873 [00:40<00:07, 247.43it/s]\u001b[A\n"," 84% 10016/11873 [00:40<00:07, 252.03it/s]\u001b[A\n"," 85% 10044/11873 [00:40<00:07, 259.60it/s]\u001b[A\n"," 85% 10071/11873 [00:40<00:07, 257.35it/s]\u001b[A\n"," 85% 10097/11873 [00:40<00:07, 252.82it/s]\u001b[A\n"," 85% 10123/11873 [00:41<00:06, 250.08it/s]\u001b[A\n"," 85% 10151/11873 [00:41<00:06, 257.28it/s]\u001b[A\n"," 86% 10178/11873 [00:41<00:06, 259.93it/s]\u001b[A\n"," 86% 10205/11873 [00:41<00:06, 258.68it/s]\u001b[A\n"," 86% 10233/11873 [00:41<00:06, 262.09it/s]\u001b[A\n"," 86% 10260/11873 [00:41<00:06, 252.44it/s]\u001b[A\n"," 87% 10286/11873 [00:41<00:06, 242.21it/s]\u001b[A\n"," 87% 10311/11873 [00:41<00:06, 235.27it/s]\u001b[A\n"," 87% 10335/11873 [00:41<00:06, 234.78it/s]\u001b[A\n"," 87% 10359/11873 [00:42<00:06, 229.70it/s]\u001b[A\n"," 87% 10385/11873 [00:42<00:06, 235.96it/s]\u001b[A\n"," 88% 10412/11873 [00:42<00:06, 243.16it/s]\u001b[A\n"," 88% 10437/11873 [00:42<00:05, 244.31it/s]\u001b[A\n"," 88% 10462/11873 [00:42<00:05, 243.42it/s]\u001b[A\n"," 88% 10489/11873 [00:42<00:05, 249.02it/s]\u001b[A\n"," 89% 10516/11873 [00:42<00:05, 254.78it/s]\u001b[A\n"," 89% 10543/11873 [00:42<00:05, 256.93it/s]\u001b[A\n"," 89% 10571/11873 [00:42<00:04, 261.27it/s]\u001b[A\n"," 89% 10598/11873 [00:42<00:04, 257.25it/s]\u001b[A\n"," 89% 10625/11873 [00:43<00:04, 259.11it/s]\u001b[A\n"," 90% 10651/11873 [00:43<00:04, 256.75it/s]\u001b[A\n"," 90% 10677/11873 [00:43<00:04, 257.55it/s]\u001b[A\n"," 90% 10703/11873 [00:43<00:04, 257.80it/s]\u001b[A\n"," 90% 10729/11873 [00:43<00:04, 253.01it/s]\u001b[A\n"," 91% 10755/11873 [00:43<00:04, 252.05it/s]\u001b[A\n"," 91% 10782/11873 [00:43<00:04, 254.83it/s]\u001b[A\n"," 91% 10808/11873 [00:43<00:04, 250.15it/s]\u001b[A\n"," 91% 10834/11873 [00:43<00:04, 252.50it/s]\u001b[A\n"," 91% 10860/11873 [00:43<00:03, 254.18it/s]\u001b[A\n"," 92% 10886/11873 [00:44<00:03, 254.82it/s]\u001b[A\n"," 92% 10913/11873 [00:44<00:03, 258.05it/s]\u001b[A\n"," 92% 10939/11873 [00:44<00:03, 252.21it/s]\u001b[A\n"," 92% 10965/11873 [00:44<00:03, 251.57it/s]\u001b[A\n"," 93% 10992/11873 [00:44<00:03, 256.64it/s]\u001b[A\n"," 93% 11018/11873 [00:44<00:03, 243.79it/s]\u001b[A\n"," 93% 11043/11873 [00:44<00:03, 239.32it/s]\u001b[A\n"," 93% 11068/11873 [00:44<00:03, 237.47it/s]\u001b[A\n"," 93% 11092/11873 [00:44<00:03, 232.09it/s]\u001b[A\n"," 94% 11116/11873 [00:45<00:03, 230.83it/s]\u001b[A\n"," 94% 11143/11873 [00:45<00:03, 241.57it/s]\u001b[A\n"," 94% 11171/11873 [00:45<00:02, 250.50it/s]\u001b[A\n"," 94% 11197/11873 [00:45<00:02, 248.66it/s]\u001b[A\n"," 95% 11222/11873 [00:45<00:02, 245.70it/s]\u001b[A\n"," 95% 11247/11873 [00:45<00:02, 246.01it/s]\u001b[A\n"," 95% 11273/11873 [00:45<00:02, 249.01it/s]\u001b[A\n"," 95% 11300/11873 [00:45<00:02, 254.62it/s]\u001b[A\n"," 95% 11326/11873 [00:45<00:02, 253.29it/s]\u001b[A\n"," 96% 11353/11873 [00:45<00:02, 256.59it/s]\u001b[A\n"," 96% 11379/11873 [00:46<00:01, 252.53it/s]\u001b[A\n"," 96% 11405/11873 [00:46<00:01, 251.20it/s]\u001b[A\n"," 96% 11431/11873 [00:46<00:01, 248.92it/s]\u001b[A\n"," 96% 11456/11873 [00:46<00:01, 242.07it/s]\u001b[A\n"," 97% 11481/11873 [00:46<00:01, 236.86it/s]\u001b[A\n"," 97% 11505/11873 [00:46<00:01, 230.99it/s]\u001b[A\n"," 97% 11529/11873 [00:46<00:01, 229.55it/s]\u001b[A\n"," 97% 11552/11873 [00:46<00:01, 220.61it/s]\u001b[A\n"," 97% 11575/11873 [00:46<00:01, 221.70it/s]\u001b[A\n"," 98% 11598/11873 [00:47<00:01, 221.95it/s]\u001b[A\n"," 98% 11622/11873 [00:47<00:01, 225.09it/s]\u001b[A\n"," 98% 11645/11873 [00:47<00:01, 225.93it/s]\u001b[A\n"," 98% 11669/11873 [00:47<00:00, 228.81it/s]\u001b[A\n"," 99% 11695/11873 [00:47<00:00, 235.53it/s]\u001b[A\n"," 99% 11722/11873 [00:47<00:00, 242.72it/s]\u001b[A\n"," 99% 11750/11873 [00:47<00:00, 251.94it/s]\u001b[A\n"," 99% 11777/11873 [00:47<00:00, 255.28it/s]\u001b[A\n"," 99% 11804/11873 [00:47<00:00, 257.39it/s]\u001b[A\n","100% 11831/11873 [00:47<00:00, 258.21it/s]\u001b[A\n","100% 11873/11873 [00:48<00:00, 246.68it/s]\n","04/07/2022 23:31:09 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/eval_predictions.json.\n","04/07/2022 23:31:09 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/eval_nbest_predictions.json.\n","04/07/2022 23:31:11 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/albert-base-v2/squad_v2/eval_null_odds.json.\n","04/07/2022 23:31:15 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1496/1496 [05:34<00:00,  4.47it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 75.8603\n","  eval_HasAns_f1         = 82.2574\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 82.6409\n","  eval_NoAns_f1          = 82.6409\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 79.2639\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 82.4578\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 79.2555\n","  eval_f1                = 82.4494\n","  eval_samples           =   11968\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-07 23:31:15,887 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]}]}