{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1649303569246,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"},"user_tz":240},"id":"70oLi9mZP6oK","outputId":"2ec66fe1-7f2d-47e4-d839-73a2deb2405a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Apr  7 03:52:48 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8069,"status":"ok","timestamp":1649303581833,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"},"user_tz":240},"id":"fGqcN-zXTvvo","outputId":"87875bb9-3742-455c-a246-eb1b354ee0c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n","\u001b[K     |████████████████████████████████| 325 kB 13.4 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 79.2 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 7.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 92.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 65.5 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 90.3 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 91.6 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.7 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 84.1 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.1 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23552,"status":"ok","timestamp":1649303605377,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"},"user_tz":240},"id":"tI_RBT1FSotu","outputId":"e1585d3b-610e-4ff1-8511-3210bc93d6c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-a4e0f2d1\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-a4e0f2d1\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (3.6.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 14.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.63.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 76.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (21.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 58.1 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.11.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (1.21.5)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.5.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0.dev0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.0.dev0) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.0.dev0) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (1.25.11)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.15.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.19.0.dev0-py3-none-any.whl size=3965509 sha256=2cadf1aaf43fb462822623ffcb2f41eed616c32b70a76d80f16330b22f54c8a4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-hia6cuj2/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.19.0.dev0\n"]}],"source":["!pip install git+https://github.com/huggingface/transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8251,"status":"ok","timestamp":1649303613620,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"},"user_tz":240},"id":"DZ3Ma-pCRJDJ"},"outputs":[],"source":["import torch\n","import datetime\n","import json\n","import os\n","import time\n","import datasets\n","import pprint\n","import random\n","import string\n","import sys\n","import transformers\n","from datasets import load_dataset\n","from datasets.tasks import QuestionAnsweringExtractive"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":48,"status":"ok","timestamp":1649303613621,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"},"user_tz":240},"id":"HNMUVyBpRGw8"},"outputs":[],"source":["device = torch.device('cuda:0' if torch.cuda.is_available()\n","                      else 'cpu')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7311,"status":"ok","timestamp":1649303620887,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"},"user_tz":240},"id":"WUVkgX-IQIiR","outputId":"2cf20339-9995-4a25-e3b3-a5faa1d5a745"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 108786, done.\u001b[K\n","remote: Total 108786 (delta 0), reused 0 (delta 0), pack-reused 108786\u001b[K\n","Receiving objects: 100% (108786/108786), 95.61 MiB | 29.17 MiB/s, done.\n","Resolving deltas: 100% (79286/79286), done.\n"]}],"source":["!git clone https://github.com/huggingface/transformers.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":148,"referenced_widgets":["c53b552b3046422cbbc1a9314010a9ef","059a3821d64c4b5394b5ec0617952b42","1a1643c836694a52a45c89a8ac27bf7e","abed2bd4e6704091a1abc092c83e4f27","068b167d0b9846e2917fc4995e83382e","665143d116fe47bd9a4367b27f539a27","821ccca36cd549fb8d0afae9591b0331","6a47b278ee534bb9a558fd9ca48759cb","15496ce6e05c492f8f5f236b754bed74","1559ab30c6684f47a4b7ed5ce99bb6f8","59706f5001974dd3ad4c34144b800fc4","02a0411d1b174005a18091234a9d67a2","21996906329446538b50c22a21865e7d","7472b853cad348499798a924a1b2ff23","05a543f86be74ce1839262a94c987966","a913988c53d949579e4a7482d540546e","929b29353ef94a52b784e4ed43a740e1","54e2bc21dbea48f3944946ae89d0cb6e","9670c85997b844908eda42f68729e4e4","e429eacc0446467c8ba4aae08202c65d","8bb38c18b23b4043a34e12ca27942428","981a88d996d942baabfde1d479e90acf","44e798e35c764164a879c8011a19bf71","e0fbead99d4e4a40a710c87edd4c6cc6","7a5681bd5ca249bc9bd51448207594b2","9059187f87284e278e97e36bcf1e62a6","9be731c3e52545d09b5c9a5dcd4fdbf5","d9a33d89db894e799292ee7d2aa9b818","8e797935275b4334a7d46a07498cff69","1e57b68b1a2e4826ae8d9400c22babee","c02c35d2883a45498d31b02e259a5281","c83b6e30652343e18d34a8889ced5f7e","72216258ffa64482b8ee5269ae65f4da","6b4b66b2dc384f209dbb18fa1574dd5e","c98e2cfe98614c2d88a4099167de87d3","74544654e50a47de9e01353a9c7e28b3","3a737310a7894ca99899aca198ca842e","276254bd90314a84ae10cf25c87e4b85","a2ee9ff13e40458b88034611cd754fd8","4ade28ba713f459d8d17555cb28707ca","5b99c1a4265d42ad9c3031643364b50c","6583788bb3fd4c3e92bc8ff3741eccdd","98864334e94544e7802edc13b347de6a","a0970da474cc4134916bf1626887fe4e","9f7a7915d2814280b0b2c839cbc2c1ad","acb6c40790d74aa792049af0215bc8f0","fa0b91dfcf1748e090483d2f1f3c890d","ae439403d6a44f56ad446fb5eeec9bcb","d34c43cd27fc466082103a4fd5731d8e","1988accb45e546f7ad9bea91e98d4c62","219f8f658bfc4602acdeaaa76f6c8d10","0fbd7a369a1c4918991fbf09d510d9e2","b3b5ec66d7384c2e8b462fd5b1d78d1c","38d85533215b4e54b7602ee3568752f3","54dffadc44f74175b7c2330860c1e400"]},"executionInfo":{"elapsed":14062,"status":"ok","timestamp":1648665030407,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"},"user_tz":240},"id":"A2gdlPTvcmfL","outputId":"c73ae981-4b3b-4074-bd7c-ba5ec9c8be51"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset squad_v2_aug/squad_v2 to /root/.cache/huggingface/datasets/squad_v2_aug/squad_v2/2.0.0/a6ff19c6a82702c2aa0c4d1d27ec21e800e6c54b42db865092833d363b6ce641...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c53b552b3046422cbbc1a9314010a9ef","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02a0411d1b174005a18091234a9d67a2","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44e798e35c764164a879c8011a19bf71","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b4b66b2dc384f209dbb18fa1574dd5e","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset squad_v2_aug downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2_aug/squad_v2/2.0.0/a6ff19c6a82702c2aa0c4d1d27ec21e800e6c54b42db865092833d363b6ce641. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f7a7915d2814280b0b2c839cbc2c1ad","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset = load_dataset('/content/drive/MyDrive/QA/squad_v2_context_aug.py')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":99804,"status":"ok","timestamp":1648665130206,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"},"user_tz":240},"id":"ttpAfwxBWdR0","outputId":"ab901899-9fd7-49ff-b51a-dbc4a20f0026"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/token.\n","        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n","        \n","Token: \n","Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}],"source":["!huggingface-cli login"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":150,"referenced_widgets":["94e690859db3472eac5dd0bf85693759","96562ad8892c49928e5b58f3a0a190ac","49061f8bf2554f228aae6a54db470e98","41edc0d9698c4664904f1a515d5cf9df","bb4fffff64274e948f138352508dac77","1c5b969516984f559e98ae0ad799018a","0280aac295f1493593239fb2b074bcb2","29b918ac261f41d7949235da9206bea9","819a787be3cc49d2b09addccf9175571","3e805b74f9ca428eb31596a4c9857ed4","c6a00020495b4c009ae2b83f48235dda","563e525dac334b87bf92ee42fa0e84dd","593561532e43409c9d7c01d02c625d3d","db1f92a2e01e4e23ac077fe2fedf6359","e1ef8012f2e24c7d81fe1dcc5ab6f54e","9fc753e451314823b55fd33a6831c08d","db044458deaa40a9a3a9bb48b67784e5","6d7cf18112d848839291891beeb4f287","9638c3f4d8424097a376ca01a61cf9a8","15fbb61b20254d838b69e08cf3f8e3db","3f67171dfa55404a9aee758fa21f8b32","38201d944dfa4922a7aafecab9b44ce7"]},"executionInfo":{"elapsed":9793,"status":"ok","timestamp":1648665139990,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"},"user_tz":240},"id":"WGAeWENJWR_F","outputId":"1b3e66a0-fb6f-4458-8459-d7566df62ef1"},"outputs":[{"name":"stderr","output_type":"stream","text":["Pushing split train to the Hub.\n","The repository already exists: the `private` keyword argument will be ignored.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94e690859db3472eac5dd0bf85693759","version_major":2,"version_minor":0},"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Pushing split validation to the Hub.\n","The repository already exists: the `private` keyword argument will be ignored.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"563e525dac334b87bf92ee42fa0e84dd","version_major":2,"version_minor":0},"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset.push_to_hub(\"sichenzhong/squad_v2_context_aug\")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1649303656472,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"},"user_tz":240},"id":"5uLaizsXQrk9","outputId":"86abc910-4aa2-4ff4-dcb4-bb0f8683d6c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/transformers/examples/pytorch/question-answering\n"]}],"source":["%cd /content/transformers/examples/pytorch/question-answering/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16201382,"status":"ok","timestamp":1649272326410,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"},"user_tz":240},"id":"aI-ipSbaHqF6","outputId":"de14b7a5-4591-48be-bc67-e587efafc369"},"outputs":[{"name":"stdout","output_type":"stream","text":["04/06/2022 14:42:08 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 14:42:08 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/runs/Apr06_14-42-08_17ba0eb63658,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 14:42:10 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/sichenzhong/squad_v2_context_aug/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp2korn5_h\n","Downloading: 100% 2.13k/2.13k [00:00<00:00, 2.12MB/s]\n","04/06/2022 14:42:11 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/sichenzhong/squad_v2_context_aug/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/5d96c9f73e10ee77ef531d62c77defd567888f093d2605b35e61077687d9f357.dc0163e043302d51d7d8cf3063bc1fc39774efd5ef1fed729be7638c509f1765\n","04/06/2022 14:42:11 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5d96c9f73e10ee77ef531d62c77defd567888f093d2605b35e61077687d9f357.dc0163e043302d51d7d8cf3063bc1fc39774efd5ef1fed729be7638c509f1765\n","04/06/2022 14:42:11 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7\n","04/06/2022 14:42:11 - INFO - datasets.builder - Generating dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","Downloading and preparing dataset squad_v2_aug/squad_v2 (download: 17.20 MiB, generated: 122.52 MiB, post-processed: Unknown size, total: 139.71 MiB) to /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n","04/06/2022 14:42:11 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]04/06/2022 14:42:11 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/sichenzhong/squad_v2_context_aug/resolve/f6ebcececd0703d02de2a54a8aa934fc69363379/data/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpk3nsn1vi\n","\n","Downloading data:   0% 0.00/16.8M [00:00<?, ?B/s]\u001b[A\n","Downloading data:   0% 52.2k/16.8M [00:00<00:54, 306kB/s]\u001b[A\n","Downloading data:   2% 261k/16.8M [00:00<00:20, 823kB/s] \u001b[A\n","Downloading data:   5% 912k/16.8M [00:00<00:05, 2.65MB/s]\u001b[A\n","Downloading data:  14% 2.30M/16.8M [00:00<00:02, 5.76MB/s]\u001b[A\n","Downloading data:  28% 4.68M/16.8M [00:00<00:01, 10.1MB/s]\u001b[A\n","Downloading data:  48% 8.08M/16.8M [00:00<00:00, 16.8MB/s]\u001b[A\n","Downloading data:  68% 11.5M/16.8M [00:00<00:00, 21.6MB/s]\u001b[A\n","Downloading data: 100% 16.8M/16.8M [00:01<00:00, 15.5MB/s]\n","04/06/2022 14:42:13 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/sichenzhong/squad_v2_context_aug/resolve/f6ebcececd0703d02de2a54a8aa934fc69363379/data/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/466c1fcec0e7d0ff0b91af0ec8bb3a8605e4866e4b553302a61edee160c5a842\n","04/06/2022 14:42:13 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/466c1fcec0e7d0ff0b91af0ec8bb3a8605e4866e4b553302a61edee160c5a842\n","Downloading data files:  50% 1/2 [00:02<00:02,  2.77s/it]04/06/2022 14:42:14 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/sichenzhong/squad_v2_context_aug/resolve/f6ebcececd0703d02de2a54a8aa934fc69363379/data/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp9w9gx1pg\n","\n","Downloading data:   0% 0.00/1.26M [00:00<?, ?B/s]\u001b[A\n","Downloading data:   3% 34.8k/1.26M [00:00<00:06, 198kB/s]\u001b[A\n","Downloading data:  19% 244k/1.26M [00:00<00:01, 773kB/s] \u001b[A\n","Downloading data: 100% 1.26M/1.26M [00:00<00:00, 2.34MB/s]\n","04/06/2022 14:42:15 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/sichenzhong/squad_v2_context_aug/resolve/f6ebcececd0703d02de2a54a8aa934fc69363379/data/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/20d1cdd89847805c2e5c746c86671034d0019def38aa9fa47592e6eca039699f\n","04/06/2022 14:42:15 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/20d1cdd89847805c2e5c746c86671034d0019def38aa9fa47592e6eca039699f\n","Downloading data files: 100% 2/2 [00:04<00:00,  2.33s/it]\n","04/06/2022 14:42:15 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","04/06/2022 14:42:15 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1392.76it/s]\n","04/06/2022 14:42:15 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n","04/06/2022 14:42:15 - INFO - datasets.builder - Generating train split\n","04/06/2022 14:42:16 - INFO - datasets.builder - Generating validation split\n","04/06/2022 14:42:16 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 338.76it/s]\n","[INFO|hub.py:583] 2022-04-06 14:42:16,458 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpt6yof06w\n","Downloading: 100% 570/570 [00:00<00:00, 594kB/s]\n","[INFO|hub.py:587] 2022-04-06 14:42:16,815 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|hub.py:595] 2022-04-06 14:42:16,815 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:654] 2022-04-06 14:42:16,815 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 14:42:16,816 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 14:42:17,174 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptqwautvq\n","Downloading: 100% 29.0/29.0 [00:00<00:00, 25.9kB/s]\n","[INFO|hub.py:587] 2022-04-06 14:42:17,531 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|hub.py:595] 2022-04-06 14:42:17,531 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:654] 2022-04-06 14:42:17,890 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 14:42:17,891 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 14:42:18,609 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp17a_5jsu\n","Downloading: 100% 208k/208k [00:00<00:00, 631kB/s]\n","[INFO|hub.py:587] 2022-04-06 14:42:19,307 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:595] 2022-04-06 14:42:19,307 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:583] 2022-04-06 14:42:19,670 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplsxflvfu\n","Downloading: 100% 426k/426k [00:00<00:00, 1.03MB/s]\n","[INFO|hub.py:587] 2022-04-06 14:42:20,458 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|hub.py:595] 2022-04-06 14:42:20,458 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:42:21,528 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:42:21,528 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:42:21,528 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:42:21,528 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:42:21,528 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:654] 2022-04-06 14:42:21,887 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 14:42:21,888 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 14:42:22,290 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpk_grfiv7\n","Downloading: 100% 416M/416M [00:06<00:00, 72.3MB/s]\n","[INFO|hub.py:587] 2022-04-06 14:42:28,447 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|hub.py:595] 2022-04-06 14:42:28,447 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|modeling_utils.py:1772] 2022-04-06 14:42:28,448 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:2049] 2022-04-06 14:42:31,441 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-04-06 14:42:31,441 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/06/2022 14:42:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-091f45f2274bf280.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:42<00:00,  3.11ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/06/2022 14:43:13 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-d9ad59e21faba2b9.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:01<00:00,  5.13s/ba]\n","04/06/2022 14:44:15 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpziznmygy\n","Downloading builder script: 6.46kB [00:00, 7.76MB/s]       \n","04/06/2022 14:44:15 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/06/2022 14:44:15 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/06/2022 14:44:15 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpl_d7ktym\n","Downloading extra modules: 11.3kB [00:00, 14.4MB/s]       \n","04/06/2022 14:44:15 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","04/06/2022 14:44:15 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 14:44:25,759 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 14:44:25,759 >>   Num examples = 132067\n","[INFO|trainer.py:1292] 2022-04-06 14:44:25,759 >>   Num Epochs = 3\n","[INFO|trainer.py:1293] 2022-04-06 14:44:25,759 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1294] 2022-04-06 14:44:25,759 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1295] 2022-04-06 14:44:25,759 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 14:44:25,759 >>   Total optimization steps = 24765\n","{'loss': 2.7063, 'learning_rate': 3.9192408641227546e-05, 'epoch': 0.06}\n","  2% 500/24765 [05:16<4:15:16,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 14:49:42,182 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 14:49:42,188 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:49:43,210 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:49:43,215 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:49:43,218 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 2.1136, 'learning_rate': 3.838481728245508e-05, 'epoch': 0.12}\n","  4% 1000/24765 [10:35<4:10:19,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 14:55:01,563 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 14:55:01,567 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:55:02,552 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:55:02,555 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:55:02,558 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.9893, 'learning_rate': 3.7577225923682624e-05, 'epoch': 0.18}\n","  6% 1500/24765 [15:55<4:05:03,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:00:20,903 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:00:20,908 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:00:21,888 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:00:21,891 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:00:21,894 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.8912, 'learning_rate': 3.676963456491016e-05, 'epoch': 0.24}\n","  8% 2000/24765 [21:14<3:59:53,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:05:40,320 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:05:40,325 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:05:41,293 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:05:41,297 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:05:41,299 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.8414, 'learning_rate': 3.5962043206137695e-05, 'epoch': 0.3}\n"," 10% 2500/24765 [26:33<3:54:54,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:10:59,580 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:10:59,584 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:11:00,553 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:11:00,557 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:11:00,559 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.8138, 'learning_rate': 3.515445184736523e-05, 'epoch': 0.36}\n"," 12% 3000/24765 [31:53<3:49:14,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:16:18,818 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:16:18,822 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:16:19,798 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:16:19,802 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:16:19,805 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.7272, 'learning_rate': 3.4346860488592774e-05, 'epoch': 0.42}\n"," 14% 3500/24765 [37:12<3:43:46,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:21:38,175 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:21:38,182 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:21:39,161 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:21:39,165 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:21:39,167 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.7302, 'learning_rate': 3.3539269129820316e-05, 'epoch': 0.48}\n"," 16% 4000/24765 [42:31<3:38:51,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:26:57,526 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:26:57,530 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:26:58,513 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:26:58,968 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:26:58,970 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.6582, 'learning_rate': 3.273167777104785e-05, 'epoch': 0.55}\n"," 18% 4500/24765 [47:51<3:33:19,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:32:17,278 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:32:17,283 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:32:18,278 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:32:18,281 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:32:18,594 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.6623, 'learning_rate': 3.1924086412275394e-05, 'epoch': 0.61}\n"," 20% 5000/24765 [53:11<3:28:09,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:37:36,956 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:37:36,961 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:37:37,951 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:37:37,955 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:37:37,974 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.638, 'learning_rate': 3.111649505350293e-05, 'epoch': 0.67}\n"," 22% 5500/24765 [58:30<3:22:58,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:42:56,331 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:42:56,336 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:42:57,318 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:42:57,322 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:42:57,324 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.5991, 'learning_rate': 3.030890369473047e-05, 'epoch': 0.73}\n"," 24% 6000/24765 [1:03:50<3:17:37,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:48:16,132 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:48:16,137 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:48:17,115 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:48:17,119 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:48:17,122 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.5797, 'learning_rate': 2.9501312335958005e-05, 'epoch': 0.79}\n"," 26% 6500/24765 [1:09:09<3:12:21,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:53:35,497 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:53:35,503 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:53:36,471 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:53:36,474 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:53:36,477 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.5855, 'learning_rate': 2.8693720977185547e-05, 'epoch': 0.85}\n"," 28% 7000/24765 [1:14:29<3:07:11,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:58:54,827 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:58:54,832 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:58:55,818 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:58:55,822 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:58:55,825 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.5431, 'learning_rate': 2.7886129618413086e-05, 'epoch': 0.91}\n"," 30% 7500/24765 [1:19:48<3:01:55,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:04:14,334 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:04:14,339 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:04:15,310 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:04:15,313 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:04:15,316 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 1.5399, 'learning_rate': 2.7078538259640622e-05, 'epoch': 0.97}\n"," 32% 8000/24765 [1:25:11<2:56:28,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:09:37,030 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:09:37,035 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:09:38,019 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:09:38,023 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:09:38,026 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 1.3847, 'learning_rate': 2.6270946900868165e-05, 'epoch': 1.03}\n"," 34% 8500/24765 [1:30:33<2:51:17,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:14:58,876 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:14:58,881 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:14:59,890 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:14:59,894 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:14:59,897 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 1.1861, 'learning_rate': 2.54633555420957e-05, 'epoch': 1.09}\n"," 36% 9000/24765 [1:35:55<2:45:59,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:20:20,833 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:20:20,838 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:20:21,835 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:20:21,838 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:20:21,841 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 1.1984, 'learning_rate': 2.465576418332324e-05, 'epoch': 1.15}\n"," 38% 9500/24765 [1:41:17<2:40:53,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:25:43,175 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:25:43,180 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:25:44,162 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:25:44,166 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:25:44,168 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 1.1842, 'learning_rate': 2.384817282455078e-05, 'epoch': 1.21}\n"," 40% 10000/24765 [1:46:36<2:35:34,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:31:02,471 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:31:02,477 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:31:03,472 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:31:03,476 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:31:03,479 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 1.1841, 'learning_rate': 2.3040581465778318e-05, 'epoch': 1.27}\n"," 42% 10500/24765 [1:51:55<2:30:13,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:36:21,436 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:36:21,440 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:36:22,433 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:36:22,437 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:36:22,440 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-10500/special_tokens_map.json\n","{'loss': 1.1718, 'learning_rate': 2.223299010700586e-05, 'epoch': 1.33}\n"," 44% 11000/24765 [1:57:14<2:25:06,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:41:40,780 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:41:40,785 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:41:41,779 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:41:41,783 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:41:41,785 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-11000/special_tokens_map.json\n","{'loss': 1.1739, 'learning_rate': 2.1425398748233396e-05, 'epoch': 1.39}\n"," 46% 11500/24765 [2:02:34<2:19:31,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:47:00,242 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-11500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:47:00,246 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:47:01,265 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:47:01,272 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:47:01,275 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-11500/special_tokens_map.json\n","{'loss': 1.144, 'learning_rate': 2.0617807389460935e-05, 'epoch': 1.45}\n"," 48% 12000/24765 [2:07:53<2:14:27,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:52:19,318 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-12000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:52:19,323 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:52:20,312 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:52:20,316 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:52:20,318 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-12000/special_tokens_map.json\n","{'loss': 1.1803, 'learning_rate': 1.9810216030688474e-05, 'epoch': 1.51}\n"," 50% 12500/24765 [2:13:12<2:09:06,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:57:38,517 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-12500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:57:38,522 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:57:39,519 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:57:39,523 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:57:39,526 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-12500/special_tokens_map.json\n","{'loss': 1.1934, 'learning_rate': 1.9002624671916013e-05, 'epoch': 1.57}\n"," 52% 13000/24765 [2:18:31<2:03:54,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:02:57,704 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-13000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:02:57,709 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:02:58,717 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:02:58,720 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:02:58,723 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-13000/special_tokens_map.json\n","{'loss': 1.1866, 'learning_rate': 1.8195033313143552e-05, 'epoch': 1.64}\n"," 55% 13500/24765 [2:23:51<1:58:38,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:08:17,050 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-13500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:08:17,055 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:08:18,055 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:08:18,058 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:08:18,061 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-13500/special_tokens_map.json\n","{'loss': 1.1831, 'learning_rate': 1.7387441954371088e-05, 'epoch': 1.7}\n"," 57% 14000/24765 [2:29:10<1:53:18,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:13:36,336 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-14000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:13:36,342 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:13:37,341 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:13:37,344 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:13:37,347 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-14000/special_tokens_map.json\n","{'loss': 1.1603, 'learning_rate': 1.6579850595598627e-05, 'epoch': 1.76}\n"," 59% 14500/24765 [2:34:29<1:48:06,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:18:55,432 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-14500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:18:55,449 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:18:56,434 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:18:56,437 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:18:56,439 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-14500/special_tokens_map.json\n","{'loss': 1.1608, 'learning_rate': 1.5772259236826166e-05, 'epoch': 1.82}\n"," 61% 15000/24765 [2:39:48<1:42:48,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:24:14,465 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-15000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:24:14,470 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:24:15,487 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:24:15,491 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:24:15,493 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-15000/special_tokens_map.json\n","{'loss': 1.1532, 'learning_rate': 1.4964667878053707e-05, 'epoch': 1.88}\n"," 63% 15500/24765 [2:45:07<1:37:30,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:29:33,593 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-15500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:29:33,598 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:29:34,606 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:29:34,610 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:29:34,630 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-15500/special_tokens_map.json\n","{'loss': 1.1617, 'learning_rate': 1.4157076519281246e-05, 'epoch': 1.94}\n"," 65% 16000/24765 [2:50:27<1:32:20,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:34:53,102 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-16000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:34:53,107 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:34:54,105 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:34:54,109 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:34:54,130 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-16000/special_tokens_map.json\n","{'loss': 1.1577, 'learning_rate': 1.3349485160508783e-05, 'epoch': 2.0}\n"," 67% 16500/24765 [2:55:46<1:26:55,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:40:12,448 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-16500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:40:12,452 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-16500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:40:13,463 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-16500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:40:13,483 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-16500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:40:13,486 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-16500/special_tokens_map.json\n","{'loss': 0.823, 'learning_rate': 1.2541893801736322e-05, 'epoch': 2.06}\n"," 69% 17000/24765 [3:01:05<1:21:43,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:45:31,217 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-17000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:45:31,222 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-17000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:45:32,232 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-17000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:45:32,235 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-17000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:45:32,253 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-17000/special_tokens_map.json\n","{'loss': 0.7835, 'learning_rate': 1.1734302442963861e-05, 'epoch': 2.12}\n"," 71% 17500/24765 [3:06:24<1:16:27,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:50:50,421 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-17500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:50:50,425 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-17500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:50:51,422 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-17500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:50:51,426 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-17500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:50:51,429 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-17500/special_tokens_map.json\n","{'loss': 0.8051, 'learning_rate': 1.0926711084191399e-05, 'epoch': 2.18}\n"," 73% 18000/24765 [3:11:43<1:11:12,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:56:09,642 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-18000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:56:09,647 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-18000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:56:10,712 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-18000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:56:10,716 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-18000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:56:10,719 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-18000/special_tokens_map.json\n","{'loss': 0.7956, 'learning_rate': 1.011911972541894e-05, 'epoch': 2.24}\n"," 75% 18500/24765 [3:17:03<1:05:54,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:01:29,249 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-18500\n","[INFO|configuration_utils.py:441] 2022-04-06 18:01:29,254 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-18500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:01:30,265 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-18500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:01:30,268 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-18500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:01:30,271 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-18500/special_tokens_map.json\n","{'loss': 0.809, 'learning_rate': 9.311528366646477e-06, 'epoch': 2.3}\n"," 77% 19000/24765 [3:22:25<1:00:34,  1.59it/s][INFO|trainer.py:2166] 2022-04-06 18:06:51,415 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-19000\n","[INFO|configuration_utils.py:441] 2022-04-06 18:06:51,420 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-19000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:06:52,422 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-19000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:06:52,425 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-19000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:06:52,428 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-19000/special_tokens_map.json\n","{'loss': 0.8042, 'learning_rate': 8.503937007874016e-06, 'epoch': 2.36}\n"," 79% 19500/24765 [3:27:44<55:22,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:12:10,298 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-19500\n","[INFO|configuration_utils.py:441] 2022-04-06 18:12:10,302 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-19500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:12:11,305 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-19500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:12:11,310 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-19500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:12:11,312 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-19500/special_tokens_map.json\n","{'loss': 0.7893, 'learning_rate': 7.696345649101555e-06, 'epoch': 2.42}\n"," 81% 20000/24765 [3:33:03<50:08,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:17:29,435 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-20000\n","[INFO|configuration_utils.py:441] 2022-04-06 18:17:29,441 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-20000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:17:30,438 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-20000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:17:30,441 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-20000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:17:30,444 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-20000/special_tokens_map.json\n","{'loss': 0.7869, 'learning_rate': 6.8887542903290935e-06, 'epoch': 2.48}\n"," 83% 20500/24765 [3:38:22<44:50,  1.59it/s][INFO|trainer.py:2166] 2022-04-06 18:22:48,579 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-20500\n","[INFO|configuration_utils.py:441] 2022-04-06 18:22:48,584 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-20500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:22:49,595 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-20500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:22:49,598 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-20500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:22:49,601 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-20500/special_tokens_map.json\n","{'loss': 0.7885, 'learning_rate': 6.0811629315566326e-06, 'epoch': 2.54}\n"," 85% 21000/24765 [3:43:42<39:39,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:28:07,838 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-21000\n","[INFO|configuration_utils.py:441] 2022-04-06 18:28:07,842 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-21000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:28:08,857 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-21000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:28:08,860 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-21000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:28:08,863 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-21000/special_tokens_map.json\n","{'loss': 0.7872, 'learning_rate': 5.273571572784172e-06, 'epoch': 2.6}\n"," 87% 21500/24765 [3:49:01<34:23,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:33:27,075 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-21500\n","[INFO|configuration_utils.py:441] 2022-04-06 18:33:27,080 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-21500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:33:28,079 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-21500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:33:28,083 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-21500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:33:28,086 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-21500/special_tokens_map.json\n","{'loss': 0.7953, 'learning_rate': 4.46598021401171e-06, 'epoch': 2.67}\n"," 89% 22000/24765 [3:54:20<29:07,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:38:46,194 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-22000\n","[INFO|configuration_utils.py:441] 2022-04-06 18:38:46,199 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-22000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:38:47,191 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-22000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:38:47,194 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-22000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:38:47,197 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-22000/special_tokens_map.json\n","{'loss': 0.7865, 'learning_rate': 3.6583888552392494e-06, 'epoch': 2.73}\n"," 91% 22500/24765 [3:59:42<23:48,  1.59it/s][INFO|trainer.py:2166] 2022-04-06 18:44:08,672 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-22500\n","[INFO|configuration_utils.py:441] 2022-04-06 18:44:08,678 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-22500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:44:09,674 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-22500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:44:09,678 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-22500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:44:09,682 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-22500/special_tokens_map.json\n","{'loss': 0.7868, 'learning_rate': 2.8507974964667877e-06, 'epoch': 2.79}\n"," 93% 23000/24765 [4:05:01<18:34,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:49:27,730 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-23000\n","[INFO|configuration_utils.py:441] 2022-04-06 18:49:27,735 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-23000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:49:28,726 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-23000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:49:28,730 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-23000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:49:28,732 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-23000/special_tokens_map.json\n","{'loss': 0.778, 'learning_rate': 2.0432061376943268e-06, 'epoch': 2.85}\n"," 95% 23500/24765 [4:10:21<13:18,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:54:46,842 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-23500\n","[INFO|configuration_utils.py:441] 2022-04-06 18:54:46,846 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-23500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:54:47,835 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-23500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:54:47,838 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-23500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:54:47,840 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-23500/special_tokens_map.json\n","{'loss': 0.7698, 'learning_rate': 1.2356147789218657e-06, 'epoch': 2.91}\n"," 97% 24000/24765 [4:15:40<08:03,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 19:00:05,870 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-24000\n","[INFO|configuration_utils.py:441] 2022-04-06 19:00:05,875 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-24000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 19:00:06,871 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-24000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 19:00:06,875 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-24000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 19:00:06,879 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-24000/special_tokens_map.json\n","{'loss': 0.786, 'learning_rate': 4.280234201494044e-07, 'epoch': 2.97}\n"," 99% 24500/24765 [4:20:59<02:47,  1.59it/s][INFO|trainer.py:2166] 2022-04-06 19:05:24,923 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-24500\n","[INFO|configuration_utils.py:441] 2022-04-06 19:05:24,928 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-24500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 19:05:25,935 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-24500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 19:05:25,956 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-24500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 19:05:25,958 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/checkpoint-24500/special_tokens_map.json\n","100% 24765/24765 [4:23:49<00:00,  2.06it/s][INFO|trainer.py:1530] 2022-04-06 19:08:15,259 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 15829.5022, 'train_samples_per_second': 25.029, 'train_steps_per_second': 1.564, 'train_loss': 1.249089519486099, 'epoch': 3.0}\n","100% 24765/24765 [4:23:49<00:00,  1.56it/s]\n","[INFO|trainer.py:2166] 2022-04-06 19:08:15,267 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug\n","[INFO|configuration_utils.py:441] 2022-04-06 19:08:15,271 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 19:08:16,396 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 19:08:16,400 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 19:08:16,403 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =     1.2491\n","  train_runtime            = 4:23:49.50\n","  train_samples            =     132067\n","  train_samples_per_second =     25.029\n","  train_steps_per_second   =      1.564\n","04/06/2022 19:08:16 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 19:08:16,468 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 19:08:16,484 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 19:08:16,485 >>   Num examples = 12199\n","[INFO|trainer.py:2421] 2022-04-06 19:08:16,485 >>   Batch size = 8\n","100% 1525/1525 [02:56<00:00,  8.82it/s]04/06/2022 19:11:25 - INFO - utils_qa - Post-processing 11873 example predictions split into 12199 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 38/11873 [00:00<00:31, 377.44it/s]\u001b[A\n","  1% 76/11873 [00:00<00:31, 377.47it/s]\u001b[A\n","  1% 122/11873 [00:00<00:28, 414.99it/s]\u001b[A\n","  1% 171/11873 [00:00<00:26, 443.47it/s]\u001b[A\n","  2% 219/11873 [00:00<00:25, 454.63it/s]\u001b[A\n","  2% 266/11873 [00:00<00:25, 459.70it/s]\u001b[A\n","  3% 315/11873 [00:00<00:24, 469.47it/s]\u001b[A\n","  3% 362/11873 [00:00<00:25, 460.11it/s]\u001b[A\n","  3% 409/11873 [00:00<00:24, 461.93it/s]\u001b[A\n","  4% 458/11873 [00:01<00:24, 468.30it/s]\u001b[A\n","  4% 505/11873 [00:01<00:24, 465.47it/s]\u001b[A\n","  5% 552/11873 [00:01<00:24, 456.56it/s]\u001b[A\n","  5% 600/11873 [00:01<00:24, 462.15it/s]\u001b[A\n","  5% 647/11873 [00:01<00:24, 463.87it/s]\u001b[A\n","  6% 695/11873 [00:01<00:23, 466.27it/s]\u001b[A\n","  6% 742/11873 [00:01<00:23, 466.91it/s]\u001b[A\n","  7% 789/11873 [00:01<00:23, 467.47it/s]\u001b[A\n","  7% 838/11873 [00:01<00:23, 473.01it/s]\u001b[A\n","  7% 890/11873 [00:01<00:22, 485.14it/s]\u001b[A\n","  8% 939/11873 [00:02<00:22, 482.55it/s]\u001b[A\n","  8% 988/11873 [00:02<00:22, 475.14it/s]\u001b[A\n","  9% 1036/11873 [00:02<00:24, 448.84it/s]\u001b[A\n","  9% 1082/11873 [00:02<00:25, 424.38it/s]\u001b[A\n","  9% 1125/11873 [00:02<00:26, 406.14it/s]\u001b[A\n"," 10% 1166/11873 [00:02<00:26, 397.53it/s]\u001b[A\n"," 10% 1206/11873 [00:02<00:27, 391.67it/s]\u001b[A\n"," 10% 1246/11873 [00:02<00:27, 388.42it/s]\u001b[A\n"," 11% 1285/11873 [00:02<00:27, 384.05it/s]\u001b[A\n"," 11% 1324/11873 [00:03<00:27, 381.38it/s]\u001b[A\n"," 11% 1363/11873 [00:03<00:27, 381.15it/s]\u001b[A\n"," 12% 1402/11873 [00:03<00:27, 379.73it/s]\u001b[A\n"," 12% 1440/11873 [00:03<00:27, 377.16it/s]\u001b[A\n"," 12% 1478/11873 [00:03<00:27, 374.13it/s]\u001b[A\n"," 13% 1516/11873 [00:03<00:27, 373.77it/s]\u001b[A\n"," 13% 1554/11873 [00:03<00:27, 374.38it/s]\u001b[A\n"," 13% 1592/11873 [00:03<00:27, 375.12it/s]\u001b[A\n"," 14% 1630/11873 [00:03<00:27, 373.29it/s]\u001b[A\n"," 14% 1668/11873 [00:03<00:27, 373.33it/s]\u001b[A\n"," 14% 1706/11873 [00:04<00:27, 372.99it/s]\u001b[A\n"," 15% 1744/11873 [00:04<00:27, 374.02it/s]\u001b[A\n"," 15% 1782/11873 [00:04<00:27, 371.10it/s]\u001b[A\n"," 15% 1820/11873 [00:04<00:27, 371.44it/s]\u001b[A\n"," 16% 1858/11873 [00:04<00:26, 371.32it/s]\u001b[A\n"," 16% 1896/11873 [00:04<00:26, 370.94it/s]\u001b[A\n"," 16% 1934/11873 [00:04<00:26, 373.16it/s]\u001b[A\n"," 17% 1972/11873 [00:04<00:26, 373.12it/s]\u001b[A\n"," 17% 2010/11873 [00:04<00:26, 374.40it/s]\u001b[A\n"," 17% 2048/11873 [00:04<00:26, 375.01it/s]\u001b[A\n"," 18% 2086/11873 [00:05<00:26, 374.54it/s]\u001b[A\n"," 18% 2124/11873 [00:05<00:26, 373.41it/s]\u001b[A\n"," 18% 2162/11873 [00:05<00:25, 374.06it/s]\u001b[A\n"," 19% 2200/11873 [00:05<00:25, 373.82it/s]\u001b[A\n"," 19% 2238/11873 [00:05<00:25, 375.05it/s]\u001b[A\n"," 19% 2276/11873 [00:05<00:26, 368.25it/s]\u001b[A\n"," 19% 2314/11873 [00:05<00:25, 369.28it/s]\u001b[A\n"," 20% 2352/11873 [00:05<00:25, 370.38it/s]\u001b[A\n"," 20% 2391/11873 [00:05<00:25, 373.28it/s]\u001b[A\n"," 20% 2429/11873 [00:05<00:25, 366.06it/s]\u001b[A\n"," 21% 2466/11873 [00:06<00:25, 363.03it/s]\u001b[A\n"," 21% 2504/11873 [00:06<00:25, 365.69it/s]\u001b[A\n"," 21% 2541/11873 [00:06<00:25, 362.12it/s]\u001b[A\n"," 22% 2579/11873 [00:06<00:25, 366.57it/s]\u001b[A\n"," 22% 2616/11873 [00:06<00:25, 366.30it/s]\u001b[A\n"," 22% 2653/11873 [00:06<00:25, 366.55it/s]\u001b[A\n"," 23% 2691/11873 [00:06<00:24, 369.36it/s]\u001b[A\n"," 23% 2729/11873 [00:06<00:24, 370.38it/s]\u001b[A\n"," 23% 2767/11873 [00:06<00:24, 368.32it/s]\u001b[A\n"," 24% 2805/11873 [00:07<00:24, 369.35it/s]\u001b[A\n"," 24% 2843/11873 [00:07<00:24, 371.68it/s]\u001b[A\n"," 24% 2881/11873 [00:07<00:24, 371.14it/s]\u001b[A\n"," 25% 2919/11873 [00:07<00:24, 369.24it/s]\u001b[A\n"," 25% 2956/11873 [00:07<00:24, 368.72it/s]\u001b[A\n"," 25% 2993/11873 [00:07<00:24, 357.61it/s]\u001b[A\n"," 26% 3029/11873 [00:07<00:26, 334.96it/s]\u001b[A\n"," 26% 3064/11873 [00:07<00:26, 336.68it/s]\u001b[A\n"," 26% 3098/11873 [00:07<00:26, 333.25it/s]\u001b[A\n"," 26% 3132/11873 [00:08<00:30, 284.76it/s]\u001b[A\n"," 27% 3162/11873 [00:08<00:32, 267.24it/s]\u001b[A\n"," 27% 3198/11873 [00:08<00:29, 289.89it/s]\u001b[A\n"," 27% 3235/11873 [00:08<00:27, 310.89it/s]\u001b[A\n"," 28% 3268/11873 [00:08<00:27, 313.53it/s]\u001b[A\n"," 28% 3301/11873 [00:08<00:36, 233.66it/s]\u001b[A\n"," 28% 3328/11873 [00:08<00:39, 215.01it/s]\u001b[A\n"," 28% 3352/11873 [00:08<00:40, 211.23it/s]\u001b[A\n"," 28% 3375/11873 [00:09<00:41, 206.16it/s]\u001b[A\n"," 29% 3412/11873 [00:09<00:34, 244.96it/s]\u001b[A\n"," 29% 3450/11873 [00:09<00:30, 277.97it/s]\u001b[A\n"," 29% 3487/11873 [00:09<00:27, 300.67it/s]\u001b[A\n"," 30% 3524/11873 [00:09<00:26, 319.22it/s]\u001b[A\n"," 30% 3561/11873 [00:09<00:25, 332.41it/s]\u001b[A\n"," 30% 3598/11873 [00:09<00:24, 336.40it/s]\u001b[A\n"," 31% 3636/11873 [00:09<00:23, 348.34it/s]\u001b[A\n"," 31% 3672/11873 [00:09<00:23, 351.39it/s]\u001b[A\n"," 31% 3708/11873 [00:10<00:23, 351.37it/s]\u001b[A\n"," 32% 3745/11873 [00:10<00:22, 355.69it/s]\u001b[A\n"," 32% 3781/11873 [00:10<00:22, 353.07it/s]\u001b[A\n"," 32% 3817/11873 [00:10<00:23, 347.38it/s]\u001b[A\n"," 32% 3852/11873 [00:10<00:24, 331.67it/s]\u001b[A\n"," 33% 3889/11873 [00:10<00:23, 340.52it/s]\u001b[A\n"," 33% 3924/11873 [00:10<00:24, 324.24it/s]\u001b[A\n"," 33% 3960/11873 [00:10<00:23, 330.83it/s]\u001b[A\n"," 34% 3994/11873 [00:10<00:24, 327.45it/s]\u001b[A\n"," 34% 4031/11873 [00:10<00:23, 338.76it/s]\u001b[A\n"," 34% 4069/11873 [00:11<00:22, 348.49it/s]\u001b[A\n"," 35% 4107/11873 [00:11<00:21, 355.51it/s]\u001b[A\n"," 35% 4143/11873 [00:11<00:21, 354.59it/s]\u001b[A\n"," 35% 4179/11873 [00:11<00:23, 332.72it/s]\u001b[A\n"," 36% 4215/11873 [00:11<00:22, 339.68it/s]\u001b[A\n"," 36% 4253/11873 [00:11<00:21, 348.62it/s]\u001b[A\n"," 36% 4290/11873 [00:11<00:21, 354.55it/s]\u001b[A\n"," 36% 4328/11873 [00:11<00:20, 361.26it/s]\u001b[A\n"," 37% 4366/11873 [00:11<00:20, 366.06it/s]\u001b[A\n"," 37% 4403/11873 [00:12<00:20, 360.63it/s]\u001b[A\n"," 37% 4440/11873 [00:12<00:25, 286.32it/s]\u001b[A\n"," 38% 4479/11873 [00:12<00:23, 311.98it/s]\u001b[A\n"," 38% 4517/11873 [00:12<00:22, 327.69it/s]\u001b[A\n"," 38% 4555/11873 [00:12<00:21, 340.75it/s]\u001b[A\n"," 39% 4593/11873 [00:12<00:20, 350.77it/s]\u001b[A\n"," 39% 4631/11873 [00:12<00:20, 357.89it/s]\u001b[A\n"," 39% 4668/11873 [00:12<00:20, 359.36it/s]\u001b[A\n"," 40% 4705/11873 [00:12<00:19, 362.38it/s]\u001b[A\n"," 40% 4742/11873 [00:13<00:19, 362.61it/s]\u001b[A\n"," 40% 4779/11873 [00:13<00:19, 363.38it/s]\u001b[A\n"," 41% 4816/11873 [00:13<00:19, 361.97it/s]\u001b[A\n"," 41% 4854/11873 [00:13<00:19, 364.93it/s]\u001b[A\n"," 41% 4891/11873 [00:13<00:19, 359.90it/s]\u001b[A\n"," 42% 4928/11873 [00:13<00:19, 361.38it/s]\u001b[A\n"," 42% 4966/11873 [00:13<00:18, 364.25it/s]\u001b[A\n"," 42% 5003/11873 [00:13<00:18, 364.88it/s]\u001b[A\n"," 42% 5040/11873 [00:13<00:18, 364.15it/s]\u001b[A\n"," 43% 5077/11873 [00:13<00:18, 364.86it/s]\u001b[A\n"," 43% 5115/11873 [00:14<00:18, 368.94it/s]\u001b[A\n"," 43% 5153/11873 [00:14<00:18, 370.36it/s]\u001b[A\n"," 44% 5191/11873 [00:14<00:17, 372.66it/s]\u001b[A\n"," 44% 5230/11873 [00:14<00:17, 375.10it/s]\u001b[A\n"," 44% 5268/11873 [00:14<00:19, 344.67it/s]\u001b[A\n"," 45% 5306/11873 [00:14<00:18, 353.95it/s]\u001b[A\n"," 45% 5345/11873 [00:14<00:17, 363.03it/s]\u001b[A\n"," 45% 5384/11873 [00:14<00:17, 369.04it/s]\u001b[A\n"," 46% 5422/11873 [00:14<00:17, 368.31it/s]\u001b[A\n"," 46% 5460/11873 [00:14<00:17, 369.64it/s]\u001b[A\n"," 46% 5498/11873 [00:15<00:17, 368.47it/s]\u001b[A\n"," 47% 5537/11873 [00:15<00:16, 373.48it/s]\u001b[A\n"," 47% 5575/11873 [00:15<00:17, 368.68it/s]\u001b[A\n"," 47% 5613/11873 [00:15<00:16, 369.34it/s]\u001b[A\n"," 48% 5650/11873 [00:15<00:16, 367.81it/s]\u001b[A\n"," 48% 5687/11873 [00:15<00:16, 368.23it/s]\u001b[A\n"," 48% 5724/11873 [00:15<00:16, 367.28it/s]\u001b[A\n"," 49% 5761/11873 [00:15<00:16, 362.36it/s]\u001b[A\n"," 49% 5799/11873 [00:15<00:16, 366.36it/s]\u001b[A\n"," 49% 5838/11873 [00:16<00:16, 370.30it/s]\u001b[A\n"," 49% 5876/11873 [00:16<00:16, 372.05it/s]\u001b[A\n"," 50% 5914/11873 [00:16<00:15, 373.21it/s]\u001b[A\n"," 50% 5952/11873 [00:16<00:15, 370.28it/s]\u001b[A\n"," 50% 5990/11873 [00:16<00:15, 369.76it/s]\u001b[A\n"," 51% 6029/11873 [00:16<00:15, 373.46it/s]\u001b[A\n"," 51% 6067/11873 [00:16<00:15, 372.48it/s]\u001b[A\n"," 51% 6105/11873 [00:16<00:15, 372.29it/s]\u001b[A\n"," 52% 6143/11873 [00:16<00:15, 371.90it/s]\u001b[A\n"," 52% 6181/11873 [00:16<00:15, 372.02it/s]\u001b[A\n"," 52% 6219/11873 [00:17<00:15, 370.53it/s]\u001b[A\n"," 53% 6257/11873 [00:17<00:15, 371.02it/s]\u001b[A\n"," 53% 6296/11873 [00:17<00:14, 374.78it/s]\u001b[A\n"," 53% 6334/11873 [00:17<00:14, 375.18it/s]\u001b[A\n"," 54% 6372/11873 [00:17<00:14, 373.33it/s]\u001b[A\n"," 54% 6410/11873 [00:17<00:14, 372.91it/s]\u001b[A\n"," 54% 6448/11873 [00:17<00:14, 370.50it/s]\u001b[A\n"," 55% 6486/11873 [00:17<00:14, 373.09it/s]\u001b[A\n"," 55% 6524/11873 [00:17<00:14, 372.69it/s]\u001b[A\n"," 55% 6562/11873 [00:17<00:14, 371.04it/s]\u001b[A\n"," 56% 6600/11873 [00:18<00:14, 369.97it/s]\u001b[A\n"," 56% 6638/11873 [00:18<00:14, 370.49it/s]\u001b[A\n"," 56% 6676/11873 [00:18<00:13, 372.07it/s]\u001b[A\n"," 57% 6714/11873 [00:18<00:15, 335.69it/s]\u001b[A\n"," 57% 6751/11873 [00:18<00:14, 342.40it/s]\u001b[A\n"," 57% 6786/11873 [00:18<00:15, 331.25it/s]\u001b[A\n"," 57% 6822/11873 [00:18<00:14, 337.23it/s]\u001b[A\n"," 58% 6859/11873 [00:18<00:14, 345.13it/s]\u001b[A\n"," 58% 6895/11873 [00:18<00:14, 348.69it/s]\u001b[A\n"," 58% 6933/11873 [00:19<00:13, 356.47it/s]\u001b[A\n"," 59% 6970/11873 [00:19<00:13, 359.21it/s]\u001b[A\n"," 59% 7007/11873 [00:19<00:13, 359.12it/s]\u001b[A\n"," 59% 7044/11873 [00:19<00:13, 361.34it/s]\u001b[A\n"," 60% 7081/11873 [00:19<00:13, 361.88it/s]\u001b[A\n"," 60% 7119/11873 [00:19<00:12, 367.14it/s]\u001b[A\n"," 60% 7156/11873 [00:19<00:12, 365.83it/s]\u001b[A\n"," 61% 7193/11873 [00:19<00:12, 363.16it/s]\u001b[A\n"," 61% 7231/11873 [00:19<00:12, 366.10it/s]\u001b[A\n"," 61% 7268/11873 [00:19<00:12, 357.74it/s]\u001b[A\n"," 62% 7305/11873 [00:20<00:12, 361.05it/s]\u001b[A\n"," 62% 7342/11873 [00:20<00:12, 360.98it/s]\u001b[A\n"," 62% 7380/11873 [00:20<00:12, 364.55it/s]\u001b[A\n"," 62% 7417/11873 [00:20<00:12, 349.42it/s]\u001b[A\n"," 63% 7454/11873 [00:20<00:12, 351.46it/s]\u001b[A\n"," 63% 7492/11873 [00:20<00:12, 357.33it/s]\u001b[A\n"," 63% 7528/11873 [00:20<00:12, 353.98it/s]\u001b[A\n"," 64% 7566/11873 [00:20<00:11, 361.28it/s]\u001b[A\n"," 64% 7603/11873 [00:20<00:11, 362.39it/s]\u001b[A\n"," 64% 7640/11873 [00:20<00:11, 360.03it/s]\u001b[A\n"," 65% 7677/11873 [00:21<00:11, 358.41it/s]\u001b[A\n"," 65% 7713/11873 [00:21<00:12, 336.56it/s]\u001b[A\n"," 65% 7750/11873 [00:21<00:11, 343.99it/s]\u001b[A\n"," 66% 7787/11873 [00:21<00:11, 349.92it/s]\u001b[A\n"," 66% 7824/11873 [00:21<00:11, 353.69it/s]\u001b[A\n"," 66% 7860/11873 [00:21<00:11, 353.28it/s]\u001b[A\n"," 67% 7896/11873 [00:21<00:11, 332.29it/s]\u001b[A\n"," 67% 7934/11873 [00:21<00:11, 344.18it/s]\u001b[A\n"," 67% 7970/11873 [00:21<00:11, 346.05it/s]\u001b[A\n"," 67% 8008/11873 [00:22<00:10, 354.33it/s]\u001b[A\n"," 68% 8044/11873 [00:22<00:10, 355.55it/s]\u001b[A\n"," 68% 8082/11873 [00:22<00:10, 360.85it/s]\u001b[A\n"," 68% 8119/11873 [00:22<00:10, 362.44it/s]\u001b[A\n"," 69% 8156/11873 [00:22<00:10, 361.74it/s]\u001b[A\n"," 69% 8193/11873 [00:22<00:10, 361.22it/s]\u001b[A\n"," 69% 8230/11873 [00:22<00:10, 361.03it/s]\u001b[A\n"," 70% 8267/11873 [00:22<00:10, 360.29it/s]\u001b[A\n"," 70% 8305/11873 [00:22<00:09, 364.30it/s]\u001b[A\n"," 70% 8343/11873 [00:22<00:09, 367.58it/s]\u001b[A\n"," 71% 8380/11873 [00:23<00:09, 366.58it/s]\u001b[A\n"," 71% 8417/11873 [00:23<00:09, 363.82it/s]\u001b[A\n"," 71% 8454/11873 [00:23<00:09, 360.65it/s]\u001b[A\n"," 72% 8491/11873 [00:23<00:09, 361.76it/s]\u001b[A\n"," 72% 8528/11873 [00:23<00:09, 363.19it/s]\u001b[A\n"," 72% 8566/11873 [00:23<00:09, 366.07it/s]\u001b[A\n"," 72% 8603/11873 [00:23<00:08, 365.64it/s]\u001b[A\n"," 73% 8640/11873 [00:23<00:08, 359.50it/s]\u001b[A\n"," 73% 8677/11873 [00:23<00:08, 362.38it/s]\u001b[A\n"," 73% 8714/11873 [00:23<00:08, 363.30it/s]\u001b[A\n"," 74% 8752/11873 [00:24<00:08, 366.55it/s]\u001b[A\n"," 74% 8789/11873 [00:24<00:08, 361.81it/s]\u001b[A\n"," 74% 8827/11873 [00:24<00:08, 364.75it/s]\u001b[A\n"," 75% 8865/11873 [00:24<00:08, 368.51it/s]\u001b[A\n"," 75% 8902/11873 [00:24<00:08, 366.14it/s]\u001b[A\n"," 75% 8939/11873 [00:24<00:07, 367.05it/s]\u001b[A\n"," 76% 8976/11873 [00:24<00:07, 365.39it/s]\u001b[A\n"," 76% 9014/11873 [00:24<00:07, 369.30it/s]\u001b[A\n"," 76% 9051/11873 [00:24<00:07, 368.85it/s]\u001b[A\n"," 77% 9089/11873 [00:25<00:07, 370.22it/s]\u001b[A\n"," 77% 9127/11873 [00:25<00:07, 363.04it/s]\u001b[A\n"," 77% 9165/11873 [00:25<00:07, 366.78it/s]\u001b[A\n"," 78% 9202/11873 [00:25<00:07, 365.12it/s]\u001b[A\n"," 78% 9239/11873 [00:25<00:07, 366.33it/s]\u001b[A\n"," 78% 9277/11873 [00:25<00:07, 369.18it/s]\u001b[A\n"," 78% 9314/11873 [00:25<00:06, 368.90it/s]\u001b[A\n"," 79% 9351/11873 [00:25<00:06, 363.07it/s]\u001b[A\n"," 79% 9389/11873 [00:25<00:06, 367.57it/s]\u001b[A\n"," 79% 9426/11873 [00:25<00:06, 366.96it/s]\u001b[A\n"," 80% 9463/11873 [00:26<00:06, 361.53it/s]\u001b[A\n"," 80% 9501/11873 [00:26<00:06, 365.81it/s]\u001b[A\n"," 80% 9539/11873 [00:26<00:06, 369.27it/s]\u001b[A\n"," 81% 9577/11873 [00:26<00:06, 372.41it/s]\u001b[A\n"," 81% 9615/11873 [00:26<00:06, 371.36it/s]\u001b[A\n"," 81% 9653/11873 [00:26<00:05, 371.97it/s]\u001b[A\n"," 82% 9692/11873 [00:26<00:05, 375.14it/s]\u001b[A\n"," 82% 9730/11873 [00:26<00:05, 376.39it/s]\u001b[A\n"," 82% 9769/11873 [00:26<00:05, 377.88it/s]\u001b[A\n"," 83% 9807/11873 [00:26<00:05, 373.28it/s]\u001b[A\n"," 83% 9845/11873 [00:27<00:05, 362.54it/s]\u001b[A\n"," 83% 9883/11873 [00:27<00:05, 365.59it/s]\u001b[A\n"," 84% 9921/11873 [00:27<00:05, 368.68it/s]\u001b[A\n"," 84% 9959/11873 [00:27<00:05, 371.59it/s]\u001b[A\n"," 84% 9997/11873 [00:27<00:05, 371.91it/s]\u001b[A\n"," 85% 10035/11873 [00:27<00:05, 362.08it/s]\u001b[A\n"," 85% 10073/11873 [00:27<00:04, 366.33it/s]\u001b[A\n"," 85% 10111/11873 [00:27<00:04, 370.06it/s]\u001b[A\n"," 85% 10149/11873 [00:27<00:04, 371.74it/s]\u001b[A\n"," 86% 10187/11873 [00:27<00:04, 372.49it/s]\u001b[A\n"," 86% 10225/11873 [00:28<00:04, 368.14it/s]\u001b[A\n"," 86% 10263/11873 [00:28<00:04, 369.00it/s]\u001b[A\n"," 87% 10300/11873 [00:28<00:04, 365.41it/s]\u001b[A\n"," 87% 10338/11873 [00:28<00:04, 368.33it/s]\u001b[A\n"," 87% 10376/11873 [00:28<00:04, 369.64it/s]\u001b[A\n"," 88% 10414/11873 [00:28<00:03, 370.53it/s]\u001b[A\n"," 88% 10452/11873 [00:28<00:04, 346.06it/s]\u001b[A\n"," 88% 10491/11873 [00:28<00:03, 356.92it/s]\u001b[A\n"," 89% 10529/11873 [00:28<00:03, 360.95it/s]\u001b[A\n"," 89% 10566/11873 [00:29<00:03, 342.12it/s]\u001b[A\n"," 89% 10604/11873 [00:29<00:03, 350.57it/s]\u001b[A\n"," 90% 10640/11873 [00:29<00:03, 352.82it/s]\u001b[A\n"," 90% 10676/11873 [00:29<00:03, 351.07it/s]\u001b[A\n"," 90% 10714/11873 [00:29<00:03, 358.02it/s]\u001b[A\n"," 91% 10751/11873 [00:29<00:03, 361.10it/s]\u001b[A\n"," 91% 10788/11873 [00:29<00:03, 358.97it/s]\u001b[A\n"," 91% 10824/11873 [00:29<00:03, 343.45it/s]\u001b[A\n"," 91% 10859/11873 [00:29<00:02, 341.96it/s]\u001b[A\n"," 92% 10896/11873 [00:29<00:02, 349.32it/s]\u001b[A\n"," 92% 10932/11873 [00:30<00:02, 346.76it/s]\u001b[A\n"," 92% 10970/11873 [00:30<00:02, 354.51it/s]\u001b[A\n"," 93% 11009/11873 [00:30<00:02, 362.57it/s]\u001b[A\n"," 93% 11048/11873 [00:30<00:02, 368.06it/s]\u001b[A\n"," 93% 11085/11873 [00:30<00:02, 359.74it/s]\u001b[A\n"," 94% 11122/11873 [00:30<00:02, 349.35it/s]\u001b[A\n"," 94% 11158/11873 [00:30<00:02, 348.33it/s]\u001b[A\n"," 94% 11196/11873 [00:30<00:01, 355.62it/s]\u001b[A\n"," 95% 11232/11873 [00:30<00:01, 353.72it/s]\u001b[A\n"," 95% 11269/11873 [00:31<00:01, 358.37it/s]\u001b[A\n"," 95% 11306/11873 [00:31<00:01, 361.32it/s]\u001b[A\n"," 96% 11344/11873 [00:31<00:01, 364.89it/s]\u001b[A\n"," 96% 11382/11873 [00:31<00:01, 368.38it/s]\u001b[A\n"," 96% 11419/11873 [00:31<00:01, 367.57it/s]\u001b[A\n"," 96% 11457/11873 [00:31<00:01, 371.05it/s]\u001b[A\n"," 97% 11495/11873 [00:31<00:01, 369.22it/s]\u001b[A\n"," 97% 11532/11873 [00:31<00:00, 367.10it/s]\u001b[A\n"," 97% 11569/11873 [00:31<00:00, 365.99it/s]\u001b[A\n"," 98% 11607/11873 [00:31<00:00, 367.63it/s]\u001b[A\n"," 98% 11644/11873 [00:32<00:00, 367.10it/s]\u001b[A\n"," 98% 11681/11873 [00:32<00:00, 362.39it/s]\u001b[A\n"," 99% 11718/11873 [00:32<00:00, 363.26it/s]\u001b[A\n"," 99% 11756/11873 [00:32<00:00, 367.64it/s]\u001b[A\n"," 99% 11794/11873 [00:32<00:00, 370.90it/s]\u001b[A\n","100% 11833/11873 [00:32<00:00, 374.80it/s]\u001b[A\n","100% 11873/11873 [00:32<00:00, 363.56it/s]\n","04/06/2022 19:11:57 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/eval_predictions.json.\n","04/06/2022 19:11:57 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/eval_nbest_predictions.json.\n","04/06/2022 19:12:00 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug/eval_null_odds.json.\n","04/06/2022 19:12:03 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1525/1525 [03:46<00:00,  6.73it/s]\n","***** eval metrics *****\n","  epoch                  =     3.0\n","  eval_HasAns_exact      = 71.0864\n","  eval_HasAns_f1         = 78.1304\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 42.8595\n","  eval_NoAns_f1          = 42.8595\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 57.2896\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 60.4864\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 56.9527\n","  eval_f1                = 60.4697\n","  eval_samples           =   12199\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 19:12:03,752 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_context_aug', 'type': 'sichenzhong/squad_v2_context_aug', 'args': 'squad_v2'}}\n"]}],"source":["!python run_qa.py \\\n","  --model_name_or_path bert-base-cased \\\n","  --dataset_name sichenzhong/squad_v2_context_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 3 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/bert-base-cased/context-aug"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YC1SX-FRTOxk","executionInfo":{"status":"ok","timestamp":1649287573933,"user_tz":240,"elapsed":5100797,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"8acfd036-f12e-4108-d031-1af45b9810d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["04/06/2022 19:12:14 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 19:12:14 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/runs/Apr06_19-12-14_17ba0eb63658,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 19:12:15 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7\n","04/06/2022 19:12:15 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/06/2022 19:12:15 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","04/06/2022 19:12:15 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","04/06/2022 19:12:15 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","100% 2/2 [00:00<00:00, 19.00it/s]\n","[INFO|hub.py:583] 2022-04-06 19:12:16,180 >> https://huggingface.co/albert-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpitkn4g6z\n","Downloading: 100% 684/684 [00:00<00:00, 1.03MB/s]\n","[INFO|hub.py:587] 2022-04-06 19:12:16,538 >> storing https://huggingface.co/albert-base-v2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|hub.py:595] 2022-04-06 19:12:16,538 >> creating metadata file for /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:654] 2022-04-06 19:12:16,539 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-06 19:12:16,541 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-06 19:12:16,901 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-06 19:12:17,259 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-06 19:12:17,260 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 19:12:17,976 >> https://huggingface.co/albert-base-v2/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdede57l8\n","Downloading: 100% 742k/742k [00:00<00:00, 1.78MB/s]\n","[INFO|hub.py:587] 2022-04-06 19:12:18,767 >> storing https://huggingface.co/albert-base-v2/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:595] 2022-04-06 19:12:18,767 >> creating metadata file for /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:583] 2022-04-06 19:12:19,125 >> https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmb56arvm\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 2.58MB/s]\n","[INFO|hub.py:587] 2022-04-06 19:12:20,000 >> storing https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|hub.py:595] 2022-04-06 19:12:20,001 >> creating metadata file for /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 19:12:21,090 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 19:12:21,091 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 19:12:21,091 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 19:12:21,091 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 19:12:21,091 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-06 19:12:21,448 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-06 19:12:21,448 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 19:12:21,930 >> https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpk29acckc\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 71.2MB/s]\n","[INFO|hub.py:587] 2022-04-06 19:12:22,659 >> storing https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|hub.py:595] 2022-04-06 19:12:22,659 >> creating metadata file for /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1772] 2022-04-06 19:12:22,660 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[WARNING|modeling_utils.py:2049] 2022-04-06 19:12:22,774 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']\n","- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-04-06 19:12:22,775 >> Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/06/2022 19:12:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-fb8ef1f109092495.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:55<00:00,  2.36ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/06/2022 19:13:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-1281049c37f584fe.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:13<00:00,  6.08s/ba]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 19:14:35,126 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 19:14:35,126 >>   Num examples = 130550\n","[INFO|trainer.py:1292] 2022-04-06 19:14:35,126 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-06 19:14:35,126 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1294] 2022-04-06 19:14:35,126 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1295] 2022-04-06 19:14:35,126 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 19:14:35,126 >>   Total optimization steps = 16320\n","{'loss': 2.272, 'learning_rate': 1.9387254901960785e-05, 'epoch': 0.06}\n","  3% 500/16320 [07:31<3:57:52,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 19:22:06,509 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 19:22:06,513 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 19:22:06,626 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 19:22:06,630 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 19:22:06,633 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 1.8413, 'learning_rate': 1.877450980392157e-05, 'epoch': 0.12}\n","  6% 1000/16320 [15:03<3:50:21,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 19:29:38,346 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 19:29:38,350 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 19:29:38,463 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 19:29:38,467 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 19:29:38,469 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.7324, 'learning_rate': 1.8161764705882355e-05, 'epoch': 0.18}\n","  9% 1500/16320 [22:35<3:43:16,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 19:37:10,421 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 19:37:10,426 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 19:37:10,540 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 19:37:10,545 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 19:37:10,547 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.6471, 'learning_rate': 1.7549019607843138e-05, 'epoch': 0.25}\n"," 12% 2000/16320 [30:07<3:36:00,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 19:44:42,696 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 19:44:42,701 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 19:44:42,816 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 19:44:42,833 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 19:44:42,836 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.6475, 'learning_rate': 1.693627450980392e-05, 'epoch': 0.31}\n"," 15% 2500/16320 [37:39<3:28:25,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 19:52:15,131 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 19:52:15,136 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 19:52:15,257 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 19:52:15,260 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 19:52:15,263 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.594, 'learning_rate': 1.6323529411764708e-05, 'epoch': 0.37}\n"," 18% 3000/16320 [45:12<3:20:37,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 19:59:47,563 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 19:59:47,567 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 19:59:47,684 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 19:59:47,688 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 19:59:47,691 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.5384, 'learning_rate': 1.571078431372549e-05, 'epoch': 0.43}\n"," 21% 3500/16320 [52:44<3:13:27,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 20:07:20,100 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 20:07:20,105 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 20:07:20,215 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 20:07:20,218 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 20:07:20,221 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.5036, 'learning_rate': 1.5098039215686276e-05, 'epoch': 0.49}\n"," 25% 4000/16320 [1:00:17<3:05:53,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 20:14:52,719 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 20:14:52,724 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 20:14:52,835 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 20:14:52,840 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 20:14:52,843 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.5295, 'learning_rate': 1.448529411764706e-05, 'epoch': 0.55}\n"," 28% 4500/16320 [1:07:50<2:58:15,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 20:22:25,438 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 20:22:25,442 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 20:22:25,563 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 20:22:25,567 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 20:22:25,570 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.502, 'learning_rate': 1.3872549019607844e-05, 'epoch': 0.61}\n"," 31% 5000/16320 [1:15:23<2:50:28,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 20:29:58,287 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 20:29:58,292 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 20:29:58,402 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 20:29:58,406 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 20:29:58,409 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.4791, 'learning_rate': 1.3259803921568627e-05, 'epoch': 0.67}\n"," 34% 5500/16320 [1:22:56<2:43:17,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 20:37:31,216 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 20:37:31,221 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 20:37:31,329 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 20:37:31,333 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 20:37:31,336 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.4645, 'learning_rate': 1.2647058823529412e-05, 'epoch': 0.74}\n"," 37% 6000/16320 [1:30:29<2:35:48,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 20:45:04,181 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 20:45:04,186 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 20:45:04,298 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 20:45:04,302 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 20:45:04,306 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.422, 'learning_rate': 1.2034313725490197e-05, 'epoch': 0.8}\n"," 40% 6500/16320 [1:38:01<2:28:13,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 20:52:37,085 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 20:52:37,090 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 20:52:37,201 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 20:52:37,205 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 20:52:37,208 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.409, 'learning_rate': 1.142156862745098e-05, 'epoch': 0.86}\n"," 43% 7000/16320 [1:45:35<2:20:44,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 21:00:10,207 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 21:00:10,212 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 21:00:10,323 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 21:00:10,327 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 21:00:10,329 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.4045, 'learning_rate': 1.0808823529411765e-05, 'epoch': 0.92}\n"," 46% 7500/16320 [1:53:08<2:12:58,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 21:07:43,292 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 21:07:43,297 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 21:07:43,406 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 21:07:43,410 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 21:07:43,413 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 1.4094, 'learning_rate': 1.0196078431372549e-05, 'epoch': 0.98}\n"," 49% 8000/16320 [2:00:41<2:05:30,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 21:15:16,310 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 21:15:16,315 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 21:15:16,425 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 21:15:16,429 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 21:15:16,431 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 1.2463, 'learning_rate': 9.583333333333335e-06, 'epoch': 1.04}\n"," 52% 8500/16320 [2:08:13<1:58:08,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 21:22:48,999 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 21:22:49,004 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 21:22:49,114 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 21:22:49,118 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 21:22:49,120 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 1.1924, 'learning_rate': 8.970588235294119e-06, 'epoch': 1.1}\n"," 55% 9000/16320 [2:15:46<1:50:30,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 21:30:21,914 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 21:30:21,919 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 21:30:22,040 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 21:30:22,044 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 21:30:22,047 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 1.159, 'learning_rate': 8.357843137254903e-06, 'epoch': 1.16}\n"," 58% 9500/16320 [2:23:19<1:42:49,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 21:37:54,813 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 21:37:54,818 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 21:37:54,926 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 21:37:54,930 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 21:37:54,932 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 1.1798, 'learning_rate': 7.745098039215687e-06, 'epoch': 1.23}\n"," 61% 10000/16320 [2:30:52<1:35:22,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 21:45:27,708 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 21:45:27,712 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 21:45:27,823 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 21:45:27,827 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 21:45:27,830 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 1.1543, 'learning_rate': 7.132352941176472e-06, 'epoch': 1.29}\n"," 64% 10500/16320 [2:38:25<1:27:49,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 21:53:00,720 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 21:53:00,725 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 21:53:00,834 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 21:53:00,838 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 21:53:00,840 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-10500/special_tokens_map.json\n","{'loss': 1.1374, 'learning_rate': 6.519607843137256e-06, 'epoch': 1.35}\n"," 67% 11000/16320 [2:45:58<1:20:21,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 22:00:33,927 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-06 22:00:33,945 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 22:00:34,060 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 22:00:34,064 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 22:00:34,066 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-11000/special_tokens_map.json\n","{'loss': 1.1563, 'learning_rate': 5.90686274509804e-06, 'epoch': 1.41}\n"," 70% 11500/16320 [2:53:31<1:12:41,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 22:08:07,085 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-11500\n","[INFO|configuration_utils.py:441] 2022-04-06 22:08:07,105 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 22:08:07,217 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 22:08:07,221 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 22:08:07,224 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-11500/special_tokens_map.json\n","{'loss': 1.1462, 'learning_rate': 5.294117647058824e-06, 'epoch': 1.47}\n"," 74% 12000/16320 [3:01:05<1:05:14,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 22:15:40,171 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-12000\n","[INFO|configuration_utils.py:441] 2022-04-06 22:15:40,176 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 22:15:40,288 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 22:15:40,292 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 22:15:40,295 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-12000/special_tokens_map.json\n","{'loss': 1.144, 'learning_rate': 4.681372549019608e-06, 'epoch': 1.53}\n"," 77% 12500/16320 [3:08:38<57:35,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 22:23:13,299 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-12500\n","[INFO|configuration_utils.py:441] 2022-04-06 22:23:13,304 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 22:23:13,421 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 22:23:13,426 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 22:23:13,430 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-12500/special_tokens_map.json\n","{'loss': 1.1288, 'learning_rate': 4.068627450980392e-06, 'epoch': 1.59}\n"," 80% 13000/16320 [3:16:11<50:01,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 22:30:46,325 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-13000\n","[INFO|configuration_utils.py:441] 2022-04-06 22:30:46,344 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 22:30:46,460 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 22:30:46,464 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 22:30:46,467 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-13000/special_tokens_map.json\n","{'loss': 1.1404, 'learning_rate': 3.4558823529411766e-06, 'epoch': 1.65}\n"," 83% 13500/16320 [3:23:44<42:29,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 22:38:19,353 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-13500\n","[INFO|configuration_utils.py:441] 2022-04-06 22:38:19,357 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 22:38:19,472 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 22:38:19,475 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 22:38:19,478 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-13500/special_tokens_map.json\n","{'loss': 1.143, 'learning_rate': 2.843137254901961e-06, 'epoch': 1.72}\n"," 86% 14000/16320 [3:31:17<35:00,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 22:45:52,256 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-14000\n","[INFO|configuration_utils.py:441] 2022-04-06 22:45:52,261 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 22:45:52,372 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 22:45:52,376 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 22:45:52,379 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-14000/special_tokens_map.json\n","{'loss': 1.1067, 'learning_rate': 2.2303921568627456e-06, 'epoch': 1.78}\n"," 89% 14500/16320 [3:38:50<27:27,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 22:53:25,305 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-14500\n","[INFO|configuration_utils.py:441] 2022-04-06 22:53:25,309 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 22:53:25,418 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 22:53:25,422 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 22:53:25,424 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-14500/special_tokens_map.json\n","{'loss': 1.0996, 'learning_rate': 1.6176470588235297e-06, 'epoch': 1.84}\n"," 92% 15000/16320 [3:46:23<19:54,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 23:00:58,289 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-15000\n","[INFO|configuration_utils.py:441] 2022-04-06 23:00:58,294 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 23:00:58,402 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 23:00:58,406 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 23:00:58,409 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-15000/special_tokens_map.json\n","{'loss': 1.1046, 'learning_rate': 1.0049019607843138e-06, 'epoch': 1.9}\n"," 95% 15500/16320 [3:53:56<12:21,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 23:08:31,334 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-15500\n","[INFO|configuration_utils.py:441] 2022-04-06 23:08:31,339 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 23:08:31,449 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 23:08:31,453 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 23:08:31,455 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-15500/special_tokens_map.json\n","{'loss': 1.1105, 'learning_rate': 3.921568627450981e-07, 'epoch': 1.96}\n"," 98% 16000/16320 [4:01:29<04:49,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 23:16:04,372 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-16000\n","[INFO|configuration_utils.py:441] 2022-04-06 23:16:04,377 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 23:16:04,485 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 23:16:04,489 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 23:16:04,492 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/checkpoint-16000/special_tokens_map.json\n","100% 16320/16320 [4:06:18<00:00,  1.35it/s][INFO|trainer.py:1530] 2022-04-06 23:20:53,809 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 14778.683, 'train_samples_per_second': 17.667, 'train_steps_per_second': 1.104, 'train_loss': 1.3627019601709702, 'epoch': 2.0}\n","100% 16320/16320 [4:06:18<00:00,  1.10it/s]\n","[INFO|trainer.py:2166] 2022-04-06 23:20:53,813 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug\n","[INFO|configuration_utils.py:441] 2022-04-06 23:20:53,818 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 23:20:53,927 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 23:20:53,931 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 23:20:53,934 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     1.3627\n","  train_runtime            = 4:06:18.68\n","  train_samples            =     130550\n","  train_samples_per_second =     17.667\n","  train_steps_per_second   =      1.104\n","04/06/2022 23:20:53 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 23:20:53,956 >> The following columns in the evaluation set  don't have a corresponding argument in `AlbertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `AlbertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 23:20:53,958 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 23:20:53,958 >>   Num examples = 11968\n","[INFO|trainer.py:2421] 2022-04-06 23:20:53,959 >>   Batch size = 8\n","100% 1496/1496 [04:17<00:00,  5.80it/s]04/06/2022 23:25:26 - INFO - utils_qa - Post-processing 11873 example predictions split into 11968 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 38/11873 [00:00<00:31, 378.40it/s]\u001b[A\n","  1% 76/11873 [00:00<00:31, 376.30it/s]\u001b[A\n","  1% 116/11873 [00:00<00:30, 383.70it/s]\u001b[A\n","  1% 157/11873 [00:00<00:29, 392.26it/s]\u001b[A\n","  2% 199/11873 [00:00<00:29, 398.76it/s]\u001b[A\n","  2% 240/11873 [00:00<00:29, 400.41it/s]\u001b[A\n","  2% 281/11873 [00:00<00:28, 401.91it/s]\u001b[A\n","  3% 322/11873 [00:00<00:28, 401.20it/s]\u001b[A\n","  3% 363/11873 [00:00<00:29, 395.33it/s]\u001b[A\n","  3% 404/11873 [00:01<00:28, 397.38it/s]\u001b[A\n","  4% 447/11873 [00:01<00:28, 404.43it/s]\u001b[A\n","  4% 488/11873 [00:01<00:28, 405.79it/s]\u001b[A\n","  4% 529/11873 [00:01<00:28, 400.05it/s]\u001b[A\n","  5% 570/11873 [00:01<00:28, 394.45it/s]\u001b[A\n","  5% 611/11873 [00:01<00:28, 396.06it/s]\u001b[A\n","  5% 652/11873 [00:01<00:28, 397.67it/s]\u001b[A\n","  6% 693/11873 [00:01<00:28, 398.54it/s]\u001b[A\n","  6% 733/11873 [00:01<00:28, 395.84it/s]\u001b[A\n","  7% 773/11873 [00:01<00:28, 389.95it/s]\u001b[A\n","  7% 816/11873 [00:02<00:27, 400.42it/s]\u001b[A\n","  7% 859/11873 [00:02<00:27, 406.48it/s]\u001b[A\n","  8% 903/11873 [00:02<00:26, 414.30it/s]\u001b[A\n","  8% 945/11873 [00:02<00:26, 413.88it/s]\u001b[A\n","  8% 987/11873 [00:02<00:26, 407.41it/s]\u001b[A\n","  9% 1028/11873 [00:02<00:28, 381.23it/s]\u001b[A\n","  9% 1067/11873 [00:02<00:30, 351.50it/s]\u001b[A\n","  9% 1103/11873 [00:02<00:32, 335.41it/s]\u001b[A\n"," 10% 1137/11873 [00:02<00:32, 325.38it/s]\u001b[A\n"," 10% 1170/11873 [00:03<00:33, 318.84it/s]\u001b[A\n"," 10% 1203/11873 [00:03<00:33, 315.37it/s]\u001b[A\n"," 10% 1235/11873 [00:03<00:34, 310.13it/s]\u001b[A\n"," 11% 1267/11873 [00:03<00:34, 309.78it/s]\u001b[A\n"," 11% 1299/11873 [00:03<00:34, 306.87it/s]\u001b[A\n"," 11% 1330/11873 [00:03<00:34, 306.25it/s]\u001b[A\n"," 11% 1361/11873 [00:03<00:34, 307.03it/s]\u001b[A\n"," 12% 1392/11873 [00:03<00:34, 303.16it/s]\u001b[A\n"," 12% 1423/11873 [00:03<00:34, 303.18it/s]\u001b[A\n"," 12% 1454/11873 [00:04<00:34, 302.41it/s]\u001b[A\n"," 13% 1485/11873 [00:04<00:34, 298.99it/s]\u001b[A\n"," 13% 1517/11873 [00:04<00:34, 302.41it/s]\u001b[A\n"," 13% 1548/11873 [00:04<00:33, 304.36it/s]\u001b[A\n"," 13% 1579/11873 [00:04<00:34, 302.31it/s]\u001b[A\n"," 14% 1610/11873 [00:04<00:33, 302.48it/s]\u001b[A\n"," 14% 1641/11873 [00:04<00:33, 301.44it/s]\u001b[A\n"," 14% 1672/11873 [00:04<00:33, 301.89it/s]\u001b[A\n"," 14% 1703/11873 [00:04<00:34, 294.73it/s]\u001b[A\n"," 15% 1734/11873 [00:04<00:34, 297.90it/s]\u001b[A\n"," 15% 1764/11873 [00:05<00:33, 298.19it/s]\u001b[A\n"," 15% 1794/11873 [00:05<00:33, 298.14it/s]\u001b[A\n"," 15% 1825/11873 [00:05<00:33, 299.00it/s]\u001b[A\n"," 16% 1856/11873 [00:05<00:33, 301.43it/s]\u001b[A\n"," 16% 1887/11873 [00:05<00:33, 299.63it/s]\u001b[A\n"," 16% 1918/11873 [00:05<00:32, 301.98it/s]\u001b[A\n"," 16% 1949/11873 [00:05<00:32, 301.46it/s]\u001b[A\n"," 17% 1980/11873 [00:05<00:32, 302.46it/s]\u001b[A\n"," 17% 2011/11873 [00:05<00:32, 299.35it/s]\u001b[A\n"," 17% 2042/11873 [00:05<00:32, 300.72it/s]\u001b[A\n"," 17% 2073/11873 [00:06<00:32, 301.06it/s]\u001b[A\n"," 18% 2105/11873 [00:06<00:32, 303.52it/s]\u001b[A\n"," 18% 2136/11873 [00:06<00:32, 298.97it/s]\u001b[A\n"," 18% 2166/11873 [00:06<00:32, 298.88it/s]\u001b[A\n"," 19% 2197/11873 [00:06<00:32, 300.76it/s]\u001b[A\n"," 19% 2228/11873 [00:06<00:31, 301.50it/s]\u001b[A\n"," 19% 2259/11873 [00:06<00:31, 302.98it/s]\u001b[A\n"," 19% 2290/11873 [00:06<00:31, 302.04it/s]\u001b[A\n"," 20% 2321/11873 [00:06<00:32, 297.73it/s]\u001b[A\n"," 20% 2353/11873 [00:06<00:31, 301.87it/s]\u001b[A\n"," 20% 2384/11873 [00:07<00:31, 301.05it/s]\u001b[A\n"," 20% 2415/11873 [00:07<00:31, 300.57it/s]\u001b[A\n"," 21% 2446/11873 [00:07<00:31, 300.87it/s]\u001b[A\n"," 21% 2477/11873 [00:07<00:31, 301.60it/s]\u001b[A\n"," 21% 2508/11873 [00:07<00:30, 302.22it/s]\u001b[A\n"," 21% 2539/11873 [00:07<00:30, 304.37it/s]\u001b[A\n"," 22% 2570/11873 [00:07<00:30, 303.36it/s]\u001b[A\n"," 22% 2601/11873 [00:07<00:30, 302.17it/s]\u001b[A\n"," 22% 2632/11873 [00:07<00:30, 301.50it/s]\u001b[A\n"," 22% 2663/11873 [00:08<00:30, 299.63it/s]\u001b[A\n"," 23% 2694/11873 [00:08<00:30, 301.37it/s]\u001b[A\n"," 23% 2725/11873 [00:08<00:30, 302.46it/s]\u001b[A\n"," 23% 2756/11873 [00:08<00:30, 299.53it/s]\u001b[A\n"," 23% 2786/11873 [00:08<00:30, 297.29it/s]\u001b[A\n"," 24% 2818/11873 [00:08<00:30, 301.48it/s]\u001b[A\n"," 24% 2849/11873 [00:08<00:29, 301.98it/s]\u001b[A\n"," 24% 2880/11873 [00:08<00:29, 301.32it/s]\u001b[A\n"," 25% 2911/11873 [00:08<00:29, 299.41it/s]\u001b[A\n"," 25% 2941/11873 [00:08<00:30, 292.08it/s]\u001b[A\n"," 25% 2971/11873 [00:09<00:30, 292.25it/s]\u001b[A\n"," 25% 3001/11873 [00:09<00:30, 291.22it/s]\u001b[A\n"," 26% 3031/11873 [00:09<00:30, 291.97it/s]\u001b[A\n"," 26% 3061/11873 [00:09<00:30, 289.34it/s]\u001b[A\n"," 26% 3090/11873 [00:09<00:30, 286.56it/s]\u001b[A\n"," 26% 3119/11873 [00:09<00:34, 256.57it/s]\u001b[A\n"," 27% 3147/11873 [00:09<00:33, 261.06it/s]\u001b[A\n"," 27% 3174/11873 [00:09<00:35, 245.25it/s]\u001b[A\n"," 27% 3204/11873 [00:09<00:33, 259.77it/s]\u001b[A\n"," 27% 3235/11873 [00:10<00:31, 271.98it/s]\u001b[A\n"," 27% 3263/11873 [00:10<00:31, 270.89it/s]\u001b[A\n"," 28% 3291/11873 [00:10<00:36, 233.72it/s]\u001b[A\n"," 28% 3316/11873 [00:10<00:38, 221.34it/s]\u001b[A\n"," 28% 3340/11873 [00:10<00:38, 224.49it/s]\u001b[A\n"," 28% 3364/11873 [00:10<00:41, 204.97it/s]\u001b[A\n"," 29% 3392/11873 [00:10<00:38, 221.38it/s]\u001b[A\n"," 29% 3423/11873 [00:10<00:34, 242.77it/s]\u001b[A\n"," 29% 3453/11873 [00:10<00:32, 258.34it/s]\u001b[A\n"," 29% 3483/11873 [00:11<00:31, 269.49it/s]\u001b[A\n"," 30% 3514/11873 [00:11<00:29, 279.59it/s]\u001b[A\n"," 30% 3545/11873 [00:11<00:29, 286.97it/s]\u001b[A\n"," 30% 3576/11873 [00:11<00:28, 290.89it/s]\u001b[A\n"," 30% 3607/11873 [00:11<00:28, 295.02it/s]\u001b[A\n"," 31% 3638/11873 [00:11<00:27, 298.72it/s]\u001b[A\n"," 31% 3668/11873 [00:11<00:27, 296.01it/s]\u001b[A\n"," 31% 3698/11873 [00:11<00:27, 294.62it/s]\u001b[A\n"," 31% 3728/11873 [00:11<00:27, 294.03it/s]\u001b[A\n"," 32% 3759/11873 [00:12<00:27, 297.64it/s]\u001b[A\n"," 32% 3789/11873 [00:12<00:27, 297.26it/s]\u001b[A\n"," 32% 3819/11873 [00:12<00:27, 296.91it/s]\u001b[A\n"," 32% 3849/11873 [00:12<00:26, 297.35it/s]\u001b[A\n"," 33% 3879/11873 [00:12<00:26, 298.13it/s]\u001b[A\n"," 33% 3910/11873 [00:12<00:26, 299.71it/s]\u001b[A\n"," 33% 3940/11873 [00:12<00:26, 295.14it/s]\u001b[A\n"," 33% 3970/11873 [00:12<00:27, 290.70it/s]\u001b[A\n"," 34% 4001/11873 [00:12<00:26, 294.11it/s]\u001b[A\n"," 34% 4032/11873 [00:12<00:26, 296.33it/s]\u001b[A\n"," 34% 4063/11873 [00:13<00:26, 298.27it/s]\u001b[A\n"," 34% 4093/11873 [00:13<00:26, 297.67it/s]\u001b[A\n"," 35% 4124/11873 [00:13<00:25, 299.44it/s]\u001b[A\n"," 35% 4154/11873 [00:13<00:26, 296.49it/s]\u001b[A\n"," 35% 4184/11873 [00:13<00:26, 293.23it/s]\u001b[A\n"," 36% 4215/11873 [00:13<00:25, 296.09it/s]\u001b[A\n"," 36% 4247/11873 [00:13<00:25, 300.68it/s]\u001b[A\n"," 36% 4278/11873 [00:13<00:25, 301.24it/s]\u001b[A\n"," 36% 4310/11873 [00:13<00:24, 305.01it/s]\u001b[A\n"," 37% 4341/11873 [00:13<00:24, 303.03it/s]\u001b[A\n"," 37% 4372/11873 [00:14<00:24, 304.65it/s]\u001b[A\n"," 37% 4403/11873 [00:14<00:24, 303.01it/s]\u001b[A\n"," 37% 4434/11873 [00:14<00:28, 263.35it/s]\u001b[A\n"," 38% 4466/11873 [00:14<00:26, 276.90it/s]\u001b[A\n"," 38% 4497/11873 [00:14<00:25, 283.96it/s]\u001b[A\n"," 38% 4527/11873 [00:14<00:25, 288.02it/s]\u001b[A\n"," 38% 4558/11873 [00:14<00:25, 291.96it/s]\u001b[A\n"," 39% 4589/11873 [00:14<00:24, 295.66it/s]\u001b[A\n"," 39% 4619/11873 [00:14<00:24, 296.36it/s]\u001b[A\n"," 39% 4649/11873 [00:15<00:24, 297.42it/s]\u001b[A\n"," 39% 4680/11873 [00:15<00:24, 299.41it/s]\u001b[A\n"," 40% 4711/11873 [00:15<00:23, 299.99it/s]\u001b[A\n"," 40% 4742/11873 [00:15<00:23, 299.25it/s]\u001b[A\n"," 40% 4773/11873 [00:15<00:23, 302.24it/s]\u001b[A\n"," 40% 4804/11873 [00:15<00:23, 296.42it/s]\u001b[A\n"," 41% 4834/11873 [00:15<00:23, 294.18it/s]\u001b[A\n"," 41% 4864/11873 [00:15<00:24, 291.55it/s]\u001b[A\n"," 41% 4895/11873 [00:15<00:23, 294.69it/s]\u001b[A\n"," 41% 4926/11873 [00:15<00:23, 296.68it/s]\u001b[A\n"," 42% 4956/11873 [00:16<00:23, 295.36it/s]\u001b[A\n"," 42% 4987/11873 [00:16<00:23, 298.22it/s]\u001b[A\n"," 42% 5017/11873 [00:16<00:23, 296.42it/s]\u001b[A\n"," 43% 5047/11873 [00:16<00:23, 295.38it/s]\u001b[A\n"," 43% 5077/11873 [00:16<00:22, 295.60it/s]\u001b[A\n"," 43% 5107/11873 [00:16<00:22, 296.71it/s]\u001b[A\n"," 43% 5137/11873 [00:16<00:22, 297.38it/s]\u001b[A\n"," 44% 5167/11873 [00:16<00:22, 294.86it/s]\u001b[A\n"," 44% 5198/11873 [00:16<00:22, 297.63it/s]\u001b[A\n"," 44% 5228/11873 [00:16<00:22, 298.30it/s]\u001b[A\n"," 44% 5258/11873 [00:17<00:23, 284.18it/s]\u001b[A\n"," 45% 5287/11873 [00:17<00:23, 280.55it/s]\u001b[A\n"," 45% 5318/11873 [00:17<00:22, 288.17it/s]\u001b[A\n"," 45% 5349/11873 [00:17<00:22, 294.42it/s]\u001b[A\n"," 45% 5380/11873 [00:17<00:21, 296.87it/s]\u001b[A\n"," 46% 5411/11873 [00:17<00:21, 299.21it/s]\u001b[A\n"," 46% 5441/11873 [00:17<00:21, 298.64it/s]\u001b[A\n"," 46% 5471/11873 [00:17<00:21, 296.02it/s]\u001b[A\n"," 46% 5502/11873 [00:17<00:21, 298.58it/s]\u001b[A\n"," 47% 5533/11873 [00:18<00:21, 300.43it/s]\u001b[A\n"," 47% 5564/11873 [00:18<00:21, 297.24it/s]\u001b[A\n"," 47% 5594/11873 [00:18<00:21, 296.75it/s]\u001b[A\n"," 47% 5624/11873 [00:18<00:21, 297.05it/s]\u001b[A\n"," 48% 5654/11873 [00:18<00:21, 292.84it/s]\u001b[A\n"," 48% 5684/11873 [00:18<00:21, 292.69it/s]\u001b[A\n"," 48% 5714/11873 [00:18<00:21, 292.48it/s]\u001b[A\n"," 48% 5744/11873 [00:18<00:20, 292.56it/s]\u001b[A\n"," 49% 5774/11873 [00:18<00:20, 291.33it/s]\u001b[A\n"," 49% 5804/11873 [00:18<00:20, 293.55it/s]\u001b[A\n"," 49% 5834/11873 [00:19<00:20, 294.13it/s]\u001b[A\n"," 49% 5865/11873 [00:19<00:20, 298.68it/s]\u001b[A\n"," 50% 5896/11873 [00:19<00:19, 299.57it/s]\u001b[A\n"," 50% 5927/11873 [00:19<00:19, 301.89it/s]\u001b[A\n"," 50% 5959/11873 [00:19<00:19, 304.36it/s]\u001b[A\n"," 50% 5990/11873 [00:19<00:19, 304.44it/s]\u001b[A\n"," 51% 6021/11873 [00:19<00:19, 303.60it/s]\u001b[A\n"," 51% 6052/11873 [00:19<00:19, 301.81it/s]\u001b[A\n"," 51% 6083/11873 [00:19<00:19, 302.09it/s]\u001b[A\n"," 51% 6114/11873 [00:19<00:19, 299.30it/s]\u001b[A\n"," 52% 6144/11873 [00:20<00:19, 298.55it/s]\u001b[A\n"," 52% 6175/11873 [00:20<00:18, 300.02it/s]\u001b[A\n"," 52% 6206/11873 [00:20<00:18, 299.28it/s]\u001b[A\n"," 53% 6237/11873 [00:20<00:18, 300.80it/s]\u001b[A\n"," 53% 6268/11873 [00:20<00:18, 301.32it/s]\u001b[A\n"," 53% 6299/11873 [00:20<00:18, 302.40it/s]\u001b[A\n"," 53% 6330/11873 [00:20<00:18, 303.48it/s]\u001b[A\n"," 54% 6361/11873 [00:20<00:19, 285.33it/s]\u001b[A\n"," 54% 6392/11873 [00:20<00:18, 290.62it/s]\u001b[A\n"," 54% 6422/11873 [00:21<00:18, 292.06it/s]\u001b[A\n"," 54% 6452/11873 [00:21<00:18, 291.40it/s]\u001b[A\n"," 55% 6483/11873 [00:21<00:18, 295.52it/s]\u001b[A\n"," 55% 6513/11873 [00:21<00:18, 296.65it/s]\u001b[A\n"," 55% 6543/11873 [00:21<00:18, 296.00it/s]\u001b[A\n"," 55% 6573/11873 [00:21<00:17, 295.11it/s]\u001b[A\n"," 56% 6604/11873 [00:21<00:17, 298.05it/s]\u001b[A\n"," 56% 6634/11873 [00:21<00:17, 298.23it/s]\u001b[A\n"," 56% 6665/11873 [00:21<00:17, 301.00it/s]\u001b[A\n"," 56% 6696/11873 [00:21<00:17, 300.28it/s]\u001b[A\n"," 57% 6727/11873 [00:22<00:17, 293.49it/s]\u001b[A\n"," 57% 6759/11873 [00:22<00:17, 298.57it/s]\u001b[A\n"," 57% 6789/11873 [00:22<00:17, 293.15it/s]\u001b[A\n"," 57% 6820/11873 [00:22<00:17, 296.73it/s]\u001b[A\n"," 58% 6851/11873 [00:22<00:16, 299.18it/s]\u001b[A\n"," 58% 6881/11873 [00:22<00:23, 209.86it/s]\u001b[A\n"," 58% 6908/11873 [00:22<00:22, 222.43it/s]\u001b[A\n"," 58% 6938/11873 [00:22<00:20, 241.10it/s]\u001b[A\n"," 59% 6968/11873 [00:23<00:19, 254.82it/s]\u001b[A\n"," 59% 6998/11873 [00:23<00:18, 265.70it/s]\u001b[A\n"," 59% 7028/11873 [00:23<00:17, 273.97it/s]\u001b[A\n"," 59% 7058/11873 [00:23<00:17, 279.82it/s]\u001b[A\n"," 60% 7088/11873 [00:23<00:16, 284.89it/s]\u001b[A\n"," 60% 7119/11873 [00:23<00:16, 289.58it/s]\u001b[A\n"," 60% 7149/11873 [00:23<00:16, 290.62it/s]\u001b[A\n"," 60% 7179/11873 [00:23<00:16, 291.31it/s]\u001b[A\n"," 61% 7209/11873 [00:23<00:15, 291.82it/s]\u001b[A\n"," 61% 7239/11873 [00:23<00:15, 290.26it/s]\u001b[A\n"," 61% 7269/11873 [00:24<00:15, 288.82it/s]\u001b[A\n"," 61% 7298/11873 [00:24<00:15, 287.78it/s]\u001b[A\n"," 62% 7327/11873 [00:24<00:15, 288.34it/s]\u001b[A\n"," 62% 7357/11873 [00:24<00:15, 291.39it/s]\u001b[A\n"," 62% 7387/11873 [00:24<00:15, 289.58it/s]\u001b[A\n"," 62% 7416/11873 [00:24<00:15, 288.23it/s]\u001b[A\n"," 63% 7446/11873 [00:24<00:15, 289.39it/s]\u001b[A\n"," 63% 7476/11873 [00:24<00:15, 290.56it/s]\u001b[A\n"," 63% 7507/11873 [00:24<00:14, 294.02it/s]\u001b[A\n"," 63% 7537/11873 [00:24<00:14, 295.36it/s]\u001b[A\n"," 64% 7567/11873 [00:25<00:14, 295.09it/s]\u001b[A\n"," 64% 7597/11873 [00:25<00:14, 292.87it/s]\u001b[A\n"," 64% 7627/11873 [00:25<00:14, 292.14it/s]\u001b[A\n"," 64% 7657/11873 [00:25<00:14, 292.73it/s]\u001b[A\n"," 65% 7687/11873 [00:25<00:14, 287.59it/s]\u001b[A\n"," 65% 7716/11873 [00:25<00:14, 287.13it/s]\u001b[A\n"," 65% 7745/11873 [00:25<00:14, 286.64it/s]\u001b[A\n"," 65% 7774/11873 [00:25<00:14, 287.50it/s]\u001b[A\n"," 66% 7803/11873 [00:25<00:14, 286.64it/s]\u001b[A\n"," 66% 7832/11873 [00:25<00:14, 280.60it/s]\u001b[A\n"," 66% 7861/11873 [00:26<00:14, 280.34it/s]\u001b[A\n"," 66% 7890/11873 [00:26<00:14, 281.69it/s]\u001b[A\n"," 67% 7919/11873 [00:26<00:13, 282.55it/s]\u001b[A\n"," 67% 7949/11873 [00:26<00:13, 285.33it/s]\u001b[A\n"," 67% 7978/11873 [00:26<00:13, 284.96it/s]\u001b[A\n"," 67% 8008/11873 [00:26<00:13, 289.10it/s]\u001b[A\n"," 68% 8038/11873 [00:26<00:13, 290.52it/s]\u001b[A\n"," 68% 8068/11873 [00:26<00:13, 289.94it/s]\u001b[A\n"," 68% 8098/11873 [00:26<00:12, 291.87it/s]\u001b[A\n"," 68% 8128/11873 [00:27<00:12, 290.07it/s]\u001b[A\n"," 69% 8158/11873 [00:27<00:12, 290.82it/s]\u001b[A\n"," 69% 8188/11873 [00:27<00:12, 290.85it/s]\u001b[A\n"," 69% 8218/11873 [00:27<00:12, 287.82it/s]\u001b[A\n"," 69% 8248/11873 [00:27<00:12, 291.19it/s]\u001b[A\n"," 70% 8279/11873 [00:27<00:12, 294.87it/s]\u001b[A\n"," 70% 8309/11873 [00:27<00:12, 293.29it/s]\u001b[A\n"," 70% 8339/11873 [00:27<00:11, 294.72it/s]\u001b[A\n"," 70% 8369/11873 [00:27<00:11, 294.29it/s]\u001b[A\n"," 71% 8399/11873 [00:27<00:11, 292.61it/s]\u001b[A\n"," 71% 8429/11873 [00:28<00:11, 289.68it/s]\u001b[A\n"," 71% 8458/11873 [00:28<00:11, 285.80it/s]\u001b[A\n"," 71% 8487/11873 [00:28<00:11, 286.77it/s]\u001b[A\n"," 72% 8517/11873 [00:28<00:11, 290.37it/s]\u001b[A\n"," 72% 8547/11873 [00:28<00:11, 292.35it/s]\u001b[A\n"," 72% 8577/11873 [00:28<00:11, 291.37it/s]\u001b[A\n"," 72% 8607/11873 [00:28<00:11, 292.64it/s]\u001b[A\n"," 73% 8637/11873 [00:28<00:11, 290.70it/s]\u001b[A\n"," 73% 8667/11873 [00:28<00:11, 291.27it/s]\u001b[A\n"," 73% 8697/11873 [00:28<00:10, 291.75it/s]\u001b[A\n"," 74% 8727/11873 [00:29<00:10, 293.60it/s]\u001b[A\n"," 74% 8757/11873 [00:29<00:10, 292.76it/s]\u001b[A\n"," 74% 8787/11873 [00:29<00:10, 293.51it/s]\u001b[A\n"," 74% 8817/11873 [00:29<00:10, 291.72it/s]\u001b[A\n"," 75% 8847/11873 [00:29<00:10, 292.40it/s]\u001b[A\n"," 75% 8877/11873 [00:29<00:10, 293.32it/s]\u001b[A\n"," 75% 8907/11873 [00:29<00:10, 292.60it/s]\u001b[A\n"," 75% 8937/11873 [00:29<00:10, 284.84it/s]\u001b[A\n"," 76% 8967/11873 [00:29<00:10, 287.98it/s]\u001b[A\n"," 76% 8998/11873 [00:29<00:09, 291.78it/s]\u001b[A\n"," 76% 9028/11873 [00:30<00:09, 291.13it/s]\u001b[A\n"," 76% 9058/11873 [00:30<00:09, 291.46it/s]\u001b[A\n"," 77% 9088/11873 [00:30<00:09, 292.39it/s]\u001b[A\n"," 77% 9118/11873 [00:30<00:09, 286.63it/s]\u001b[A\n"," 77% 9148/11873 [00:30<00:09, 288.43it/s]\u001b[A\n"," 77% 9178/11873 [00:30<00:09, 290.70it/s]\u001b[A\n"," 78% 9208/11873 [00:30<00:09, 291.61it/s]\u001b[A\n"," 78% 9238/11873 [00:30<00:09, 289.00it/s]\u001b[A\n"," 78% 9268/11873 [00:30<00:08, 289.96it/s]\u001b[A\n"," 78% 9298/11873 [00:31<00:08, 289.14it/s]\u001b[A\n"," 79% 9327/11873 [00:31<00:08, 288.77it/s]\u001b[A\n"," 79% 9357/11873 [00:31<00:08, 289.45it/s]\u001b[A\n"," 79% 9388/11873 [00:31<00:08, 293.15it/s]\u001b[A\n"," 79% 9418/11873 [00:31<00:08, 292.67it/s]\u001b[A\n"," 80% 9448/11873 [00:31<00:08, 293.50it/s]\u001b[A\n"," 80% 9478/11873 [00:31<00:08, 293.91it/s]\u001b[A\n"," 80% 9508/11873 [00:31<00:08, 294.51it/s]\u001b[A\n"," 80% 9538/11873 [00:31<00:07, 295.35it/s]\u001b[A\n"," 81% 9568/11873 [00:31<00:07, 296.51it/s]\u001b[A\n"," 81% 9598/11873 [00:32<00:07, 295.57it/s]\u001b[A\n"," 81% 9628/11873 [00:32<00:07, 294.47it/s]\u001b[A\n"," 81% 9658/11873 [00:32<00:07, 290.49it/s]\u001b[A\n"," 82% 9688/11873 [00:32<00:07, 292.93it/s]\u001b[A\n"," 82% 9718/11873 [00:32<00:07, 294.75it/s]\u001b[A\n"," 82% 9748/11873 [00:32<00:07, 294.53it/s]\u001b[A\n"," 82% 9778/11873 [00:32<00:07, 294.36it/s]\u001b[A\n"," 83% 9808/11873 [00:32<00:07, 294.33it/s]\u001b[A\n"," 83% 9838/11873 [00:32<00:06, 294.65it/s]\u001b[A\n"," 83% 9868/11873 [00:32<00:06, 295.79it/s]\u001b[A\n"," 83% 9898/11873 [00:33<00:06, 294.62it/s]\u001b[A\n"," 84% 9928/11873 [00:33<00:06, 290.21it/s]\u001b[A\n"," 84% 9959/11873 [00:33<00:06, 293.51it/s]\u001b[A\n"," 84% 9989/11873 [00:33<00:06, 291.44it/s]\u001b[A\n"," 84% 10020/11873 [00:33<00:06, 294.67it/s]\u001b[A\n"," 85% 10051/11873 [00:33<00:06, 296.84it/s]\u001b[A\n"," 85% 10081/11873 [00:33<00:06, 296.42it/s]\u001b[A\n"," 85% 10111/11873 [00:33<00:06, 293.29it/s]\u001b[A\n"," 85% 10141/11873 [00:33<00:05, 295.23it/s]\u001b[A\n"," 86% 10171/11873 [00:33<00:05, 295.75it/s]\u001b[A\n"," 86% 10201/11873 [00:34<00:05, 292.50it/s]\u001b[A\n"," 86% 10231/11873 [00:34<00:05, 293.53it/s]\u001b[A\n"," 86% 10261/11873 [00:34<00:05, 290.82it/s]\u001b[A\n"," 87% 10291/11873 [00:34<00:05, 288.23it/s]\u001b[A\n"," 87% 10321/11873 [00:34<00:05, 289.49it/s]\u001b[A\n"," 87% 10351/11873 [00:34<00:05, 291.11it/s]\u001b[A\n"," 87% 10381/11873 [00:34<00:05, 293.68it/s]\u001b[A\n"," 88% 10411/11873 [00:34<00:04, 295.15it/s]\u001b[A\n"," 88% 10441/11873 [00:34<00:04, 292.85it/s]\u001b[A\n"," 88% 10472/11873 [00:35<00:04, 296.41it/s]\u001b[A\n"," 88% 10502/11873 [00:35<00:04, 285.86it/s]\u001b[A\n"," 89% 10532/11873 [00:35<00:04, 288.75it/s]\u001b[A\n"," 89% 10561/11873 [00:35<00:04, 288.00it/s]\u001b[A\n"," 89% 10591/11873 [00:35<00:04, 289.36it/s]\u001b[A\n"," 89% 10621/11873 [00:35<00:04, 291.17it/s]\u001b[A\n"," 90% 10651/11873 [00:35<00:04, 289.93it/s]\u001b[A\n"," 90% 10681/11873 [00:35<00:04, 289.18it/s]\u001b[A\n"," 90% 10711/11873 [00:35<00:03, 292.18it/s]\u001b[A\n"," 90% 10741/11873 [00:35<00:03, 292.85it/s]\u001b[A\n"," 91% 10771/11873 [00:36<00:03, 292.50it/s]\u001b[A\n"," 91% 10801/11873 [00:36<00:03, 290.40it/s]\u001b[A\n"," 91% 10831/11873 [00:36<00:03, 290.09it/s]\u001b[A\n"," 91% 10861/11873 [00:36<00:03, 291.59it/s]\u001b[A\n"," 92% 10891/11873 [00:36<00:03, 291.96it/s]\u001b[A\n"," 92% 10921/11873 [00:36<00:03, 292.36it/s]\u001b[A\n"," 92% 10951/11873 [00:36<00:03, 289.37it/s]\u001b[A\n"," 92% 10982/11873 [00:36<00:03, 292.50it/s]\u001b[A\n"," 93% 11012/11873 [00:36<00:02, 293.97it/s]\u001b[A\n"," 93% 11042/11873 [00:36<00:02, 293.23it/s]\u001b[A\n"," 93% 11072/11873 [00:37<00:02, 294.68it/s]\u001b[A\n"," 94% 11102/11873 [00:37<00:02, 292.66it/s]\u001b[A\n"," 94% 11132/11873 [00:37<00:02, 292.68it/s]\u001b[A\n"," 94% 11163/11873 [00:37<00:02, 295.84it/s]\u001b[A\n"," 94% 11193/11873 [00:37<00:02, 295.67it/s]\u001b[A\n"," 95% 11223/11873 [00:37<00:02, 296.56it/s]\u001b[A\n"," 95% 11253/11873 [00:37<00:02, 294.39it/s]\u001b[A\n"," 95% 11283/11873 [00:37<00:02, 294.30it/s]\u001b[A\n"," 95% 11313/11873 [00:37<00:01, 295.27it/s]\u001b[A\n"," 96% 11343/11873 [00:38<00:01, 292.47it/s]\u001b[A\n"," 96% 11373/11873 [00:38<00:01, 291.96it/s]\u001b[A\n"," 96% 11403/11873 [00:38<00:01, 291.96it/s]\u001b[A\n"," 96% 11433/11873 [00:38<00:01, 292.21it/s]\u001b[A\n"," 97% 11464/11873 [00:38<00:01, 294.55it/s]\u001b[A\n"," 97% 11494/11873 [00:38<00:01, 294.09it/s]\u001b[A\n"," 97% 11524/11873 [00:38<00:01, 294.58it/s]\u001b[A\n"," 97% 11554/11873 [00:38<00:01, 291.55it/s]\u001b[A\n"," 98% 11584/11873 [00:38<00:00, 293.80it/s]\u001b[A\n"," 98% 11614/11873 [00:38<00:00, 292.03it/s]\u001b[A\n"," 98% 11644/11873 [00:39<00:00, 289.62it/s]\u001b[A\n"," 98% 11673/11873 [00:39<00:00, 289.27it/s]\u001b[A\n"," 99% 11702/11873 [00:39<00:00, 288.61it/s]\u001b[A\n"," 99% 11731/11873 [00:39<00:00, 288.26it/s]\u001b[A\n"," 99% 11760/11873 [00:39<00:00, 287.32it/s]\u001b[A\n"," 99% 11791/11873 [00:39<00:00, 291.88it/s]\u001b[A\n","100% 11821/11873 [00:39<00:00, 291.57it/s]\u001b[A\n","100% 11873/11873 [00:39<00:00, 298.10it/s]\n","04/06/2022 23:26:06 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/eval_predictions.json.\n","04/06/2022 23:26:06 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/eval_nbest_predictions.json.\n","04/06/2022 23:26:08 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug/eval_null_odds.json.\n","04/06/2022 23:26:11 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1496/1496 [05:17<00:00,  4.71it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      =   73.33\n","  eval_HasAns_f1         = 79.2066\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 62.1531\n","  eval_NoAns_f1          = 62.1531\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 67.7419\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           =  70.676\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 67.7335\n","  eval_f1                = 70.6676\n","  eval_samples           =   11968\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 23:26:12,165 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_context_aug', 'type': 'sichenzhong/squad_v2_context_aug', 'args': 'squad_v2'}}\n"]}],"source":["!python run_qa.py \\\n","  --model_name_or_path albert-base-v2 \\\n","  --dataset_name sichenzhong/squad_v2_context_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 512 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/albert-base-v2/context-aug"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Au53tORZUy9I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649310043320,"user_tz":240,"elapsed":6167159,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"c1773322-91dc-4fbd-bd9c-53d7a3a25188"},"outputs":[{"output_type":"stream","name":"stdout","text":["04/07/2022 03:57:59 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/07/2022 03:57:59 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/roberta-base/context-aug/runs/Apr07_03-57-59_5b2b897fe0af,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/roberta-base/context-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/roberta-base/context-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/07/2022 03:58:00 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7\n","04/07/2022 03:58:00 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/07/2022 03:58:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","04/07/2022 03:58:00 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","04/07/2022 03:58:00 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","100% 2/2 [00:00<00:00, 633.01it/s]\n","[INFO|configuration_utils.py:654] 2022-04-07 03:58:01,127 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-07 03:58:01,128 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-07 03:58:01,487 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-07 03:58:01,848 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-07 03:58:01,849 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 03:58:04,384 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 03:58:04,384 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 03:58:04,384 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 03:58:04,384 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 03:58:04,384 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 03:58:04,385 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-07 03:58:04,745 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-07 03:58:04,746 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|modeling_utils.py:1772] 2022-04-07 03:58:05,194 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[WARNING|modeling_utils.py:2049] 2022-04-07 03:58:06,483 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-04-07 03:58:06,484 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/07/2022 03:58:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-d24b383e37ca8ccc.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:41<00:00,  3.15ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/07/2022 03:58:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_context_aug-4e28f6dbac5172a7/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-dafb4f4c24a536fd.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:05<00:00,  5.47s/ba]\n","04/07/2022 03:59:54 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpsdy440c9\n","Downloading builder script: 6.46kB [00:00, 6.41MB/s]       \n","04/07/2022 03:59:54 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/07/2022 03:59:54 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/07/2022 03:59:54 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp4zaaol5o\n","Downloading extra modules: 11.3kB [00:00, 11.9MB/s]       \n","04/07/2022 03:59:54 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","04/07/2022 03:59:54 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-07 04:00:07,035 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-07 04:00:07,035 >>   Num examples = 131809\n","[INFO|trainer.py:1292] 2022-04-07 04:00:07,035 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-07 04:00:07,035 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1294] 2022-04-07 04:00:07,035 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1295] 2022-04-07 04:00:07,035 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-07 04:00:07,035 >>   Total optimization steps = 10986\n","{'loss': 2.2748, 'learning_rate': 3.8179501183324234e-05, 'epoch': 0.09}\n","  5% 500/10986 [04:21<1:31:46,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:04:28,925 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:04:28,930 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:04:30,237 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:04:30,242 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:04:30,245 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 1.8166, 'learning_rate': 3.6359002366648465e-05, 'epoch': 0.18}\n","  9% 1000/10986 [08:48<1:27:14,  1.91it/s][INFO|trainer.py:2166] 2022-04-07 04:08:55,712 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-07 04:08:55,721 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:08:57,014 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:08:57,018 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:08:57,022 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.7566, 'learning_rate': 3.4538503549972695e-05, 'epoch': 0.27}\n"," 14% 1500/10986 [13:15<1:23:15,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:13:22,756 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:13:22,766 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:13:24,130 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:13:24,135 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:13:24,138 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.6698, 'learning_rate': 3.2718004733296926e-05, 'epoch': 0.36}\n"," 18% 2000/10986 [17:43<1:18:40,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:17:50,118 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-07 04:17:50,136 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:17:51,461 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:17:51,466 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:17:51,470 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.6215, 'learning_rate': 3.0897505916621156e-05, 'epoch': 0.46}\n"," 23% 2500/10986 [22:10<1:14:27,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:22:17,673 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:22:17,691 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:22:19,019 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:22:19,023 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:22:19,027 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.577, 'learning_rate': 2.9077007099945387e-05, 'epoch': 0.55}\n"," 27% 3000/10986 [26:38<1:09:59,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:26:45,495 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-07 04:26:45,514 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:26:46,846 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:26:46,850 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:26:46,853 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.5231, 'learning_rate': 2.7256508283269618e-05, 'epoch': 0.64}\n"," 32% 3500/10986 [31:06<1:05:43,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:31:13,267 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:31:13,271 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:31:14,609 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:31:14,614 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:31:14,618 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.4968, 'learning_rate': 2.543600946659385e-05, 'epoch': 0.73}\n"," 36% 4000/10986 [35:33<1:01:03,  1.91it/s][INFO|trainer.py:2166] 2022-04-07 04:35:40,781 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-07 04:35:40,786 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:35:42,087 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:35:42,092 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:35:42,095 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.4718, 'learning_rate': 2.3615510649918082e-05, 'epoch': 0.82}\n"," 41% 4500/10986 [40:01<56:43,  1.91it/s][INFO|trainer.py:2166] 2022-04-07 04:40:08,121 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:40:08,126 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:40:09,429 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:40:09,433 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:40:09,437 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.4577, 'learning_rate': 2.1795011833242313e-05, 'epoch': 0.91}\n"," 46% 5000/10986 [44:28<52:23,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:44:35,493 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-07 04:44:35,499 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:44:36,831 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:44:36,835 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:44:36,838 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.4444, 'learning_rate': 1.997451301656654e-05, 'epoch': 1.0}\n"," 50% 5500/10986 [48:55<46:37,  1.96it/s][INFO|trainer.py:2166] 2022-04-07 04:49:02,461 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:49:02,466 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:49:03,788 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:49:03,792 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:49:03,796 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.1872, 'learning_rate': 1.8154014199890774e-05, 'epoch': 1.09}\n"," 55% 6000/10986 [53:22<43:37,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:53:30,002 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-07 04:53:30,008 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:53:31,322 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:53:31,327 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:53:31,330 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.169, 'learning_rate': 1.6333515383215e-05, 'epoch': 1.18}\n"," 59% 6500/10986 [57:50<39:18,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:57:57,269 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:57:57,274 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:57:58,594 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:57:58,599 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:57:58,602 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.1465, 'learning_rate': 1.4513016566539234e-05, 'epoch': 1.27}\n"," 64% 7000/10986 [1:02:17<34:55,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 05:02:24,915 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-07 05:02:24,921 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:02:26,294 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:02:26,324 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:02:26,344 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.1287, 'learning_rate': 1.2692517749863464e-05, 'epoch': 1.37}\n"," 68% 7500/10986 [1:06:45<30:31,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 05:06:52,776 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-07 05:06:52,782 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:06:54,221 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:06:54,226 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:06:54,230 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 1.1304, 'learning_rate': 1.0872018933187693e-05, 'epoch': 1.46}\n"," 73% 8000/10986 [1:11:16<26:12,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 05:11:24,051 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-07 05:11:24,056 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:11:25,373 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:11:25,378 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:11:25,383 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 1.1476, 'learning_rate': 9.051520116511924e-06, 'epoch': 1.55}\n"," 77% 8500/10986 [1:15:47<21:53,  1.89it/s][INFO|trainer.py:2166] 2022-04-07 05:15:55,049 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-07 05:15:55,056 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:15:56,358 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:15:56,362 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:15:56,365 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 1.125, 'learning_rate': 7.231021299836156e-06, 'epoch': 1.64}\n"," 82% 9000/10986 [1:20:17<17:24,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 05:20:24,907 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-07 05:20:24,913 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:20:26,180 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:20:26,184 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:20:26,188 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 1.0939, 'learning_rate': 5.410522483160386e-06, 'epoch': 1.73}\n"," 86% 9500/10986 [1:24:48<12:59,  1.91it/s][INFO|trainer.py:2166] 2022-04-07 05:24:55,087 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-07 05:24:55,092 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:24:56,350 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:24:56,355 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:24:56,359 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 1.0856, 'learning_rate': 3.5900236664846172e-06, 'epoch': 1.82}\n"," 91% 10000/10986 [1:29:16<08:37,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 05:29:23,597 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-07 05:29:23,602 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:29:24,862 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:29:24,867 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:29:24,870 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 1.1044, 'learning_rate': 1.7695248498088476e-06, 'epoch': 1.91}\n"," 96% 10500/10986 [1:33:45<04:15,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 05:33:52,128 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-07 05:33:52,133 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:33:53,393 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:33:53,400 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:33:53,403 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/checkpoint-10500/special_tokens_map.json\n","100% 10985/10986 [1:38:05<00:00,  1.91it/s][INFO|trainer.py:1530] 2022-04-07 05:38:12,694 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 5885.6591, 'train_samples_per_second': 44.79, 'train_steps_per_second': 1.867, 'train_loss': 1.3866269194795506, 'epoch': 2.0}\n","100% 10986/10986 [1:38:05<00:00,  1.87it/s]\n","[INFO|trainer.py:2166] 2022-04-07 05:38:12,699 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug\n","[INFO|configuration_utils.py:441] 2022-04-07 05:38:12,704 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:38:14,002 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:38:14,007 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:38:14,010 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     1.3866\n","  train_runtime            = 1:38:05.65\n","  train_samples            =     131809\n","  train_samples_per_second =      44.79\n","  train_steps_per_second   =      1.867\n","04/07/2022 05:38:14 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-07 05:38:14,986 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-07 05:38:14,988 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-07 05:38:14,988 >>   Num examples = 12165\n","[INFO|trainer.py:2421] 2022-04-07 05:38:14,988 >>   Batch size = 8\n","100% 1520/1521 [01:34<00:00, 16.15it/s]04/07/2022 05:40:00 - INFO - utils_qa - Post-processing 11873 example predictions split into 12165 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 43/11873 [00:00<00:27, 426.22it/s]\u001b[A\n","  1% 86/11873 [00:00<00:29, 397.16it/s]\u001b[A\n","  1% 131/11873 [00:00<00:27, 420.11it/s]\u001b[A\n","  2% 179/11873 [00:00<00:26, 442.42it/s]\u001b[A\n","  2% 226/11873 [00:00<00:25, 450.74it/s]\u001b[A\n","  2% 273/11873 [00:00<00:25, 455.75it/s]\u001b[A\n","  3% 321/11873 [00:00<00:25, 461.71it/s]\u001b[A\n","  3% 368/11873 [00:00<00:25, 459.14it/s]\u001b[A\n","  4% 417/11873 [00:00<00:24, 465.74it/s]\u001b[A\n","  4% 465/11873 [00:01<00:24, 467.78it/s]\u001b[A\n","  4% 512/11873 [00:01<00:24, 455.07it/s]\u001b[A\n","100% 1521/1521 [01:47<00:00, 16.15it/s]\n","  5% 604/11873 [00:01<00:25, 442.87it/s]\u001b[A\n","  5% 650/11873 [00:01<00:25, 446.02it/s]\u001b[A\n","  6% 696/11873 [00:01<00:24, 447.74it/s]\u001b[A\n","  6% 742/11873 [00:01<00:24, 449.09it/s]\u001b[A\n","  7% 787/11873 [00:01<00:24, 448.51it/s]\u001b[A\n","  7% 834/11873 [00:01<00:24, 454.61it/s]\u001b[A\n","  7% 884/11873 [00:01<00:23, 465.12it/s]\u001b[A\n","  8% 931/11873 [00:02<00:23, 463.22it/s]\u001b[A\n","  8% 978/11873 [00:02<00:23, 462.18it/s]\u001b[A\n","  9% 1025/11873 [00:02<00:25, 433.35it/s]\u001b[A\n","  9% 1069/11873 [00:02<00:26, 409.93it/s]\u001b[A\n","  9% 1111/11873 [00:02<00:27, 386.45it/s]\u001b[A\n"," 10% 1151/11873 [00:02<00:28, 380.16it/s]\u001b[A\n"," 10% 1190/11873 [00:02<00:28, 374.70it/s]\u001b[A\n"," 10% 1228/11873 [00:02<00:28, 370.12it/s]\u001b[A\n"," 11% 1266/11873 [00:02<00:29, 365.53it/s]\u001b[A\n"," 11% 1303/11873 [00:03<00:28, 364.75it/s]\u001b[A\n"," 11% 1340/11873 [00:03<00:29, 361.44it/s]\u001b[A\n"," 12% 1377/11873 [00:03<00:29, 361.05it/s]\u001b[A\n"," 12% 1414/11873 [00:03<00:29, 359.86it/s]\u001b[A\n"," 12% 1450/11873 [00:03<00:29, 358.59it/s]\u001b[A\n"," 13% 1486/11873 [00:03<00:28, 358.31it/s]\u001b[A\n"," 13% 1523/11873 [00:03<00:28, 360.74it/s]\u001b[A\n"," 13% 1560/11873 [00:03<00:28, 361.28it/s]\u001b[A\n"," 13% 1597/11873 [00:03<00:28, 358.51it/s]\u001b[A\n"," 14% 1633/11873 [00:03<00:28, 354.68it/s]\u001b[A\n"," 14% 1669/11873 [00:04<00:28, 354.35it/s]\u001b[A\n"," 14% 1705/11873 [00:04<00:28, 355.19it/s]\u001b[A\n"," 15% 1741/11873 [00:04<00:28, 350.60it/s]\u001b[A\n"," 15% 1777/11873 [00:04<00:29, 346.00it/s]\u001b[A\n"," 15% 1812/11873 [00:04<00:29, 345.69it/s]\u001b[A\n"," 16% 1847/11873 [00:04<00:29, 344.74it/s]\u001b[A\n"," 16% 1882/11873 [00:04<00:28, 345.40it/s]\u001b[A\n"," 16% 1918/11873 [00:04<00:28, 348.95it/s]\u001b[A\n"," 16% 1955/11873 [00:04<00:28, 354.02it/s]\u001b[A\n"," 17% 1991/11873 [00:05<00:28, 352.50it/s]\u001b[A\n"," 17% 2029/11873 [00:05<00:27, 358.68it/s]\u001b[A\n"," 17% 2065/11873 [00:05<00:27, 357.91it/s]\u001b[A\n"," 18% 2103/11873 [00:05<00:26, 362.56it/s]\u001b[A\n"," 18% 2140/11873 [00:05<00:27, 357.11it/s]\u001b[A\n"," 18% 2176/11873 [00:05<00:27, 354.66it/s]\u001b[A\n"," 19% 2212/11873 [00:05<00:27, 352.67it/s]\u001b[A\n"," 19% 2248/11873 [00:05<00:27, 354.14it/s]\u001b[A\n"," 19% 2284/11873 [00:05<00:27, 351.67it/s]\u001b[A\n"," 20% 2320/11873 [00:05<00:27, 351.23it/s]\u001b[A\n"," 20% 2357/11873 [00:06<00:26, 355.52it/s]\u001b[A\n"," 20% 2393/11873 [00:06<00:26, 355.26it/s]\u001b[A\n"," 20% 2429/11873 [00:06<00:26, 353.87it/s]\u001b[A\n"," 21% 2465/11873 [00:06<00:26, 352.46it/s]\u001b[A\n"," 21% 2501/11873 [00:06<00:26, 353.54it/s]\u001b[A\n"," 21% 2537/11873 [00:06<00:26, 353.07it/s]\u001b[A\n"," 22% 2573/11873 [00:06<00:26, 354.72it/s]\u001b[A\n"," 22% 2609/11873 [00:06<00:26, 356.03it/s]\u001b[A\n"," 22% 2645/11873 [00:06<00:25, 356.06it/s]\u001b[A\n"," 23% 2681/11873 [00:06<00:25, 356.10it/s]\u001b[A\n"," 23% 2718/11873 [00:07<00:25, 357.77it/s]\u001b[A\n"," 23% 2754/11873 [00:07<00:25, 354.44it/s]\u001b[A\n"," 23% 2790/11873 [00:07<00:25, 351.90it/s]\u001b[A\n"," 24% 2828/11873 [00:07<00:25, 358.80it/s]\u001b[A\n"," 24% 2864/11873 [00:07<00:25, 358.14it/s]\u001b[A\n"," 24% 2900/11873 [00:07<00:25, 355.53it/s]\u001b[A\n"," 25% 2936/11873 [00:07<00:25, 352.64it/s]\u001b[A\n"," 25% 2972/11873 [00:07<00:25, 349.94it/s]\u001b[A\n"," 25% 3008/11873 [00:07<00:25, 345.54it/s]\u001b[A\n"," 26% 3043/11873 [00:07<00:25, 340.13it/s]\u001b[A\n"," 26% 3078/11873 [00:08<00:26, 337.58it/s]\u001b[A\n"," 26% 3112/11873 [00:08<00:27, 318.88it/s]\u001b[A\n"," 26% 3145/11873 [00:08<00:30, 282.83it/s]\u001b[A\n"," 27% 3175/11873 [00:08<00:31, 277.19it/s]\u001b[A\n"," 27% 3210/11873 [00:08<00:29, 296.00it/s]\u001b[A\n"," 27% 3242/11873 [00:08<00:28, 300.79it/s]\u001b[A\n"," 28% 3273/11873 [00:08<00:29, 290.14it/s]\u001b[A\n"," 28% 3303/11873 [00:09<00:38, 224.33it/s]\u001b[A\n"," 28% 3328/11873 [00:09<00:40, 208.80it/s]\u001b[A\n"," 28% 3351/11873 [00:09<00:41, 205.45it/s]\u001b[A\n"," 28% 3373/11873 [00:09<00:42, 200.52it/s]\u001b[A\n"," 29% 3408/11873 [00:09<00:35, 236.23it/s]\u001b[A\n"," 29% 3443/11873 [00:09<00:31, 265.45it/s]\u001b[A\n"," 29% 3479/11873 [00:09<00:29, 289.05it/s]\u001b[A\n"," 30% 3514/11873 [00:09<00:27, 303.81it/s]\u001b[A\n"," 30% 3548/11873 [00:09<00:26, 313.51it/s]\u001b[A\n"," 30% 3584/11873 [00:10<00:25, 326.31it/s]\u001b[A\n"," 31% 3622/11873 [00:10<00:24, 341.42it/s]\u001b[A\n"," 31% 3657/11873 [00:10<00:23, 342.72it/s]\u001b[A\n"," 31% 3692/11873 [00:10<00:23, 343.19it/s]\u001b[A\n"," 31% 3727/11873 [00:10<00:23, 344.84it/s]\u001b[A\n"," 32% 3763/11873 [00:10<00:23, 348.11it/s]\u001b[A\n"," 32% 3798/11873 [00:10<00:23, 345.72it/s]\u001b[A\n"," 32% 3833/11873 [00:10<00:25, 320.79it/s]\u001b[A\n"," 33% 3868/11873 [00:10<00:24, 326.32it/s]\u001b[A\n"," 33% 3904/11873 [00:10<00:23, 333.23it/s]\u001b[A\n"," 33% 3938/11873 [00:11<00:25, 312.11it/s]\u001b[A\n"," 33% 3972/11873 [00:11<00:24, 319.60it/s]\u001b[A\n"," 34% 4009/11873 [00:11<00:23, 331.92it/s]\u001b[A\n"," 34% 4045/11873 [00:11<00:23, 339.88it/s]\u001b[A\n"," 34% 4082/11873 [00:11<00:22, 346.31it/s]\u001b[A\n"," 35% 4118/11873 [00:11<00:22, 349.00it/s]\u001b[A\n"," 35% 4154/11873 [00:11<00:23, 325.06it/s]\u001b[A\n"," 35% 4188/11873 [00:11<00:23, 327.71it/s]\u001b[A\n"," 36% 4223/11873 [00:11<00:22, 333.00it/s]\u001b[A\n"," 36% 4259/11873 [00:12<00:22, 338.83it/s]\u001b[A\n"," 36% 4296/11873 [00:12<00:21, 346.20it/s]\u001b[A\n"," 36% 4332/11873 [00:12<00:21, 347.90it/s]\u001b[A\n"," 37% 4368/11873 [00:12<00:21, 350.58it/s]\u001b[A\n"," 37% 4404/11873 [00:12<00:21, 351.28it/s]\u001b[A\n"," 37% 4440/11873 [00:12<00:26, 284.47it/s]\u001b[A\n"," 38% 4477/11873 [00:12<00:24, 305.35it/s]\u001b[A\n"," 38% 4513/11873 [00:12<00:23, 317.60it/s]\u001b[A\n"," 38% 4548/11873 [00:12<00:22, 325.04it/s]\u001b[A\n"," 39% 4583/11873 [00:13<00:22, 330.08it/s]\u001b[A\n"," 39% 4619/11873 [00:13<00:21, 335.92it/s]\u001b[A\n"," 39% 4654/11873 [00:13<00:21, 338.10it/s]\u001b[A\n"," 40% 4691/11873 [00:13<00:20, 345.58it/s]\u001b[A\n"," 40% 4727/11873 [00:13<00:20, 348.59it/s]\u001b[A\n"," 40% 4764/11873 [00:13<00:20, 354.03it/s]\u001b[A\n"," 40% 4800/11873 [00:13<00:20, 350.57it/s]\u001b[A\n"," 41% 4836/11873 [00:13<00:20, 350.89it/s]\u001b[A\n"," 41% 4872/11873 [00:13<00:20, 348.74it/s]\u001b[A\n"," 41% 4907/11873 [00:13<00:20, 343.78it/s]\u001b[A\n"," 42% 4943/11873 [00:14<00:20, 345.78it/s]\u001b[A\n"," 42% 4978/11873 [00:14<00:20, 344.71it/s]\u001b[A\n"," 42% 5013/11873 [00:14<00:19, 345.59it/s]\u001b[A\n"," 43% 5048/11873 [00:14<00:19, 342.94it/s]\u001b[A\n"," 43% 5086/11873 [00:14<00:19, 352.21it/s]\u001b[A\n"," 43% 5122/11873 [00:14<00:19, 353.08it/s]\u001b[A\n"," 43% 5158/11873 [00:14<00:19, 353.13it/s]\u001b[A\n"," 44% 5194/11873 [00:14<00:18, 354.90it/s]\u001b[A\n"," 44% 5230/11873 [00:14<00:18, 352.39it/s]\u001b[A\n"," 44% 5266/11873 [00:14<00:20, 323.34it/s]\u001b[A\n"," 45% 5302/11873 [00:15<00:19, 332.13it/s]\u001b[A\n"," 45% 5339/11873 [00:15<00:19, 340.64it/s]\u001b[A\n"," 45% 5375/11873 [00:15<00:18, 344.82it/s]\u001b[A\n"," 46% 5413/11873 [00:15<00:18, 353.95it/s]\u001b[A\n"," 46% 5449/11873 [00:15<00:18, 352.22it/s]\u001b[A\n"," 46% 5485/11873 [00:15<00:18, 354.21it/s]\u001b[A\n"," 47% 5521/11873 [00:15<00:18, 351.32it/s]\u001b[A\n"," 47% 5557/11873 [00:15<00:17, 352.38it/s]\u001b[A\n"," 47% 5593/11873 [00:15<00:17, 352.71it/s]\u001b[A\n"," 47% 5629/11873 [00:16<00:18, 344.38it/s]\u001b[A\n"," 48% 5664/11873 [00:16<00:17, 345.75it/s]\u001b[A\n"," 48% 5699/11873 [00:16<00:17, 344.38it/s]\u001b[A\n"," 48% 5736/11873 [00:16<00:17, 349.99it/s]\u001b[A\n"," 49% 5772/11873 [00:16<00:17, 343.99it/s]\u001b[A\n"," 49% 5811/11873 [00:16<00:17, 355.89it/s]\u001b[A\n"," 49% 5848/11873 [00:16<00:16, 358.18it/s]\u001b[A\n"," 50% 5885/11873 [00:16<00:16, 359.17it/s]\u001b[A\n"," 50% 5921/11873 [00:16<00:16, 357.97it/s]\u001b[A\n"," 50% 5957/11873 [00:16<00:16, 356.58it/s]\u001b[A\n"," 50% 5993/11873 [00:17<00:16, 355.90it/s]\u001b[A\n"," 51% 6029/11873 [00:17<00:16, 356.48it/s]\u001b[A\n"," 51% 6065/11873 [00:17<00:16, 355.81it/s]\u001b[A\n"," 51% 6101/11873 [00:17<00:16, 353.47it/s]\u001b[A\n"," 52% 6137/11873 [00:17<00:16, 351.74it/s]\u001b[A\n"," 52% 6174/11873 [00:17<00:16, 354.50it/s]\u001b[A\n"," 52% 6210/11873 [00:17<00:15, 355.50it/s]\u001b[A\n"," 53% 6246/11873 [00:17<00:15, 355.55it/s]\u001b[A\n"," 53% 6283/11873 [00:17<00:15, 358.25it/s]\u001b[A\n"," 53% 6319/11873 [00:17<00:15, 352.44it/s]\u001b[A\n"," 54% 6355/11873 [00:18<00:15, 349.59it/s]\u001b[A\n"," 54% 6390/11873 [00:18<00:15, 349.03it/s]\u001b[A\n"," 54% 6425/11873 [00:18<00:15, 341.86it/s]\u001b[A\n"," 54% 6460/11873 [00:18<00:15, 342.41it/s]\u001b[A\n"," 55% 6495/11873 [00:18<00:15, 343.63it/s]\u001b[A\n"," 55% 6531/11873 [00:18<00:15, 347.09it/s]\u001b[A\n"," 55% 6566/11873 [00:18<00:15, 343.32it/s]\u001b[A\n"," 56% 6602/11873 [00:18<00:15, 346.45it/s]\u001b[A\n"," 56% 6637/11873 [00:18<00:15, 345.47it/s]\u001b[A\n"," 56% 6673/11873 [00:18<00:14, 349.17it/s]\u001b[A\n"," 56% 6708/11873 [00:19<00:15, 327.82it/s]\u001b[A\n"," 57% 6742/11873 [00:19<00:16, 318.95it/s]\u001b[A\n"," 57% 6775/11873 [00:19<00:15, 320.97it/s]\u001b[A\n"," 57% 6810/11873 [00:19<00:15, 326.87it/s]\u001b[A\n"," 58% 6846/11873 [00:19<00:14, 335.28it/s]\u001b[A\n"," 58% 6880/11873 [00:19<00:14, 334.18it/s]\u001b[A\n"," 58% 6915/11873 [00:19<00:14, 338.21it/s]\u001b[A\n"," 59% 6950/11873 [00:19<00:14, 339.47it/s]\u001b[A\n"," 59% 6986/11873 [00:19<00:14, 343.92it/s]\u001b[A\n"," 59% 7022/11873 [00:20<00:13, 347.29it/s]\u001b[A\n"," 59% 7059/11873 [00:20<00:13, 351.23it/s]\u001b[A\n"," 60% 7095/11873 [00:20<00:13, 351.79it/s]\u001b[A\n"," 60% 7131/11873 [00:20<00:13, 347.20it/s]\u001b[A\n"," 60% 7167/11873 [00:20<00:13, 349.89it/s]\u001b[A\n"," 61% 7203/11873 [00:20<00:13, 349.05it/s]\u001b[A\n"," 61% 7239/11873 [00:20<00:13, 350.21it/s]\u001b[A\n"," 61% 7275/11873 [00:20<00:13, 351.78it/s]\u001b[A\n"," 62% 7312/11873 [00:20<00:12, 354.29it/s]\u001b[A\n"," 62% 7348/11873 [00:20<00:12, 353.66it/s]\u001b[A\n"," 62% 7385/11873 [00:21<00:12, 357.54it/s]\u001b[A\n"," 63% 7421/11873 [00:21<00:13, 334.32it/s]\u001b[A\n"," 63% 7457/11873 [00:21<00:12, 339.79it/s]\u001b[A\n"," 63% 7493/11873 [00:21<00:12, 345.20it/s]\u001b[A\n"," 63% 7530/11873 [00:21<00:12, 350.70it/s]\u001b[A\n"," 64% 7568/11873 [00:21<00:12, 357.82it/s]\u001b[A\n"," 64% 7604/11873 [00:21<00:12, 354.34it/s]\u001b[A\n"," 64% 7640/11873 [00:21<00:12, 352.58it/s]\u001b[A\n"," 65% 7676/11873 [00:21<00:11, 353.98it/s]\u001b[A\n"," 65% 7712/11873 [00:22<00:12, 333.92it/s]\u001b[A\n"," 65% 7746/11873 [00:22<00:12, 334.44it/s]\u001b[A\n"," 66% 7782/11873 [00:22<00:11, 341.29it/s]\u001b[A\n"," 66% 7817/11873 [00:22<00:11, 342.63it/s]\u001b[A\n"," 66% 7852/11873 [00:22<00:11, 343.58it/s]\u001b[A\n"," 66% 7887/11873 [00:22<00:12, 319.60it/s]\u001b[A\n"," 67% 7924/11873 [00:22<00:11, 332.28it/s]\u001b[A\n"," 67% 7960/11873 [00:22<00:11, 337.64it/s]\u001b[A\n"," 67% 7995/11873 [00:22<00:11, 340.56it/s]\u001b[A\n"," 68% 8032/11873 [00:22<00:11, 347.46it/s]\u001b[A\n"," 68% 8068/11873 [00:23<00:10, 350.23it/s]\u001b[A\n"," 68% 8105/11873 [00:23<00:10, 354.05it/s]\u001b[A\n"," 69% 8141/11873 [00:23<00:10, 354.37it/s]\u001b[A\n"," 69% 8178/11873 [00:23<00:10, 356.13it/s]\u001b[A\n"," 69% 8214/11873 [00:23<00:10, 348.92it/s]\u001b[A\n"," 69% 8251/11873 [00:23<00:10, 353.83it/s]\u001b[A\n"," 70% 8287/11873 [00:23<00:10, 354.66it/s]\u001b[A\n"," 70% 8323/11873 [00:23<00:09, 356.14it/s]\u001b[A\n"," 70% 8359/11873 [00:23<00:09, 357.21it/s]\u001b[A\n"," 71% 8395/11873 [00:23<00:09, 356.89it/s]\u001b[A\n"," 71% 8431/11873 [00:24<00:09, 350.48it/s]\u001b[A\n"," 71% 8467/11873 [00:24<00:09, 345.17it/s]\u001b[A\n"," 72% 8502/11873 [00:24<00:09, 346.40it/s]\u001b[A\n"," 72% 8537/11873 [00:24<00:09, 344.52it/s]\u001b[A\n"," 72% 8572/11873 [00:24<00:09, 343.10it/s]\u001b[A\n"," 72% 8607/11873 [00:24<00:09, 344.85it/s]\u001b[A\n"," 73% 8642/11873 [00:24<00:09, 344.16it/s]\u001b[A\n"," 73% 8678/11873 [00:24<00:09, 347.55it/s]\u001b[A\n"," 73% 8713/11873 [00:24<00:09, 348.10it/s]\u001b[A\n"," 74% 8750/11873 [00:24<00:08, 352.31it/s]\u001b[A\n"," 74% 8786/11873 [00:25<00:08, 350.48it/s]\u001b[A\n"," 74% 8822/11873 [00:25<00:08, 350.25it/s]\u001b[A\n"," 75% 8858/11873 [00:25<00:08, 348.09it/s]\u001b[A\n"," 75% 8894/11873 [00:25<00:08, 351.14it/s]\u001b[A\n"," 75% 8930/11873 [00:25<00:08, 350.73it/s]\u001b[A\n"," 76% 8966/11873 [00:25<00:08, 349.66it/s]\u001b[A\n"," 76% 9004/11873 [00:25<00:08, 355.91it/s]\u001b[A\n"," 76% 9040/11873 [00:25<00:08, 351.97it/s]\u001b[A\n"," 76% 9077/11873 [00:25<00:07, 354.83it/s]\u001b[A\n"," 77% 9113/11873 [00:26<00:07, 354.19it/s]\u001b[A\n"," 77% 9149/11873 [00:26<00:07, 353.78it/s]\u001b[A\n"," 77% 9185/11873 [00:26<00:07, 355.03it/s]\u001b[A\n"," 78% 9221/11873 [00:26<00:07, 354.63it/s]\u001b[A\n"," 78% 9257/11873 [00:26<00:07, 350.50it/s]\u001b[A\n"," 78% 9293/11873 [00:26<00:07, 350.63it/s]\u001b[A\n"," 79% 9329/11873 [00:26<00:07, 350.92it/s]\u001b[A\n"," 79% 9365/11873 [00:26<00:07, 353.50it/s]\u001b[A\n"," 79% 9401/11873 [00:26<00:06, 353.84it/s]\u001b[A\n"," 79% 9437/11873 [00:26<00:06, 352.13it/s]\u001b[A\n"," 80% 9473/11873 [00:27<00:06, 350.87it/s]\u001b[A\n"," 80% 9509/11873 [00:27<00:06, 343.62it/s]\u001b[A\n"," 80% 9545/11873 [00:27<00:06, 346.82it/s]\u001b[A\n"," 81% 9581/11873 [00:27<00:06, 349.25it/s]\u001b[A\n"," 81% 9617/11873 [00:27<00:06, 349.64it/s]\u001b[A\n"," 81% 9652/11873 [00:27<00:06, 349.73it/s]\u001b[A\n"," 82% 9688/11873 [00:27<00:06, 350.64it/s]\u001b[A\n"," 82% 9724/11873 [00:27<00:06, 343.83it/s]\u001b[A\n"," 82% 9761/11873 [00:27<00:06, 351.08it/s]\u001b[A\n"," 83% 9797/11873 [00:27<00:05, 349.31it/s]\u001b[A\n"," 83% 9832/11873 [00:28<00:05, 348.86it/s]\u001b[A\n"," 83% 9868/11873 [00:28<00:05, 351.52it/s]\u001b[A\n"," 83% 9904/11873 [00:28<00:05, 350.38it/s]\u001b[A\n"," 84% 9940/11873 [00:28<00:05, 347.89it/s]\u001b[A\n"," 84% 9976/11873 [00:28<00:05, 349.44it/s]\u001b[A\n"," 84% 10011/11873 [00:28<00:05, 349.40it/s]\u001b[A\n"," 85% 10046/11873 [00:28<00:05, 348.80it/s]\u001b[A\n"," 85% 10083/11873 [00:28<00:05, 352.27it/s]\u001b[A\n"," 85% 10119/11873 [00:28<00:05, 348.31it/s]\u001b[A\n"," 86% 10154/11873 [00:29<00:04, 348.21it/s]\u001b[A\n"," 86% 10189/11873 [00:29<00:04, 345.81it/s]\u001b[A\n"," 86% 10224/11873 [00:29<00:04, 346.85it/s]\u001b[A\n"," 86% 10260/11873 [00:29<00:04, 348.54it/s]\u001b[A\n"," 87% 10295/11873 [00:29<00:04, 344.92it/s]\u001b[A\n"," 87% 10330/11873 [00:29<00:04, 345.29it/s]\u001b[A\n"," 87% 10366/11873 [00:29<00:04, 348.35it/s]\u001b[A\n"," 88% 10402/11873 [00:29<00:04, 349.31it/s]\u001b[A\n"," 88% 10437/11873 [00:29<00:04, 327.40it/s]\u001b[A\n"," 88% 10474/11873 [00:29<00:04, 338.69it/s]\u001b[A\n"," 89% 10509/11873 [00:30<00:04, 332.13it/s]\u001b[A\n"," 89% 10545/11873 [00:30<00:03, 338.96it/s]\u001b[A\n"," 89% 10580/11873 [00:30<00:03, 338.49it/s]\u001b[A\n"," 89% 10615/11873 [00:30<00:03, 340.26it/s]\u001b[A\n"," 90% 10650/11873 [00:30<00:03, 337.62it/s]\u001b[A\n"," 90% 10686/11873 [00:30<00:03, 341.99it/s]\u001b[A\n"," 90% 10721/11873 [00:30<00:03, 342.81it/s]\u001b[A\n"," 91% 10757/11873 [00:30<00:03, 346.60it/s]\u001b[A\n"," 91% 10793/11873 [00:30<00:03, 349.02it/s]\u001b[A\n"," 91% 10828/11873 [00:30<00:03, 325.98it/s]\u001b[A\n"," 91% 10863/11873 [00:31<00:03, 332.46it/s]\u001b[A\n"," 92% 10897/11873 [00:31<00:02, 330.99it/s]\u001b[A\n"," 92% 10931/11873 [00:31<00:02, 327.60it/s]\u001b[A\n"," 92% 10966/11873 [00:31<00:02, 332.58it/s]\u001b[A\n"," 93% 11002/11873 [00:31<00:02, 337.88it/s]\u001b[A\n"," 93% 11038/11873 [00:31<00:02, 341.88it/s]\u001b[A\n"," 93% 11075/11873 [00:31<00:02, 348.22it/s]\u001b[A\n"," 94% 11110/11873 [00:31<00:02, 348.24it/s]\u001b[A\n"," 94% 11147/11873 [00:31<00:02, 352.07it/s]\u001b[A\n"," 94% 11183/11873 [00:32<00:01, 346.93it/s]\u001b[A\n"," 95% 11220/11873 [00:32<00:01, 350.86it/s]\u001b[A\n"," 95% 11256/11873 [00:32<00:01, 349.83it/s]\u001b[A\n"," 95% 11292/11873 [00:32<00:01, 346.32it/s]\u001b[A\n"," 95% 11330/11873 [00:32<00:01, 355.85it/s]\u001b[A\n"," 96% 11366/11873 [00:32<00:01, 349.22it/s]\u001b[A\n"," 96% 11403/11873 [00:32<00:01, 353.60it/s]\u001b[A\n"," 96% 11439/11873 [00:32<00:01, 353.20it/s]\u001b[A\n"," 97% 11475/11873 [00:32<00:01, 353.20it/s]\u001b[A\n"," 97% 11511/11873 [00:32<00:01, 353.37it/s]\u001b[A\n"," 97% 11547/11873 [00:33<00:00, 352.97it/s]\u001b[A\n"," 98% 11583/11873 [00:33<00:00, 354.31it/s]\u001b[A\n"," 98% 11619/11873 [00:33<00:00, 349.18it/s]\u001b[A\n"," 98% 11655/11873 [00:33<00:00, 350.38it/s]\u001b[A\n"," 98% 11691/11873 [00:33<00:00, 339.90it/s]\u001b[A\n"," 99% 11727/11873 [00:33<00:00, 344.97it/s]\u001b[A\n"," 99% 11763/11873 [00:33<00:00, 346.89it/s]\u001b[A\n"," 99% 11798/11873 [00:33<00:00, 342.33it/s]\u001b[A\n","100% 11834/11873 [00:33<00:00, 347.34it/s]\u001b[A\n","100% 11873/11873 [00:33<00:00, 349.29it/s]\n","04/07/2022 05:40:34 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/eval_predictions.json.\n","04/07/2022 05:40:35 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/eval_nbest_predictions.json.\n","04/07/2022 05:40:37 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/roberta-base/context-aug/eval_null_odds.json.\n","04/07/2022 05:40:40 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1521/1521 [02:25<00:00, 10.44it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 77.9521\n","  eval_HasAns_f1         = 84.0016\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 54.2304\n","  eval_NoAns_f1          = 54.2304\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 66.0827\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 69.1031\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 66.0743\n","  eval_f1                = 69.0947\n","  eval_samples           =   12165\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-07 05:40:41,195 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_context_aug', 'type': 'sichenzhong/squad_v2_context_aug', 'args': 'squad_v2'}}\n"]}],"source":["!python run_qa.py \\\n","  --model_name_or_path roberta-base \\\n","  --dataset_name sichenzhong/squad_v2_context_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/roberta-base/context-aug"]},{"cell_type":"code","source":[""],"metadata":{"id":"DijcvEe1eEXz"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"Context word embedding augmented.ipynb","provenance":[],"mount_file_id":"1WXPdKYSM6ohU4E5efGa53jcfERBqbnor","authorship_tag":"ABX9TyMdzhVl1mbiwFGOgy5ojwao"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0280aac295f1493593239fb2b074bcb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02a0411d1b174005a18091234a9d67a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21996906329446538b50c22a21865e7d","IPY_MODEL_7472b853cad348499798a924a1b2ff23","IPY_MODEL_05a543f86be74ce1839262a94c987966"],"layout":"IPY_MODEL_a913988c53d949579e4a7482d540546e"}},"059a3821d64c4b5394b5ec0617952b42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_665143d116fe47bd9a4367b27f539a27","placeholder":"​","style":"IPY_MODEL_821ccca36cd549fb8d0afae9591b0331","value":"Downloading data files: 100%"}},"05a543f86be74ce1839262a94c987966":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bb38c18b23b4043a34e12ca27942428","placeholder":"​","style":"IPY_MODEL_981a88d996d942baabfde1d479e90acf","value":" 2/2 [00:00&lt;00:00, 47.86it/s]"}},"068b167d0b9846e2917fc4995e83382e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fbd7a369a1c4918991fbf09d510d9e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15496ce6e05c492f8f5f236b754bed74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1559ab30c6684f47a4b7ed5ce99bb6f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15fbb61b20254d838b69e08cf3f8e3db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1988accb45e546f7ad9bea91e98d4c62":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a1643c836694a52a45c89a8ac27bf7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a47b278ee534bb9a558fd9ca48759cb","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15496ce6e05c492f8f5f236b754bed74","value":2}},"1c5b969516984f559e98ae0ad799018a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e57b68b1a2e4826ae8d9400c22babee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"21996906329446538b50c22a21865e7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_929b29353ef94a52b784e4ed43a740e1","placeholder":"​","style":"IPY_MODEL_54e2bc21dbea48f3944946ae89d0cb6e","value":"Extracting data files: 100%"}},"219f8f658bfc4602acdeaaa76f6c8d10":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"276254bd90314a84ae10cf25c87e4b85":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29b918ac261f41d7949235da9206bea9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38201d944dfa4922a7aafecab9b44ce7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38d85533215b4e54b7602ee3568752f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a737310a7894ca99899aca198ca842e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98864334e94544e7802edc13b347de6a","placeholder":"​","style":"IPY_MODEL_a0970da474cc4134916bf1626887fe4e","value":" 11613/0 [00:00&lt;00:00, 14001.58 examples/s]"}},"3e805b74f9ca428eb31596a4c9857ed4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f67171dfa55404a9aee758fa21f8b32":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41edc0d9698c4664904f1a515d5cf9df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e805b74f9ca428eb31596a4c9857ed4","placeholder":"​","style":"IPY_MODEL_c6a00020495b4c009ae2b83f48235dda","value":" 1/1 [00:05&lt;00:00,  5.50s/it]"}},"44e798e35c764164a879c8011a19bf71":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0fbead99d4e4a40a710c87edd4c6cc6","IPY_MODEL_7a5681bd5ca249bc9bd51448207594b2","IPY_MODEL_9059187f87284e278e97e36bcf1e62a6"],"layout":"IPY_MODEL_9be731c3e52545d09b5c9a5dcd4fdbf5"}},"49061f8bf2554f228aae6a54db470e98":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_29b918ac261f41d7949235da9206bea9","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_819a787be3cc49d2b09addccf9175571","value":1}},"4ade28ba713f459d8d17555cb28707ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54dffadc44f74175b7c2330860c1e400":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54e2bc21dbea48f3944946ae89d0cb6e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"563e525dac334b87bf92ee42fa0e84dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_593561532e43409c9d7c01d02c625d3d","IPY_MODEL_db1f92a2e01e4e23ac077fe2fedf6359","IPY_MODEL_e1ef8012f2e24c7d81fe1dcc5ab6f54e"],"layout":"IPY_MODEL_9fc753e451314823b55fd33a6831c08d"}},"593561532e43409c9d7c01d02c625d3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db044458deaa40a9a3a9bb48b67784e5","placeholder":"​","style":"IPY_MODEL_6d7cf18112d848839291891beeb4f287","value":"Pushing dataset shards to the dataset hub: 100%"}},"59706f5001974dd3ad4c34144b800fc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b99c1a4265d42ad9c3031643364b50c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"6583788bb3fd4c3e92bc8ff3741eccdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"665143d116fe47bd9a4367b27f539a27":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a47b278ee534bb9a558fd9ca48759cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b4b66b2dc384f209dbb18fa1574dd5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c98e2cfe98614c2d88a4099167de87d3","IPY_MODEL_74544654e50a47de9e01353a9c7e28b3","IPY_MODEL_3a737310a7894ca99899aca198ca842e"],"layout":"IPY_MODEL_276254bd90314a84ae10cf25c87e4b85"}},"6d7cf18112d848839291891beeb4f287":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72216258ffa64482b8ee5269ae65f4da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74544654e50a47de9e01353a9c7e28b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b99c1a4265d42ad9c3031643364b50c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6583788bb3fd4c3e92bc8ff3741eccdd","value":1}},"7472b853cad348499798a924a1b2ff23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9670c85997b844908eda42f68729e4e4","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e429eacc0446467c8ba4aae08202c65d","value":2}},"7a5681bd5ca249bc9bd51448207594b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e57b68b1a2e4826ae8d9400c22babee","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c02c35d2883a45498d31b02e259a5281","value":1}},"819a787be3cc49d2b09addccf9175571":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"821ccca36cd549fb8d0afae9591b0331":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8bb38c18b23b4043a34e12ca27942428":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e797935275b4334a7d46a07498cff69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9059187f87284e278e97e36bcf1e62a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c83b6e30652343e18d34a8889ced5f7e","placeholder":"​","style":"IPY_MODEL_72216258ffa64482b8ee5269ae65f4da","value":" 128998/0 [00:10&lt;00:00, 14363.41 examples/s]"}},"929b29353ef94a52b784e4ed43a740e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94e690859db3472eac5dd0bf85693759":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96562ad8892c49928e5b58f3a0a190ac","IPY_MODEL_49061f8bf2554f228aae6a54db470e98","IPY_MODEL_41edc0d9698c4664904f1a515d5cf9df"],"layout":"IPY_MODEL_bb4fffff64274e948f138352508dac77"}},"9638c3f4d8424097a376ca01a61cf9a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96562ad8892c49928e5b58f3a0a190ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c5b969516984f559e98ae0ad799018a","placeholder":"​","style":"IPY_MODEL_0280aac295f1493593239fb2b074bcb2","value":"Pushing dataset shards to the dataset hub: 100%"}},"9670c85997b844908eda42f68729e4e4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"981a88d996d942baabfde1d479e90acf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98864334e94544e7802edc13b347de6a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9be731c3e52545d09b5c9a5dcd4fdbf5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f7a7915d2814280b0b2c839cbc2c1ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_acb6c40790d74aa792049af0215bc8f0","IPY_MODEL_fa0b91dfcf1748e090483d2f1f3c890d","IPY_MODEL_ae439403d6a44f56ad446fb5eeec9bcb"],"layout":"IPY_MODEL_d34c43cd27fc466082103a4fd5731d8e"}},"9fc753e451314823b55fd33a6831c08d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0970da474cc4134916bf1626887fe4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2ee9ff13e40458b88034611cd754fd8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a913988c53d949579e4a7482d540546e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abed2bd4e6704091a1abc092c83e4f27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1559ab30c6684f47a4b7ed5ce99bb6f8","placeholder":"​","style":"IPY_MODEL_59706f5001974dd3ad4c34144b800fc4","value":" 2/2 [00:00&lt;00:00, 81.31it/s]"}},"acb6c40790d74aa792049af0215bc8f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1988accb45e546f7ad9bea91e98d4c62","placeholder":"​","style":"IPY_MODEL_219f8f658bfc4602acdeaaa76f6c8d10","value":"100%"}},"ae439403d6a44f56ad446fb5eeec9bcb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38d85533215b4e54b7602ee3568752f3","placeholder":"​","style":"IPY_MODEL_54dffadc44f74175b7c2330860c1e400","value":" 2/2 [00:00&lt;00:00, 64.94it/s]"}},"b3b5ec66d7384c2e8b462fd5b1d78d1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb4fffff64274e948f138352508dac77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c02c35d2883a45498d31b02e259a5281":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c53b552b3046422cbbc1a9314010a9ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_059a3821d64c4b5394b5ec0617952b42","IPY_MODEL_1a1643c836694a52a45c89a8ac27bf7e","IPY_MODEL_abed2bd4e6704091a1abc092c83e4f27"],"layout":"IPY_MODEL_068b167d0b9846e2917fc4995e83382e"}},"c6a00020495b4c009ae2b83f48235dda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c83b6e30652343e18d34a8889ced5f7e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c98e2cfe98614c2d88a4099167de87d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2ee9ff13e40458b88034611cd754fd8","placeholder":"​","style":"IPY_MODEL_4ade28ba713f459d8d17555cb28707ca","value":"Generating validation split: "}},"d34c43cd27fc466082103a4fd5731d8e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9a33d89db894e799292ee7d2aa9b818":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db044458deaa40a9a3a9bb48b67784e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db1f92a2e01e4e23ac077fe2fedf6359":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9638c3f4d8424097a376ca01a61cf9a8","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15fbb61b20254d838b69e08cf3f8e3db","value":1}},"e0fbead99d4e4a40a710c87edd4c6cc6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9a33d89db894e799292ee7d2aa9b818","placeholder":"​","style":"IPY_MODEL_8e797935275b4334a7d46a07498cff69","value":"Generating train split: "}},"e1ef8012f2e24c7d81fe1dcc5ab6f54e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f67171dfa55404a9aee758fa21f8b32","placeholder":"​","style":"IPY_MODEL_38201d944dfa4922a7aafecab9b44ce7","value":" 1/1 [00:01&lt;00:00,  1.84s/it]"}},"e429eacc0446467c8ba4aae08202c65d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa0b91dfcf1748e090483d2f1f3c890d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fbd7a369a1c4918991fbf09d510d9e2","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b3b5ec66d7384c2e8b462fd5b1d78d1c","value":2}}}}},"nbformat":4,"nbformat_minor":0}