{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Synonym augmented.ipynb","provenance":[],"machine_shape":"hm","collapsed_sections":[],"mount_file_id":"1Cc_Ty8oAxMPULDvpaH_mI5Fm0ElAmH48","authorship_tag":"ABX9TyPpa6OPwCExSOEHHGvYeH6c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"932b453a4230490da79cc3d11fa68f5c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_361c4cfe8ea140e4a5a4688562fbe848","IPY_MODEL_8f4036ef1ce74bd48eb7981a15af8d2f","IPY_MODEL_74d924331c03472893667330feed4b5f"],"layout":"IPY_MODEL_c91667912c62464f8b4fbb56f3fc50d5"}},"361c4cfe8ea140e4a5a4688562fbe848":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da70aacc95764f45b57d97232ffd79cd","placeholder":"​","style":"IPY_MODEL_731b5c5dc7d24cb28d38e5b24e0f9932","value":"Downloading data files: 100%"}},"8f4036ef1ce74bd48eb7981a15af8d2f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db5c36634cab40fc9f8e56f11ab1ff6f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a639403488747bf9b96aa4a2c26d49a","value":2}},"74d924331c03472893667330feed4b5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6a0201e0df14f08aba95b9ce871e8d0","placeholder":"​","style":"IPY_MODEL_0de01f4653424caa815adcca03d022d2","value":" 2/2 [00:00&lt;00:00, 61.25it/s]"}},"c91667912c62464f8b4fbb56f3fc50d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da70aacc95764f45b57d97232ffd79cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"731b5c5dc7d24cb28d38e5b24e0f9932":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db5c36634cab40fc9f8e56f11ab1ff6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a639403488747bf9b96aa4a2c26d49a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6a0201e0df14f08aba95b9ce871e8d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0de01f4653424caa815adcca03d022d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5015d3f759c44e7abd5cbc53ebbb79f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_76de16cfc09d4fe393efa751f94468c6","IPY_MODEL_c273e20896ca439186d4786dc1972a4b","IPY_MODEL_0d9d85036f0145239c7b79b83fdf8532"],"layout":"IPY_MODEL_ccffd93fbd1b453c8d15ef28aadd3b94"}},"76de16cfc09d4fe393efa751f94468c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53c1fea3236b409e9734d1df93d71136","placeholder":"​","style":"IPY_MODEL_505c9e0bacad4c6e985637302ba548ea","value":"Extracting data files: 100%"}},"c273e20896ca439186d4786dc1972a4b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14310bf25b234cd9a992d6ae6501945e","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_309fe22cdcc142a89f9a56b82999b040","value":2}},"0d9d85036f0145239c7b79b83fdf8532":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3422b55078f84a6ba26f84f599bea2d7","placeholder":"​","style":"IPY_MODEL_c1a4da54c5ed4b1f9eb3d73c630f6dc0","value":" 2/2 [00:00&lt;00:00, 36.27it/s]"}},"ccffd93fbd1b453c8d15ef28aadd3b94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53c1fea3236b409e9734d1df93d71136":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"505c9e0bacad4c6e985637302ba548ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14310bf25b234cd9a992d6ae6501945e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"309fe22cdcc142a89f9a56b82999b040":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3422b55078f84a6ba26f84f599bea2d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1a4da54c5ed4b1f9eb3d73c630f6dc0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1659c2211694146bab56798a9ecb1b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78bdc7006ec9408299c8f866931e0524","IPY_MODEL_5a90992073c54cb999d53a1c15e1727c","IPY_MODEL_19697754d7db4a5faab02fe9646d30f5"],"layout":"IPY_MODEL_a48483e2ef03444a8bb6d02459614838"}},"78bdc7006ec9408299c8f866931e0524":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c72f1d423b234cc4b6223f39c096adb1","placeholder":"​","style":"IPY_MODEL_9226950341ca4574ba609fa34bce3acc","value":"Generating train split: "}},"5a90992073c54cb999d53a1c15e1727c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed5243934bde4acd9402e9d5baff1e4f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7859f927016349b68c192119a5b1d409","value":1}},"19697754d7db4a5faab02fe9646d30f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37e8750612474eb2bc27304b06eeb863","placeholder":"​","style":"IPY_MODEL_d4160a3624404f2abc204cc4a493b44b","value":" 129466/0 [00:12&lt;00:00, 12535.02 examples/s]"}},"a48483e2ef03444a8bb6d02459614838":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c72f1d423b234cc4b6223f39c096adb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9226950341ca4574ba609fa34bce3acc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed5243934bde4acd9402e9d5baff1e4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7859f927016349b68c192119a5b1d409":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37e8750612474eb2bc27304b06eeb863":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4160a3624404f2abc204cc4a493b44b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfa19d68077044f9b3d7757f05d7a57d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0131d88db719473eaeccf3c1e40d4743","IPY_MODEL_9781979d96d44e04b92c6c4a2dcc13b1","IPY_MODEL_127a3edf451c4ae7b35997ff540f9f1d"],"layout":"IPY_MODEL_5b3eba29d2084ae1857cfa3203311a4d"}},"0131d88db719473eaeccf3c1e40d4743":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3653b735566454eb061475b8acff2dd","placeholder":"​","style":"IPY_MODEL_36b91b43b2a643a2b978c491f06bcbb1","value":"Generating validation split: "}},"9781979d96d44e04b92c6c4a2dcc13b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee2d0bd378de4b1a99173474dd958b02","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a139f32b5dde41d1a7982dff8f40a2b5","value":1}},"127a3edf451c4ae7b35997ff540f9f1d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_574b04f8cb744da5b406e80b3ea9c0dc","placeholder":"​","style":"IPY_MODEL_28a3b7bf74c04f5585dc95dafaf426ae","value":" 11483/0 [00:01&lt;00:00, 11201.86 examples/s]"}},"5b3eba29d2084ae1857cfa3203311a4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3653b735566454eb061475b8acff2dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36b91b43b2a643a2b978c491f06bcbb1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee2d0bd378de4b1a99173474dd958b02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a139f32b5dde41d1a7982dff8f40a2b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"574b04f8cb744da5b406e80b3ea9c0dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28a3b7bf74c04f5585dc95dafaf426ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a06c9db67a8c41569ce2c7ef3a2ab2ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2daf74d1078c427f96239dfff35b4d07","IPY_MODEL_7f3fb8c2fc654968be8a91a85abb6729","IPY_MODEL_30b0d5c76e434541804f61dd17491741"],"layout":"IPY_MODEL_db93032314be4dc5b23c3cba85145cca"}},"2daf74d1078c427f96239dfff35b4d07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3be3891fe16b49b9a116fa384dc99b47","placeholder":"​","style":"IPY_MODEL_7bbeb669e616444ea2823ccc713cbdf5","value":"100%"}},"7f3fb8c2fc654968be8a91a85abb6729":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_238bf65ae6334419b1b7f6736c0d9ab0","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_820c6c55ebf34a5aaa19991e0e33b534","value":2}},"30b0d5c76e434541804f61dd17491741":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f71209bdbf34f84a089e8621c0fa3a8","placeholder":"​","style":"IPY_MODEL_49c13cf231824359b358d20819c75317","value":" 2/2 [00:00&lt;00:00, 48.45it/s]"}},"db93032314be4dc5b23c3cba85145cca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3be3891fe16b49b9a116fa384dc99b47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bbeb669e616444ea2823ccc713cbdf5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"238bf65ae6334419b1b7f6736c0d9ab0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"820c6c55ebf34a5aaa19991e0e33b534":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f71209bdbf34f84a089e8621c0fa3a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49c13cf231824359b358d20819c75317":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cd6a65946014f02a124d1df07ddccdb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eccf52b87faf45988b3bc8fc0b4715cc","IPY_MODEL_462e25c2b6984fe2bb62c298f5a04477","IPY_MODEL_85f0d896dfb04230ad2ea17fb4a9404d"],"layout":"IPY_MODEL_1ded75089f13451e8a89b4385b9d870b"}},"eccf52b87faf45988b3bc8fc0b4715cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc82964fc015416e95a443e4e5e2226c","placeholder":"​","style":"IPY_MODEL_854583d9d4e94a7ea8574eb2bdd32ee0","value":"Pushing dataset shards to the dataset hub: 100%"}},"462e25c2b6984fe2bb62c298f5a04477":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0133dbbc53b445ca524b768c9a0bcfd","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0fd2ec12a054410f9f12c3c19c7d24e6","value":1}},"85f0d896dfb04230ad2ea17fb4a9404d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29ff7bed8b2d4b1cb1686b54b90536e7","placeholder":"​","style":"IPY_MODEL_93137ab8ce5142e1af5b7d49d1be4bcd","value":" 1/1 [00:05&lt;00:00,  5.87s/it]"}},"1ded75089f13451e8a89b4385b9d870b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc82964fc015416e95a443e4e5e2226c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"854583d9d4e94a7ea8574eb2bdd32ee0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0133dbbc53b445ca524b768c9a0bcfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fd2ec12a054410f9f12c3c19c7d24e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"29ff7bed8b2d4b1cb1686b54b90536e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93137ab8ce5142e1af5b7d49d1be4bcd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36b2b7dd801648af8ccbcbbde0120faf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0055a1ea4a8b45eb8ad20f6415a1f2b9","IPY_MODEL_8628818abf2c44db9d5eb3cafc0bde18","IPY_MODEL_335ad6ae970d47b3a1c279c12dca6257"],"layout":"IPY_MODEL_cc56722253af49f39864d9e53e75fbee"}},"0055a1ea4a8b45eb8ad20f6415a1f2b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c65e89e2368408e9c30818b777ae931","placeholder":"​","style":"IPY_MODEL_02307c64599e4ce88a9f649ef6a5759d","value":"Pushing dataset shards to the dataset hub: 100%"}},"8628818abf2c44db9d5eb3cafc0bde18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_af40f7914bcc45658e5a80037d3517bb","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f6afb726f0d941dcb22947c484bb16c2","value":1}},"335ad6ae970d47b3a1c279c12dca6257":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80b9d60c1ad448e89e3b9948ed93eaf2","placeholder":"​","style":"IPY_MODEL_ccf5708bf7f2418abc164a0e4fc535dc","value":" 1/1 [00:01&lt;00:00,  1.83s/it]"}},"cc56722253af49f39864d9e53e75fbee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c65e89e2368408e9c30818b777ae931":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02307c64599e4ce88a9f649ef6a5759d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af40f7914bcc45658e5a80037d3517bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6afb726f0d941dcb22947c484bb16c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"80b9d60c1ad448e89e3b9948ed93eaf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccf5708bf7f2418abc164a0e4fc535dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70oLi9mZP6oK","executionInfo":{"status":"ok","timestamp":1649225278376,"user_tz":240,"elapsed":202,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"8f08da77-0102-4b9a-d6e1-ce428be59ac6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Apr  6 06:07:56 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGqcN-zXTvvo","executionInfo":{"status":"ok","timestamp":1649225286485,"user_tz":240,"elapsed":8112,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"1f03d9b7-15f8-4c52-85ac-b08e313ff637"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n","\u001b[K     |████████████████████████████████| 325 kB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 48.8 MB/s \n","\u001b[?25hCollecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 59.8 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Collecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.0-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 7.2 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 54.5 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 93.2 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 86.2 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 67.2 MB/s \n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tI_RBT1FSotu","outputId":"c482f72b-0bc2-4d23-d2f0-d45f08088cf6","executionInfo":{"status":"ok","timestamp":1649225308311,"user_tz":240,"elapsed":21833,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-sh53vper\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-sh53vper\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 2.9 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 75.0 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (3.6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (0.5.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (1.21.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.11.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 69.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.63.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0.dev0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0.dev0) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0.dev0) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.15.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.18.0.dev0-py3-none-any.whl size=3958915 sha256=6422d7688827003f552f64a3fdf853d32e644862364f4670ccc42e29ea295de1\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-a8yvzxej/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0.dev0\n"]}]},{"cell_type":"code","source":["import torch\n","import datetime\n","import json\n","import os\n","import time\n","import datasets\n","import pprint\n","import random\n","import string\n","import sys\n","import transformers\n","from datasets import load_dataset\n","from datasets.tasks import QuestionAnsweringExtractive"],"metadata":{"id":"DZ3Ma-pCRJDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available()\n","                      else 'cpu')"],"metadata":{"id":"HNMUVyBpRGw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/huggingface/transformers.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUVkgX-IQIiR","executionInfo":{"status":"ok","timestamp":1649225322718,"user_tz":240,"elapsed":6007,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"5d46e73e-6524-4fc7-9abc-b4a7d2d2e952"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 108477, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 108477 (delta 0), reused 2 (delta 0), pack-reused 108474\u001b[K\n","Receiving objects: 100% (108477/108477), 95.20 MiB | 38.13 MiB/s, done.\n","Resolving deltas: 100% (79042/79042), done.\n"]}]},{"cell_type":"code","source":["dataset = load_dataset('/content/drive/MyDrive/QA/squad_v2_synonym_aug.py')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":148,"referenced_widgets":["932b453a4230490da79cc3d11fa68f5c","361c4cfe8ea140e4a5a4688562fbe848","8f4036ef1ce74bd48eb7981a15af8d2f","74d924331c03472893667330feed4b5f","c91667912c62464f8b4fbb56f3fc50d5","da70aacc95764f45b57d97232ffd79cd","731b5c5dc7d24cb28d38e5b24e0f9932","db5c36634cab40fc9f8e56f11ab1ff6f","9a639403488747bf9b96aa4a2c26d49a","e6a0201e0df14f08aba95b9ce871e8d0","0de01f4653424caa815adcca03d022d2","d5015d3f759c44e7abd5cbc53ebbb79f","76de16cfc09d4fe393efa751f94468c6","c273e20896ca439186d4786dc1972a4b","0d9d85036f0145239c7b79b83fdf8532","ccffd93fbd1b453c8d15ef28aadd3b94","53c1fea3236b409e9734d1df93d71136","505c9e0bacad4c6e985637302ba548ea","14310bf25b234cd9a992d6ae6501945e","309fe22cdcc142a89f9a56b82999b040","3422b55078f84a6ba26f84f599bea2d7","c1a4da54c5ed4b1f9eb3d73c630f6dc0","e1659c2211694146bab56798a9ecb1b0","78bdc7006ec9408299c8f866931e0524","5a90992073c54cb999d53a1c15e1727c","19697754d7db4a5faab02fe9646d30f5","a48483e2ef03444a8bb6d02459614838","c72f1d423b234cc4b6223f39c096adb1","9226950341ca4574ba609fa34bce3acc","ed5243934bde4acd9402e9d5baff1e4f","7859f927016349b68c192119a5b1d409","37e8750612474eb2bc27304b06eeb863","d4160a3624404f2abc204cc4a493b44b","dfa19d68077044f9b3d7757f05d7a57d","0131d88db719473eaeccf3c1e40d4743","9781979d96d44e04b92c6c4a2dcc13b1","127a3edf451c4ae7b35997ff540f9f1d","5b3eba29d2084ae1857cfa3203311a4d","c3653b735566454eb061475b8acff2dd","36b91b43b2a643a2b978c491f06bcbb1","ee2d0bd378de4b1a99173474dd958b02","a139f32b5dde41d1a7982dff8f40a2b5","574b04f8cb744da5b406e80b3ea9c0dc","28a3b7bf74c04f5585dc95dafaf426ae","a06c9db67a8c41569ce2c7ef3a2ab2ca","2daf74d1078c427f96239dfff35b4d07","7f3fb8c2fc654968be8a91a85abb6729","30b0d5c76e434541804f61dd17491741","db93032314be4dc5b23c3cba85145cca","3be3891fe16b49b9a116fa384dc99b47","7bbeb669e616444ea2823ccc713cbdf5","238bf65ae6334419b1b7f6736c0d9ab0","820c6c55ebf34a5aaa19991e0e33b534","6f71209bdbf34f84a089e8621c0fa3a8","49c13cf231824359b358d20819c75317"]},"id":"A2gdlPTvcmfL","executionInfo":{"status":"ok","timestamp":1648601427003,"user_tz":240,"elapsed":15298,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"43e93b78-cf14-4558-832b-366162549e6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset squad_v2_aug/squad_v2 to /root/.cache/huggingface/datasets/squad_v2_aug/squad_v2/2.0.0/5d74a6f4711d259e4a39604a1d557508317903610cdb09ee7321111a1f2105f1...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"932b453a4230490da79cc3d11fa68f5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5015d3f759c44e7abd5cbc53ebbb79f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1659c2211694146bab56798a9ecb1b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfa19d68077044f9b3d7757f05d7a57d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset squad_v2_aug downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2_aug/squad_v2/2.0.0/5d74a6f4711d259e4a39604a1d557508317903610cdb09ee7321111a1f2105f1. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a06c9db67a8c41569ce2c7ef3a2ab2ca"}},"metadata":{}}]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttpAfwxBWdR0","executionInfo":{"status":"ok","timestamp":1648601504996,"user_tz":240,"elapsed":77789,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"fadb49eb-38cc-4b6f-f330-be051029d351"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/token.\n","        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n","        \n","Token: \n","Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}]},{"cell_type":"code","source":["dataset.push_to_hub(\"sichenzhong/squad_v2_synonym_aug\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":150,"referenced_widgets":["4cd6a65946014f02a124d1df07ddccdb","eccf52b87faf45988b3bc8fc0b4715cc","462e25c2b6984fe2bb62c298f5a04477","85f0d896dfb04230ad2ea17fb4a9404d","1ded75089f13451e8a89b4385b9d870b","bc82964fc015416e95a443e4e5e2226c","854583d9d4e94a7ea8574eb2bdd32ee0","b0133dbbc53b445ca524b768c9a0bcfd","0fd2ec12a054410f9f12c3c19c7d24e6","29ff7bed8b2d4b1cb1686b54b90536e7","93137ab8ce5142e1af5b7d49d1be4bcd","36b2b7dd801648af8ccbcbbde0120faf","0055a1ea4a8b45eb8ad20f6415a1f2b9","8628818abf2c44db9d5eb3cafc0bde18","335ad6ae970d47b3a1c279c12dca6257","cc56722253af49f39864d9e53e75fbee","4c65e89e2368408e9c30818b777ae931","02307c64599e4ce88a9f649ef6a5759d","af40f7914bcc45658e5a80037d3517bb","f6afb726f0d941dcb22947c484bb16c2","80b9d60c1ad448e89e3b9948ed93eaf2","ccf5708bf7f2418abc164a0e4fc535dc"]},"id":"WGAeWENJWR_F","executionInfo":{"status":"ok","timestamp":1648601515371,"user_tz":240,"elapsed":10378,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"a5584cb5-8224-456c-a591-f580ddd6a0e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Pushing split train to the Hub.\n","The repository already exists: the `private` keyword argument will be ignored.\n"]},{"output_type":"display_data","data":{"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cd6a65946014f02a124d1df07ddccdb"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Pushing split validation to the Hub.\n","The repository already exists: the `private` keyword argument will be ignored.\n"]},{"output_type":"display_data","data":{"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36b2b7dd801648af8ccbcbbde0120faf"}},"metadata":{}}]},{"cell_type":"code","source":["%cd /content/transformers/examples/pytorch/question-answering/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uLaizsXQrk9","executionInfo":{"status":"ok","timestamp":1649225322719,"user_tz":240,"elapsed":6,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"a8d43cc1-446d-47a4-f3bd-3237e7ca43ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/transformers/examples/pytorch/question-answering\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path bert-base-cased \\\n","  --dataset_name sichenzhong/squad_v2_synonym_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 3 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aI-ipSbaHqF6","executionInfo":{"status":"ok","timestamp":1649241606879,"user_tz":240,"elapsed":16284165,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"d9dec658-82b7-4447-94ee-cd6b1262096d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["04/06/2022 06:08:45 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 06:08:45 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/runs/Apr06_06-08-45_2baca14bc166,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 06:08:46 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/sichenzhong/squad_v2_synonym_aug/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpv5ctonli\n","Downloading: 100% 2.13k/2.13k [00:00<00:00, 1.95MB/s]\n","04/06/2022 06:08:46 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/sichenzhong/squad_v2_synonym_aug/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/5d7bdb75ee9bb22b5c63b5d66e0f73dc9cdbcddc491a721e2b7c0a24be9c8d4f.697037dca9a88606d3b69b8eb3ae8a23550bb5f802c56c606d6d1a4a12d322d8\n","04/06/2022 06:08:46 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5d7bdb75ee9bb22b5c63b5d66e0f73dc9cdbcddc491a721e2b7c0a24be9c8d4f.697037dca9a88606d3b69b8eb3ae8a23550bb5f802c56c606d6d1a4a12d322d8\n","04/06/2022 06:08:46 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b\n","04/06/2022 06:08:46 - INFO - datasets.builder - Generating dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","Downloading and preparing dataset squad_v2_aug/squad_v2 (download: 17.06 MiB, generated: 123.25 MiB, post-processed: Unknown size, total: 140.31 MiB) to /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n","04/06/2022 06:08:46 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]04/06/2022 06:08:46 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/sichenzhong/squad_v2_synonym_aug/resolve/5cbbb5a5f5fcdf9ab91b0ebbbb35cb25cb90b985/data/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpl_tqnfwv\n","\n","Downloading data:   0% 0.00/1.26M [00:00<?, ?B/s]\u001b[A\n","Downloading data: 100% 1.26M/1.26M [00:00<00:00, 8.27MB/s]\n","04/06/2022 06:08:47 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/sichenzhong/squad_v2_synonym_aug/resolve/5cbbb5a5f5fcdf9ab91b0ebbbb35cb25cb90b985/data/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/6b76b652d8334cc1db78ce55cc20fc0e5932b009b6e4c331edbf5d36fd149d57\n","04/06/2022 06:08:47 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6b76b652d8334cc1db78ce55cc20fc0e5932b009b6e4c331edbf5d36fd149d57\n","Downloading data files:  50% 1/2 [00:00<00:00,  1.05it/s]04/06/2022 06:08:47 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/sichenzhong/squad_v2_synonym_aug/resolve/5cbbb5a5f5fcdf9ab91b0ebbbb35cb25cb90b985/data/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp2lzp275w\n","\n","Downloading data:   0% 0.00/16.6M [00:00<?, ?B/s]\u001b[A\n","Downloading data:   2% 280k/16.6M [00:00<00:05, 2.77MB/s]\u001b[A\n","Downloading data:  26% 4.26M/16.6M [00:00<00:00, 24.5MB/s]\u001b[A\n","Downloading data: 100% 16.6M/16.6M [00:00<00:00, 45.0MB/s]\n","04/06/2022 06:08:48 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/sichenzhong/squad_v2_synonym_aug/resolve/5cbbb5a5f5fcdf9ab91b0ebbbb35cb25cb90b985/data/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/027f55632b71919511df79b78687373482e35c72692024729c057662b2d4a655\n","04/06/2022 06:08:48 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/027f55632b71919511df79b78687373482e35c72692024729c057662b2d4a655\n","Downloading data files: 100% 2/2 [00:02<00:00,  1.05s/it]\n","04/06/2022 06:08:48 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","04/06/2022 06:08:48 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1131.15it/s]\n","04/06/2022 06:08:48 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n","04/06/2022 06:08:48 - INFO - datasets.builder - Generating validation split\n","04/06/2022 06:08:48 - INFO - datasets.builder - Generating train split\n","04/06/2022 06:08:48 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 284.22it/s]\n","[INFO|hub.py:583] 2022-04-06 06:08:48,768 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpm2p38veq\n","Downloading: 100% 570/570 [00:00<00:00, 498kB/s]\n","[INFO|hub.py:587] 2022-04-06 06:08:48,897 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|hub.py:595] 2022-04-06 06:08:48,897 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:654] 2022-04-06 06:08:48,898 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 06:08:48,898 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 06:08:49,025 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvzqvkmh4\n","Downloading: 100% 29.0/29.0 [00:00<00:00, 25.1kB/s]\n","[INFO|hub.py:587] 2022-04-06 06:08:49,156 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|hub.py:595] 2022-04-06 06:08:49,156 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:654] 2022-04-06 06:08:49,253 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 06:08:49,254 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 06:08:49,523 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_4_7ytkt\n","Downloading: 100% 208k/208k [00:00<00:00, 2.88MB/s]\n","[INFO|hub.py:587] 2022-04-06 06:08:49,693 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:595] 2022-04-06 06:08:49,693 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:583] 2022-04-06 06:08:49,793 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi36hvlh7\n","Downloading: 100% 426k/426k [00:00<00:00, 3.31MB/s]\n","[INFO|hub.py:587] 2022-04-06 06:08:50,054 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|hub.py:595] 2022-04-06 06:08:50,054 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:08:50,432 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:08:50,433 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:08:50,433 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:08:50,433 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:08:50,433 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:654] 2022-04-06 06:08:50,564 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 06:08:50,565 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 06:08:50,713 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpyn9f5dxn\n","Downloading: 100% 416M/416M [00:06<00:00, 71.1MB/s]\n","[INFO|hub.py:587] 2022-04-06 06:08:56,938 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|hub.py:595] 2022-04-06 06:08:58,476 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|modeling_utils.py:1772] 2022-04-06 06:08:58,477 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:2050] 2022-04-06 06:09:00,013 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-06 06:09:00,013 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/06/2022 06:09:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-8206e98cfe92d2e1.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:43<00:00,  3.04ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/06/2022 06:09:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-daafda211fce38bb.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:03<00:00,  5.27s/ba]\n","04/06/2022 06:10:46 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp_9myy5ov\n","Downloading builder script: 6.46kB [00:00, 7.86MB/s]       \n","04/06/2022 06:10:46 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/06/2022 06:10:46 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/06/2022 06:10:47 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpm78cibl6\n","Downloading extra modules: 11.3kB [00:00, 11.8MB/s]       \n","04/06/2022 06:10:47 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","04/06/2022 06:10:47 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 06:10:58,061 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 06:10:58,062 >>   Num examples = 132124\n","[INFO|trainer.py:1292] 2022-04-06 06:10:58,062 >>   Num Epochs = 3\n","[INFO|trainer.py:1293] 2022-04-06 06:10:58,062 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1294] 2022-04-06 06:10:58,062 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1295] 2022-04-06 06:10:58,062 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 06:10:58,062 >>   Total optimization steps = 24774\n","{'loss': 2.3999, 'learning_rate': 3.919270202631792e-05, 'epoch': 0.06}\n","  2% 500/24774 [05:17<4:16:48,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 06:16:15,617 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:16:15,622 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:16:16,684 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:16:16,687 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:16:16,690 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 1.7809, 'learning_rate': 3.838540405263583e-05, 'epoch': 0.12}\n","  4% 1000/24774 [10:38<4:11:31,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 06:21:36,453 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:21:36,457 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:21:37,452 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:21:37,456 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:21:37,458 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.627, 'learning_rate': 3.7578106078953745e-05, 'epoch': 0.18}\n","  6% 1500/24774 [15:59<4:06:14,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 06:26:57,197 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:26:57,213 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:26:58,175 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:26:58,178 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:26:58,181 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.5467, 'learning_rate': 3.677080810527166e-05, 'epoch': 0.24}\n","  8% 2000/24774 [21:20<4:00:52,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 06:32:18,270 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:32:18,275 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:32:19,256 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:32:19,259 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:32:19,277 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.4917, 'learning_rate': 3.596351013158957e-05, 'epoch': 0.3}\n"," 10% 2500/24774 [26:40<3:56:06,  1.57it/s][INFO|trainer.py:2166] 2022-04-06 06:37:38,889 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:37:38,895 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:37:39,887 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:37:39,890 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:37:39,893 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.435, 'learning_rate': 3.515621215790749e-05, 'epoch': 0.36}\n"," 12% 3000/24774 [32:01<3:50:06,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 06:42:59,660 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:42:59,664 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:43:00,686 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:43:00,689 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:43:00,692 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.4261, 'learning_rate': 3.43489141842254e-05, 'epoch': 0.42}\n"," 14% 3500/24774 [37:25<3:45:12,  1.57it/s][INFO|trainer.py:2166] 2022-04-06 06:48:23,576 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:48:23,581 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:48:24,589 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:48:24,593 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:48:24,596 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.3769, 'learning_rate': 3.3541616210543315e-05, 'epoch': 0.48}\n"," 16% 4000/24774 [42:49<3:40:03,  1.57it/s][INFO|trainer.py:2166] 2022-04-06 06:53:47,411 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:53:47,416 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:53:48,455 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:53:48,459 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:53:48,462 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.3302, 'learning_rate': 3.273431823686123e-05, 'epoch': 0.54}\n"," 18% 4500/24774 [48:13<3:34:30,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 06:59:11,504 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:59:11,509 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:59:12,485 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:59:12,488 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:59:12,491 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.2819, 'learning_rate': 3.192702026317914e-05, 'epoch': 0.61}\n"," 20% 5000/24774 [53:37<3:28:35,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 07:04:35,460 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 07:04:35,465 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:04:36,469 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:04:36,473 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:04:36,476 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.2919, 'learning_rate': 3.111972228949706e-05, 'epoch': 0.67}\n"," 22% 5500/24774 [58:58<3:23:28,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 07:09:56,172 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:09:56,177 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:09:57,184 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:09:57,188 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:09:57,191 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.261, 'learning_rate': 3.0312424315814968e-05, 'epoch': 0.73}\n"," 24% 6000/24774 [1:04:18<3:18:11,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 07:15:16,967 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 07:15:16,971 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:15:17,967 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:15:17,970 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:15:17,973 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.2277, 'learning_rate': 2.950512634213288e-05, 'epoch': 0.79}\n"," 26% 6500/24774 [1:09:39<3:13:05,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 07:20:37,533 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:20:37,538 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:20:38,592 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:20:38,596 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:20:38,600 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.2292, 'learning_rate': 2.86978283684508e-05, 'epoch': 0.85}\n"," 28% 7000/24774 [1:15:00<3:07:57,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 07:25:58,103 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 07:25:58,108 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:25:59,176 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:25:59,180 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:25:59,183 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.2133, 'learning_rate': 2.7890530394768713e-05, 'epoch': 0.91}\n"," 30% 7500/24774 [1:20:20<3:03:08,  1.57it/s][INFO|trainer.py:2166] 2022-04-06 07:31:18,543 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:31:18,548 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:31:19,569 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:31:19,573 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:31:19,576 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 1.2055, 'learning_rate': 2.7083232421086627e-05, 'epoch': 0.97}\n"," 32% 8000/24774 [1:25:44<2:57:02,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 07:36:42,483 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 07:36:42,489 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:36:43,509 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:36:43,514 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:36:43,517 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 1.0125, 'learning_rate': 2.627593444740454e-05, 'epoch': 1.03}\n"," 34% 8500/24774 [1:31:07<2:51:52,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 07:42:05,124 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:42:05,129 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:42:06,129 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:42:06,133 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:42:06,135 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.8776, 'learning_rate': 2.5468636473722455e-05, 'epoch': 1.09}\n"," 36% 9000/24774 [1:36:27<2:47:06,  1.57it/s][INFO|trainer.py:2166] 2022-04-06 07:47:25,391 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 07:47:25,395 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:47:26,431 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:47:26,435 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:47:26,438 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.8805, 'learning_rate': 2.466133850004037e-05, 'epoch': 1.15}\n"," 38% 9500/24774 [1:41:51<2:41:20,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 07:52:49,132 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:52:49,151 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:52:50,180 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:52:50,184 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:52:50,187 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.8811, 'learning_rate': 2.3854040526358283e-05, 'epoch': 1.21}\n"," 40% 10000/24774 [1:47:11<2:36:16,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 07:58:09,438 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 07:58:09,445 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:58:10,462 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:58:10,466 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:58:10,469 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.8583, 'learning_rate': 2.3046742552676197e-05, 'epoch': 1.27}\n"," 42% 10500/24774 [1:52:31<2:30:34,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 08:03:29,901 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:03:29,906 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:03:30,933 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:03:30,936 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:03:30,939 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.867, 'learning_rate': 2.223944457899411e-05, 'epoch': 1.33}\n"," 44% 11000/24774 [1:57:52<2:25:48,  1.57it/s][INFO|trainer.py:2166] 2022-04-06 08:08:50,245 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:08:50,255 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:08:51,301 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:08:51,305 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:08:51,308 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.8798, 'learning_rate': 2.1432146605312025e-05, 'epoch': 1.39}\n"," 46% 11500/24774 [2:03:12<2:20:13,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 08:14:10,665 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-11500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:14:10,669 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:14:11,708 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:14:11,712 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:14:11,733 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.8592, 'learning_rate': 2.0624848631629935e-05, 'epoch': 1.45}\n"," 48% 12000/24774 [2:08:33<2:15:09,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 08:19:31,179 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-12000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:19:31,184 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:19:32,184 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:19:32,188 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:19:32,206 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.8804, 'learning_rate': 1.981755065794785e-05, 'epoch': 1.51}\n"," 50% 12500/24774 [2:13:53<2:09:37,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 08:24:51,736 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-12500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:24:51,741 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:24:52,743 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:24:52,746 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:24:52,749 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.8464, 'learning_rate': 1.9010252684265763e-05, 'epoch': 1.57}\n"," 52% 13000/24774 [2:19:14<2:04:20,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 08:30:12,094 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-13000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:30:12,098 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:30:13,084 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:30:13,087 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:30:13,090 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.8618, 'learning_rate': 1.8202954710583677e-05, 'epoch': 1.63}\n"," 54% 13500/24774 [2:24:34<1:59:04,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 08:35:32,406 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-13500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:35:32,411 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:35:33,442 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:35:33,446 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:35:33,448 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.8532, 'learning_rate': 1.739565673690159e-05, 'epoch': 1.7}\n"," 57% 14000/24774 [2:29:55<1:53:40,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 08:40:53,179 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-14000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:40:53,184 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:40:54,211 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:40:54,215 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:40:54,217 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.8413, 'learning_rate': 1.6588358763219505e-05, 'epoch': 1.76}\n"," 59% 14500/24774 [2:35:15<1:48:30,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 08:46:13,910 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-14500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:46:13,914 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:46:14,915 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:46:14,918 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:46:14,921 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.8294, 'learning_rate': 1.578106078953742e-05, 'epoch': 1.82}\n"," 61% 15000/24774 [2:40:39<1:43:17,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 08:51:37,592 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-15000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:51:37,596 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:51:38,588 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:51:38,592 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:51:38,594 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.8393, 'learning_rate': 1.4973762815855333e-05, 'epoch': 1.88}\n"," 63% 15500/24774 [2:46:03<1:37:54,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 08:57:01,240 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-15500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:57:01,244 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:57:02,250 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:57:02,253 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:57:02,256 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.8255, 'learning_rate': 1.4166464842173247e-05, 'epoch': 1.94}\n"," 65% 16000/24774 [2:51:26<1:32:52,  1.57it/s][INFO|trainer.py:2166] 2022-04-06 09:02:24,898 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-16000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:02:24,902 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:02:25,906 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:02:25,910 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:02:25,913 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-16000/special_tokens_map.json\n","{'loss': 0.8238, 'learning_rate': 1.3359166868491161e-05, 'epoch': 2.0}\n"," 67% 16500/24774 [2:56:49<1:27:15,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 09:07:47,215 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-16500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:07:47,220 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-16500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:07:48,235 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-16500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:07:48,238 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-16500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:07:48,241 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-16500/special_tokens_map.json\n","{'loss': 0.5522, 'learning_rate': 1.2551868894809077e-05, 'epoch': 2.06}\n"," 69% 17000/24774 [3:02:09<1:22:11,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 09:13:07,496 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-17000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:13:07,500 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-17000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:13:08,538 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-17000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:13:08,542 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-17000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:13:08,545 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-17000/special_tokens_map.json\n","{'loss': 0.5232, 'learning_rate': 1.174457092112699e-05, 'epoch': 2.12}\n"," 71% 17500/24774 [3:07:30<1:17:03,  1.57it/s][INFO|trainer.py:2166] 2022-04-06 09:18:28,099 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-17500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:18:28,104 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-17500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:18:29,113 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-17500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:18:29,116 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-17500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:18:29,119 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-17500/special_tokens_map.json\n","{'loss': 0.5255, 'learning_rate': 1.0937272947444903e-05, 'epoch': 2.18}\n"," 73% 18000/24774 [3:12:50<1:11:28,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 09:23:48,507 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-18000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:23:48,515 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-18000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:23:49,558 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-18000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:23:49,562 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-18000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:23:49,564 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-18000/special_tokens_map.json\n","{'loss': 0.5324, 'learning_rate': 1.0129974973762817e-05, 'epoch': 2.24}\n"," 75% 18500/24774 [3:18:13<1:06:16,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 09:29:11,451 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-18500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:29:11,456 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-18500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:29:12,495 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-18500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:29:12,498 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-18500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:29:12,501 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-18500/special_tokens_map.json\n","{'loss': 0.5397, 'learning_rate': 9.32267700008073e-06, 'epoch': 2.3}\n"," 77% 19000/24774 [3:23:36<1:00:54,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 09:34:34,155 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-19000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:34:34,160 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-19000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:34:35,187 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-19000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:34:35,191 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-19000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:34:35,194 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-19000/special_tokens_map.json\n","{'loss': 0.5217, 'learning_rate': 8.515379026398645e-06, 'epoch': 2.36}\n"," 79% 19500/24774 [3:28:56<55:41,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 09:39:54,354 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-19500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:39:54,358 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-19500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:39:55,367 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-19500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:39:55,371 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-19500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:39:55,374 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-19500/special_tokens_map.json\n","{'loss': 0.5122, 'learning_rate': 7.708081052716559e-06, 'epoch': 2.42}\n"," 81% 20000/24774 [3:34:16<50:25,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 09:45:14,953 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-20000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:45:14,957 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-20000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:45:15,960 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-20000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:45:15,963 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-20000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:45:15,966 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-20000/special_tokens_map.json\n","{'loss': 0.5376, 'learning_rate': 6.900783079034472e-06, 'epoch': 2.48}\n"," 83% 20500/24774 [3:39:37<45:06,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 09:50:35,152 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-20500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:50:35,157 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-20500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:50:36,164 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-20500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:50:36,167 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-20500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:50:36,170 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-20500/special_tokens_map.json\n","{'loss': 0.5241, 'learning_rate': 6.093485105352386e-06, 'epoch': 2.54}\n"," 85% 21000/24774 [3:44:57<39:50,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 09:55:55,384 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-21000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:55:55,389 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-21000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:55:56,389 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-21000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:55:56,392 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-21000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:55:56,395 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-21000/special_tokens_map.json\n","{'loss': 0.5211, 'learning_rate': 5.2861871316703e-06, 'epoch': 2.6}\n"," 87% 21500/24774 [3:50:17<34:35,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 10:01:15,594 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-21500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:01:15,598 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-21500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:01:16,626 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-21500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:01:16,630 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-21500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:01:16,633 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-21500/special_tokens_map.json\n","{'loss': 0.5269, 'learning_rate': 4.478889157988214e-06, 'epoch': 2.66}\n"," 89% 22000/24774 [3:55:37<29:15,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 10:06:35,956 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-22000\n","[INFO|configuration_utils.py:441] 2022-04-06 10:06:35,961 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-22000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:06:36,991 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-22000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:06:36,995 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-22000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:06:36,997 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-22000/special_tokens_map.json\n","{'loss': 0.4983, 'learning_rate': 3.6715911843061274e-06, 'epoch': 2.72}\n"," 91% 22500/24774 [4:00:58<23:59,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 10:11:56,502 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-22500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:11:56,511 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-22500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:11:57,529 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-22500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:11:57,532 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-22500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:11:57,534 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-22500/special_tokens_map.json\n","{'loss': 0.5177, 'learning_rate': 2.8642932106240418e-06, 'epoch': 2.79}\n"," 93% 23000/24774 [4:06:18<18:43,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 10:17:16,695 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-23000\n","[INFO|configuration_utils.py:441] 2022-04-06 10:17:16,700 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-23000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:17:17,725 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-23000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:17:17,728 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-23000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:17:17,746 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-23000/special_tokens_map.json\n","{'loss': 0.5142, 'learning_rate': 2.0569952369419553e-06, 'epoch': 2.85}\n"," 95% 23500/24774 [4:11:39<13:29,  1.57it/s][INFO|trainer.py:2166] 2022-04-06 10:22:37,197 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-23500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:22:37,202 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-23500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:22:38,212 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-23500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:22:38,216 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-23500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:22:38,219 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-23500/special_tokens_map.json\n","{'loss': 0.5093, 'learning_rate': 1.2496972632598693e-06, 'epoch': 2.91}\n"," 97% 24000/24774 [4:16:59<08:10,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 10:27:57,520 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-24000\n","[INFO|configuration_utils.py:441] 2022-04-06 10:27:57,525 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-24000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:27:58,588 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-24000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:27:58,592 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-24000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:27:58,610 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-24000/special_tokens_map.json\n","{'loss': 0.5152, 'learning_rate': 4.4239928957778315e-07, 'epoch': 2.97}\n"," 99% 24500/24774 [4:22:19<02:53,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 10:33:17,952 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-24500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:33:17,957 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-24500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:33:19,026 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-24500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:33:19,030 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-24500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:33:19,033 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/checkpoint-24500/special_tokens_map.json\n","100% 24774/24774 [4:25:16<00:00,  1.71it/s][INFO|trainer.py:1530] 2022-04-06 10:36:15,000 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 15916.9402, 'train_samples_per_second': 24.903, 'train_steps_per_second': 1.556, 'train_loss': 0.9381613466153954, 'epoch': 3.0}\n","100% 24774/24774 [4:25:16<00:00,  1.56it/s]\n","[INFO|trainer.py:2166] 2022-04-06 10:36:15,010 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug\n","[INFO|configuration_utils.py:441] 2022-04-06 10:36:15,015 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:36:16,239 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:36:16,243 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:36:16,245 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =     0.9382\n","  train_runtime            = 4:25:16.94\n","  train_samples            =     132124\n","  train_samples_per_second =     24.903\n","  train_steps_per_second   =      1.556\n","04/06/2022 10:36:16 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 10:36:16,317 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 10:36:16,325 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 10:36:16,325 >>   Num examples = 12199\n","[INFO|trainer.py:2421] 2022-04-06 10:36:16,325 >>   Batch size = 8\n","100% 1525/1525 [02:57<00:00,  8.77it/s]04/06/2022 10:39:26 - INFO - utils_qa - Post-processing 11873 example predictions split into 12199 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 31/11873 [00:00<00:38, 307.07it/s]\u001b[A\n","  1% 68/11873 [00:00<00:35, 336.38it/s]\u001b[A\n","  1% 106/11873 [00:00<00:33, 352.53it/s]\u001b[A\n","  1% 151/11873 [00:00<00:30, 388.61it/s]\u001b[A\n","  2% 198/11873 [00:00<00:28, 414.04it/s]\u001b[A\n","  2% 244/11873 [00:00<00:27, 427.39it/s]\u001b[A\n","  2% 290/11873 [00:00<00:26, 434.28it/s]\u001b[A\n","  3% 334/11873 [00:00<00:27, 423.78it/s]\u001b[A\n","  3% 379/11873 [00:00<00:26, 430.92it/s]\u001b[A\n","  4% 423/11873 [00:01<00:26, 426.63it/s]\u001b[A\n","  4% 469/11873 [00:01<00:26, 433.60it/s]\u001b[A\n","  4% 513/11873 [00:01<00:26, 423.52it/s]\u001b[A\n","  5% 556/11873 [00:01<00:26, 423.10it/s]\u001b[A\n","  5% 599/11873 [00:01<00:26, 422.37it/s]\u001b[A\n","  5% 643/11873 [00:01<00:26, 426.87it/s]\u001b[A\n","  6% 689/11873 [00:01<00:25, 434.75it/s]\u001b[A\n","  6% 738/11873 [00:01<00:24, 448.80it/s]\u001b[A\n","  7% 784/11873 [00:01<00:24, 452.00it/s]\u001b[A\n","  7% 835/11873 [00:01<00:23, 467.36it/s]\u001b[A\n","  7% 888/11873 [00:02<00:22, 485.61it/s]\u001b[A\n","  8% 937/11873 [00:02<00:22, 483.47it/s]\u001b[A\n","  8% 986/11873 [00:02<00:23, 464.16it/s]\u001b[A\n","  9% 1033/11873 [00:02<00:25, 431.75it/s]\u001b[A\n","  9% 1077/11873 [00:02<00:26, 408.68it/s]\u001b[A\n","  9% 1119/11873 [00:02<00:27, 389.37it/s]\u001b[A\n"," 10% 1159/11873 [00:02<00:28, 380.05it/s]\u001b[A\n"," 10% 1198/11873 [00:02<00:29, 365.14it/s]\u001b[A\n"," 10% 1236/11873 [00:02<00:28, 368.91it/s]\u001b[A\n"," 11% 1275/11873 [00:03<00:28, 373.74it/s]\u001b[A\n"," 11% 1315/11873 [00:03<00:27, 378.63it/s]\u001b[A\n"," 11% 1354/11873 [00:03<00:27, 381.75it/s]\u001b[A\n"," 12% 1394/11873 [00:03<00:27, 385.45it/s]\u001b[A\n"," 12% 1434/11873 [00:03<00:26, 387.49it/s]\u001b[A\n"," 12% 1473/11873 [00:03<00:26, 386.90it/s]\u001b[A\n"," 13% 1513/11873 [00:03<00:26, 389.55it/s]\u001b[A\n"," 13% 1553/11873 [00:03<00:26, 390.63it/s]\u001b[A\n"," 13% 1593/11873 [00:03<00:26, 389.24it/s]\u001b[A\n"," 14% 1632/11873 [00:03<00:26, 380.50it/s]\u001b[A\n"," 14% 1672/11873 [00:04<00:26, 383.85it/s]\u001b[A\n"," 14% 1711/11873 [00:04<00:27, 373.55it/s]\u001b[A\n"," 15% 1749/11873 [00:04<00:28, 359.12it/s]\u001b[A\n"," 15% 1787/11873 [00:04<00:27, 363.98it/s]\u001b[A\n"," 15% 1826/11873 [00:04<00:27, 370.99it/s]\u001b[A\n"," 16% 1865/11873 [00:04<00:26, 374.06it/s]\u001b[A\n"," 16% 1904/11873 [00:04<00:26, 376.62it/s]\u001b[A\n"," 16% 1942/11873 [00:04<00:26, 375.46it/s]\u001b[A\n"," 17% 1980/11873 [00:04<00:26, 375.30it/s]\u001b[A\n"," 17% 2020/11873 [00:05<00:25, 381.93it/s]\u001b[A\n"," 17% 2060/11873 [00:05<00:25, 384.47it/s]\u001b[A\n"," 18% 2099/11873 [00:05<00:25, 378.55it/s]\u001b[A\n"," 18% 2138/11873 [00:05<00:25, 381.62it/s]\u001b[A\n"," 18% 2177/11873 [00:05<00:25, 382.72it/s]\u001b[A\n"," 19% 2217/11873 [00:05<00:24, 386.83it/s]\u001b[A\n"," 19% 2257/11873 [00:05<00:24, 387.94it/s]\u001b[A\n"," 19% 2296/11873 [00:05<00:24, 386.09it/s]\u001b[A\n"," 20% 2335/11873 [00:05<00:24, 387.06it/s]\u001b[A\n"," 20% 2375/11873 [00:05<00:24, 389.76it/s]\u001b[A\n"," 20% 2414/11873 [00:06<00:24, 386.79it/s]\u001b[A\n"," 21% 2453/11873 [00:06<00:24, 385.46it/s]\u001b[A\n"," 21% 2492/11873 [00:06<00:24, 385.73it/s]\u001b[A\n"," 21% 2532/11873 [00:06<00:24, 387.70it/s]\u001b[A\n"," 22% 2572/11873 [00:06<00:23, 389.69it/s]\u001b[A\n"," 22% 2612/11873 [00:06<00:23, 390.04it/s]\u001b[A\n"," 22% 2652/11873 [00:06<00:23, 388.86it/s]\u001b[A\n"," 23% 2692/11873 [00:06<00:23, 390.43it/s]\u001b[A\n"," 23% 2732/11873 [00:06<00:23, 387.86it/s]\u001b[A\n"," 23% 2771/11873 [00:06<00:23, 383.52it/s]\u001b[A\n"," 24% 2811/11873 [00:07<00:23, 386.14it/s]\u001b[A\n"," 24% 2850/11873 [00:07<00:23, 383.49it/s]\u001b[A\n"," 24% 2889/11873 [00:07<00:24, 372.12it/s]\u001b[A\n"," 25% 2927/11873 [00:07<00:23, 373.43it/s]\u001b[A\n"," 25% 2967/11873 [00:07<00:23, 378.89it/s]\u001b[A\n"," 25% 3005/11873 [00:07<00:25, 343.04it/s]\u001b[A\n"," 26% 3043/11873 [00:07<00:25, 352.15it/s]\u001b[A\n"," 26% 3079/11873 [00:07<00:24, 353.85it/s]\u001b[A\n"," 26% 3115/11873 [00:07<00:27, 321.74it/s]\u001b[A\n"," 27% 3148/11873 [00:08<00:29, 296.78it/s]\u001b[A\n"," 27% 3179/11873 [00:08<00:30, 286.83it/s]\u001b[A\n"," 27% 3213/11873 [00:08<00:28, 299.71it/s]\u001b[A\n"," 27% 3247/11873 [00:08<00:27, 308.96it/s]\u001b[A\n"," 28% 3279/11873 [00:08<00:31, 272.46it/s]\u001b[A\n"," 28% 3308/11873 [00:08<00:39, 216.20it/s]\u001b[A\n"," 28% 3332/11873 [00:08<00:42, 200.81it/s]\u001b[A\n"," 28% 3354/11873 [00:09<00:44, 191.41it/s]\u001b[A\n"," 28% 3378/11873 [00:09<00:42, 201.41it/s]\u001b[A\n"," 29% 3413/11873 [00:09<00:35, 237.92it/s]\u001b[A\n"," 29% 3452/11873 [00:09<00:30, 276.76it/s]\u001b[A\n"," 29% 3491/11873 [00:09<00:27, 306.49it/s]\u001b[A\n"," 30% 3530/11873 [00:09<00:25, 328.95it/s]\u001b[A\n"," 30% 3570/11873 [00:09<00:23, 347.51it/s]\u001b[A\n"," 30% 3610/11873 [00:09<00:22, 360.19it/s]\u001b[A\n"," 31% 3650/11873 [00:09<00:22, 369.59it/s]\u001b[A\n"," 31% 3688/11873 [00:09<00:22, 367.87it/s]\u001b[A\n"," 31% 3726/11873 [00:10<00:22, 363.65it/s]\u001b[A\n"," 32% 3765/11873 [00:10<00:21, 371.07it/s]\u001b[A\n"," 32% 3804/11873 [00:10<00:21, 374.03it/s]\u001b[A\n"," 32% 3842/11873 [00:10<00:22, 351.71it/s]\u001b[A\n"," 33% 3881/11873 [00:10<00:22, 360.17it/s]\u001b[A\n"," 33% 3918/11873 [00:10<00:22, 346.91it/s]\u001b[A\n"," 33% 3953/11873 [00:10<00:22, 344.92it/s]\u001b[A\n"," 34% 3988/11873 [00:10<00:23, 335.59it/s]\u001b[A\n"," 34% 4028/11873 [00:10<00:22, 352.98it/s]\u001b[A\n"," 34% 4067/11873 [00:11<00:21, 363.01it/s]\u001b[A\n"," 35% 4107/11873 [00:11<00:20, 371.39it/s]\u001b[A\n"," 35% 4145/11873 [00:11<00:21, 360.65it/s]\u001b[A\n"," 35% 4182/11873 [00:11<00:22, 345.59it/s]\u001b[A\n"," 36% 4219/11873 [00:11<00:21, 350.31it/s]\u001b[A\n"," 36% 4259/11873 [00:11<00:21, 362.09it/s]\u001b[A\n"," 36% 4299/11873 [00:11<00:20, 371.50it/s]\u001b[A\n"," 37% 4339/11873 [00:11<00:19, 379.42it/s]\u001b[A\n"," 37% 4378/11873 [00:11<00:19, 377.59it/s]\u001b[A\n"," 37% 4416/11873 [00:12<00:21, 347.50it/s]\u001b[A\n"," 37% 4452/11873 [00:12<00:23, 321.21it/s]\u001b[A\n"," 38% 4490/11873 [00:12<00:22, 334.44it/s]\u001b[A\n"," 38% 4525/11873 [00:12<00:21, 334.58it/s]\u001b[A\n"," 38% 4559/11873 [00:12<00:21, 335.54it/s]\u001b[A\n"," 39% 4596/11873 [00:12<00:21, 342.70it/s]\u001b[A\n"," 39% 4632/11873 [00:12<00:20, 346.75it/s]\u001b[A\n"," 39% 4667/11873 [00:12<00:20, 345.58it/s]\u001b[A\n"," 40% 4702/11873 [00:12<00:21, 336.78it/s]\u001b[A\n"," 40% 4736/11873 [00:12<00:21, 332.95it/s]\u001b[A\n"," 40% 4771/11873 [00:13<00:21, 336.98it/s]\u001b[A\n"," 40% 4805/11873 [00:13<00:21, 334.50it/s]\u001b[A\n"," 41% 4842/11873 [00:13<00:20, 343.38it/s]\u001b[A\n"," 41% 4880/11873 [00:13<00:19, 353.05it/s]\u001b[A\n"," 41% 4918/11873 [00:13<00:19, 360.61it/s]\u001b[A\n"," 42% 4958/11873 [00:13<00:18, 371.08it/s]\u001b[A\n"," 42% 4997/11873 [00:13<00:18, 374.49it/s]\u001b[A\n"," 42% 5035/11873 [00:13<00:18, 374.63it/s]\u001b[A\n"," 43% 5074/11873 [00:13<00:18, 377.01it/s]\u001b[A\n"," 43% 5114/11873 [00:13<00:17, 382.68it/s]\u001b[A\n"," 43% 5153/11873 [00:14<00:17, 384.10it/s]\u001b[A\n"," 44% 5193/11873 [00:14<00:17, 387.41it/s]\u001b[A\n"," 44% 5232/11873 [00:14<00:17, 378.71it/s]\u001b[A\n"," 44% 5270/11873 [00:14<00:18, 350.14it/s]\u001b[A\n"," 45% 5310/11873 [00:14<00:18, 362.10it/s]\u001b[A\n"," 45% 5350/11873 [00:14<00:17, 370.73it/s]\u001b[A\n"," 45% 5390/11873 [00:14<00:17, 377.46it/s]\u001b[A\n"," 46% 5430/11873 [00:14<00:16, 382.88it/s]\u001b[A\n"," 46% 5469/11873 [00:14<00:16, 380.34it/s]\u001b[A\n"," 46% 5509/11873 [00:15<00:16, 383.31it/s]\u001b[A\n"," 47% 5548/11873 [00:15<00:16, 385.09it/s]\u001b[A\n"," 47% 5587/11873 [00:15<00:16, 379.89it/s]\u001b[A\n"," 47% 5626/11873 [00:15<00:16, 378.87it/s]\u001b[A\n"," 48% 5665/11873 [00:15<00:16, 378.68it/s]\u001b[A\n"," 48% 5703/11873 [00:15<00:16, 378.85it/s]\u001b[A\n"," 48% 5741/11873 [00:15<00:16, 376.73it/s]\u001b[A\n"," 49% 5779/11873 [00:15<00:16, 377.43it/s]\u001b[A\n"," 49% 5820/11873 [00:15<00:15, 385.09it/s]\u001b[A\n"," 49% 5860/11873 [00:15<00:15, 387.11it/s]\u001b[A\n"," 50% 5899/11873 [00:16<00:15, 387.24it/s]\u001b[A\n"," 50% 5939/11873 [00:16<00:15, 388.57it/s]\u001b[A\n"," 50% 5978/11873 [00:16<00:15, 382.25it/s]\u001b[A\n"," 51% 6017/11873 [00:16<00:15, 370.06it/s]\u001b[A\n"," 51% 6055/11873 [00:16<00:15, 365.35it/s]\u001b[A\n"," 51% 6092/11873 [00:16<00:16, 355.12it/s]\u001b[A\n"," 52% 6128/11873 [00:16<00:16, 355.24it/s]\u001b[A\n"," 52% 6164/11873 [00:16<00:16, 350.92it/s]\u001b[A\n"," 52% 6200/11873 [00:16<00:16, 351.41it/s]\u001b[A\n"," 53% 6236/11873 [00:17<00:16, 351.56it/s]\u001b[A\n"," 53% 6272/11873 [00:17<00:16, 346.07it/s]\u001b[A\n"," 53% 6308/11873 [00:17<00:16, 347.32it/s]\u001b[A\n"," 53% 6347/11873 [00:17<00:15, 358.40it/s]\u001b[A\n"," 54% 6386/11873 [00:17<00:14, 366.27it/s]\u001b[A\n"," 54% 6425/11873 [00:17<00:14, 370.95it/s]\u001b[A\n"," 54% 6464/11873 [00:17<00:14, 375.30it/s]\u001b[A\n"," 55% 6502/11873 [00:17<00:14, 376.01it/s]\u001b[A\n"," 55% 6541/11873 [00:17<00:14, 378.23it/s]\u001b[A\n"," 55% 6580/11873 [00:17<00:13, 380.74it/s]\u001b[A\n"," 56% 6619/11873 [00:18<00:13, 378.58it/s]\u001b[A\n"," 56% 6658/11873 [00:18<00:13, 380.92it/s]\u001b[A\n"," 56% 6697/11873 [00:18<00:13, 382.84it/s]\u001b[A\n"," 57% 6736/11873 [00:18<00:15, 333.60it/s]\u001b[A\n"," 57% 6774/11873 [00:18<00:14, 344.77it/s]\u001b[A\n"," 57% 6813/11873 [00:18<00:14, 356.89it/s]\u001b[A\n"," 58% 6852/11873 [00:18<00:13, 365.96it/s]\u001b[A\n"," 58% 6891/11873 [00:18<00:13, 370.86it/s]\u001b[A\n"," 58% 6929/11873 [00:18<00:13, 371.50it/s]\u001b[A\n"," 59% 6968/11873 [00:19<00:13, 375.62it/s]\u001b[A\n"," 59% 7006/11873 [00:19<00:12, 374.45it/s]\u001b[A\n"," 59% 7045/11873 [00:19<00:12, 377.48it/s]\u001b[A\n"," 60% 7083/11873 [00:19<00:12, 369.21it/s]\u001b[A\n"," 60% 7121/11873 [00:19<00:12, 372.06it/s]\u001b[A\n"," 60% 7161/11873 [00:19<00:12, 378.19it/s]\u001b[A\n"," 61% 7200/11873 [00:19<00:12, 380.76it/s]\u001b[A\n"," 61% 7239/11873 [00:19<00:12, 382.35it/s]\u001b[A\n"," 61% 7278/11873 [00:19<00:12, 378.22it/s]\u001b[A\n"," 62% 7317/11873 [00:19<00:12, 379.28it/s]\u001b[A\n"," 62% 7356/11873 [00:20<00:11, 381.53it/s]\u001b[A\n"," 62% 7395/11873 [00:20<00:11, 380.68it/s]\u001b[A\n"," 63% 7434/11873 [00:20<00:12, 362.70it/s]\u001b[A\n"," 63% 7471/11873 [00:20<00:12, 362.52it/s]\u001b[A\n"," 63% 7511/11873 [00:20<00:11, 371.43it/s]\u001b[A\n"," 64% 7551/11873 [00:20<00:11, 377.04it/s]\u001b[A\n"," 64% 7590/11873 [00:20<00:11, 380.33it/s]\u001b[A\n"," 64% 7629/11873 [00:20<00:11, 378.23it/s]\u001b[A\n"," 65% 7667/11873 [00:20<00:11, 373.73it/s]\u001b[A\n"," 65% 7705/11873 [00:20<00:11, 360.52it/s]\u001b[A\n"," 65% 7742/11873 [00:21<00:11, 357.36it/s]\u001b[A\n"," 66% 7780/11873 [00:21<00:11, 362.83it/s]\u001b[A\n"," 66% 7818/11873 [00:21<00:11, 367.40it/s]\u001b[A\n"," 66% 7855/11873 [00:21<00:10, 367.37it/s]\u001b[A\n"," 66% 7892/11873 [00:21<00:11, 347.83it/s]\u001b[A\n"," 67% 7932/11873 [00:21<00:10, 360.29it/s]\u001b[A\n"," 67% 7970/11873 [00:21<00:10, 365.17it/s]\u001b[A\n"," 67% 8008/11873 [00:21<00:10, 368.85it/s]\u001b[A\n"," 68% 8047/11873 [00:21<00:10, 373.32it/s]\u001b[A\n"," 68% 8086/11873 [00:22<00:10, 377.91it/s]\u001b[A\n"," 68% 8125/11873 [00:22<00:09, 378.83it/s]\u001b[A\n"," 69% 8165/11873 [00:22<00:09, 382.24it/s]\u001b[A\n"," 69% 8204/11873 [00:22<00:09, 374.15it/s]\u001b[A\n"," 69% 8242/11873 [00:22<00:09, 367.74it/s]\u001b[A\n"," 70% 8279/11873 [00:22<00:09, 366.95it/s]\u001b[A\n"," 70% 8316/11873 [00:22<00:09, 362.46it/s]\u001b[A\n"," 70% 8353/11873 [00:22<00:09, 359.81it/s]\u001b[A\n"," 71% 8390/11873 [00:22<00:09, 354.28it/s]\u001b[A\n"," 71% 8426/11873 [00:22<00:09, 346.68it/s]\u001b[A\n"," 71% 8461/11873 [00:23<00:09, 342.47it/s]\u001b[A\n"," 72% 8496/11873 [00:23<00:09, 342.91it/s]\u001b[A\n"," 72% 8534/11873 [00:23<00:09, 351.23it/s]\u001b[A\n"," 72% 8572/11873 [00:23<00:09, 359.57it/s]\u001b[A\n"," 73% 8611/11873 [00:23<00:08, 367.54it/s]\u001b[A\n"," 73% 8650/11873 [00:23<00:08, 371.47it/s]\u001b[A\n"," 73% 8689/11873 [00:23<00:08, 376.45it/s]\u001b[A\n"," 74% 8728/11873 [00:23<00:08, 378.31it/s]\u001b[A\n"," 74% 8767/11873 [00:23<00:08, 381.14it/s]\u001b[A\n"," 74% 8806/11873 [00:23<00:08, 380.85it/s]\u001b[A\n"," 74% 8845/11873 [00:24<00:07, 380.63it/s]\u001b[A\n"," 75% 8884/11873 [00:24<00:07, 382.56it/s]\u001b[A\n"," 75% 8923/11873 [00:24<00:07, 382.00it/s]\u001b[A\n"," 75% 8962/11873 [00:24<00:07, 376.55it/s]\u001b[A\n"," 76% 9002/11873 [00:24<00:07, 382.78it/s]\u001b[A\n"," 76% 9041/11873 [00:24<00:07, 380.38it/s]\u001b[A\n"," 76% 9080/11873 [00:24<00:07, 382.73it/s]\u001b[A\n"," 77% 9119/11873 [00:24<00:07, 384.41it/s]\u001b[A\n"," 77% 9158/11873 [00:24<00:07, 373.48it/s]\u001b[A\n"," 77% 9196/11873 [00:25<00:07, 373.18it/s]\u001b[A\n"," 78% 9235/11873 [00:25<00:07, 376.82it/s]\u001b[A\n"," 78% 9274/11873 [00:25<00:06, 379.25it/s]\u001b[A\n"," 78% 9312/11873 [00:25<00:06, 379.08it/s]\u001b[A\n"," 79% 9350/11873 [00:25<00:06, 376.43it/s]\u001b[A\n"," 79% 9389/11873 [00:25<00:06, 380.32it/s]\u001b[A\n"," 79% 9428/11873 [00:25<00:06, 381.39it/s]\u001b[A\n"," 80% 9467/11873 [00:25<00:06, 382.17it/s]\u001b[A\n"," 80% 9507/11873 [00:25<00:06, 386.24it/s]\u001b[A\n"," 80% 9546/11873 [00:25<00:06, 386.83it/s]\u001b[A\n"," 81% 9585/11873 [00:26<00:05, 387.40it/s]\u001b[A\n"," 81% 9624/11873 [00:26<00:05, 386.03it/s]\u001b[A\n"," 81% 9663/11873 [00:26<00:05, 385.85it/s]\u001b[A\n"," 82% 9703/11873 [00:26<00:05, 388.15it/s]\u001b[A\n"," 82% 9742/11873 [00:26<00:05, 388.07it/s]\u001b[A\n"," 82% 9782/11873 [00:26<00:05, 389.29it/s]\u001b[A\n"," 83% 9821/11873 [00:26<00:05, 387.83it/s]\u001b[A\n"," 83% 9861/11873 [00:26<00:05, 389.77it/s]\u001b[A\n"," 83% 9900/11873 [00:26<00:05, 389.37it/s]\u001b[A\n"," 84% 9939/11873 [00:26<00:05, 376.06it/s]\u001b[A\n"," 84% 9977/11873 [00:27<00:05, 365.74it/s]\u001b[A\n"," 84% 10014/11873 [00:27<00:05, 348.79it/s]\u001b[A\n"," 85% 10052/11873 [00:27<00:05, 355.19it/s]\u001b[A\n"," 85% 10091/11873 [00:27<00:04, 365.04it/s]\u001b[A\n"," 85% 10130/11873 [00:27<00:04, 371.37it/s]\u001b[A\n"," 86% 10170/11873 [00:27<00:04, 378.31it/s]\u001b[A\n"," 86% 10208/11873 [00:27<00:04, 377.18it/s]\u001b[A\n"," 86% 10248/11873 [00:27<00:04, 382.54it/s]\u001b[A\n"," 87% 10287/11873 [00:27<00:04, 374.96it/s]\u001b[A\n"," 87% 10325/11873 [00:28<00:04, 361.16it/s]\u001b[A\n"," 87% 10362/11873 [00:28<00:04, 356.05it/s]\u001b[A\n"," 88% 10398/11873 [00:28<00:04, 348.26it/s]\u001b[A\n"," 88% 10433/11873 [00:28<00:04, 321.54it/s]\u001b[A\n"," 88% 10469/11873 [00:28<00:04, 330.43it/s]\u001b[A\n"," 88% 10505/11873 [00:28<00:04, 336.27it/s]\u001b[A\n"," 89% 10539/11873 [00:28<00:04, 332.39it/s]\u001b[A\n"," 89% 10573/11873 [00:28<00:04, 311.92it/s]\u001b[A\n"," 89% 10606/11873 [00:28<00:04, 316.48it/s]\u001b[A\n"," 90% 10639/11873 [00:28<00:03, 320.21it/s]\u001b[A\n"," 90% 10673/11873 [00:29<00:03, 323.78it/s]\u001b[A\n"," 90% 10707/11873 [00:29<00:03, 328.03it/s]\u001b[A\n"," 90% 10743/11873 [00:29<00:03, 336.28it/s]\u001b[A\n"," 91% 10781/11873 [00:29<00:03, 348.93it/s]\u001b[A\n"," 91% 10819/11873 [00:29<00:02, 352.76it/s]\u001b[A\n"," 91% 10855/11873 [00:29<00:03, 338.60it/s]\u001b[A\n"," 92% 10893/11873 [00:29<00:02, 349.18it/s]\u001b[A\n"," 92% 10929/11873 [00:29<00:02, 352.24it/s]\u001b[A\n"," 92% 10968/11873 [00:29<00:02, 362.48it/s]\u001b[A\n"," 93% 11008/11873 [00:30<00:02, 372.39it/s]\u001b[A\n"," 93% 11048/11873 [00:30<00:02, 379.64it/s]\u001b[A\n"," 93% 11087/11873 [00:30<00:02, 373.71it/s]\u001b[A\n"," 94% 11125/11873 [00:30<00:01, 374.17it/s]\u001b[A\n"," 94% 11163/11873 [00:30<00:01, 367.11it/s]\u001b[A\n"," 94% 11200/11873 [00:30<00:01, 359.71it/s]\u001b[A\n"," 95% 11237/11873 [00:30<00:01, 355.83it/s]\u001b[A\n"," 95% 11273/11873 [00:30<00:01, 351.11it/s]\u001b[A\n"," 95% 11309/11873 [00:30<00:01, 351.86it/s]\u001b[A\n"," 96% 11347/11873 [00:30<00:01, 357.54it/s]\u001b[A\n"," 96% 11386/11873 [00:31<00:01, 366.32it/s]\u001b[A\n"," 96% 11424/11873 [00:31<00:01, 365.22it/s]\u001b[A\n"," 97% 11463/11873 [00:31<00:01, 372.26it/s]\u001b[A\n"," 97% 11501/11873 [00:31<00:00, 374.46it/s]\u001b[A\n"," 97% 11539/11873 [00:31<00:00, 374.14it/s]\u001b[A\n"," 98% 11578/11873 [00:31<00:00, 378.08it/s]\u001b[A\n"," 98% 11617/11873 [00:31<00:00, 379.59it/s]\u001b[A\n"," 98% 11656/11873 [00:31<00:00, 382.44it/s]\u001b[A\n"," 99% 11695/11873 [00:31<00:00, 378.51it/s]\u001b[A\n"," 99% 11733/11873 [00:31<00:00, 376.57it/s]\u001b[A\n"," 99% 11771/11873 [00:32<00:00, 371.26it/s]\u001b[A\n"," 99% 11809/11873 [00:32<00:00, 367.43it/s]\u001b[A\n","100% 11873/11873 [00:32<00:00, 366.82it/s]\n","04/06/2022 10:39:58 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/eval_predictions.json.\n","04/06/2022 10:39:58 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/eval_nbest_predictions.json.\n","04/06/2022 10:40:00 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/bert-base-cased/synonym-aug/eval_null_odds.json.\n","04/06/2022 10:40:04 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1525/1525 [03:47<00:00,  6.70it/s]\n","***** eval metrics *****\n","  epoch                  =     3.0\n","  eval_HasAns_exact      = 72.3178\n","  eval_HasAns_f1         = 79.1201\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 66.3415\n","  eval_NoAns_f1          = 66.3415\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 69.3254\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 72.7217\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 69.3254\n","  eval_f1                = 72.7217\n","  eval_samples           =   12199\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 10:40:04,178 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_synonym_aug', 'type': 'sichenzhong/squad_v2_synonym_aug', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path albert-base-v2 \\\n","  --dataset_name sichenzhong/squad_v2_synonym_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 512 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YC1SX-FRTOxk","outputId":"a08fd207-67d0-4e2d-84c5-dfb538ee62cc","executionInfo":{"status":"ok","timestamp":1649256889361,"user_tz":240,"elapsed":696052,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["04/06/2022 10:40:15 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 10:40:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/runs/Apr06_10-40-15_2baca14bc166,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 10:40:15 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b\n","04/06/2022 10:40:15 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/06/2022 10:40:15 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","04/06/2022 10:40:15 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","04/06/2022 10:40:15 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","100% 2/2 [00:00<00:00, 20.03it/s]\n","[INFO|hub.py:583] 2022-04-06 10:40:15,921 >> https://huggingface.co/albert-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7c4hs55x\n","Downloading: 100% 684/684 [00:00<00:00, 785kB/s]\n","[INFO|hub.py:587] 2022-04-06 10:40:16,026 >> storing https://huggingface.co/albert-base-v2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|hub.py:595] 2022-04-06 10:40:16,026 >> creating metadata file for /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:654] 2022-04-06 10:40:16,026 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-06 10:40:16,028 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-06 10:40:16,137 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-06 10:40:16,235 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-06 10:40:16,236 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 10:40:16,433 >> https://huggingface.co/albert-base-v2/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2itlbp9m\n","Downloading: 100% 742k/742k [00:00<00:00, 7.61MB/s]\n","[INFO|hub.py:587] 2022-04-06 10:40:16,637 >> storing https://huggingface.co/albert-base-v2/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:595] 2022-04-06 10:40:16,637 >> creating metadata file for /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:583] 2022-04-06 10:40:16,739 >> https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfrpzu6nk\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 11.0MB/s]\n","[INFO|hub.py:587] 2022-04-06 10:40:16,966 >> storing https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|hub.py:595] 2022-04-06 10:40:16,966 >> creating metadata file for /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 10:40:17,272 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 10:40:17,272 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 10:40:17,272 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 10:40:17,272 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 10:40:17,272 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-06 10:40:17,369 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-06 10:40:17,370 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 10:40:17,595 >> https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqoui0u2a\n","Downloading: 100% 45.2M/45.2M [00:01<00:00, 47.3MB/s]\n","[INFO|hub.py:587] 2022-04-06 10:40:18,926 >> storing https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|hub.py:595] 2022-04-06 10:40:18,926 >> creating metadata file for /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1772] 2022-04-06 10:40:18,927 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[WARNING|modeling_utils.py:2050] 2022-04-06 10:40:19,038 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']\n","- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-06 10:40:19,038 >> Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/06/2022 10:40:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-246830930b936e74.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:56<00:00,  2.34ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/06/2022 10:41:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-c9a2cade0400b8f1.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:17<00:00,  6.43s/ba]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 10:42:36,266 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 10:42:36,266 >>   Num examples = 130559\n","[INFO|trainer.py:1292] 2022-04-06 10:42:36,266 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-06 10:42:36,266 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1294] 2022-04-06 10:42:36,266 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1295] 2022-04-06 10:42:36,266 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 10:42:36,266 >>   Total optimization steps = 16320\n","{'loss': 1.9029, 'learning_rate': 1.9387254901960785e-05, 'epoch': 0.06}\n","  3% 500/16320 [07:32<3:58:16,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 10:50:09,026 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:50:09,032 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:50:09,153 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:50:09,157 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:50:09,161 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 1.4627, 'learning_rate': 1.877450980392157e-05, 'epoch': 0.12}\n","  6% 1000/16320 [15:06<3:51:12,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:57:42,441 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 10:57:42,447 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:57:42,569 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:57:42,573 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:57:42,576 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.3359, 'learning_rate': 1.8161764705882355e-05, 'epoch': 0.18}\n","  9% 1500/16320 [22:39<3:43:38,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:05:15,844 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 11:05:15,849 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:05:15,961 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:05:15,964 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:05:15,967 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.3057, 'learning_rate': 1.7549019607843138e-05, 'epoch': 0.25}\n"," 12% 2000/16320 [30:13<3:36:15,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:12:49,374 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 11:12:49,380 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:12:49,500 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:12:49,503 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:12:49,507 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.2631, 'learning_rate': 1.693627450980392e-05, 'epoch': 0.31}\n"," 15% 2500/16320 [37:46<3:28:47,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:20:23,235 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 11:20:23,240 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:20:23,356 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:20:23,360 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:20:23,363 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.2238, 'learning_rate': 1.6323529411764708e-05, 'epoch': 0.37}\n"," 18% 3000/16320 [45:20<3:21:07,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:27:57,112 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 11:27:57,117 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:27:57,244 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:27:57,248 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:27:57,250 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.1697, 'learning_rate': 1.571078431372549e-05, 'epoch': 0.43}\n"," 21% 3500/16320 [52:54<3:13:42,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:35:31,003 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 11:35:31,008 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:35:31,122 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:35:31,126 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:35:31,128 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.1625, 'learning_rate': 1.5098039215686276e-05, 'epoch': 0.49}\n"," 25% 4000/16320 [1:00:28<3:06:23,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:43:04,896 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 11:43:04,900 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:43:05,014 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:43:05,018 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:43:05,021 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.1314, 'learning_rate': 1.448529411764706e-05, 'epoch': 0.55}\n"," 28% 4500/16320 [1:08:02<2:58:39,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:50:39,015 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 11:50:39,021 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:50:39,153 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:50:39,156 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:50:39,175 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.1249, 'learning_rate': 1.3872549019607844e-05, 'epoch': 0.61}\n"," 31% 5000/16320 [1:15:36<2:51:00,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:58:13,100 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 11:58:13,105 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:58:13,221 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:58:13,241 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:58:13,245 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.1131, 'learning_rate': 1.3259803921568627e-05, 'epoch': 0.67}\n"," 34% 5500/16320 [1:23:10<2:43:37,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 12:05:47,068 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 12:05:47,073 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:05:47,185 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:05:47,205 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:05:47,207 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.0999, 'learning_rate': 1.2647058823529412e-05, 'epoch': 0.74}\n"," 37% 6000/16320 [1:30:44<2:36:15,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 12:13:20,982 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 12:13:20,987 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:13:21,115 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:13:21,118 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:13:21,121 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.0804, 'learning_rate': 1.2034313725490197e-05, 'epoch': 0.8}\n"," 40% 6500/16320 [1:38:18<2:28:13,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 12:20:54,891 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 12:20:54,896 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:20:55,021 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:20:55,025 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:20:55,028 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.07, 'learning_rate': 1.142156862745098e-05, 'epoch': 0.86}\n"," 43% 7000/16320 [1:45:52<2:20:41,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 12:28:28,812 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 12:28:28,817 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:28:28,930 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:28:28,934 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:28:28,936 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.0171, 'learning_rate': 1.0808823529411765e-05, 'epoch': 0.92}\n"," 46% 7500/16320 [1:53:26<2:13:18,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 12:36:02,851 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 12:36:02,857 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:36:02,975 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:36:02,979 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:36:02,982 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 1.0464, 'learning_rate': 1.0196078431372549e-05, 'epoch': 0.98}\n"," 49% 8000/16320 [2:01:00<2:05:42,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 12:43:36,707 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 12:43:36,715 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:43:36,833 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:43:36,838 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:43:36,841 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.8966, 'learning_rate': 9.583333333333335e-06, 'epoch': 1.04}\n"," 52% 8500/16320 [2:08:34<1:58:17,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 12:51:10,409 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 12:51:10,416 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:51:10,532 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:51:10,536 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:51:10,539 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.8214, 'learning_rate': 8.970588235294119e-06, 'epoch': 1.1}\n"," 55% 9000/16320 [2:16:08<1:50:37,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 12:58:44,408 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 12:58:44,415 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:58:44,528 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:58:44,532 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:58:44,534 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.8158, 'learning_rate': 8.357843137254903e-06, 'epoch': 1.16}\n"," 58% 9500/16320 [2:23:42<1:43:05,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:06:18,511 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 13:06:18,516 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:06:18,630 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:06:18,633 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:06:18,636 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.8139, 'learning_rate': 7.745098039215687e-06, 'epoch': 1.23}\n"," 61% 10000/16320 [2:31:16<1:35:30,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:13:52,520 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 13:13:52,525 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:13:52,633 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:13:52,637 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:13:52,640 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.8004, 'learning_rate': 7.132352941176472e-06, 'epoch': 1.29}\n"," 64% 10500/16320 [2:38:50<1:27:54,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:21:26,419 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 13:21:26,424 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:21:26,535 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:21:26,540 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:21:26,543 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.813, 'learning_rate': 6.519607843137256e-06, 'epoch': 1.35}\n"," 67% 11000/16320 [2:46:24<1:20:35,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:29:00,594 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-06 13:29:00,599 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:29:00,719 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:29:00,723 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:29:00,726 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.7944, 'learning_rate': 5.90686274509804e-06, 'epoch': 1.41}\n"," 70% 11500/16320 [2:53:58<1:12:51,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:36:34,664 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-11500\n","[INFO|configuration_utils.py:441] 2022-04-06 13:36:34,669 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:36:34,783 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:36:34,787 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:36:34,790 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.7985, 'learning_rate': 5.294117647058824e-06, 'epoch': 1.47}\n"," 74% 12000/16320 [3:01:32<1:05:26,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:44:08,546 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-12000\n","[INFO|configuration_utils.py:441] 2022-04-06 13:44:08,551 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:44:08,663 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:44:08,667 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:44:08,670 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.8014, 'learning_rate': 4.681372549019608e-06, 'epoch': 1.53}\n"," 77% 12500/16320 [3:09:06<57:44,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:51:42,449 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-12500\n","[INFO|configuration_utils.py:441] 2022-04-06 13:51:42,454 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:51:42,572 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:51:42,575 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:51:42,578 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.7905, 'learning_rate': 4.068627450980392e-06, 'epoch': 1.59}\n"," 80% 13000/16320 [3:16:39<50:04,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 13:59:16,142 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-13000\n","[INFO|configuration_utils.py:441] 2022-04-06 13:59:16,147 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:59:16,263 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:59:16,267 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:59:16,270 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.7753, 'learning_rate': 3.4558823529411766e-06, 'epoch': 1.65}\n"," 83% 13500/16320 [3:24:13<42:37,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 14:06:49,783 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-13500\n","[INFO|configuration_utils.py:441] 2022-04-06 14:06:49,788 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:06:49,900 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:06:49,904 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:06:49,907 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.7796, 'learning_rate': 2.843137254901961e-06, 'epoch': 1.72}\n"," 86% 14000/16320 [3:31:47<35:02,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 14:14:23,418 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-14000\n","[INFO|configuration_utils.py:441] 2022-04-06 14:14:23,423 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:14:23,549 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:14:23,552 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:14:23,557 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.7579, 'learning_rate': 2.2303921568627456e-06, 'epoch': 1.78}\n"," 89% 14500/16320 [3:39:20<27:29,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 14:21:56,995 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-14500\n","[INFO|configuration_utils.py:441] 2022-04-06 14:21:57,000 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:21:57,113 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:21:57,117 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:21:57,119 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.7569, 'learning_rate': 1.6176470588235297e-06, 'epoch': 1.84}\n"," 92% 15000/16320 [3:46:54<19:56,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 14:29:30,657 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-15000\n","[INFO|configuration_utils.py:441] 2022-04-06 14:29:30,662 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:29:30,775 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:29:30,779 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:29:30,781 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.7521, 'learning_rate': 1.0049019607843138e-06, 'epoch': 1.9}\n"," 95% 15500/16320 [3:54:28<12:22,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 14:37:04,441 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-15500\n","[INFO|configuration_utils.py:441] 2022-04-06 14:37:04,446 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:37:04,560 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:37:04,564 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:37:04,567 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.7469, 'learning_rate': 3.921568627450981e-07, 'epoch': 1.96}\n"," 98% 16000/16320 [4:02:01<04:50,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 14:44:38,231 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-16000\n","[INFO|configuration_utils.py:441] 2022-04-06 14:44:38,237 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:44:38,354 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:44:38,358 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:44:38,361 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/checkpoint-16000/special_tokens_map.json\n","100% 16320/16320 [4:06:52<00:00,  1.12it/s][INFO|trainer.py:1530] 2022-04-06 14:49:28,713 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 14812.4469, 'train_samples_per_second': 17.628, 'train_steps_per_second': 1.102, 'train_loss': 1.0021039074542475, 'epoch': 2.0}\n","100% 16320/16320 [4:06:52<00:00,  1.10it/s]\n","[INFO|trainer.py:2166] 2022-04-06 14:49:28,717 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug\n","[INFO|configuration_utils.py:441] 2022-04-06 14:49:28,722 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:49:28,837 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:49:28,841 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:49:28,844 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     1.0021\n","  train_runtime            = 4:06:52.44\n","  train_samples            =     130559\n","  train_samples_per_second =     17.628\n","  train_steps_per_second   =      1.102\n","04/06/2022 14:49:28 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 14:49:28,870 >> The following columns in the evaluation set  don't have a corresponding argument in `AlbertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `AlbertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 14:49:28,872 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 14:49:28,872 >>   Num examples = 11968\n","[INFO|trainer.py:2421] 2022-04-06 14:49:28,872 >>   Batch size = 8\n","100% 1496/1496 [04:19<00:00,  5.75it/s]04/06/2022 14:54:04 - INFO - utils_qa - Post-processing 11873 example predictions split into 11968 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 38/11873 [00:00<00:31, 375.49it/s]\u001b[A\n","  1% 76/11873 [00:00<00:31, 373.55it/s]\u001b[A\n","  1% 115/11873 [00:00<00:31, 377.21it/s]\u001b[A\n","  1% 156/11873 [00:00<00:30, 388.24it/s]\u001b[A\n","  2% 199/11873 [00:00<00:29, 399.75it/s]\u001b[A\n","  2% 241/11873 [00:00<00:28, 404.66it/s]\u001b[A\n","  2% 284/11873 [00:00<00:28, 411.26it/s]\u001b[A\n","  3% 326/11873 [00:00<00:28, 408.76it/s]\u001b[A\n","  3% 367/11873 [00:00<00:28, 407.80it/s]\u001b[A\n","  3% 410/11873 [00:01<00:27, 414.28it/s]\u001b[A\n","  4% 452/11873 [00:01<00:27, 413.25it/s]\u001b[A\n","  4% 494/11873 [00:01<00:27, 410.72it/s]\u001b[A\n","  5% 536/11873 [00:01<00:27, 406.19it/s]\u001b[A\n","  5% 577/11873 [00:01<00:28, 402.34it/s]\u001b[A\n","  5% 619/11873 [00:01<00:27, 407.32it/s]\u001b[A\n","  6% 660/11873 [00:01<00:27, 403.35it/s]\u001b[A\n","  6% 703/11873 [00:01<00:27, 408.93it/s]\u001b[A\n","  6% 744/11873 [00:01<00:27, 408.43it/s]\u001b[A\n","  7% 785/11873 [00:01<00:27, 405.48it/s]\u001b[A\n","  7% 828/11873 [00:02<00:26, 411.16it/s]\u001b[A\n","  7% 872/11873 [00:02<00:26, 417.90it/s]\u001b[A\n","  8% 916/11873 [00:02<00:25, 422.66it/s]\u001b[A\n","  8% 961/11873 [00:02<00:25, 427.92it/s]\u001b[A\n","  8% 1004/11873 [00:02<00:26, 409.77it/s]\u001b[A\n","  9% 1046/11873 [00:02<00:29, 367.01it/s]\u001b[A\n","  9% 1084/11873 [00:02<00:31, 347.95it/s]\u001b[A\n","  9% 1120/11873 [00:02<00:32, 333.27it/s]\u001b[A\n"," 10% 1154/11873 [00:02<00:34, 314.78it/s]\u001b[A\n"," 10% 1186/11873 [00:03<00:34, 314.12it/s]\u001b[A\n"," 10% 1219/11873 [00:03<00:33, 315.91it/s]\u001b[A\n"," 11% 1251/11873 [00:03<00:33, 316.13it/s]\u001b[A\n"," 11% 1283/11873 [00:03<00:33, 316.38it/s]\u001b[A\n"," 11% 1315/11873 [00:03<00:33, 315.25it/s]\u001b[A\n"," 11% 1347/11873 [00:03<00:33, 313.05it/s]\u001b[A\n"," 12% 1379/11873 [00:03<00:33, 310.51it/s]\u001b[A\n"," 12% 1412/11873 [00:03<00:33, 313.56it/s]\u001b[A\n"," 12% 1444/11873 [00:03<00:33, 312.03it/s]\u001b[A\n"," 12% 1476/11873 [00:04<00:33, 312.91it/s]\u001b[A\n"," 13% 1508/11873 [00:04<00:32, 314.13it/s]\u001b[A\n"," 13% 1540/11873 [00:04<00:33, 311.74it/s]\u001b[A\n"," 13% 1572/11873 [00:04<00:32, 313.63it/s]\u001b[A\n"," 14% 1604/11873 [00:04<00:32, 313.96it/s]\u001b[A\n"," 14% 1636/11873 [00:04<00:32, 312.72it/s]\u001b[A\n"," 14% 1668/11873 [00:04<00:33, 307.38it/s]\u001b[A\n"," 14% 1699/11873 [00:04<00:33, 305.62it/s]\u001b[A\n"," 15% 1730/11873 [00:04<00:33, 304.77it/s]\u001b[A\n"," 15% 1762/11873 [00:04<00:32, 306.80it/s]\u001b[A\n"," 15% 1793/11873 [00:05<00:33, 302.77it/s]\u001b[A\n"," 15% 1824/11873 [00:05<00:34, 292.89it/s]\u001b[A\n"," 16% 1854/11873 [00:05<00:34, 288.18it/s]\u001b[A\n"," 16% 1885/11873 [00:05<00:34, 292.78it/s]\u001b[A\n"," 16% 1916/11873 [00:05<00:33, 297.58it/s]\u001b[A\n"," 16% 1946/11873 [00:05<00:33, 293.48it/s]\u001b[A\n"," 17% 1976/11873 [00:05<00:33, 292.11it/s]\u001b[A\n"," 17% 2008/11873 [00:05<00:32, 299.03it/s]\u001b[A\n"," 17% 2040/11873 [00:05<00:32, 303.68it/s]\u001b[A\n"," 17% 2073/11873 [00:05<00:31, 307.08it/s]\u001b[A\n"," 18% 2105/11873 [00:06<00:31, 310.21it/s]\u001b[A\n"," 18% 2137/11873 [00:06<00:31, 310.29it/s]\u001b[A\n"," 18% 2169/11873 [00:06<00:31, 309.01it/s]\u001b[A\n"," 19% 2201/11873 [00:06<00:31, 311.37it/s]\u001b[A\n"," 19% 2233/11873 [00:06<00:30, 312.79it/s]\u001b[A\n"," 19% 2265/11873 [00:06<00:30, 310.19it/s]\u001b[A\n"," 19% 2297/11873 [00:06<00:31, 308.19it/s]\u001b[A\n"," 20% 2329/11873 [00:06<00:30, 309.91it/s]\u001b[A\n"," 20% 2361/11873 [00:06<00:30, 311.21it/s]\u001b[A\n"," 20% 2393/11873 [00:07<00:30, 313.22it/s]\u001b[A\n"," 20% 2425/11873 [00:07<00:31, 303.61it/s]\u001b[A\n"," 21% 2456/11873 [00:07<00:31, 301.10it/s]\u001b[A\n"," 21% 2488/11873 [00:07<00:30, 303.77it/s]\u001b[A\n"," 21% 2520/11873 [00:07<00:30, 305.85it/s]\u001b[A\n"," 21% 2552/11873 [00:07<00:30, 308.36it/s]\u001b[A\n"," 22% 2584/11873 [00:07<00:29, 310.37it/s]\u001b[A\n"," 22% 2616/11873 [00:07<00:29, 311.71it/s]\u001b[A\n"," 22% 2648/11873 [00:07<00:29, 312.24it/s]\u001b[A\n"," 23% 2680/11873 [00:07<00:29, 311.66it/s]\u001b[A\n"," 23% 2712/11873 [00:08<00:29, 313.37it/s]\u001b[A\n"," 23% 2744/11873 [00:08<00:30, 302.53it/s]\u001b[A\n"," 23% 2775/11873 [00:08<00:30, 300.24it/s]\u001b[A\n"," 24% 2806/11873 [00:08<00:30, 301.28it/s]\u001b[A\n"," 24% 2837/11873 [00:08<00:29, 303.34it/s]\u001b[A\n"," 24% 2870/11873 [00:08<00:29, 308.33it/s]\u001b[A\n"," 24% 2901/11873 [00:08<00:29, 306.35it/s]\u001b[A\n"," 25% 2932/11873 [00:08<00:29, 307.15it/s]\u001b[A\n"," 25% 2964/11873 [00:08<00:28, 309.77it/s]\u001b[A\n"," 25% 2995/11873 [00:08<00:29, 305.27it/s]\u001b[A\n"," 25% 3026/11873 [00:09<00:29, 302.68it/s]\u001b[A\n"," 26% 3057/11873 [00:09<00:29, 296.75it/s]\u001b[A\n"," 26% 3087/11873 [00:09<00:30, 290.79it/s]\u001b[A\n"," 26% 3117/11873 [00:09<00:33, 264.15it/s]\u001b[A\n"," 26% 3144/11873 [00:09<00:32, 265.36it/s]\u001b[A\n"," 27% 3171/11873 [00:09<00:34, 249.13it/s]\u001b[A\n"," 27% 3203/11873 [00:09<00:32, 267.01it/s]\u001b[A\n"," 27% 3235/11873 [00:09<00:30, 279.96it/s]\u001b[A\n"," 28% 3266/11873 [00:09<00:29, 287.28it/s]\u001b[A\n"," 28% 3296/11873 [00:10<00:36, 232.59it/s]\u001b[A\n"," 28% 3322/11873 [00:10<00:38, 219.34it/s]\u001b[A\n"," 28% 3350/11873 [00:10<00:36, 230.79it/s]\u001b[A\n"," 28% 3375/11873 [00:10<00:40, 212.09it/s]\u001b[A\n"," 29% 3405/11873 [00:10<00:36, 232.45it/s]\u001b[A\n"," 29% 3435/11873 [00:10<00:33, 249.43it/s]\u001b[A\n"," 29% 3465/11873 [00:10<00:32, 262.48it/s]\u001b[A\n"," 29% 3496/11873 [00:10<00:30, 275.05it/s]\u001b[A\n"," 30% 3528/11873 [00:11<00:29, 286.69it/s]\u001b[A\n"," 30% 3560/11873 [00:11<00:28, 294.46it/s]\u001b[A\n"," 30% 3592/11873 [00:11<00:27, 301.78it/s]\u001b[A\n"," 31% 3625/11873 [00:11<00:26, 308.26it/s]\u001b[A\n"," 31% 3657/11873 [00:11<00:26, 309.31it/s]\u001b[A\n"," 31% 3689/11873 [00:11<00:26, 305.61it/s]\u001b[A\n"," 31% 3720/11873 [00:11<00:27, 300.51it/s]\u001b[A\n"," 32% 3753/11873 [00:11<00:26, 306.39it/s]\u001b[A\n"," 32% 3784/11873 [00:11<00:26, 303.73it/s]\u001b[A\n"," 32% 3815/11873 [00:11<00:26, 305.12it/s]\u001b[A\n"," 32% 3846/11873 [00:12<00:26, 304.03it/s]\u001b[A\n"," 33% 3877/11873 [00:12<00:26, 300.77it/s]\u001b[A\n"," 33% 3908/11873 [00:12<00:26, 300.40it/s]\u001b[A\n"," 33% 3939/11873 [00:12<00:26, 294.78it/s]\u001b[A\n"," 33% 3969/11873 [00:12<00:27, 292.29it/s]\u001b[A\n"," 34% 3999/11873 [00:12<00:26, 292.84it/s]\u001b[A\n"," 34% 4031/11873 [00:12<00:26, 298.24it/s]\u001b[A\n"," 34% 4063/11873 [00:12<00:25, 303.50it/s]\u001b[A\n"," 34% 4095/11873 [00:12<00:25, 307.31it/s]\u001b[A\n"," 35% 4127/11873 [00:13<00:24, 309.90it/s]\u001b[A\n"," 35% 4159/11873 [00:13<00:25, 306.04it/s]\u001b[A\n"," 35% 4190/11873 [00:13<00:25, 301.73it/s]\u001b[A\n"," 36% 4222/11873 [00:13<00:25, 304.86it/s]\u001b[A\n"," 36% 4255/11873 [00:13<00:24, 309.87it/s]\u001b[A\n"," 36% 4287/11873 [00:13<00:24, 311.92it/s]\u001b[A\n"," 36% 4320/11873 [00:13<00:23, 314.74it/s]\u001b[A\n"," 37% 4352/11873 [00:13<00:24, 312.38it/s]\u001b[A\n"," 37% 4384/11873 [00:13<00:23, 313.64it/s]\u001b[A\n"," 37% 4416/11873 [00:13<00:25, 294.46it/s]\u001b[A\n"," 37% 4446/11873 [00:14<00:26, 276.57it/s]\u001b[A\n"," 38% 4479/11873 [00:14<00:25, 288.76it/s]\u001b[A\n"," 38% 4510/11873 [00:14<00:25, 294.31it/s]\u001b[A\n"," 38% 4542/11873 [00:14<00:24, 299.36it/s]\u001b[A\n"," 39% 4574/11873 [00:14<00:24, 303.92it/s]\u001b[A\n"," 39% 4606/11873 [00:14<00:23, 308.12it/s]\u001b[A\n"," 39% 4638/11873 [00:14<00:23, 308.98it/s]\u001b[A\n"," 39% 4669/11873 [00:14<00:23, 308.04it/s]\u001b[A\n"," 40% 4701/11873 [00:14<00:23, 308.82it/s]\u001b[A\n"," 40% 4733/11873 [00:15<00:23, 310.20it/s]\u001b[A\n"," 40% 4765/11873 [00:15<00:22, 311.55it/s]\u001b[A\n"," 40% 4797/11873 [00:15<00:23, 303.05it/s]\u001b[A\n"," 41% 4828/11873 [00:15<00:23, 304.81it/s]\u001b[A\n"," 41% 4859/11873 [00:15<00:23, 304.58it/s]\u001b[A\n"," 41% 4890/11873 [00:15<00:22, 306.16it/s]\u001b[A\n"," 41% 4923/11873 [00:15<00:22, 310.46it/s]\u001b[A\n"," 42% 4955/11873 [00:15<00:22, 302.17it/s]\u001b[A\n"," 42% 4987/11873 [00:15<00:22, 304.52it/s]\u001b[A\n"," 42% 5018/11873 [00:15<00:22, 303.61it/s]\u001b[A\n"," 43% 5049/11873 [00:16<00:22, 301.03it/s]\u001b[A\n"," 43% 5081/11873 [00:16<00:22, 305.94it/s]\u001b[A\n"," 43% 5113/11873 [00:16<00:21, 307.28it/s]\u001b[A\n"," 43% 5144/11873 [00:16<00:22, 304.38it/s]\u001b[A\n"," 44% 5175/11873 [00:16<00:22, 297.97it/s]\u001b[A\n"," 44% 5205/11873 [00:16<00:22, 298.27it/s]\u001b[A\n"," 44% 5235/11873 [00:16<00:22, 292.67it/s]\u001b[A\n"," 44% 5265/11873 [00:16<00:24, 264.64it/s]\u001b[A\n"," 45% 5296/11873 [00:16<00:23, 275.57it/s]\u001b[A\n"," 45% 5328/11873 [00:17<00:22, 285.83it/s]\u001b[A\n"," 45% 5359/11873 [00:17<00:22, 292.39it/s]\u001b[A\n"," 45% 5391/11873 [00:17<00:21, 299.21it/s]\u001b[A\n"," 46% 5423/11873 [00:17<00:21, 305.19it/s]\u001b[A\n"," 46% 5454/11873 [00:17<00:21, 304.29it/s]\u001b[A\n"," 46% 5485/11873 [00:17<00:21, 303.32it/s]\u001b[A\n"," 46% 5518/11873 [00:17<00:20, 308.95it/s]\u001b[A\n"," 47% 5549/11873 [00:17<00:20, 307.23it/s]\u001b[A\n"," 47% 5580/11873 [00:17<00:20, 307.40it/s]\u001b[A\n"," 47% 5611/11873 [00:17<00:20, 305.98it/s]\u001b[A\n"," 48% 5642/11873 [00:18<00:20, 302.17it/s]\u001b[A\n"," 48% 5674/11873 [00:18<00:20, 304.79it/s]\u001b[A\n"," 48% 5705/11873 [00:18<00:20, 293.80it/s]\u001b[A\n"," 48% 5735/11873 [00:18<00:21, 289.69it/s]\u001b[A\n"," 49% 5765/11873 [00:18<00:21, 284.19it/s]\u001b[A\n"," 49% 5794/11873 [00:18<00:21, 284.97it/s]\u001b[A\n"," 49% 5823/11873 [00:18<00:21, 286.17it/s]\u001b[A\n"," 49% 5852/11873 [00:18<00:21, 285.18it/s]\u001b[A\n"," 50% 5881/11873 [00:18<00:21, 280.07it/s]\u001b[A\n"," 50% 5913/11873 [00:18<00:20, 289.78it/s]\u001b[A\n"," 50% 5944/11873 [00:19<00:20, 295.58it/s]\u001b[A\n"," 50% 5976/11873 [00:19<00:19, 301.11it/s]\u001b[A\n"," 51% 6008/11873 [00:19<00:19, 303.97it/s]\u001b[A\n"," 51% 6040/11873 [00:19<00:18, 308.53it/s]\u001b[A\n"," 51% 6071/11873 [00:19<00:18, 306.72it/s]\u001b[A\n"," 51% 6103/11873 [00:19<00:18, 307.93it/s]\u001b[A\n"," 52% 6134/11873 [00:19<00:18, 302.43it/s]\u001b[A\n"," 52% 6165/11873 [00:19<00:18, 304.51it/s]\u001b[A\n"," 52% 6198/11873 [00:19<00:18, 309.28it/s]\u001b[A\n"," 52% 6229/11873 [00:20<00:18, 307.98it/s]\u001b[A\n"," 53% 6261/11873 [00:20<00:18, 310.35it/s]\u001b[A\n"," 53% 6293/11873 [00:20<00:18, 308.13it/s]\u001b[A\n"," 53% 6325/11873 [00:20<00:17, 310.83it/s]\u001b[A\n"," 54% 6357/11873 [00:20<00:17, 311.12it/s]\u001b[A\n"," 54% 6389/11873 [00:20<00:17, 311.59it/s]\u001b[A\n"," 54% 6421/11873 [00:20<00:17, 305.55it/s]\u001b[A\n"," 54% 6452/11873 [00:20<00:17, 304.32it/s]\u001b[A\n"," 55% 6483/11873 [00:20<00:17, 302.08it/s]\u001b[A\n"," 55% 6514/11873 [00:20<00:17, 299.40it/s]\u001b[A\n"," 55% 6545/11873 [00:21<00:17, 300.86it/s]\u001b[A\n"," 55% 6576/11873 [00:21<00:17, 301.87it/s]\u001b[A\n"," 56% 6607/11873 [00:21<00:17, 303.32it/s]\u001b[A\n"," 56% 6639/11873 [00:21<00:17, 306.69it/s]\u001b[A\n"," 56% 6670/11873 [00:21<00:17, 304.79it/s]\u001b[A\n"," 56% 6701/11873 [00:21<00:16, 306.10it/s]\u001b[A\n"," 57% 6732/11873 [00:21<00:17, 300.55it/s]\u001b[A\n"," 57% 6764/11873 [00:21<00:16, 304.75it/s]\u001b[A\n"," 57% 6795/11873 [00:21<00:16, 303.88it/s]\u001b[A\n"," 58% 6827/11873 [00:21<00:16, 307.86it/s]\u001b[A\n"," 58% 6859/11873 [00:22<00:16, 310.16it/s]\u001b[A\n"," 58% 6891/11873 [00:22<00:16, 309.14it/s]\u001b[A\n"," 58% 6923/11873 [00:22<00:15, 310.78it/s]\u001b[A\n"," 59% 6955/11873 [00:22<00:15, 311.47it/s]\u001b[A\n"," 59% 6988/11873 [00:22<00:15, 314.02it/s]\u001b[A\n"," 59% 7020/11873 [00:22<00:15, 315.50it/s]\u001b[A\n"," 59% 7052/11873 [00:22<00:15, 309.41it/s]\u001b[A\n"," 60% 7083/11873 [00:22<00:15, 301.67it/s]\u001b[A\n"," 60% 7114/11873 [00:22<00:16, 295.87it/s]\u001b[A\n"," 60% 7145/11873 [00:23<00:15, 299.48it/s]\u001b[A\n"," 60% 7175/11873 [00:23<00:15, 298.16it/s]\u001b[A\n"," 61% 7205/11873 [00:23<00:15, 293.64it/s]\u001b[A\n"," 61% 7236/11873 [00:23<00:15, 297.07it/s]\u001b[A\n"," 61% 7267/11873 [00:23<00:15, 298.50it/s]\u001b[A\n"," 61% 7299/11873 [00:23<00:15, 304.74it/s]\u001b[A\n"," 62% 7331/11873 [00:23<00:14, 306.96it/s]\u001b[A\n"," 62% 7363/11873 [00:23<00:14, 310.24it/s]\u001b[A\n"," 62% 7395/11873 [00:23<00:14, 308.55it/s]\u001b[A\n"," 63% 7426/11873 [00:23<00:14, 306.68it/s]\u001b[A\n"," 63% 7457/11873 [00:24<00:14, 303.54it/s]\u001b[A\n"," 63% 7488/11873 [00:24<00:14, 303.71it/s]\u001b[A\n"," 63% 7520/11873 [00:24<00:14, 307.27it/s]\u001b[A\n"," 64% 7551/11873 [00:24<00:14, 302.43it/s]\u001b[A\n"," 64% 7582/11873 [00:24<00:14, 300.87it/s]\u001b[A\n"," 64% 7613/11873 [00:24<00:14, 295.94it/s]\u001b[A\n"," 64% 7643/11873 [00:24<00:14, 293.42it/s]\u001b[A\n"," 65% 7673/11873 [00:24<00:14, 294.46it/s]\u001b[A\n"," 65% 7703/11873 [00:24<00:14, 290.59it/s]\u001b[A\n"," 65% 7733/11873 [00:24<00:14, 289.76it/s]\u001b[A\n"," 65% 7763/11873 [00:25<00:14, 291.58it/s]\u001b[A\n"," 66% 7793/11873 [00:25<00:14, 288.78it/s]\u001b[A\n"," 66% 7823/11873 [00:25<00:13, 291.22it/s]\u001b[A\n"," 66% 7853/11873 [00:25<00:13, 292.70it/s]\u001b[A\n"," 66% 7883/11873 [00:25<00:14, 284.90it/s]\u001b[A\n"," 67% 7914/11873 [00:25<00:13, 291.68it/s]\u001b[A\n"," 67% 7946/11873 [00:25<00:13, 297.91it/s]\u001b[A\n"," 67% 7976/11873 [00:25<00:13, 298.30it/s]\u001b[A\n"," 67% 8008/11873 [00:25<00:12, 303.11it/s]\u001b[A\n"," 68% 8040/11873 [00:25<00:12, 307.30it/s]\u001b[A\n"," 68% 8072/11873 [00:26<00:12, 309.73it/s]\u001b[A\n"," 68% 8104/11873 [00:26<00:12, 310.76it/s]\u001b[A\n"," 69% 8136/11873 [00:26<00:12, 309.40it/s]\u001b[A\n"," 69% 8168/11873 [00:26<00:11, 310.79it/s]\u001b[A\n"," 69% 8200/11873 [00:26<00:12, 304.06it/s]\u001b[A\n"," 69% 8232/11873 [00:26<00:11, 306.15it/s]\u001b[A\n"," 70% 8263/11873 [00:26<00:11, 307.16it/s]\u001b[A\n"," 70% 8294/11873 [00:26<00:11, 306.22it/s]\u001b[A\n"," 70% 8325/11873 [00:26<00:11, 306.10it/s]\u001b[A\n"," 70% 8356/11873 [00:27<00:11, 302.19it/s]\u001b[A\n"," 71% 8388/11873 [00:27<00:11, 305.28it/s]\u001b[A\n"," 71% 8419/11873 [00:27<00:11, 302.53it/s]\u001b[A\n"," 71% 8450/11873 [00:27<00:11, 301.86it/s]\u001b[A\n"," 71% 8481/11873 [00:27<00:11, 299.30it/s]\u001b[A\n"," 72% 8513/11873 [00:27<00:11, 304.30it/s]\u001b[A\n"," 72% 8545/11873 [00:27<00:10, 306.05it/s]\u001b[A\n"," 72% 8577/11873 [00:27<00:10, 307.47it/s]\u001b[A\n"," 73% 8609/11873 [00:27<00:10, 307.93it/s]\u001b[A\n"," 73% 8640/11873 [00:27<00:10, 305.87it/s]\u001b[A\n"," 73% 8671/11873 [00:28<00:14, 214.34it/s]\u001b[A\n"," 73% 8701/11873 [00:28<00:13, 232.93it/s]\u001b[A\n"," 74% 8733/11873 [00:28<00:12, 252.28it/s]\u001b[A\n"," 74% 8763/11873 [00:28<00:11, 263.87it/s]\u001b[A\n"," 74% 8794/11873 [00:28<00:11, 274.09it/s]\u001b[A\n"," 74% 8825/11873 [00:28<00:10, 282.70it/s]\u001b[A\n"," 75% 8855/11873 [00:28<00:10, 286.66it/s]\u001b[A\n"," 75% 8885/11873 [00:28<00:10, 288.03it/s]\u001b[A\n"," 75% 8916/11873 [00:29<00:10, 291.55it/s]\u001b[A\n"," 75% 8947/11873 [00:29<00:09, 294.94it/s]\u001b[A\n"," 76% 8978/11873 [00:29<00:09, 297.39it/s]\u001b[A\n"," 76% 9008/11873 [00:29<00:09, 297.74it/s]\u001b[A\n"," 76% 9039/11873 [00:29<00:09, 299.14it/s]\u001b[A\n"," 76% 9070/11873 [00:29<00:09, 301.70it/s]\u001b[A\n"," 77% 9102/11873 [00:29<00:09, 304.81it/s]\u001b[A\n"," 77% 9133/11873 [00:29<00:08, 305.63it/s]\u001b[A\n"," 77% 9164/11873 [00:29<00:08, 305.18it/s]\u001b[A\n"," 77% 9195/11873 [00:29<00:08, 306.34it/s]\u001b[A\n"," 78% 9226/11873 [00:30<00:08, 304.37it/s]\u001b[A\n"," 78% 9257/11873 [00:30<00:08, 304.41it/s]\u001b[A\n"," 78% 9288/11873 [00:30<00:08, 304.06it/s]\u001b[A\n"," 78% 9319/11873 [00:30<00:08, 304.19it/s]\u001b[A\n"," 79% 9350/11873 [00:30<00:08, 299.89it/s]\u001b[A\n"," 79% 9381/11873 [00:30<00:08, 297.75it/s]\u001b[A\n"," 79% 9411/11873 [00:30<00:08, 296.80it/s]\u001b[A\n"," 80% 9442/11873 [00:30<00:08, 298.54it/s]\u001b[A\n"," 80% 9474/11873 [00:30<00:07, 303.91it/s]\u001b[A\n"," 80% 9506/11873 [00:30<00:07, 307.99it/s]\u001b[A\n"," 80% 9538/11873 [00:31<00:07, 311.43it/s]\u001b[A\n"," 81% 9570/11873 [00:31<00:07, 313.88it/s]\u001b[A\n"," 81% 9602/11873 [00:31<00:07, 313.20it/s]\u001b[A\n"," 81% 9634/11873 [00:31<00:07, 309.20it/s]\u001b[A\n"," 81% 9665/11873 [00:31<00:07, 304.96it/s]\u001b[A\n"," 82% 9696/11873 [00:31<00:07, 303.78it/s]\u001b[A\n"," 82% 9727/11873 [00:31<00:07, 305.38it/s]\u001b[A\n"," 82% 9760/11873 [00:31<00:06, 310.30it/s]\u001b[A\n"," 82% 9792/11873 [00:31<00:06, 310.12it/s]\u001b[A\n"," 83% 9824/11873 [00:31<00:06, 309.80it/s]\u001b[A\n"," 83% 9857/11873 [00:32<00:06, 313.40it/s]\u001b[A\n"," 83% 9889/11873 [00:32<00:06, 313.18it/s]\u001b[A\n"," 84% 9921/11873 [00:32<00:06, 312.45it/s]\u001b[A\n"," 84% 9953/11873 [00:32<00:06, 303.04it/s]\u001b[A\n"," 84% 9984/11873 [00:32<00:06, 297.61it/s]\u001b[A\n"," 84% 10014/11873 [00:32<00:06, 297.69it/s]\u001b[A\n"," 85% 10045/11873 [00:32<00:06, 298.55it/s]\u001b[A\n"," 85% 10075/11873 [00:32<00:06, 298.25it/s]\u001b[A\n"," 85% 10108/11873 [00:32<00:05, 304.61it/s]\u001b[A\n"," 85% 10140/11873 [00:33<00:05, 307.33it/s]\u001b[A\n"," 86% 10172/11873 [00:33<00:05, 309.09it/s]\u001b[A\n"," 86% 10203/11873 [00:33<00:05, 309.31it/s]\u001b[A\n"," 86% 10235/11873 [00:33<00:05, 310.05it/s]\u001b[A\n"," 86% 10267/11873 [00:33<00:05, 311.32it/s]\u001b[A\n"," 87% 10299/11873 [00:33<00:05, 307.72it/s]\u001b[A\n"," 87% 10331/11873 [00:33<00:04, 309.23it/s]\u001b[A\n"," 87% 10363/11873 [00:33<00:04, 310.16it/s]\u001b[A\n"," 88% 10395/11873 [00:33<00:04, 306.02it/s]\u001b[A\n"," 88% 10426/11873 [00:33<00:04, 298.94it/s]\u001b[A\n"," 88% 10457/11873 [00:34<00:04, 300.08it/s]\u001b[A\n"," 88% 10488/11873 [00:34<00:04, 300.79it/s]\u001b[A\n"," 89% 10519/11873 [00:34<00:04, 296.39it/s]\u001b[A\n"," 89% 10550/11873 [00:34<00:04, 298.38it/s]\u001b[A\n"," 89% 10582/11873 [00:34<00:04, 303.98it/s]\u001b[A\n"," 89% 10614/11873 [00:34<00:04, 306.22it/s]\u001b[A\n"," 90% 10645/11873 [00:34<00:04, 304.75it/s]\u001b[A\n"," 90% 10677/11873 [00:34<00:03, 307.43it/s]\u001b[A\n"," 90% 10709/11873 [00:34<00:03, 309.74it/s]\u001b[A\n"," 90% 10741/11873 [00:34<00:03, 312.64it/s]\u001b[A\n"," 91% 10773/11873 [00:35<00:03, 311.66it/s]\u001b[A\n"," 91% 10805/11873 [00:35<00:03, 307.50it/s]\u001b[A\n"," 91% 10836/11873 [00:35<00:03, 304.57it/s]\u001b[A\n"," 92% 10867/11873 [00:35<00:03, 299.20it/s]\u001b[A\n"," 92% 10897/11873 [00:35<00:03, 292.60it/s]\u001b[A\n"," 92% 10927/11873 [00:35<00:03, 285.58it/s]\u001b[A\n"," 92% 10957/11873 [00:35<00:03, 288.71it/s]\u001b[A\n"," 93% 10988/11873 [00:35<00:03, 292.88it/s]\u001b[A\n"," 93% 11020/11873 [00:35<00:02, 298.34it/s]\u001b[A\n"," 93% 11052/11873 [00:36<00:02, 302.94it/s]\u001b[A\n"," 93% 11084/11873 [00:36<00:02, 306.72it/s]\u001b[A\n"," 94% 11115/11873 [00:36<00:02, 307.11it/s]\u001b[A\n"," 94% 11147/11873 [00:36<00:02, 308.33it/s]\u001b[A\n"," 94% 11180/11873 [00:36<00:02, 312.34it/s]\u001b[A\n"," 94% 11212/11873 [00:36<00:02, 313.27it/s]\u001b[A\n"," 95% 11244/11873 [00:36<00:02, 309.89it/s]\u001b[A\n"," 95% 11276/11873 [00:36<00:01, 309.09it/s]\u001b[A\n"," 95% 11308/11873 [00:36<00:01, 310.03it/s]\u001b[A\n"," 96% 11340/11873 [00:36<00:01, 307.95it/s]\u001b[A\n"," 96% 11372/11873 [00:37<00:01, 309.38it/s]\u001b[A\n"," 96% 11403/11873 [00:37<00:01, 305.68it/s]\u001b[A\n"," 96% 11435/11873 [00:37<00:01, 307.53it/s]\u001b[A\n"," 97% 11466/11873 [00:37<00:01, 307.21it/s]\u001b[A\n"," 97% 11497/11873 [00:37<00:01, 307.57it/s]\u001b[A\n"," 97% 11528/11873 [00:37<00:01, 308.24it/s]\u001b[A\n"," 97% 11559/11873 [00:37<00:01, 307.47it/s]\u001b[A\n"," 98% 11592/11873 [00:37<00:00, 311.19it/s]\u001b[A\n"," 98% 11624/11873 [00:37<00:00, 309.08it/s]\u001b[A\n"," 98% 11655/11873 [00:37<00:00, 305.39it/s]\u001b[A\n"," 98% 11686/11873 [00:38<00:00, 303.03it/s]\u001b[A\n"," 99% 11718/11873 [00:38<00:00, 305.71it/s]\u001b[A\n"," 99% 11749/11873 [00:38<00:00, 306.69it/s]\u001b[A\n"," 99% 11781/11873 [00:38<00:00, 309.66it/s]\u001b[A\n"," 99% 11812/11873 [00:38<00:00, 306.07it/s]\u001b[A\n","100% 11873/11873 [00:38<00:00, 306.74it/s]\n","04/06/2022 14:54:42 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/eval_predictions.json.\n","04/06/2022 14:54:42 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/eval_nbest_predictions.json.\n","04/06/2022 14:54:45 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/albert-base-v2/synonym-aug/eval_null_odds.json.\n","04/06/2022 14:54:48 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1496/1496 [05:19<00:00,  4.69it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      =  76.805\n","  eval_HasAns_f1         =  82.835\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 77.1573\n","  eval_NoAns_f1          = 77.1573\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 76.9898\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 80.0005\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 76.9814\n","  eval_f1                = 79.9921\n","  eval_samples           =   11968\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 14:54:48,249 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_synonym_aug', 'type': 'sichenzhong/squad_v2_synonym_aug', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path roberta-base \\\n","  --dataset_name sichenzhong/squad_v2_synonym_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug"],"metadata":{"id":"x-58VmCEfZxU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649267293057,"user_tz":240,"elapsed":10403699,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"d6677da2-fa82-4814-8819-2572178b8223"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["04/06/2022 14:54:53 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 14:54:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/runs/Apr06_14-54-53_2baca14bc166,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 14:54:53 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b\n","04/06/2022 14:54:53 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/06/2022 14:54:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","04/06/2022 14:54:53 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","04/06/2022 14:54:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","100% 2/2 [00:00<00:00, 684.95it/s]\n","[INFO|hub.py:583] 2022-04-06 14:54:53,897 >> https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8vfu7d34\n","Downloading: 100% 481/481 [00:00<00:00, 839kB/s]\n","[INFO|hub.py:587] 2022-04-06 14:54:54,001 >> storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|hub.py:595] 2022-04-06 14:54:54,001 >> creating metadata file for /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:654] 2022-04-06 14:54:54,001 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-06 14:54:54,002 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-06 14:54:54,101 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-06 14:54:54,199 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-06 14:54:54,199 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 14:54:54,413 >> https://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1ny65yvp\n","Downloading: 100% 878k/878k [00:00<00:00, 9.61MB/s]\n","[INFO|hub.py:587] 2022-04-06 14:54:54,627 >> storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|hub.py:595] 2022-04-06 14:54:54,627 >> creating metadata file for /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|hub.py:583] 2022-04-06 14:54:54,720 >> https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_s4bey5l\n","Downloading: 100% 446k/446k [00:00<00:00, 4.67MB/s]\n","[INFO|hub.py:587] 2022-04-06 14:54:54,919 >> storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|hub.py:595] 2022-04-06 14:54:54,919 >> creating metadata file for /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|hub.py:583] 2022-04-06 14:54:55,017 >> https://huggingface.co/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp81aoe7n8\n","Downloading: 100% 1.29M/1.29M [00:00<00:00, 11.1MB/s]\n","[INFO|hub.py:587] 2022-04-06 14:54:55,250 >> storing https://huggingface.co/roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|hub.py:595] 2022-04-06 14:54:55,250 >> creating metadata file for /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:54:55,548 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:54:55,548 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:54:55,549 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:54:55,549 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:54:55,549 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:54:55,549 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-06 14:54:55,647 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-06 14:54:55,648 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 14:54:55,833 >> https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbsg8_jam\n","Downloading: 100% 478M/478M [00:06<00:00, 72.1MB/s]\n","[INFO|hub.py:587] 2022-04-06 14:55:02,893 >> storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[INFO|hub.py:595] 2022-04-06 14:55:02,893 >> creating metadata file for /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[INFO|modeling_utils.py:1772] 2022-04-06 14:55:02,893 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[WARNING|modeling_utils.py:2050] 2022-04-06 14:55:04,096 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-06 14:55:04,097 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/06/2022 14:55:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-6e06f107936cde3a.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:40<00:00,  3.25ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/06/2022 14:55:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_synonym_aug-f9116297b5a4192b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-ae074e006b983b7d.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:03<00:00,  5.26s/ba]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 14:56:51,657 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 14:56:51,657 >>   Num examples = 131852\n","[INFO|trainer.py:1292] 2022-04-06 14:56:51,657 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-06 14:56:51,657 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1294] 2022-04-06 14:56:51,657 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1295] 2022-04-06 14:56:51,657 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 14:56:51,657 >>   Total optimization steps = 10988\n","{'loss': 1.9506, 'learning_rate': 3.8179832544594105e-05, 'epoch': 0.09}\n","  5% 500/10988 [07:32<2:38:09,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 15:04:24,415 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:04:24,420 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:04:25,649 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:04:25,653 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:04:26,569 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 1.4486, 'learning_rate': 3.6359665089188207e-05, 'epoch': 0.18}\n","  9% 1000/10988 [15:10<2:30:58,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 15:12:02,202 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:12:02,208 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:12:03,498 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:12:03,503 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:12:03,507 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.3385, 'learning_rate': 3.4539497633782315e-05, 'epoch': 0.27}\n"," 14% 1500/10988 [22:47<2:23:09,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 15:19:39,343 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:19:39,348 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:19:40,577 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:19:40,581 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:19:40,584 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.2824, 'learning_rate': 3.271933017837641e-05, 'epoch': 0.36}\n"," 18% 2000/10988 [30:24<2:15:28,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 15:27:16,232 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:27:16,237 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:27:17,438 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:27:17,442 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:27:17,446 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.2293, 'learning_rate': 3.089916272297051e-05, 'epoch': 0.46}\n"," 23% 2500/10988 [38:02<2:08:05,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 15:34:54,243 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:34:54,248 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:34:55,789 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:34:56,352 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:34:56,355 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.1916, 'learning_rate': 2.9078995267564617e-05, 'epoch': 0.55}\n"," 27% 3000/10988 [45:40<2:00:39,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 15:42:31,764 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:42:31,768 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:42:32,960 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:42:32,964 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:42:32,968 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.1445, 'learning_rate': 2.725882781215872e-05, 'epoch': 0.64}\n"," 32% 3500/10988 [53:16<1:52:56,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 15:50:08,179 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:50:08,183 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:50:09,405 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:50:09,415 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:50:09,419 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.1104, 'learning_rate': 2.5438660356752823e-05, 'epoch': 0.73}\n"," 36% 4000/10988 [1:00:56<1:45:09,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 15:57:48,329 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:57:48,334 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:57:49,583 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:57:49,588 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:57:49,591 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.0877, 'learning_rate': 2.3618492901346928e-05, 'epoch': 0.82}\n"," 41% 4500/10988 [1:08:34<1:37:39,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 16:05:25,871 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:05:25,878 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:05:27,103 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:05:27,108 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:05:27,112 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.0658, 'learning_rate': 2.1798325445941027e-05, 'epoch': 0.91}\n"," 46% 5000/10988 [1:16:10<1:30:18,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 16:13:02,598 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:13:02,603 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:13:03,828 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:13:03,832 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:13:03,836 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.0369, 'learning_rate': 1.997815799053513e-05, 'epoch': 1.0}\n"," 50% 5500/10988 [1:23:47<1:22:29,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 16:20:39,300 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:20:39,306 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:20:40,517 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:20:40,521 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:20:40,524 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.8266, 'learning_rate': 1.8157990535129233e-05, 'epoch': 1.09}\n"," 55% 6000/10988 [1:31:27<1:15:19,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 16:28:19,185 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:28:19,190 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:28:20,410 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:28:20,414 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:28:20,417 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.8053, 'learning_rate': 1.6337823079723335e-05, 'epoch': 1.18}\n"," 59% 6500/10988 [1:39:05<1:07:53,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 16:35:57,629 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:35:57,635 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:35:58,824 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:35:58,828 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:35:58,832 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.8049, 'learning_rate': 1.451765562431744e-05, 'epoch': 1.27}\n"," 64% 7000/10988 [1:46:43<1:00:10,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 16:43:34,836 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:43:34,841 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:43:36,627 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:43:36,631 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:43:36,634 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.8002, 'learning_rate': 1.269748816891154e-05, 'epoch': 1.37}\n"," 68% 7500/10988 [1:54:20<52:47,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 16:51:12,534 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:51:12,539 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:51:13,772 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:51:13,777 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:51:13,780 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.7772, 'learning_rate': 1.0877320713505643e-05, 'epoch': 1.46}\n"," 73% 8000/10988 [2:02:01<45:03,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 16:58:53,314 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:58:53,319 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:58:54,576 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:58:54,580 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:58:55,524 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.7798, 'learning_rate': 9.057153258099745e-06, 'epoch': 1.55}\n"," 77% 8500/10988 [2:09:39<37:28,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 17:06:31,355 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:06:31,360 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:06:32,505 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:06:32,508 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:06:32,511 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.7512, 'learning_rate': 7.236985802693849e-06, 'epoch': 1.64}\n"," 82% 9000/10988 [2:17:16<30:05,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 17:14:08,253 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:14:08,258 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:14:09,426 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:14:09,430 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:14:09,433 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.7583, 'learning_rate': 5.416818347287951e-06, 'epoch': 1.73}\n"," 86% 9500/10988 [2:24:53<22:28,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 17:21:44,959 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:21:44,964 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:21:46,127 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:21:46,131 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:21:46,134 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.7599, 'learning_rate': 3.5966508918820536e-06, 'epoch': 1.82}\n"," 91% 10000/10988 [2:32:31<14:56,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 17:29:23,675 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:29:23,680 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:29:24,847 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:29:25,622 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:29:25,798 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.7454, 'learning_rate': 1.776483436476156e-06, 'epoch': 1.91}\n"," 96% 10500/10988 [2:40:09<07:23,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 17:37:01,199 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:37:01,204 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:37:02,349 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:37:02,353 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:37:02,357 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/checkpoint-10500/special_tokens_map.json\n","100% 10988/10988 [2:47:35<00:00,  1.16it/s][INFO|trainer.py:1530] 2022-04-06 17:44:26,742 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10055.0873, 'train_samples_per_second': 26.226, 'train_steps_per_second': 1.093, 'train_loss': 1.0199427762637365, 'epoch': 2.0}\n","100% 10988/10988 [2:47:35<00:00,  1.09it/s]\n","[INFO|trainer.py:2166] 2022-04-06 17:44:26,749 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug\n","[INFO|configuration_utils.py:441] 2022-04-06 17:44:26,753 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:44:27,970 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:44:27,974 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:44:27,977 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     1.0199\n","  train_runtime            = 2:47:35.08\n","  train_samples            =     131852\n","  train_samples_per_second =     26.226\n","  train_steps_per_second   =      1.093\n","04/06/2022 17:44:28 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 17:44:28,115 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 17:44:28,117 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 17:44:28,117 >>   Num examples = 12165\n","[INFO|trainer.py:2421] 2022-04-06 17:44:28,117 >>   Batch size = 8\n","100% 1520/1521 [02:56<00:00,  8.68it/s]04/06/2022 17:47:36 - INFO - utils_qa - Post-processing 11873 example predictions split into 12165 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 50/11873 [00:00<00:23, 499.19it/s]\u001b[A\n","  1% 100/11873 [00:00<00:25, 465.31it/s]\u001b[A\n","  1% 153/11873 [00:00<00:23, 490.62it/s]\u001b[A\n","  2% 206/11873 [00:00<00:23, 503.66it/s]\u001b[A\n","  2% 261/11873 [00:00<00:22, 517.78it/s]\u001b[A\n","  3% 315/11873 [00:00<00:22, 524.84it/s]\u001b[A\n","  3% 368/11873 [00:00<00:22, 522.40it/s]\u001b[A\n","  4% 423/11873 [00:00<00:21, 529.14it/s]\u001b[A\n","  4% 478/11873 [00:00<00:21, 533.43it/s]\u001b[A\n","  4% 532/11873 [00:01<00:21, 525.96it/s]\u001b[A\n","  5% 585/11873 [00:01<00:21, 519.43it/s]\u001b[A\n","  5% 642/11873 [00:01<00:21, 531.96it/s]\u001b[A\n","  6% 696/11873 [00:01<00:21, 524.68it/s]\u001b[A\n","  6% 749/11873 [00:01<00:21, 521.57it/s]\u001b[A\n","  7% 802/11873 [00:01<00:21, 523.45it/s]\u001b[A\n","  7% 856/11873 [00:01<00:20, 527.70it/s]\u001b[A\n","  8% 914/11873 [00:01<00:20, 541.81it/s]\u001b[A\n","  8% 969/11873 [00:01<00:20, 544.22it/s]\u001b[A\n","  9% 1024/11873 [00:01<00:20, 516.77it/s]\u001b[A\n","  9% 1076/11873 [00:02<00:22, 477.73it/s]\u001b[A\n","  9% 1125/11873 [00:02<00:24, 446.90it/s]\u001b[A\n"," 10% 1171/11873 [00:02<00:24, 435.52it/s]\u001b[A\n"," 10% 1216/11873 [00:02<00:24, 428.55it/s]\u001b[A\n"," 11% 1260/11873 [00:02<00:25, 422.30it/s]\u001b[A\n"," 11% 1303/11873 [00:02<00:25, 415.71it/s]\u001b[A\n"," 11% 1345/11873 [00:02<00:25, 415.08it/s]\u001b[A\n"," 12% 1387/11873 [00:02<00:25, 416.33it/s]\u001b[A\n"," 12% 1429/11873 [00:02<00:25, 415.28it/s]\u001b[A\n"," 12% 1471/11873 [00:03<00:25, 414.01it/s]\u001b[A\n"," 13% 1513/11873 [00:03<00:25, 412.88it/s]\u001b[A\n"," 13% 1555/11873 [00:03<00:25, 410.59it/s]\u001b[A\n"," 13% 1597/11873 [00:03<00:25, 402.86it/s]\u001b[A\n"," 14% 1638/11873 [00:03<00:26, 385.91it/s]\u001b[A\n"," 14% 1677/11873 [00:03<00:26, 377.79it/s]\u001b[A\n"," 14% 1716/11873 [00:03<00:26, 379.02it/s]\u001b[A\n"," 15% 1755/11873 [00:03<00:26, 380.64it/s]\u001b[A\n"," 15% 1796/11873 [00:03<00:26, 386.81it/s]\u001b[A\n"," 15% 1836/11873 [00:04<00:25, 389.06it/s]\u001b[A\n"," 16% 1875/11873 [00:04<00:25, 386.19it/s]\u001b[A\n"," 16% 1916/11873 [00:04<00:25, 391.76it/s]\u001b[A\n"," 16% 1958/11873 [00:04<00:24, 398.82it/s]\u001b[A\n"," 17% 1998/11873 [00:04<00:25, 390.07it/s]\u001b[A\n"," 17% 2038/11873 [00:04<00:25, 379.27it/s]\u001b[A\n"," 17% 2077/11873 [00:04<00:25, 376.88it/s]\u001b[A\n"," 18% 2115/11873 [00:04<00:26, 371.14it/s]\u001b[A\n"," 18% 2153/11873 [00:04<00:26, 371.11it/s]\u001b[A\n"," 18% 2192/11873 [00:04<00:25, 376.38it/s]\u001b[A\n"," 19% 2233/11873 [00:05<00:25, 384.50it/s]\u001b[A\n"," 19% 2275/11873 [00:05<00:24, 392.04it/s]\u001b[A\n"," 20% 2316/11873 [00:05<00:24, 396.29it/s]\u001b[A\n"," 20% 2358/11873 [00:05<00:23, 402.16it/s]\u001b[A\n"," 20% 2399/11873 [00:05<00:24, 391.47it/s]\u001b[A\n"," 21% 2439/11873 [00:05<00:24, 379.02it/s]\u001b[A\n"," 21% 2478/11873 [00:05<00:24, 379.04it/s]\u001b[A\n"," 21% 2518/11873 [00:05<00:24, 382.83it/s]\u001b[A\n"," 22% 2558/11873 [00:05<00:24, 387.75it/s]\u001b[A\n"," 22% 2599/11873 [00:05<00:23, 393.80it/s]\u001b[A\n"," 22% 2641/11873 [00:06<00:23, 400.38it/s]\u001b[A\n"," 23% 2682/11873 [00:06<00:22, 399.94it/s]\u001b[A\n"," 23% 2723/11873 [00:06<00:22, 402.52it/s]\u001b[A\n"," 23% 2764/11873 [00:06<00:22, 402.30it/s]\u001b[A\n"," 24% 2805/11873 [00:06<00:22, 401.60it/s]\u001b[A\n"," 24% 2847/11873 [00:06<00:22, 406.53it/s]\u001b[A\n"," 24% 2888/11873 [00:06<00:22, 404.78it/s]\u001b[A\n"," 25% 2929/11873 [00:06<00:22, 403.89it/s]\u001b[A\n"," 25% 2971/11873 [00:06<00:21, 405.43it/s]\u001b[A\n"," 25% 3012/11873 [00:07<00:23, 384.56it/s]\u001b[A\n"," 26% 3051/11873 [00:07<00:23, 375.25it/s]\u001b[A\n"," 26% 3089/11873 [00:07<00:23, 368.31it/s]\u001b[A\n"," 26% 3126/11873 [00:07<00:27, 323.73it/s]\u001b[A\n"," 27% 3160/11873 [00:07<00:29, 299.54it/s]\u001b[A\n"," 27% 3195/11873 [00:07<00:27, 310.80it/s]\u001b[A\n"," 27% 3231/11873 [00:07<00:26, 322.24it/s]\u001b[A\n"," 28% 3266/11873 [00:07<00:26, 325.40it/s]\u001b[A\n"," 28% 3300/11873 [00:08<00:34, 245.82it/s]\u001b[A\n"," 28% 3328/11873 [00:08<00:37, 228.95it/s]\u001b[A\n"," 28% 3354/11873 [00:08<00:38, 221.13it/s]\u001b[A\n"," 28% 3378/11873 [00:08<00:37, 224.10it/s]\u001b[A\n"," 29% 3416/11873 [00:08<00:32, 262.20it/s]\u001b[A\n"," 29% 3453/11873 [00:08<00:29, 288.98it/s]\u001b[A\n"," 29% 3493/11873 [00:08<00:26, 317.24it/s]\u001b[A\n"," 30% 3534/11873 [00:08<00:24, 340.78it/s]\u001b[A\n"," 30% 3573/11873 [00:08<00:23, 353.07it/s]\u001b[A\n"," 30% 3611/11873 [00:09<00:23, 358.57it/s]\u001b[A\n"," 31% 3650/11873 [00:09<00:22, 367.29it/s]\u001b[A\n"," 31% 3688/11873 [00:09<00:22, 361.74it/s]\u001b[A\n"," 31% 3725/11873 [00:09<00:22, 361.94it/s]\u001b[A\n"," 32% 3762/11873 [00:09<00:22, 364.24it/s]\u001b[A\n"," 32% 3799/11873 [00:09<00:22, 361.63it/s]\u001b[A\n"," 32% 3836/11873 [00:09<00:23, 336.51it/s]\u001b[A\n"," 33% 3876/11873 [00:09<00:22, 354.20it/s]\u001b[A\n"," 33% 3913/11873 [00:09<00:22, 358.09it/s]\u001b[A\n"," 33% 3950/11873 [00:10<00:23, 340.62it/s]\u001b[A\n"," 34% 3987/11873 [00:10<00:22, 346.78it/s]\u001b[A\n"," 34% 4026/11873 [00:10<00:22, 356.64it/s]\u001b[A\n"," 34% 4066/11873 [00:10<00:21, 368.56it/s]\u001b[A\n"," 35% 4108/11873 [00:10<00:20, 380.71it/s]\u001b[A\n"," 35% 4147/11873 [00:10<00:20, 370.11it/s]\u001b[A\n"," 35% 4185/11873 [00:10<00:21, 361.52it/s]\u001b[A\n"," 36% 4225/11873 [00:10<00:20, 370.93it/s]\u001b[A\n"," 36% 4267/11873 [00:10<00:19, 383.43it/s]\u001b[A\n"," 36% 4308/11873 [00:10<00:19, 389.42it/s]\u001b[A\n"," 37% 4348/11873 [00:11<00:19, 386.79it/s]\u001b[A\n"," 37% 4387/11873 [00:11<00:19, 381.89it/s]\u001b[A\n"," 37% 4426/11873 [00:11<00:24, 306.53it/s]\u001b[A\n"," 38% 4469/11873 [00:11<00:22, 331.03it/s]\u001b[A\n"," 38% 4510/11873 [00:11<00:21, 350.52it/s]\u001b[A\n"," 38% 4551/11873 [00:11<00:20, 365.37it/s]\u001b[A\n"," 39% 4594/11873 [00:11<00:19, 380.97it/s]\u001b[A\n"," 39% 4635/11873 [00:11<00:18, 388.47it/s]\u001b[A\n"," 39% 4675/11873 [00:11<00:18, 389.68it/s]\u001b[A\n"," 40% 4715/11873 [00:12<00:18, 382.50it/s]\u001b[A\n"," 40% 4754/11873 [00:12<00:18, 377.15it/s]\u001b[A\n"," 40% 4792/11873 [00:12<00:19, 370.61it/s]\u001b[A\n"," 41% 4830/11873 [00:12<00:19, 366.99it/s]\u001b[A\n"," 41% 4869/11873 [00:12<00:18, 371.21it/s]\u001b[A\n"," 41% 4910/11873 [00:12<00:18, 381.51it/s]\u001b[A\n"," 42% 4951/11873 [00:12<00:17, 388.32it/s]\u001b[A\n"," 42% 4991/11873 [00:12<00:17, 391.10it/s]\u001b[A\n"," 42% 5031/11873 [00:12<00:17, 391.05it/s]\u001b[A\n"," 43% 5071/11873 [00:12<00:17, 393.09it/s]\u001b[A\n"," 43% 5113/11873 [00:13<00:16, 400.83it/s]\u001b[A\n"," 43% 5155/11873 [00:13<00:16, 405.53it/s]\u001b[A\n"," 44% 5197/11873 [00:13<00:16, 407.39it/s]\u001b[A\n"," 44% 5239/11873 [00:13<00:16, 408.87it/s]\u001b[A\n"," 44% 5280/11873 [00:13<00:17, 381.02it/s]\u001b[A\n"," 45% 5322/11873 [00:13<00:16, 389.82it/s]\u001b[A\n"," 45% 5364/11873 [00:13<00:16, 397.70it/s]\u001b[A\n"," 46% 5406/11873 [00:13<00:16, 404.05it/s]\u001b[A\n"," 46% 5447/11873 [00:13<00:16, 401.62it/s]\u001b[A\n"," 46% 5488/11873 [00:14<00:16, 391.42it/s]\u001b[A\n"," 47% 5528/11873 [00:14<00:16, 392.66it/s]\u001b[A\n"," 47% 5568/11873 [00:14<00:16, 389.41it/s]\u001b[A\n"," 47% 5608/11873 [00:14<00:16, 382.80it/s]\u001b[A\n"," 48% 5647/11873 [00:14<00:16, 379.73it/s]\u001b[A\n"," 48% 5687/11873 [00:14<00:16, 384.19it/s]\u001b[A\n"," 48% 5727/11873 [00:14<00:15, 388.60it/s]\u001b[A\n"," 49% 5768/11873 [00:14<00:15, 392.73it/s]\u001b[A\n"," 49% 5810/11873 [00:14<00:15, 399.92it/s]\u001b[A\n"," 49% 5852/11873 [00:14<00:14, 403.70it/s]\u001b[A\n"," 50% 5893/11873 [00:15<00:14, 403.58it/s]\u001b[A\n"," 50% 5934/11873 [00:15<00:14, 405.09it/s]\u001b[A\n"," 50% 5976/11873 [00:15<00:14, 408.64it/s]\u001b[A\n"," 51% 6018/11873 [00:15<00:14, 411.25it/s]\u001b[A\n"," 51% 6060/11873 [00:15<00:14, 409.23it/s]\u001b[A\n"," 51% 6101/11873 [00:15<00:14, 404.10it/s]\u001b[A\n"," 52% 6142/11873 [00:15<00:14, 403.53it/s]\u001b[A\n"," 52% 6183/11873 [00:15<00:14, 404.07it/s]\u001b[A\n"," 52% 6224/11873 [00:15<00:13, 404.39it/s]\u001b[A\n"," 53% 6266/11873 [00:15<00:13, 406.11it/s]\u001b[A\n"," 53% 6307/11873 [00:16<00:13, 400.30it/s]\u001b[A\n"," 53% 6348/11873 [00:16<00:13, 394.87it/s]\u001b[A\n"," 54% 6388/11873 [00:16<00:14, 391.02it/s]\u001b[A\n"," 54% 6428/11873 [00:16<00:14, 383.11it/s]\u001b[A\n"," 54% 6467/11873 [00:16<00:14, 375.10it/s]\u001b[A\n"," 55% 6505/11873 [00:16<00:14, 366.39it/s]\u001b[A\n"," 55% 6542/11873 [00:16<00:14, 364.43it/s]\u001b[A\n"," 55% 6579/11873 [00:16<00:14, 365.49it/s]\u001b[A\n"," 56% 6617/11873 [00:16<00:14, 368.37it/s]\u001b[A\n"," 56% 6654/11873 [00:17<00:14, 366.41it/s]\u001b[A\n"," 56% 6691/11873 [00:17<00:14, 366.61it/s]\u001b[A\n"," 57% 6728/11873 [00:17<00:15, 325.40it/s]\u001b[A\n"," 57% 6768/11873 [00:17<00:14, 343.37it/s]\u001b[A\n"," 57% 6808/11873 [00:17<00:14, 358.00it/s]\u001b[A\n"," 58% 6845/11873 [00:17<00:13, 361.16it/s]\u001b[A\n"," 58% 6884/11873 [00:17<00:13, 368.38it/s]\u001b[A\n"," 58% 6924/11873 [00:17<00:13, 375.38it/s]\u001b[A\n"," 59% 6964/11873 [00:17<00:12, 380.52it/s]\u001b[A\n"," 59% 7005/11873 [00:17<00:12, 389.10it/s]\u001b[A\n"," 59% 7046/11873 [00:18<00:12, 394.93it/s]\u001b[A\n"," 60% 7086/11873 [00:18<00:12, 388.90it/s]\u001b[A\n"," 60% 7126/11873 [00:18<00:12, 389.57it/s]\u001b[A\n"," 60% 7167/11873 [00:18<00:11, 395.23it/s]\u001b[A\n"," 61% 7208/11873 [00:18<00:11, 398.54it/s]\u001b[A\n"," 61% 7248/11873 [00:18<00:12, 382.47it/s]\u001b[A\n"," 61% 7287/11873 [00:18<00:11, 383.27it/s]\u001b[A\n"," 62% 7327/11873 [00:18<00:11, 385.69it/s]\u001b[A\n"," 62% 7369/11873 [00:18<00:11, 393.47it/s]\u001b[A\n"," 62% 7409/11873 [00:19<00:11, 390.89it/s]\u001b[A\n"," 63% 7449/11873 [00:19<00:11, 375.90it/s]\u001b[A\n"," 63% 7491/11873 [00:19<00:11, 386.74it/s]\u001b[A\n"," 63% 7533/11873 [00:19<00:11, 393.57it/s]\u001b[A\n"," 64% 7574/11873 [00:19<00:10, 397.73it/s]\u001b[A\n"," 64% 7614/11873 [00:19<00:10, 388.43it/s]\u001b[A\n"," 64% 7653/11873 [00:19<00:11, 376.24it/s]\u001b[A\n"," 65% 7693/11873 [00:19<00:10, 381.15it/s]\u001b[A\n"," 65% 7732/11873 [00:19<00:11, 362.45it/s]\u001b[A\n"," 65% 7771/11873 [00:19<00:11, 368.50it/s]\u001b[A\n"," 66% 7811/11873 [00:20<00:10, 376.75it/s]\u001b[A\n"," 66% 7850/11873 [00:20<00:10, 378.75it/s]\u001b[A\n"," 66% 7889/11873 [00:20<00:11, 359.97it/s]\u001b[A\n"," 67% 7930/11873 [00:20<00:10, 373.08it/s]\u001b[A\n"," 67% 7970/11873 [00:20<00:10, 379.12it/s]\u001b[A\n"," 67% 8010/11873 [00:20<00:10, 384.62it/s]\u001b[A\n"," 68% 8051/11873 [00:20<00:09, 389.87it/s]\u001b[A\n"," 68% 8093/11873 [00:20<00:09, 397.18it/s]\u001b[A\n"," 69% 8134/11873 [00:20<00:09, 400.05it/s]\u001b[A\n"," 69% 8176/11873 [00:21<00:09, 403.66it/s]\u001b[A\n"," 69% 8217/11873 [00:21<00:09, 397.31it/s]\u001b[A\n"," 70% 8260/11873 [00:21<00:08, 404.67it/s]\u001b[A\n"," 70% 8302/11873 [00:21<00:08, 407.27it/s]\u001b[A\n"," 70% 8343/11873 [00:21<00:08, 407.25it/s]\u001b[A\n"," 71% 8384/11873 [00:21<00:08, 404.55it/s]\u001b[A\n"," 71% 8425/11873 [00:21<00:08, 388.56it/s]\u001b[A\n"," 71% 8464/11873 [00:21<00:08, 386.10it/s]\u001b[A\n"," 72% 8505/11873 [00:21<00:08, 391.83it/s]\u001b[A\n"," 72% 8546/11873 [00:21<00:08, 396.26it/s]\u001b[A\n"," 72% 8587/11873 [00:22<00:08, 399.19it/s]\u001b[A\n"," 73% 8627/11873 [00:22<00:08, 387.11it/s]\u001b[A\n"," 73% 8666/11873 [00:22<00:08, 385.88it/s]\u001b[A\n"," 73% 8705/11873 [00:22<00:08, 380.82it/s]\u001b[A\n"," 74% 8744/11873 [00:22<00:08, 374.57it/s]\u001b[A\n"," 74% 8782/11873 [00:22<00:08, 375.47it/s]\u001b[A\n"," 74% 8822/11873 [00:22<00:08, 380.25it/s]\u001b[A\n"," 75% 8863/11873 [00:22<00:07, 388.94it/s]\u001b[A\n"," 75% 8903/11873 [00:22<00:07, 389.98it/s]\u001b[A\n"," 75% 8943/11873 [00:22<00:07, 392.25it/s]\u001b[A\n"," 76% 8984/11873 [00:23<00:07, 395.07it/s]\u001b[A\n"," 76% 9024/11873 [00:23<00:07, 395.93it/s]\u001b[A\n"," 76% 9064/11873 [00:23<00:07, 396.73it/s]\u001b[A\n"," 77% 9106/11873 [00:23<00:06, 401.45it/s]\u001b[A\n"," 77% 9147/11873 [00:23<00:06, 399.88it/s]\u001b[A\n"," 77% 9188/11873 [00:23<00:06, 400.89it/s]\u001b[A\n"," 78% 9229/11873 [00:23<00:06, 401.33it/s]\u001b[A\n"," 78% 9270/11873 [00:23<00:06, 400.64it/s]\u001b[A\n"," 78% 9311/11873 [00:23<00:06, 402.06it/s]\u001b[A\n"," 79% 9352/11873 [00:24<00:06, 402.08it/s]\u001b[A\n"," 79% 9393/11873 [00:24<00:06, 390.23it/s]\u001b[A\n"," 79% 9434/11873 [00:24<00:06, 393.58it/s]\u001b[A\n"," 80% 9475/11873 [00:24<00:06, 398.11it/s]\u001b[A\n"," 80% 9516/11873 [00:24<00:05, 400.71it/s]\u001b[A\n"," 80% 9557/11873 [00:24<00:05, 402.60it/s]\u001b[A\n"," 81% 9598/11873 [00:24<00:05, 389.37it/s]\u001b[A\n"," 81% 9639/11873 [00:24<00:05, 392.79it/s]\u001b[A\n"," 82% 9679/11873 [00:24<00:05, 393.49it/s]\u001b[A\n"," 82% 9720/11873 [00:24<00:05, 396.32it/s]\u001b[A\n"," 82% 9762/11873 [00:25<00:05, 403.03it/s]\u001b[A\n"," 83% 9803/11873 [00:25<00:05, 402.38it/s]\u001b[A\n"," 83% 9845/11873 [00:25<00:04, 405.61it/s]\u001b[A\n"," 83% 9886/11873 [00:25<00:04, 406.79it/s]\u001b[A\n"," 84% 9927/11873 [00:25<00:04, 406.94it/s]\u001b[A\n"," 84% 9969/11873 [00:25<00:04, 408.40it/s]\u001b[A\n"," 84% 10010/11873 [00:25<00:04, 390.91it/s]\u001b[A\n"," 85% 10052/11873 [00:25<00:04, 397.34it/s]\u001b[A\n"," 85% 10095/11873 [00:25<00:04, 403.99it/s]\u001b[A\n"," 85% 10136/11873 [00:25<00:04, 402.95it/s]\u001b[A\n"," 86% 10177/11873 [00:26<00:04, 404.66it/s]\u001b[A\n"," 86% 10218/11873 [00:26<00:04, 401.37it/s]\u001b[A\n"," 86% 10260/11873 [00:26<00:03, 405.24it/s]\u001b[A\n"," 87% 10301/11873 [00:26<00:03, 399.05it/s]\u001b[A\n"," 87% 10343/11873 [00:26<00:03, 402.64it/s]\u001b[A\n"," 87% 10384/11873 [00:26<00:03, 398.11it/s]\u001b[A\n"," 88% 10424/11873 [00:26<00:03, 376.34it/s]\u001b[A\n"," 88% 10462/11873 [00:26<00:03, 367.54it/s]\u001b[A\n"," 88% 10501/11873 [00:26<00:03, 373.01it/s]\u001b[A\n"," 89% 10539/11873 [00:27<00:03, 374.41it/s]\u001b[A\n"," 89% 10577/11873 [00:27<00:03, 372.31it/s]\u001b[A\n"," 89% 10616/11873 [00:27<00:03, 375.02it/s]\u001b[A\n"," 90% 10654/11873 [00:27<00:03, 367.89it/s]\u001b[A\n"," 90% 10694/11873 [00:27<00:03, 375.64it/s]\u001b[A\n"," 90% 10736/11873 [00:27<00:02, 387.88it/s]\u001b[A\n"," 91% 10775/11873 [00:27<00:02, 388.22it/s]\u001b[A\n"," 91% 10816/11873 [00:27<00:02, 392.35it/s]\u001b[A\n"," 91% 10856/11873 [00:27<00:02, 368.32it/s]\u001b[A\n"," 92% 10895/11873 [00:27<00:02, 373.31it/s]\u001b[A\n"," 92% 10934/11873 [00:28<00:02, 376.46it/s]\u001b[A\n"," 92% 10974/11873 [00:28<00:02, 382.75it/s]\u001b[A\n"," 93% 11015/11873 [00:28<00:02, 390.01it/s]\u001b[A\n"," 93% 11057/11873 [00:28<00:02, 398.83it/s]\u001b[A\n"," 93% 11099/11873 [00:28<00:01, 403.43it/s]\u001b[A\n"," 94% 11140/11873 [00:28<00:01, 404.47it/s]\u001b[A\n"," 94% 11181/11873 [00:28<00:01, 399.48it/s]\u001b[A\n"," 95% 11221/11873 [00:28<00:01, 398.61it/s]\u001b[A\n"," 95% 11261/11873 [00:28<00:01, 394.20it/s]\u001b[A\n"," 95% 11302/11873 [00:28<00:01, 397.69it/s]\u001b[A\n"," 96% 11344/11873 [00:29<00:01, 402.30it/s]\u001b[A\n"," 96% 11385/11873 [00:29<00:01, 399.73it/s]\u001b[A\n"," 96% 11426/11873 [00:29<00:01, 400.45it/s]\u001b[A\n"," 97% 11467/11873 [00:29<00:01, 398.77it/s]\u001b[A\n"," 97% 11507/11873 [00:29<00:00, 398.92it/s]\u001b[A\n"," 97% 11547/11873 [00:29<00:00, 398.11it/s]\u001b[A\n"," 98% 11588/11873 [00:29<00:00, 400.68it/s]\u001b[A\n"," 98% 11629/11873 [00:29<00:00, 398.73it/s]\u001b[A\n"," 98% 11669/11873 [00:29<00:00, 395.90it/s]\u001b[A\n"," 99% 11709/11873 [00:30<00:00, 394.30it/s]\u001b[A\n"," 99% 11750/11873 [00:30<00:00, 397.24it/s]\u001b[A\n"," 99% 11791/11873 [00:30<00:00, 399.06it/s]\u001b[A\n","100% 11873/11873 [00:30<00:00, 390.46it/s]\n","04/06/2022 17:48:06 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/eval_predictions.json.\n","04/06/2022 17:48:06 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/eval_nbest_predictions.json.\n","04/06/2022 17:48:08 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/roberta-base/synonym-aug/eval_null_odds.json.\n","04/06/2022 17:48:11 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1521/1521 [03:43<00:00,  6.80it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 78.2726\n","  eval_HasAns_f1         = 84.5729\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 77.4601\n","  eval_NoAns_f1          = 77.4601\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 77.8657\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 81.0114\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 77.8657\n","  eval_f1                = 81.0114\n","  eval_samples           =   12165\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 17:48:12,003 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_synonym_aug', 'type': 'sichenzhong/squad_v2_synonym_aug', 'args': 'squad_v2'}}\n"]}]}]}