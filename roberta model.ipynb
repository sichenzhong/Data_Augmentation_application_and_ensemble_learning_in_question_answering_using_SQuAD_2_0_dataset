{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"roberta model.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70oLi9mZP6oK","executionInfo":{"status":"ok","timestamp":1649181449082,"user_tz":240,"elapsed":13,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"8fda05ff-47ad-4bc0-8c15-45fbde247319"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Apr  5 17:57:28 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGqcN-zXTvvo","executionInfo":{"status":"ok","timestamp":1649181460385,"user_tz":240,"elapsed":11310,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"ec658844-6f85-4e4f-eee9-eb415edee5f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n","\u001b[K     |████████████████████████████████| 325 kB 5.0 MB/s \n","\u001b[?25hCollecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 61.4 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 51.7 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Collecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.0-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 5.2 MB/s \n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 64.2 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 49.0 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 47.5 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 2.3 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 51.2 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tI_RBT1FSotu","outputId":"0bd90181-6391-458a-d82a-2a03079753c0","executionInfo":{"status":"ok","timestamp":1649181487050,"user_tz":240,"elapsed":26673,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-kj41qwtc\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-kj41qwtc\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (1.21.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (3.6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 5.1 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 62.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.11.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 63.1 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.63.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (0.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0.dev0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0.dev0) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0.dev0) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (7.1.2)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.18.0.dev0-py3-none-any.whl size=3958813 sha256=25648106eb7c0a98e8e122e45041e0b4fea34f974191109b8a5bd6f971d9f486\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9zufp846/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0.dev0\n"]}]},{"cell_type":"code","source":["import torch\n","import datetime\n","import json\n","import os\n","import time\n","import datasets\n","import pprint\n","import random\n","import string\n","import sys\n","import transformers"],"metadata":{"id":"DZ3Ma-pCRJDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available()\n","                      else 'cpu')"],"metadata":{"id":"HNMUVyBpRGw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/huggingface/transformers.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUVkgX-IQIiR","executionInfo":{"status":"ok","timestamp":1649181502669,"user_tz":240,"elapsed":6980,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"4d81d315-0af1-4591-cfd6-993e26a4fed2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 108426, done.\u001b[K\n","remote: Counting objects: 100% (253/253), done.\u001b[K\n","remote: Compressing objects: 100% (86/86), done.\u001b[K\n","remote: Total 108426 (delta 133), reused 195 (delta 125), pack-reused 108173\u001b[K\n","Receiving objects: 100% (108426/108426), 95.08 MiB | 31.69 MiB/s, done.\n","Resolving deltas: 100% (78992/78992), done.\n"]}]},{"cell_type":"code","source":["%cd /content/transformers/examples/pytorch/question-answering/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uLaizsXQrk9","executionInfo":{"status":"ok","timestamp":1649181502669,"user_tz":240,"elapsed":18,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"4c3959dc-f419-40a7-fa63-d47f5a506016"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/transformers/examples/pytorch/question-answering\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path roberta-base \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 3e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOm5ck1RQqE_","executionInfo":{"status":"ok","timestamp":1649147290650,"user_tz":240,"elapsed":10385685,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"8d557cbd-214e-494c-a144-eab71ecac692"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["04/05/2022 05:35:08 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/05/2022 05:35:08 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Apr05_05-35-08_103dd436e4a7,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/05/2022 05:35:08 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpvnhyefla\n","Downloading builder script: 5.28kB [00:00, 7.18MB/s]       \n","04/05/2022 05:35:08 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","04/05/2022 05:35:08 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","04/05/2022 05:35:08 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp914g_qfi\n","Downloading metadata: 2.40kB [00:00, 4.25MB/s]       \n","04/05/2022 05:35:08 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","04/05/2022 05:35:08 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","04/05/2022 05:35:08 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","04/05/2022 05:35:08 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/05/2022 05:35:08 - INFO - datasets.builder - Generating dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","04/05/2022 05:35:08 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]04/05/2022 05:35:09 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpejxrcto4\n","\n","Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n","Downloading data:  89% 8.50M/9.55M [00:00<00:00, 85.0MB/s]\u001b[A\n","Downloading data: 19.2MB [00:00, 98.0MB/s]                \u001b[A\n","Downloading data: 29.9MB [00:00, 102MB/s] \u001b[A\n","Downloading data: 42.1MB [00:00, 102MB/s]\n","04/05/2022 05:35:10 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","04/05/2022 05:35:10 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","Downloading data files:  50% 1/2 [00:01<00:01,  1.41s/it]04/05/2022 05:35:10 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpeku1_sd7\n","\n","Downloading data: 4.37MB [00:00, 80.7MB/s]      \n","04/05/2022 05:35:10 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","04/05/2022 05:35:10 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","Downloading data files: 100% 2/2 [00:01<00:00,  1.20it/s]\n","04/05/2022 05:35:10 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","04/05/2022 05:35:10 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1472.98it/s]\n","04/05/2022 05:35:10 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n","04/05/2022 05:35:10 - INFO - datasets.builder - Generating train split\n","04/05/2022 05:35:21 - INFO - datasets.builder - Generating validation split\n","04/05/2022 05:35:22 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 338.88it/s]\n","[INFO|hub.py:583] 2022-04-05 05:35:22,673 >> https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmprtmsvygf\n","Downloading: 100% 481/481 [00:00<00:00, 612kB/s]\n","[INFO|hub.py:587] 2022-04-05 05:35:22,813 >> storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|hub.py:595] 2022-04-05 05:35:22,813 >> creating metadata file for /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:654] 2022-04-05 05:35:22,813 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-05 05:35:22,815 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-05 05:35:22,955 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-05 05:35:23,091 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-05 05:35:23,092 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|hub.py:583] 2022-04-05 05:35:23,362 >> https://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7p1c5i33\n","Downloading: 100% 878k/878k [00:00<00:00, 5.06MB/s]\n","[INFO|hub.py:587] 2022-04-05 05:35:23,686 >> storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|hub.py:595] 2022-04-05 05:35:23,686 >> creating metadata file for /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|hub.py:583] 2022-04-05 05:35:23,815 >> https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp551d207y\n","Downloading: 100% 446k/446k [00:00<00:00, 3.16MB/s]\n","[INFO|hub.py:587] 2022-04-05 05:35:24,098 >> storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|hub.py:595] 2022-04-05 05:35:24,098 >> creating metadata file for /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|hub.py:583] 2022-04-05 05:35:24,226 >> https://huggingface.co/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_luriq0_\n","Downloading: 100% 1.29M/1.29M [00:00<00:00, 9.66MB/s]\n","[INFO|hub.py:587] 2022-04-05 05:35:24,531 >> storing https://huggingface.co/roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|hub.py:595] 2022-04-05 05:35:24,531 >> creating metadata file for /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 05:35:24,930 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 05:35:24,930 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 05:35:24,930 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 05:35:24,930 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 05:35:24,930 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 05:35:24,930 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-05 05:35:25,058 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-05 05:35:25,059 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|hub.py:583] 2022-04-05 05:35:25,278 >> https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxxosylfu\n","Downloading: 100% 478M/478M [00:07<00:00, 71.3MB/s]\n","[INFO|hub.py:587] 2022-04-05 05:35:32,407 >> storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[INFO|hub.py:595] 2022-04-05 05:35:32,407 >> creating metadata file for /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[INFO|modeling_utils.py:1772] 2022-04-05 05:35:33,086 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[WARNING|modeling_utils.py:2050] 2022-04-05 05:35:34,445 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-05 05:35:34,445 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/05/2022 05:35:34 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-f70d4265b8a69d5f.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:38<00:00,  3.38ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/05/2022 05:36:13 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-9f333badd4a4f030.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:02<00:00,  5.19s/ba]\n","04/05/2022 05:37:15 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp_e25ld_j\n","Downloading builder script: 6.46kB [00:00, 7.96MB/s]       \n","04/05/2022 05:37:15 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/05/2022 05:37:15 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/05/2022 05:37:15 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpclhqha8k\n","Downloading extra modules: 11.3kB [00:00, 13.9MB/s]       \n","04/05/2022 05:37:16 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","04/05/2022 05:37:16 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-05 05:37:26,267 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-05 05:37:26,267 >>   Num examples = 131823\n","[INFO|trainer.py:1292] 2022-04-05 05:37:26,267 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-05 05:37:26,267 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1294] 2022-04-05 05:37:26,267 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1295] 2022-04-05 05:37:26,267 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-05 05:37:26,267 >>   Total optimization steps = 10986\n","{'loss': 1.8215, 'learning_rate': 2.8634625887493174e-05, 'epoch': 0.09}\n","  5% 500/10986 [07:32<2:38:05,  1.11it/s][INFO|trainer.py:2166] 2022-04-05 05:44:58,944 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-05 05:44:58,945 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 05:44:59,877 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 05:44:59,878 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 05:44:59,878 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.2333, 'learning_rate': 2.7269251774986347e-05, 'epoch': 0.18}\n","  9% 1000/10986 [15:08<2:30:42,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 05:52:34,866 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-05 05:52:34,867 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 05:52:35,788 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 05:52:35,789 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 05:52:35,789 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.114, 'learning_rate': 2.5903877662479523e-05, 'epoch': 0.27}\n"," 14% 1500/10986 [22:44<2:23:15,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 06:00:10,795 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-05 06:00:10,796 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 06:00:11,732 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 06:00:11,733 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 06:00:11,733 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.0596, 'learning_rate': 2.4538503549972693e-05, 'epoch': 0.36}\n"," 18% 2000/10986 [30:20<2:15:48,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 06:07:46,695 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-05 06:07:46,695 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 06:07:47,617 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 06:07:47,617 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 06:07:47,617 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.0204, 'learning_rate': 2.3173129437465866e-05, 'epoch': 0.46}\n"," 23% 2500/10986 [37:56<2:08:19,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 06:15:22,509 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-05 06:15:22,510 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 06:15:23,445 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 06:15:23,445 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 06:15:23,445 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 0.9937, 'learning_rate': 2.180775532495904e-05, 'epoch': 0.55}\n"," 27% 3000/10986 [45:32<2:00:32,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 06:22:58,795 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-05 06:22:58,795 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 06:22:59,734 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 06:22:59,735 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 06:22:59,735 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.9572, 'learning_rate': 2.044238121245221e-05, 'epoch': 0.64}\n"," 32% 3500/10986 [53:08<1:52:55,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 06:30:35,222 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-05 06:30:35,223 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 06:30:36,149 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 06:30:36,150 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 06:30:36,150 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.9284, 'learning_rate': 1.9077007099945385e-05, 'epoch': 0.73}\n"," 36% 4000/10986 [1:00:45<1:45:31,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 06:38:11,508 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-05 06:38:11,509 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 06:38:12,436 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 06:38:12,437 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 06:38:12,437 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.911, 'learning_rate': 1.771163298743856e-05, 'epoch': 0.82}\n"," 41% 4500/10986 [1:08:21<1:38:13,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 06:45:47,953 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-05 06:45:47,954 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 06:45:48,870 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 06:45:48,871 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 06:45:48,871 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.8693, 'learning_rate': 1.6346258874931734e-05, 'epoch': 0.91}\n"," 46% 5000/10986 [1:15:58<1:30:24,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 06:53:24,514 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-05 06:53:24,514 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 06:53:25,443 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 06:53:25,444 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 06:53:25,445 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.8604, 'learning_rate': 1.4980884762424905e-05, 'epoch': 1.0}\n"," 50% 5500/10986 [1:23:34<1:22:12,  1.11it/s][INFO|trainer.py:2166] 2022-04-05 07:01:00,612 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-05 07:01:00,613 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 07:01:01,544 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 07:01:01,544 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 07:01:01,545 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.6647, 'learning_rate': 1.3615510649918078e-05, 'epoch': 1.09}\n"," 55% 6000/10986 [1:31:10<1:15:18,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 07:08:37,039 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-05 07:08:37,040 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 07:08:37,994 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 07:08:37,994 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 07:08:37,995 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.6591, 'learning_rate': 1.2250136537411251e-05, 'epoch': 1.18}\n"," 59% 6500/10986 [1:38:47<1:07:57,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 07:16:13,477 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-05 07:16:13,478 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 07:16:14,433 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 07:16:14,433 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 07:16:14,434 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.6635, 'learning_rate': 1.0884762424904424e-05, 'epoch': 1.27}\n"," 64% 7000/10986 [1:46:23<1:00:14,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 07:23:49,949 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-05 07:23:49,949 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 07:23:50,892 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 07:23:50,893 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 07:23:50,893 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.6424, 'learning_rate': 9.519388312397597e-06, 'epoch': 1.37}\n"," 68% 7500/10986 [1:54:00<52:37,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 07:31:26,331 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-05 07:31:26,332 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 07:31:27,303 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 07:31:27,303 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 07:31:27,303 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.6597, 'learning_rate': 8.15401419989077e-06, 'epoch': 1.46}\n"," 73% 8000/10986 [2:01:36<45:06,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 07:39:02,998 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-05 07:39:02,998 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 07:39:03,941 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 07:39:03,941 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 07:39:03,942 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.6593, 'learning_rate': 6.788640087383943e-06, 'epoch': 1.55}\n"," 77% 8500/10986 [2:09:13<37:31,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 07:46:39,541 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-05 07:46:39,542 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 07:46:40,431 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 07:46:40,431 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 07:46:40,432 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.6444, 'learning_rate': 5.423265974877117e-06, 'epoch': 1.64}\n"," 82% 9000/10986 [2:16:49<30:01,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 07:54:15,901 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-05 07:54:15,902 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 07:54:16,769 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 07:54:16,770 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 07:54:16,770 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.6221, 'learning_rate': 4.05789186237029e-06, 'epoch': 1.73}\n"," 86% 9500/10986 [2:24:25<22:27,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 08:01:52,174 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-05 08:01:52,174 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 08:01:53,038 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 08:01:53,038 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 08:01:53,038 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.6223, 'learning_rate': 2.692517749863463e-06, 'epoch': 1.82}\n"," 91% 10000/10986 [2:32:02<14:53,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 08:09:28,375 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-05 08:09:28,376 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 08:09:29,277 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 08:09:29,277 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 08:09:29,278 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6383, 'learning_rate': 1.3271436373566358e-06, 'epoch': 1.91}\n"," 96% 10500/10986 [2:39:38<07:19,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 08:17:04,796 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-05 08:17:04,797 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 08:17:05,666 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 08:17:05,666 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 08:17:05,667 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","100% 10986/10986 [2:47:01<00:00,  1.23it/s][INFO|trainer.py:1530] 2022-04-05 08:24:28,200 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10021.9355, 'train_samples_per_second': 26.307, 'train_steps_per_second': 1.096, 'train_loss': 0.858128690554669, 'epoch': 2.0}\n","100% 10986/10986 [2:47:01<00:00,  1.10it/s]\n","[INFO|trainer.py:2166] 2022-04-05 08:24:28,205 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:441] 2022-04-05 08:24:28,205 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 08:24:29,062 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 08:24:29,062 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 08:24:29,063 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.8581\n","  train_runtime            = 2:47:01.93\n","  train_samples            =     131823\n","  train_samples_per_second =     26.307\n","  train_steps_per_second   =      1.096\n","04/05/2022 08:24:29 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-05 08:24:29,159 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-05 08:24:29,162 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-05 08:24:29,162 >>   Num examples = 12165\n","[INFO|trainer.py:2421] 2022-04-05 08:24:29,162 >>   Batch size = 8\n","100% 1520/1521 [02:54<00:00,  8.70it/s]04/05/2022 08:27:34 - INFO - utils_qa - Post-processing 11873 example predictions split into 12165 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 51/11873 [00:00<00:23, 508.74it/s]\u001b[A\n","  1% 102/11873 [00:00<00:24, 480.43it/s]\u001b[A\n","  1% 155/11873 [00:00<00:23, 499.57it/s]\u001b[A\n","  2% 208/11873 [00:00<00:22, 509.59it/s]\u001b[A\n","  2% 262/11873 [00:00<00:22, 517.60it/s]\u001b[A\n","  3% 317/11873 [00:00<00:21, 525.96it/s]\u001b[A\n","  3% 372/11873 [00:00<00:21, 533.23it/s]\u001b[A\n","  4% 430/11873 [00:00<00:20, 545.60it/s]\u001b[A\n","  4% 488/11873 [00:00<00:20, 553.68it/s]\u001b[A\n","  5% 544/11873 [00:01<00:20, 546.30it/s]\u001b[A\n","  5% 600/11873 [00:01<00:20, 548.12it/s]\u001b[A\n","  6% 657/11873 [00:01<00:20, 554.34it/s]\u001b[A\n","  6% 713/11873 [00:01<00:20, 553.88it/s]\u001b[A\n","  6% 769/11873 [00:01<00:20, 542.68it/s]\u001b[A\n","  7% 827/11873 [00:01<00:20, 551.38it/s]\u001b[A\n","  7% 888/11873 [00:01<00:19, 568.03it/s]\u001b[A\n","  8% 947/11873 [00:01<00:19, 573.82it/s]\u001b[A\n","  8% 1005/11873 [00:01<00:19, 554.58it/s]\u001b[A\n","  9% 1061/11873 [00:01<00:21, 513.85it/s]\u001b[A\n","  9% 1114/11873 [00:02<00:22, 486.80it/s]\u001b[A\n"," 10% 1164/11873 [00:02<00:22, 470.08it/s]\u001b[A\n"," 10% 1212/11873 [00:02<00:23, 459.54it/s]\u001b[A\n"," 11% 1259/11873 [00:02<00:23, 450.77it/s]\u001b[A\n"," 11% 1305/11873 [00:02<00:23, 443.35it/s]\u001b[A\n"," 11% 1350/11873 [00:02<00:23, 440.82it/s]\u001b[A\n"," 12% 1395/11873 [00:02<00:23, 440.01it/s]\u001b[A\n"," 12% 1440/11873 [00:02<00:23, 437.27it/s]\u001b[A\n"," 12% 1484/11873 [00:02<00:23, 433.08it/s]\u001b[A\n"," 13% 1528/11873 [00:03<00:23, 433.46it/s]\u001b[A\n"," 13% 1572/11873 [00:03<00:23, 432.78it/s]\u001b[A\n"," 14% 1616/11873 [00:03<00:24, 426.52it/s]\u001b[A\n"," 14% 1659/11873 [00:03<00:24, 424.73it/s]\u001b[A\n"," 14% 1702/11873 [00:03<00:24, 421.56it/s]\u001b[A\n"," 15% 1745/11873 [00:03<00:23, 422.62it/s]\u001b[A\n"," 15% 1788/11873 [00:03<00:23, 421.95it/s]\u001b[A\n"," 15% 1831/11873 [00:03<00:24, 416.82it/s]\u001b[A\n"," 16% 1874/11873 [00:03<00:23, 419.21it/s]\u001b[A\n"," 16% 1917/11873 [00:03<00:23, 421.15it/s]\u001b[A\n"," 17% 1960/11873 [00:04<00:23, 420.73it/s]\u001b[A\n"," 17% 2004/11873 [00:04<00:23, 424.10it/s]\u001b[A\n"," 17% 2047/11873 [00:04<00:23, 424.46it/s]\u001b[A\n"," 18% 2091/11873 [00:04<00:22, 427.29it/s]\u001b[A\n"," 18% 2134/11873 [00:04<00:22, 426.72it/s]\u001b[A\n"," 18% 2177/11873 [00:04<00:22, 427.28it/s]\u001b[A\n"," 19% 2221/11873 [00:04<00:22, 429.63it/s]\u001b[A\n"," 19% 2264/11873 [00:04<00:22, 429.54it/s]\u001b[A\n"," 19% 2307/11873 [00:04<00:22, 427.58it/s]\u001b[A\n"," 20% 2351/11873 [00:05<00:22, 429.85it/s]\u001b[A\n"," 20% 2395/11873 [00:05<00:21, 431.47it/s]\u001b[A\n"," 21% 2439/11873 [00:05<00:22, 425.79it/s]\u001b[A\n"," 21% 2482/11873 [00:05<00:22, 426.15it/s]\u001b[A\n"," 21% 2525/11873 [00:05<00:22, 422.96it/s]\u001b[A\n"," 22% 2568/11873 [00:05<00:22, 420.73it/s]\u001b[A\n"," 22% 2611/11873 [00:05<00:21, 421.26it/s]\u001b[A\n"," 22% 2654/11873 [00:05<00:21, 423.16it/s]\u001b[A\n"," 23% 2698/11873 [00:05<00:21, 425.13it/s]\u001b[A\n"," 23% 2741/11873 [00:05<00:22, 414.14it/s]\u001b[A\n"," 23% 2784/11873 [00:06<00:21, 416.40it/s]\u001b[A\n"," 24% 2828/11873 [00:06<00:21, 422.12it/s]\u001b[A\n"," 24% 2872/11873 [00:06<00:21, 425.88it/s]\u001b[A\n"," 25% 2915/11873 [00:06<00:21, 424.95it/s]\u001b[A\n"," 25% 2958/11873 [00:06<00:21, 419.24it/s]\u001b[A\n"," 25% 3000/11873 [00:06<00:21, 415.77it/s]\u001b[A\n"," 26% 3042/11873 [00:06<00:21, 413.32it/s]\u001b[A\n"," 26% 3084/11873 [00:06<00:21, 404.32it/s]\u001b[A\n"," 26% 3125/11873 [00:06<00:24, 352.65it/s]\u001b[A\n"," 27% 3162/11873 [00:07<00:26, 329.47it/s]\u001b[A\n"," 27% 3204/11873 [00:07<00:24, 351.71it/s]\u001b[A\n"," 27% 3247/11873 [00:07<00:23, 372.63it/s]\u001b[A\n"," 28% 3286/11873 [00:07<00:27, 313.52it/s]\u001b[A\n"," 28% 3320/11873 [00:07<00:33, 255.32it/s]\u001b[A\n"," 28% 3349/11873 [00:07<00:33, 257.83it/s]\u001b[A\n"," 28% 3377/11873 [00:07<00:34, 245.81it/s]\u001b[A\n"," 29% 3420/11873 [00:07<00:29, 288.44it/s]\u001b[A\n"," 29% 3463/11873 [00:08<00:25, 323.72it/s]\u001b[A\n"," 30% 3505/11873 [00:08<00:23, 348.70it/s]\u001b[A\n"," 30% 3548/11873 [00:08<00:22, 369.86it/s]\u001b[A\n"," 30% 3591/11873 [00:08<00:21, 386.06it/s]\u001b[A\n"," 31% 3635/11873 [00:08<00:20, 399.80it/s]\u001b[A\n"," 31% 3676/11873 [00:08<00:20, 400.65it/s]\u001b[A\n"," 31% 3717/11873 [00:08<00:20, 402.17it/s]\u001b[A\n"," 32% 3760/11873 [00:08<00:19, 407.79it/s]\u001b[A\n"," 32% 3802/11873 [00:08<00:19, 410.18it/s]\u001b[A\n"," 32% 3844/11873 [00:08<00:20, 387.83it/s]\u001b[A\n"," 33% 3886/11873 [00:09<00:20, 396.44it/s]\u001b[A\n"," 33% 3926/11873 [00:09<00:21, 377.34it/s]\u001b[A\n"," 33% 3967/11873 [00:09<00:20, 384.40it/s]\u001b[A\n"," 34% 4009/11873 [00:09<00:20, 392.01it/s]\u001b[A\n"," 34% 4049/11873 [00:09<00:19, 393.30it/s]\u001b[A\n"," 34% 4092/11873 [00:09<00:19, 401.78it/s]\u001b[A\n"," 35% 4134/11873 [00:09<00:19, 406.94it/s]\u001b[A\n"," 35% 4175/11873 [00:09<00:20, 381.30it/s]\u001b[A\n"," 35% 4214/11873 [00:09<00:20, 375.82it/s]\u001b[A\n"," 36% 4257/11873 [00:10<00:19, 390.96it/s]\u001b[A\n"," 36% 4301/11873 [00:10<00:18, 403.88it/s]\u001b[A\n"," 37% 4345/11873 [00:10<00:18, 413.11it/s]\u001b[A\n"," 37% 4388/11873 [00:10<00:17, 416.04it/s]\u001b[A\n"," 37% 4430/11873 [00:10<00:21, 343.02it/s]\u001b[A\n"," 38% 4474/11873 [00:10<00:20, 367.35it/s]\u001b[A\n"," 38% 4517/11873 [00:10<00:19, 381.95it/s]\u001b[A\n"," 38% 4560/11873 [00:10<00:18, 393.94it/s]\u001b[A\n"," 39% 4601/11873 [00:10<00:18, 397.91it/s]\u001b[A\n"," 39% 4642/11873 [00:11<00:18, 394.16it/s]\u001b[A\n"," 39% 4683/11873 [00:11<00:18, 398.54it/s]\u001b[A\n"," 40% 4724/11873 [00:11<00:17, 400.31it/s]\u001b[A\n"," 40% 4766/11873 [00:11<00:17, 405.64it/s]\u001b[A\n"," 40% 4807/11873 [00:11<00:17, 401.49it/s]\u001b[A\n"," 41% 4849/11873 [00:11<00:17, 404.41it/s]\u001b[A\n"," 41% 4892/11873 [00:11<00:17, 409.23it/s]\u001b[A\n"," 42% 4935/11873 [00:11<00:16, 415.12it/s]\u001b[A\n"," 42% 4978/11873 [00:11<00:16, 419.49it/s]\u001b[A\n"," 42% 5021/11873 [00:11<00:16, 416.67it/s]\u001b[A\n"," 43% 5063/11873 [00:12<00:16, 416.88it/s]\u001b[A\n"," 43% 5107/11873 [00:12<00:16, 422.73it/s]\u001b[A\n"," 43% 5150/11873 [00:12<00:15, 423.60it/s]\u001b[A\n"," 44% 5194/11873 [00:12<00:15, 425.74it/s]\u001b[A\n"," 44% 5238/11873 [00:12<00:15, 428.28it/s]\u001b[A\n"," 44% 5281/11873 [00:12<00:16, 397.79it/s]\u001b[A\n"," 45% 5325/11873 [00:12<00:16, 407.11it/s]\u001b[A\n"," 45% 5369/11873 [00:12<00:15, 414.51it/s]\u001b[A\n"," 46% 5413/11873 [00:12<00:15, 419.51it/s]\u001b[A\n"," 46% 5456/11873 [00:12<00:15, 418.52it/s]\u001b[A\n"," 46% 5499/11873 [00:13<00:15, 420.17it/s]\u001b[A\n"," 47% 5543/11873 [00:13<00:14, 423.68it/s]\u001b[A\n"," 47% 5586/11873 [00:13<00:14, 420.66it/s]\u001b[A\n"," 47% 5629/11873 [00:13<00:14, 418.51it/s]\u001b[A\n"," 48% 5671/11873 [00:13<00:14, 417.66it/s]\u001b[A\n"," 48% 5713/11873 [00:13<00:14, 414.40it/s]\u001b[A\n"," 48% 5756/11873 [00:13<00:14, 417.98it/s]\u001b[A\n"," 49% 5800/11873 [00:13<00:14, 421.78it/s]\u001b[A\n"," 49% 5844/11873 [00:13<00:14, 425.82it/s]\u001b[A\n"," 50% 5888/11873 [00:14<00:13, 427.74it/s]\u001b[A\n"," 50% 5931/11873 [00:14<00:13, 425.90it/s]\u001b[A\n"," 50% 5975/11873 [00:14<00:13, 427.74it/s]\u001b[A\n"," 51% 6019/11873 [00:14<00:13, 428.70it/s]\u001b[A\n"," 51% 6062/11873 [00:14<00:13, 427.95it/s]\u001b[A\n"," 51% 6105/11873 [00:14<00:13, 426.90it/s]\u001b[A\n"," 52% 6148/11873 [00:14<00:13, 417.69it/s]\u001b[A\n"," 52% 6192/11873 [00:14<00:13, 421.53it/s]\u001b[A\n"," 53% 6236/11873 [00:14<00:13, 425.39it/s]\u001b[A\n"," 53% 6279/11873 [00:14<00:13, 415.97it/s]\u001b[A\n"," 53% 6322/11873 [00:15<00:13, 419.72it/s]\u001b[A\n"," 54% 6365/11873 [00:15<00:13, 422.13it/s]\u001b[A\n"," 54% 6408/11873 [00:15<00:12, 422.14it/s]\u001b[A\n"," 54% 6451/11873 [00:15<00:12, 421.47it/s]\u001b[A\n"," 55% 6494/11873 [00:15<00:12, 415.41it/s]\u001b[A\n"," 55% 6536/11873 [00:15<00:13, 403.46it/s]\u001b[A\n"," 55% 6577/11873 [00:15<00:13, 394.79it/s]\u001b[A\n"," 56% 6617/11873 [00:15<00:13, 392.95it/s]\u001b[A\n"," 56% 6658/11873 [00:15<00:13, 396.70it/s]\u001b[A\n"," 56% 6699/11873 [00:15<00:12, 398.77it/s]\u001b[A\n"," 57% 6739/11873 [00:16<00:13, 367.00it/s]\u001b[A\n"," 57% 6781/11873 [00:16<00:13, 380.34it/s]\u001b[A\n"," 57% 6824/11873 [00:16<00:12, 393.13it/s]\u001b[A\n"," 58% 6867/11873 [00:16<00:12, 401.97it/s]\u001b[A\n"," 58% 6908/11873 [00:16<00:12, 391.72it/s]\u001b[A\n"," 59% 6950/11873 [00:16<00:12, 398.48it/s]\u001b[A\n"," 59% 6993/11873 [00:16<00:12, 405.24it/s]\u001b[A\n"," 59% 7035/11873 [00:16<00:11, 407.51it/s]\u001b[A\n"," 60% 7078/11873 [00:16<00:11, 413.53it/s]\u001b[A\n"," 60% 7122/11873 [00:17<00:11, 418.40it/s]\u001b[A\n"," 60% 7165/11873 [00:17<00:11, 421.32it/s]\u001b[A\n"," 61% 7208/11873 [00:17<00:11, 421.85it/s]\u001b[A\n"," 61% 7251/11873 [00:17<00:11, 416.01it/s]\u001b[A\n"," 61% 7293/11873 [00:17<00:11, 416.15it/s]\u001b[A\n"," 62% 7335/11873 [00:17<00:11, 396.50it/s]\u001b[A\n"," 62% 7375/11873 [00:17<00:11, 395.21it/s]\u001b[A\n"," 62% 7415/11873 [00:17<00:11, 381.05it/s]\u001b[A\n"," 63% 7455/11873 [00:17<00:11, 386.28it/s]\u001b[A\n"," 63% 7497/11873 [00:17<00:11, 395.33it/s]\u001b[A\n"," 64% 7541/11873 [00:18<00:10, 408.23it/s]\u001b[A\n"," 64% 7584/11873 [00:18<00:10, 412.67it/s]\u001b[A\n"," 64% 7626/11873 [00:18<00:10, 410.66it/s]\u001b[A\n"," 65% 7668/11873 [00:18<00:10, 412.82it/s]\u001b[A\n"," 65% 7710/11873 [00:18<00:10, 387.00it/s]\u001b[A\n"," 65% 7750/11873 [00:18<00:10, 389.28it/s]\u001b[A\n"," 66% 7791/11873 [00:18<00:10, 394.65it/s]\u001b[A\n"," 66% 7833/11873 [00:18<00:10, 399.97it/s]\u001b[A\n"," 66% 7874/11873 [00:18<00:10, 381.23it/s]\u001b[A\n"," 67% 7915/11873 [00:19<00:10, 387.58it/s]\u001b[A\n"," 67% 7957/11873 [00:19<00:09, 395.35it/s]\u001b[A\n"," 67% 7998/11873 [00:19<00:09, 397.17it/s]\u001b[A\n"," 68% 8038/11873 [00:19<00:09, 395.41it/s]\u001b[A\n"," 68% 8078/11873 [00:19<00:09, 385.65it/s]\u001b[A\n"," 68% 8117/11873 [00:19<00:09, 376.94it/s]\u001b[A\n"," 69% 8158/11873 [00:19<00:09, 384.99it/s]\u001b[A\n"," 69% 8199/11873 [00:19<00:09, 390.30it/s]\u001b[A\n"," 69% 8241/11873 [00:19<00:09, 396.23it/s]\u001b[A\n"," 70% 8284/11873 [00:19<00:08, 405.37it/s]\u001b[A\n"," 70% 8327/11873 [00:20<00:08, 412.53it/s]\u001b[A\n"," 70% 8370/11873 [00:20<00:08, 415.83it/s]\u001b[A\n"," 71% 8412/11873 [00:20<00:08, 415.14it/s]\u001b[A\n"," 71% 8454/11873 [00:20<00:08, 412.01it/s]\u001b[A\n"," 72% 8496/11873 [00:20<00:08, 413.59it/s]\u001b[A\n"," 72% 8540/11873 [00:20<00:07, 418.60it/s]\u001b[A\n"," 72% 8583/11873 [00:20<00:07, 421.09it/s]\u001b[A\n"," 73% 8626/11873 [00:20<00:07, 419.06it/s]\u001b[A\n"," 73% 8669/11873 [00:20<00:07, 420.54it/s]\u001b[A\n"," 73% 8712/11873 [00:20<00:07, 417.90it/s]\u001b[A\n"," 74% 8754/11873 [00:21<00:07, 415.36it/s]\u001b[A\n"," 74% 8796/11873 [00:21<00:07, 408.17it/s]\u001b[A\n"," 74% 8837/11873 [00:21<00:07, 405.18it/s]\u001b[A\n"," 75% 8879/11873 [00:21<00:07, 407.52it/s]\u001b[A\n"," 75% 8921/11873 [00:21<00:07, 409.73it/s]\u001b[A\n"," 75% 8963/11873 [00:21<00:07, 411.03it/s]\u001b[A\n"," 76% 9007/11873 [00:21<00:06, 418.74it/s]\u001b[A\n"," 76% 9049/11873 [00:21<00:06, 418.00it/s]\u001b[A\n"," 77% 9092/11873 [00:21<00:06, 419.89it/s]\u001b[A\n"," 77% 9134/11873 [00:22<00:06, 417.51it/s]\u001b[A\n"," 77% 9177/11873 [00:22<00:06, 418.73it/s]\u001b[A\n"," 78% 9220/11873 [00:22<00:06, 420.96it/s]\u001b[A\n"," 78% 9263/11873 [00:22<00:06, 419.66it/s]\u001b[A\n"," 78% 9305/11873 [00:22<00:06, 419.68it/s]\u001b[A\n"," 79% 9347/11873 [00:22<00:06, 418.33it/s]\u001b[A\n"," 79% 9389/11873 [00:22<00:05, 418.77it/s]\u001b[A\n"," 79% 9431/11873 [00:22<00:05, 417.95it/s]\u001b[A\n"," 80% 9474/11873 [00:22<00:05, 418.96it/s]\u001b[A\n"," 80% 9517/11873 [00:22<00:05, 421.63it/s]\u001b[A\n"," 81% 9561/11873 [00:23<00:05, 424.62it/s]\u001b[A\n"," 81% 9604/11873 [00:23<00:05, 423.41it/s]\u001b[A\n"," 81% 9647/11873 [00:23<00:05, 422.65it/s]\u001b[A\n"," 82% 9691/11873 [00:23<00:05, 426.95it/s]\u001b[A\n"," 82% 9734/11873 [00:23<00:05, 427.09it/s]\u001b[A\n"," 82% 9777/11873 [00:23<00:04, 420.76it/s]\u001b[A\n"," 83% 9820/11873 [00:23<00:04, 411.74it/s]\u001b[A\n"," 83% 9862/11873 [00:23<00:04, 407.76it/s]\u001b[A\n"," 83% 9903/11873 [00:23<00:04, 406.47it/s]\u001b[A\n"," 84% 9946/11873 [00:23<00:04, 413.27it/s]\u001b[A\n"," 84% 9989/11873 [00:24<00:04, 417.65it/s]\u001b[A\n"," 85% 10033/11873 [00:24<00:04, 422.53it/s]\u001b[A\n"," 85% 10077/11873 [00:24<00:04, 426.55it/s]\u001b[A\n"," 85% 10120/11873 [00:24<00:04, 427.55it/s]\u001b[A\n"," 86% 10164/11873 [00:24<00:03, 430.20it/s]\u001b[A\n"," 86% 10208/11873 [00:24<00:03, 424.28it/s]\u001b[A\n"," 86% 10251/11873 [00:24<00:03, 425.91it/s]\u001b[A\n"," 87% 10294/11873 [00:24<00:03, 419.50it/s]\u001b[A\n"," 87% 10336/11873 [00:24<00:03, 419.62it/s]\u001b[A\n"," 87% 10379/11873 [00:24<00:03, 420.13it/s]\u001b[A\n"," 88% 10422/11873 [00:25<00:03, 414.70it/s]\u001b[A\n"," 88% 10464/11873 [00:25<00:03, 398.42it/s]\u001b[A\n"," 88% 10507/11873 [00:25<00:03, 407.12it/s]\u001b[A\n"," 89% 10548/11873 [00:25<00:03, 406.60it/s]\u001b[A\n"," 89% 10590/11873 [00:25<00:03, 409.17it/s]\u001b[A\n"," 90% 10631/11873 [00:25<00:03, 408.28it/s]\u001b[A\n"," 90% 10672/11873 [00:25<00:02, 401.42it/s]\u001b[A\n"," 90% 10715/11873 [00:25<00:02, 407.83it/s]\u001b[A\n"," 91% 10757/11873 [00:25<00:02, 408.70it/s]\u001b[A\n"," 91% 10799/11873 [00:26<00:02, 409.36it/s]\u001b[A\n"," 91% 10840/11873 [00:26<00:02, 381.56it/s]\u001b[A\n"," 92% 10882/11873 [00:26<00:02, 390.85it/s]\u001b[A\n"," 92% 10923/11873 [00:26<00:02, 390.74it/s]\u001b[A\n"," 92% 10966/11873 [00:26<00:02, 400.79it/s]\u001b[A\n"," 93% 11007/11873 [00:26<00:02, 394.39it/s]\u001b[A\n"," 93% 11049/11873 [00:26<00:02, 398.82it/s]\u001b[A\n"," 93% 11089/11873 [00:26<00:01, 393.63it/s]\u001b[A\n"," 94% 11131/11873 [00:26<00:01, 399.79it/s]\u001b[A\n"," 94% 11174/11873 [00:26<00:01, 406.80it/s]\u001b[A\n"," 94% 11217/11873 [00:27<00:01, 411.60it/s]\u001b[A\n"," 95% 11259/11873 [00:27<00:01, 403.70it/s]\u001b[A\n"," 95% 11302/11873 [00:27<00:01, 411.23it/s]\u001b[A\n"," 96% 11344/11873 [00:27<00:01, 410.28it/s]\u001b[A\n"," 96% 11386/11873 [00:27<00:01, 412.71it/s]\u001b[A\n"," 96% 11428/11873 [00:27<00:01, 409.39it/s]\u001b[A\n"," 97% 11469/11873 [00:27<00:00, 408.54it/s]\u001b[A\n"," 97% 11510/11873 [00:27<00:00, 405.55it/s]\u001b[A\n"," 97% 11551/11873 [00:27<00:00, 403.93it/s]\u001b[A\n"," 98% 11593/11873 [00:27<00:00, 408.02it/s]\u001b[A\n"," 98% 11635/11873 [00:28<00:00, 411.54it/s]\u001b[A\n"," 98% 11677/11873 [00:28<00:00, 412.60it/s]\u001b[A\n"," 99% 11719/11873 [00:28<00:00, 413.47it/s]\u001b[A\n"," 99% 11763/11873 [00:28<00:00, 418.62it/s]\u001b[A\n"," 99% 11806/11873 [00:28<00:00, 419.57it/s]\u001b[A\n","100% 11873/11873 [00:28<00:00, 414.44it/s]\n","04/05/2022 08:28:03 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","04/05/2022 08:28:03 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","04/05/2022 08:28:05 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","04/05/2022 08:28:08 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1521/1521 [03:38<00:00,  6.95it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 78.3232\n","  eval_HasAns_f1         = 84.4562\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 82.0017\n","  eval_NoAns_f1          = 82.0017\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 80.1651\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 83.2272\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 80.1651\n","  eval_f1                = 83.2272\n","  eval_samples           =   12165\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-05 08:28:08,425 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path roberta-base \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6hhWcxeQ4AEx","executionInfo":{"status":"ok","timestamp":1649192034709,"user_tz":240,"elapsed":10532055,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"2a3638b7-0bbc-4cd1-d205-e8ff31c6ccf2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["04/05/2022 17:58:27 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/05/2022 17:58:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Apr05_17-58-27_c282ee9c4a83,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/05/2022 17:58:27 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp9dt_09vt\n","Downloading builder script: 5.28kB [00:00, 3.96MB/s]       \n","04/05/2022 17:58:27 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","04/05/2022 17:58:27 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","04/05/2022 17:58:27 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpafu0v98q\n","Downloading metadata: 2.40kB [00:00, 2.28MB/s]       \n","04/05/2022 17:58:27 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","04/05/2022 17:58:27 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","04/05/2022 17:58:27 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","04/05/2022 17:58:27 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/05/2022 17:58:27 - INFO - datasets.builder - Generating dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","04/05/2022 17:58:27 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]04/05/2022 17:58:27 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpbnu5xoht\n","\n","Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n","Downloading data:  68% 6.52M/9.55M [00:00<00:00, 65.2MB/s]\u001b[A\n","Downloading data: 14.4MB [00:00, 73.0MB/s]                \u001b[A\n","Downloading data: 22.3MB [00:00, 76.1MB/s]\u001b[A\n","Downloading data: 30.2MB [00:00, 77.1MB/s]\u001b[A\n","Downloading data: 42.1MB [00:00, 76.0MB/s]\n","04/05/2022 17:58:28 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","04/05/2022 17:58:28 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","Downloading data files:  50% 1/2 [00:00<00:00,  1.44it/s]04/05/2022 17:58:28 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpbso7vv6y\n","\n","Downloading data: 4.37MB [00:00, 66.6MB/s]      \n","04/05/2022 17:58:28 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","04/05/2022 17:58:28 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","Downloading data files: 100% 2/2 [00:00<00:00,  2.12it/s]\n","04/05/2022 17:58:28 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","04/05/2022 17:58:28 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1171.43it/s]\n","04/05/2022 17:58:28 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n","04/05/2022 17:58:28 - INFO - datasets.builder - Generating train split\n","04/05/2022 17:58:42 - INFO - datasets.builder - Generating validation split\n","04/05/2022 17:58:43 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 287.86it/s]\n","[INFO|hub.py:583] 2022-04-05 17:58:43,829 >> https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpna5xeyh6\n","Downloading: 100% 481/481 [00:00<00:00, 407kB/s]\n","[INFO|hub.py:587] 2022-04-05 17:58:43,926 >> storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|hub.py:595] 2022-04-05 17:58:43,926 >> creating metadata file for /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:654] 2022-04-05 17:58:43,926 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-05 17:58:43,928 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-05 17:58:44,028 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-05 17:58:44,126 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-05 17:58:44,127 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|hub.py:583] 2022-04-05 17:58:44,323 >> https://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdhotfhd4\n","Downloading: 100% 878k/878k [00:00<00:00, 7.39MB/s]\n","[INFO|hub.py:587] 2022-04-05 17:58:44,551 >> storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|hub.py:595] 2022-04-05 17:58:44,552 >> creating metadata file for /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|hub.py:583] 2022-04-05 17:58:44,651 >> https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpirrkpofb\n","Downloading: 100% 446k/446k [00:00<00:00, 5.89MB/s]\n","[INFO|hub.py:587] 2022-04-05 17:58:44,850 >> storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|hub.py:595] 2022-04-05 17:58:44,850 >> creating metadata file for /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|hub.py:583] 2022-04-05 17:58:44,947 >> https://huggingface.co/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpe22r677o\n","Downloading: 100% 1.29M/1.29M [00:00<00:00, 9.82MB/s]\n","[INFO|hub.py:587] 2022-04-05 17:58:45,197 >> storing https://huggingface.co/roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|hub.py:595] 2022-04-05 17:58:45,198 >> creating metadata file for /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 17:58:45,503 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 17:58:45,503 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 17:58:45,503 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 17:58:45,503 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 17:58:45,503 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 17:58:45,503 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-05 17:58:45,600 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-05 17:58:45,601 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|hub.py:583] 2022-04-05 17:58:45,800 >> https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzlvffxlq\n","Downloading: 100% 478M/478M [00:09<00:00, 51.9MB/s]\n","[INFO|hub.py:587] 2022-04-05 17:58:55,713 >> storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[INFO|hub.py:595] 2022-04-05 17:58:55,713 >> creating metadata file for /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[INFO|modeling_utils.py:1772] 2022-04-05 17:58:55,714 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[WARNING|modeling_utils.py:2050] 2022-04-05 17:58:57,232 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-05 17:58:57,232 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/05/2022 17:58:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-f70d4265b8a69d5f.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:47<00:00,  2.78ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/05/2022 17:59:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-9f333badd4a4f030.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:14<00:00,  6.20s/ba]\n","04/05/2022 18:00:59 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp_8j8pqml\n","Downloading builder script: 6.46kB [00:00, 4.33MB/s]       \n","04/05/2022 18:00:59 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/05/2022 18:00:59 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/05/2022 18:00:59 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp7u71976m\n","Downloading extra modules: 11.3kB [00:00, 7.36MB/s]       \n","04/05/2022 18:00:59 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","04/05/2022 18:00:59 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-05 18:01:10,788 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-05 18:01:10,788 >>   Num examples = 131823\n","[INFO|trainer.py:1292] 2022-04-05 18:01:10,788 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-05 18:01:10,788 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1294] 2022-04-05 18:01:10,788 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1295] 2022-04-05 18:01:10,788 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-05 18:01:10,788 >>   Total optimization steps = 10986\n","{'loss': 1.9419, 'learning_rate': 1.9089750591662117e-05, 'epoch': 0.09}\n","  5% 500/10986 [07:38<2:40:06,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 18:08:49,303 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-05 18:08:49,304 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 18:08:50,213 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 18:08:50,213 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 18:08:50,213 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.2702, 'learning_rate': 1.8179501183324232e-05, 'epoch': 0.18}\n","  9% 1000/10986 [15:19<2:32:21,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 18:16:30,689 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-05 18:16:30,690 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 18:16:31,588 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 18:16:31,589 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 18:16:31,589 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.1276, 'learning_rate': 1.7269251774986348e-05, 'epoch': 0.27}\n"," 14% 1500/10986 [23:01<2:24:54,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 18:24:11,966 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-05 18:24:11,967 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 18:24:12,870 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 18:24:12,871 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 18:24:12,871 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.0761, 'learning_rate': 1.6359002366648463e-05, 'epoch': 0.36}\n"," 18% 2000/10986 [30:42<2:17:16,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 18:31:53,460 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-05 18:31:53,461 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 18:31:54,362 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 18:31:54,363 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 18:31:54,363 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.0231, 'learning_rate': 1.5448752958310578e-05, 'epoch': 0.46}\n"," 23% 2500/10986 [38:24<2:09:56,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 18:39:35,269 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-05 18:39:35,270 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 18:39:36,172 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 18:39:36,172 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 18:39:36,173 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.0051, 'learning_rate': 1.4538503549972694e-05, 'epoch': 0.55}\n"," 27% 3000/10986 [46:06<2:02:03,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 18:47:16,992 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-05 18:47:16,993 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 18:47:17,889 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 18:47:17,889 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 18:47:17,890 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.9658, 'learning_rate': 1.3628254141634809e-05, 'epoch': 0.64}\n"," 32% 3500/10986 [53:47<1:54:28,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 18:54:58,699 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-05 18:54:58,700 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 18:54:59,594 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 18:54:59,594 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 18:54:59,595 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.9474, 'learning_rate': 1.2718004733296924e-05, 'epoch': 0.73}\n"," 36% 4000/10986 [1:01:29<1:46:53,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 19:02:40,757 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-05 19:02:40,758 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 19:02:41,652 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 19:02:41,653 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 19:02:41,653 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.9242, 'learning_rate': 1.1807755324959041e-05, 'epoch': 0.82}\n"," 41% 4500/10986 [1:09:11<1:39:07,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 19:10:22,703 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-05 19:10:22,705 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 19:10:23,600 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 19:10:23,600 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 19:10:23,601 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.8893, 'learning_rate': 1.0897505916621156e-05, 'epoch': 0.91}\n"," 46% 5000/10986 [1:16:53<1:31:34,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 19:18:04,325 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-05 19:18:04,327 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 19:18:05,179 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 19:18:05,180 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 19:18:05,180 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.8699, 'learning_rate': 9.98725650828327e-06, 'epoch': 1.0}\n"," 50% 5500/10986 [1:24:33<1:23:08,  1.10it/s][INFO|trainer.py:2166] 2022-04-05 19:25:44,577 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-05 19:25:44,578 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 19:25:45,415 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 19:25:45,416 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 19:25:45,416 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.7098, 'learning_rate': 9.077007099945387e-06, 'epoch': 1.09}\n"," 55% 6000/10986 [1:32:14<1:16:11,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 19:33:25,455 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-05 19:33:25,456 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 19:33:26,302 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 19:33:26,303 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 19:33:26,304 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.704, 'learning_rate': 8.1667576916075e-06, 'epoch': 1.18}\n"," 59% 6500/10986 [1:39:55<1:08:31,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 19:41:06,152 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-05 19:41:06,153 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 19:41:07,004 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 19:41:07,004 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 19:41:07,005 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.7147, 'learning_rate': 7.256508283269617e-06, 'epoch': 1.27}\n"," 64% 7000/10986 [1:47:36<1:00:54,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 19:48:47,088 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-05 19:48:47,089 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 19:48:47,955 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 19:48:47,956 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 19:48:47,956 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.7024, 'learning_rate': 6.346258874931732e-06, 'epoch': 1.37}\n"," 68% 7500/10986 [1:55:17<53:09,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 19:56:28,420 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-05 19:56:28,422 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 19:56:29,298 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 19:56:29,298 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 19:56:29,299 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.7066, 'learning_rate': 5.436009466593847e-06, 'epoch': 1.46}\n"," 73% 8000/10986 [2:02:58<45:35,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 20:04:09,737 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-05 20:04:09,738 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 20:04:10,560 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 20:04:10,560 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 20:04:10,561 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.7043, 'learning_rate': 4.525760058255962e-06, 'epoch': 1.55}\n"," 77% 8500/10986 [2:10:39<38:02,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 20:11:50,452 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-05 20:11:50,453 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 20:11:51,238 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 20:11:51,239 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 20:11:51,239 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.693, 'learning_rate': 3.615510649918078e-06, 'epoch': 1.64}\n"," 82% 9000/10986 [2:18:20<30:23,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 20:19:31,047 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-05 20:19:31,048 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 20:19:31,864 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 20:19:31,865 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 20:19:31,865 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.6708, 'learning_rate': 2.705261241580193e-06, 'epoch': 1.73}\n"," 86% 9500/10986 [2:26:01<22:43,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 20:27:11,825 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-05 20:27:11,826 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 20:27:12,627 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 20:27:12,628 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 20:27:12,629 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.6713, 'learning_rate': 1.7950118332423086e-06, 'epoch': 1.82}\n"," 91% 10000/10986 [2:33:41<15:04,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 20:34:52,470 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-05 20:34:52,471 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 20:34:53,250 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 20:34:53,250 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 20:34:53,251 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6911, 'learning_rate': 8.847624249044238e-07, 'epoch': 1.91}\n"," 96% 10500/10986 [2:41:22<07:24,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 20:42:33,158 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-05 20:42:33,159 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 20:42:33,954 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 20:42:33,954 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 20:42:33,955 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","100% 10986/10986 [2:48:49<00:00,  1.21it/s][INFO|trainer.py:1530] 2022-04-05 20:50:00,592 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10129.8045, 'train_samples_per_second': 26.027, 'train_steps_per_second': 1.085, 'train_loss': 0.895428582355882, 'epoch': 2.0}\n","100% 10986/10986 [2:48:49<00:00,  1.08it/s]\n","[INFO|trainer.py:2166] 2022-04-05 20:50:00,595 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:441] 2022-04-05 20:50:00,596 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 20:50:01,374 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 20:50:01,375 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 20:50:01,376 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.8954\n","  train_runtime            = 2:48:49.80\n","  train_samples            =     131823\n","  train_samples_per_second =     26.027\n","  train_steps_per_second   =      1.085\n","04/05/2022 20:50:01 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-05 20:50:01,482 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-05 20:50:01,485 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-05 20:50:01,485 >>   Num examples = 12165\n","[INFO|trainer.py:2421] 2022-04-05 20:50:01,485 >>   Batch size = 8\n","100% 1520/1521 [02:55<00:00,  8.65it/s]04/05/2022 20:53:10 - INFO - utils_qa - Post-processing 11873 example predictions split into 12165 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 42/11873 [00:00<00:28, 414.76it/s]\u001b[A\n","  1% 84/11873 [00:00<00:31, 374.35it/s]\u001b[A\n","  1% 129/11873 [00:00<00:29, 404.93it/s]\u001b[A\n","  1% 177/11873 [00:00<00:27, 432.28it/s]\u001b[A\n","  2% 224/11873 [00:00<00:26, 443.07it/s]\u001b[A\n","  2% 270/11873 [00:00<00:25, 448.21it/s]\u001b[A\n","  3% 318/11873 [00:00<00:25, 457.73it/s]\u001b[A\n","  3% 364/11873 [00:00<00:25, 454.70it/s]\u001b[A\n","  3% 412/11873 [00:00<00:24, 460.19it/s]\u001b[A\n","  4% 459/11873 [00:01<00:24, 459.85it/s]\u001b[A\n","  4% 507/11873 [00:01<00:24, 464.16it/s]\u001b[A\n","  5% 554/11873 [00:01<00:25, 447.96it/s]\u001b[A\n","  5% 601/11873 [00:01<00:24, 454.09it/s]\u001b[A\n","  5% 648/11873 [00:01<00:24, 456.14it/s]\u001b[A\n","  6% 694/11873 [00:01<00:24, 454.36it/s]\u001b[A\n","  6% 740/11873 [00:01<00:24, 453.91it/s]\u001b[A\n","  7% 786/11873 [00:01<00:25, 441.96it/s]\u001b[A\n","  7% 832/11873 [00:01<00:24, 444.70it/s]\u001b[A\n","  7% 881/11873 [00:01<00:24, 456.90it/s]\u001b[A\n","  8% 927/11873 [00:02<00:24, 446.96it/s]\u001b[A\n","  8% 972/11873 [00:02<00:24, 442.24it/s]\u001b[A\n","  9% 1017/11873 [00:02<00:26, 415.65it/s]\u001b[A\n","  9% 1059/11873 [00:02<00:27, 390.97it/s]\u001b[A\n","  9% 1099/11873 [00:02<00:28, 378.13it/s]\u001b[A\n"," 10% 1138/11873 [00:02<00:29, 366.82it/s]\u001b[A\n"," 10% 1175/11873 [00:02<00:29, 359.05it/s]\u001b[A\n"," 10% 1212/11873 [00:02<00:29, 357.06it/s]\u001b[A\n"," 11% 1248/11873 [00:02<00:30, 345.24it/s]\u001b[A\n"," 11% 1283/11873 [00:03<00:31, 340.76it/s]\u001b[A\n"," 11% 1318/11873 [00:03<00:31, 336.30it/s]\u001b[A\n"," 11% 1352/11873 [00:03<00:31, 334.74it/s]\u001b[A\n"," 12% 1387/11873 [00:03<00:31, 337.81it/s]\u001b[A\n"," 12% 1421/11873 [00:03<00:30, 338.10it/s]\u001b[A\n"," 12% 1455/11873 [00:03<00:30, 336.67it/s]\u001b[A\n"," 13% 1491/11873 [00:03<00:30, 341.05it/s]\u001b[A\n"," 13% 1526/11873 [00:03<00:30, 341.18it/s]\u001b[A\n"," 13% 1561/11873 [00:03<00:30, 338.52it/s]\u001b[A\n"," 13% 1596/11873 [00:04<00:30, 341.37it/s]\u001b[A\n"," 14% 1631/11873 [00:04<00:29, 343.08it/s]\u001b[A\n"," 14% 1666/11873 [00:04<00:29, 342.49it/s]\u001b[A\n"," 14% 1701/11873 [00:04<00:29, 344.29it/s]\u001b[A\n"," 15% 1738/11873 [00:04<00:29, 349.36it/s]\u001b[A\n"," 15% 1773/11873 [00:04<00:29, 345.87it/s]\u001b[A\n"," 15% 1808/11873 [00:04<00:29, 343.30it/s]\u001b[A\n"," 16% 1843/11873 [00:04<00:30, 332.79it/s]\u001b[A\n"," 16% 1877/11873 [00:04<00:30, 331.82it/s]\u001b[A\n"," 16% 1911/11873 [00:04<00:29, 333.84it/s]\u001b[A\n"," 16% 1946/11873 [00:05<00:29, 337.26it/s]\u001b[A\n"," 17% 1980/11873 [00:05<00:29, 334.22it/s]\u001b[A\n"," 17% 2015/11873 [00:05<00:29, 338.53it/s]\u001b[A\n"," 17% 2051/11873 [00:05<00:28, 344.81it/s]\u001b[A\n"," 18% 2088/11873 [00:05<00:28, 349.16it/s]\u001b[A\n"," 18% 2123/11873 [00:05<00:28, 344.18it/s]\u001b[A\n"," 18% 2159/11873 [00:05<00:27, 348.04it/s]\u001b[A\n"," 18% 2194/11873 [00:05<00:28, 345.15it/s]\u001b[A\n"," 19% 2229/11873 [00:05<00:27, 346.13it/s]\u001b[A\n"," 19% 2265/11873 [00:05<00:27, 348.18it/s]\u001b[A\n"," 19% 2300/11873 [00:06<00:27, 345.58it/s]\u001b[A\n"," 20% 2335/11873 [00:06<00:27, 343.97it/s]\u001b[A\n"," 20% 2371/11873 [00:06<00:27, 346.11it/s]\u001b[A\n"," 20% 2406/11873 [00:06<00:27, 343.56it/s]\u001b[A\n"," 21% 2441/11873 [00:06<00:27, 340.03it/s]\u001b[A\n"," 21% 2476/11873 [00:06<00:27, 340.12it/s]\u001b[A\n"," 21% 2513/11873 [00:06<00:27, 346.25it/s]\u001b[A\n"," 21% 2548/11873 [00:06<00:27, 344.20it/s]\u001b[A\n"," 22% 2583/11873 [00:06<00:27, 343.00it/s]\u001b[A\n"," 22% 2619/11873 [00:06<00:26, 346.54it/s]\u001b[A\n"," 22% 2655/11873 [00:07<00:26, 348.54it/s]\u001b[A\n"," 23% 2691/11873 [00:07<00:26, 349.32it/s]\u001b[A\n"," 23% 2727/11873 [00:07<00:26, 350.03it/s]\u001b[A\n"," 23% 2763/11873 [00:07<00:26, 350.30it/s]\u001b[A\n"," 24% 2800/11873 [00:07<00:25, 353.21it/s]\u001b[A\n"," 24% 2836/11873 [00:07<00:25, 349.79it/s]\u001b[A\n"," 24% 2872/11873 [00:07<00:25, 350.13it/s]\u001b[A\n"," 24% 2908/11873 [00:07<00:25, 345.00it/s]\u001b[A\n"," 25% 2943/11873 [00:07<00:26, 337.94it/s]\u001b[A\n"," 25% 2978/11873 [00:08<00:26, 339.26it/s]\u001b[A\n"," 25% 3012/11873 [00:08<00:26, 332.95it/s]\u001b[A\n"," 26% 3046/11873 [00:08<00:26, 330.32it/s]\u001b[A\n"," 26% 3080/11873 [00:08<00:26, 327.93it/s]\u001b[A\n"," 26% 3113/11873 [00:08<00:28, 304.21it/s]\u001b[A\n"," 26% 3144/11873 [00:08<00:32, 267.70it/s]\u001b[A\n"," 27% 3172/11873 [00:08<00:33, 259.25it/s]\u001b[A\n"," 27% 3207/11873 [00:08<00:30, 281.42it/s]\u001b[A\n"," 27% 3243/11873 [00:08<00:28, 300.66it/s]\u001b[A\n"," 28% 3274/11873 [00:09<00:30, 284.47it/s]\u001b[A\n"," 28% 3304/11873 [00:09<00:39, 214.92it/s]\u001b[A\n"," 28% 3329/11873 [00:09<00:42, 200.53it/s]\u001b[A\n"," 28% 3352/11873 [00:09<00:43, 194.04it/s]\u001b[A\n"," 28% 3373/11873 [00:09<00:44, 189.89it/s]\u001b[A\n"," 29% 3405/11873 [00:09<00:38, 220.28it/s]\u001b[A\n"," 29% 3438/11873 [00:09<00:34, 247.31it/s]\u001b[A\n"," 29% 3470/11873 [00:10<00:31, 266.49it/s]\u001b[A\n"," 30% 3504/11873 [00:10<00:29, 286.43it/s]\u001b[A\n"," 30% 3539/11873 [00:10<00:27, 303.75it/s]\u001b[A\n"," 30% 3573/11873 [00:10<00:26, 313.23it/s]\u001b[A\n"," 30% 3609/11873 [00:10<00:25, 325.07it/s]\u001b[A\n"," 31% 3646/11873 [00:10<00:24, 338.07it/s]\u001b[A\n"," 31% 3681/11873 [00:10<00:24, 328.83it/s]\u001b[A\n"," 31% 3715/11873 [00:10<00:25, 322.51it/s]\u001b[A\n"," 32% 3750/11873 [00:10<00:24, 329.66it/s]\u001b[A\n"," 32% 3784/11873 [00:10<00:24, 328.90it/s]\u001b[A\n"," 32% 3818/11873 [00:11<00:25, 319.00it/s]\u001b[A\n"," 32% 3851/11873 [00:11<00:25, 314.63it/s]\u001b[A\n"," 33% 3885/11873 [00:11<00:24, 321.36it/s]\u001b[A\n"," 33% 3918/11873 [00:11<00:26, 297.36it/s]\u001b[A\n"," 33% 3952/11873 [00:11<00:25, 306.88it/s]\u001b[A\n"," 34% 3986/11873 [00:11<00:25, 314.31it/s]\u001b[A\n"," 34% 4023/11873 [00:11<00:23, 328.19it/s]\u001b[A\n"," 34% 4058/11873 [00:11<00:23, 332.90it/s]\u001b[A\n"," 34% 4092/11873 [00:11<00:23, 330.18it/s]\u001b[A\n"," 35% 4126/11873 [00:12<00:23, 329.17it/s]\u001b[A\n"," 35% 4160/11873 [00:12<00:25, 304.80it/s]\u001b[A\n"," 35% 4192/11873 [00:12<00:24, 308.72it/s]\u001b[A\n"," 36% 4224/11873 [00:12<00:24, 311.21it/s]\u001b[A\n"," 36% 4260/11873 [00:12<00:23, 323.17it/s]\u001b[A\n"," 36% 4296/11873 [00:12<00:22, 333.11it/s]\u001b[A\n"," 36% 4333/11873 [00:12<00:22, 341.74it/s]\u001b[A\n"," 37% 4369/11873 [00:12<00:21, 345.81it/s]\u001b[A\n"," 37% 4404/11873 [00:12<00:22, 337.91it/s]\u001b[A\n"," 37% 4438/11873 [00:13<00:27, 273.70it/s]\u001b[A\n"," 38% 4474/11873 [00:13<00:25, 293.77it/s]\u001b[A\n"," 38% 4509/11873 [00:13<00:24, 306.66it/s]\u001b[A\n"," 38% 4542/11873 [00:13<00:23, 310.40it/s]\u001b[A\n"," 39% 4578/11873 [00:13<00:22, 323.77it/s]\u001b[A\n"," 39% 4613/11873 [00:13<00:21, 330.89it/s]\u001b[A\n"," 39% 4647/11873 [00:13<00:21, 331.55it/s]\u001b[A\n"," 39% 4683/11873 [00:13<00:21, 338.85it/s]\u001b[A\n"," 40% 4718/11873 [00:13<00:21, 339.34it/s]\u001b[A\n"," 40% 4753/11873 [00:13<00:21, 331.47it/s]\u001b[A\n"," 40% 4788/11873 [00:14<00:21, 333.78it/s]\u001b[A\n"," 41% 4823/11873 [00:14<00:20, 335.80it/s]\u001b[A\n"," 41% 4857/11873 [00:14<00:20, 335.21it/s]\u001b[A\n"," 41% 4892/11873 [00:14<00:20, 338.77it/s]\u001b[A\n"," 41% 4927/11873 [00:14<00:20, 340.94it/s]\u001b[A\n"," 42% 4962/11873 [00:14<00:20, 341.48it/s]\u001b[A\n"," 42% 4998/11873 [00:14<00:19, 345.03it/s]\u001b[A\n"," 42% 5033/11873 [00:14<00:19, 342.14it/s]\u001b[A\n"," 43% 5068/11873 [00:14<00:19, 344.22it/s]\u001b[A\n"," 43% 5105/11873 [00:14<00:19, 350.36it/s]\u001b[A\n"," 43% 5141/11873 [00:15<00:19, 350.29it/s]\u001b[A\n"," 44% 5177/11873 [00:15<00:19, 348.33it/s]\u001b[A\n"," 44% 5214/11873 [00:15<00:18, 352.58it/s]\u001b[A\n"," 44% 5250/11873 [00:15<00:18, 351.77it/s]\u001b[A\n"," 45% 5286/11873 [00:15<00:20, 324.33it/s]\u001b[A\n"," 45% 5323/11873 [00:15<00:19, 335.54it/s]\u001b[A\n"," 45% 5360/11873 [00:15<00:19, 342.63it/s]\u001b[A\n"," 45% 5395/11873 [00:15<00:18, 343.51it/s]\u001b[A\n"," 46% 5430/11873 [00:15<00:19, 338.36it/s]\u001b[A\n"," 46% 5464/11873 [00:16<00:19, 330.34it/s]\u001b[A\n"," 46% 5498/11873 [00:16<00:19, 331.50it/s]\u001b[A\n"," 47% 5534/11873 [00:16<00:18, 337.10it/s]\u001b[A\n"," 47% 5568/11873 [00:16<00:19, 331.00it/s]\u001b[A\n"," 47% 5602/11873 [00:16<00:19, 329.26it/s]\u001b[A\n"," 47% 5636/11873 [00:16<00:18, 331.26it/s]\u001b[A\n"," 48% 5672/11873 [00:16<00:18, 339.40it/s]\u001b[A\n"," 48% 5707/11873 [00:16<00:18, 339.68it/s]\u001b[A\n"," 48% 5741/11873 [00:16<00:18, 338.15it/s]\u001b[A\n"," 49% 5775/11873 [00:16<00:18, 337.52it/s]\u001b[A\n"," 49% 5810/11873 [00:17<00:17, 340.88it/s]\u001b[A\n"," 49% 5845/11873 [00:17<00:17, 337.60it/s]\u001b[A\n"," 50% 5881/11873 [00:17<00:17, 343.07it/s]\u001b[A\n"," 50% 5916/11873 [00:17<00:17, 339.56it/s]\u001b[A\n"," 50% 5950/11873 [00:17<00:17, 334.92it/s]\u001b[A\n"," 50% 5984/11873 [00:17<00:17, 333.83it/s]\u001b[A\n"," 51% 6018/11873 [00:17<00:17, 332.91it/s]\u001b[A\n"," 51% 6052/11873 [00:17<00:17, 328.58it/s]\u001b[A\n"," 51% 6087/11873 [00:17<00:17, 334.77it/s]\u001b[A\n"," 52% 6121/11873 [00:17<00:17, 336.17it/s]\u001b[A\n"," 52% 6155/11873 [00:18<00:17, 333.39it/s]\u001b[A\n"," 52% 6189/11873 [00:18<00:17, 332.01it/s]\u001b[A\n"," 52% 6223/11873 [00:18<00:17, 331.22it/s]\u001b[A\n"," 53% 6258/11873 [00:18<00:16, 334.35it/s]\u001b[A\n"," 53% 6294/11873 [00:18<00:16, 340.97it/s]\u001b[A\n"," 53% 6329/11873 [00:18<00:16, 342.44it/s]\u001b[A\n"," 54% 6364/11873 [00:18<00:16, 343.51it/s]\u001b[A\n"," 54% 6400/11873 [00:18<00:15, 347.04it/s]\u001b[A\n"," 54% 6435/11873 [00:18<00:16, 338.78it/s]\u001b[A\n"," 55% 6471/11873 [00:19<00:15, 344.04it/s]\u001b[A\n"," 55% 6506/11873 [00:19<00:15, 343.59it/s]\u001b[A\n"," 55% 6541/11873 [00:19<00:15, 343.19it/s]\u001b[A\n"," 55% 6576/11873 [00:19<00:15, 335.73it/s]\u001b[A\n"," 56% 6610/11873 [00:19<00:15, 330.31it/s]\u001b[A\n"," 56% 6645/11873 [00:19<00:15, 334.72it/s]\u001b[A\n"," 56% 6679/11873 [00:19<00:15, 333.93it/s]\u001b[A\n"," 57% 6713/11873 [00:19<00:16, 305.63it/s]\u001b[A\n"," 57% 6745/11873 [00:19<00:16, 305.70it/s]\u001b[A\n"," 57% 6777/11873 [00:19<00:16, 308.05it/s]\u001b[A\n"," 57% 6809/11873 [00:20<00:16, 310.32it/s]\u001b[A\n"," 58% 6843/11873 [00:20<00:15, 316.45it/s]\u001b[A\n"," 58% 6875/11873 [00:20<00:15, 317.24it/s]\u001b[A\n"," 58% 6907/11873 [00:20<00:15, 315.17it/s]\u001b[A\n"," 58% 6940/11873 [00:20<00:15, 318.64it/s]\u001b[A\n"," 59% 6973/11873 [00:20<00:15, 321.95it/s]\u001b[A\n"," 59% 7009/11873 [00:20<00:14, 331.94it/s]\u001b[A\n"," 59% 7044/11873 [00:20<00:14, 336.74it/s]\u001b[A\n"," 60% 7078/11873 [00:20<00:14, 332.72it/s]\u001b[A\n"," 60% 7112/11873 [00:21<00:14, 329.71it/s]\u001b[A\n"," 60% 7145/11873 [00:21<00:14, 322.87it/s]\u001b[A\n"," 60% 7179/11873 [00:21<00:14, 327.28it/s]\u001b[A\n"," 61% 7214/11873 [00:21<00:14, 332.77it/s]\u001b[A\n"," 61% 7248/11873 [00:21<00:14, 327.99it/s]\u001b[A\n"," 61% 7281/11873 [00:21<00:13, 328.20it/s]\u001b[A\n"," 62% 7315/11873 [00:21<00:13, 330.92it/s]\u001b[A\n"," 62% 7349/11873 [00:21<00:13, 333.39it/s]\u001b[A\n"," 62% 7384/11873 [00:21<00:13, 336.13it/s]\u001b[A\n"," 62% 7418/11873 [00:21<00:14, 314.65it/s]\u001b[A\n"," 63% 7453/11873 [00:22<00:13, 323.08it/s]\u001b[A\n"," 63% 7489/11873 [00:22<00:13, 333.60it/s]\u001b[A\n"," 63% 7525/11873 [00:22<00:12, 339.63it/s]\u001b[A\n"," 64% 7563/11873 [00:22<00:12, 348.80it/s]\u001b[A\n"," 64% 7598/11873 [00:22<00:12, 341.34it/s]\u001b[A\n"," 64% 7633/11873 [00:22<00:12, 333.44it/s]\u001b[A\n"," 65% 7668/11873 [00:22<00:12, 336.54it/s]\u001b[A\n"," 65% 7702/11873 [00:22<00:12, 326.30it/s]\u001b[A\n"," 65% 7735/11873 [00:22<00:13, 310.91it/s]\u001b[A\n"," 65% 7767/11873 [00:23<00:13, 312.51it/s]\u001b[A\n"," 66% 7801/11873 [00:23<00:12, 319.15it/s]\u001b[A\n"," 66% 7834/11873 [00:23<00:12, 313.86it/s]\u001b[A\n"," 66% 7867/11873 [00:23<00:12, 313.52it/s]\u001b[A\n"," 67% 7899/11873 [00:23<00:13, 302.12it/s]\u001b[A\n"," 67% 7933/11873 [00:23<00:12, 310.98it/s]\u001b[A\n"," 67% 7967/11873 [00:23<00:12, 318.79it/s]\u001b[A\n"," 67% 8001/11873 [00:23<00:11, 323.31it/s]\u001b[A\n"," 68% 8037/11873 [00:23<00:11, 332.68it/s]\u001b[A\n"," 68% 8071/11873 [00:23<00:11, 328.60it/s]\u001b[A\n"," 68% 8105/11873 [00:24<00:11, 329.12it/s]\u001b[A\n"," 69% 8138/11873 [00:24<00:11, 324.39it/s]\u001b[A\n"," 69% 8173/11873 [00:24<00:11, 329.09it/s]\u001b[A\n"," 69% 8207/11873 [00:24<00:11, 329.67it/s]\u001b[A\n"," 69% 8242/11873 [00:24<00:10, 335.11it/s]\u001b[A\n"," 70% 8276/11873 [00:24<00:10, 334.86it/s]\u001b[A\n"," 70% 8311/11873 [00:24<00:10, 338.07it/s]\u001b[A\n"," 70% 8347/11873 [00:24<00:10, 342.05it/s]\u001b[A\n"," 71% 8383/11873 [00:24<00:10, 345.59it/s]\u001b[A\n"," 71% 8418/11873 [00:24<00:10, 335.76it/s]\u001b[A\n"," 71% 8452/11873 [00:25<00:10, 327.08it/s]\u001b[A\n"," 71% 8485/11873 [00:25<00:10, 325.46it/s]\u001b[A\n"," 72% 8521/11873 [00:25<00:10, 334.03it/s]\u001b[A\n"," 72% 8556/11873 [00:25<00:09, 337.13it/s]\u001b[A\n"," 72% 8593/11873 [00:25<00:09, 343.57it/s]\u001b[A\n"," 73% 8628/11873 [00:25<00:09, 332.23it/s]\u001b[A\n"," 73% 8662/11873 [00:25<00:09, 334.39it/s]\u001b[A\n"," 73% 8697/11873 [00:25<00:09, 336.71it/s]\u001b[A\n"," 74% 8731/11873 [00:25<00:09, 336.70it/s]\u001b[A\n"," 74% 8765/11873 [00:26<00:09, 330.58it/s]\u001b[A\n"," 74% 8799/11873 [00:26<00:09, 328.97it/s]\u001b[A\n"," 74% 8833/11873 [00:26<00:09, 330.92it/s]\u001b[A\n"," 75% 8868/11873 [00:26<00:08, 334.60it/s]\u001b[A\n"," 75% 8902/11873 [00:26<00:08, 335.65it/s]\u001b[A\n"," 75% 8937/11873 [00:26<00:08, 337.55it/s]\u001b[A\n"," 76% 8973/11873 [00:26<00:08, 341.70it/s]\u001b[A\n"," 76% 9009/11873 [00:26<00:08, 344.87it/s]\u001b[A\n"," 76% 9044/11873 [00:26<00:08, 341.75it/s]\u001b[A\n"," 76% 9079/11873 [00:26<00:08, 340.09it/s]\u001b[A\n"," 77% 9114/11873 [00:27<00:08, 327.92it/s]\u001b[A\n"," 77% 9147/11873 [00:27<00:08, 322.79it/s]\u001b[A\n"," 77% 9180/11873 [00:27<00:08, 314.59it/s]\u001b[A\n"," 78% 9213/11873 [00:27<00:08, 318.38it/s]\u001b[A\n"," 78% 9247/11873 [00:27<00:08, 324.23it/s]\u001b[A\n"," 78% 9282/11873 [00:27<00:07, 329.20it/s]\u001b[A\n"," 78% 9317/11873 [00:27<00:07, 334.42it/s]\u001b[A\n"," 79% 9352/11873 [00:27<00:07, 336.19it/s]\u001b[A\n"," 79% 9386/11873 [00:27<00:07, 335.13it/s]\u001b[A\n"," 79% 9420/11873 [00:28<00:07, 334.27it/s]\u001b[A\n"," 80% 9455/11873 [00:28<00:07, 335.25it/s]\u001b[A\n"," 80% 9490/11873 [00:28<00:07, 337.00it/s]\u001b[A\n"," 80% 9525/11873 [00:28<00:06, 339.58it/s]\u001b[A\n"," 81% 9560/11873 [00:28<00:06, 341.83it/s]\u001b[A\n"," 81% 9595/11873 [00:28<00:06, 342.26it/s]\u001b[A\n"," 81% 9630/11873 [00:28<00:06, 336.85it/s]\u001b[A\n"," 81% 9664/11873 [00:28<00:06, 336.69it/s]\u001b[A\n"," 82% 9701/11873 [00:28<00:06, 344.89it/s]\u001b[A\n"," 82% 9736/11873 [00:28<00:06, 339.67it/s]\u001b[A\n"," 82% 9772/11873 [00:29<00:06, 343.61it/s]\u001b[A\n"," 83% 9807/11873 [00:29<00:05, 344.74it/s]\u001b[A\n"," 83% 9842/11873 [00:29<00:05, 341.91it/s]\u001b[A\n"," 83% 9877/11873 [00:29<00:05, 340.99it/s]\u001b[A\n"," 83% 9912/11873 [00:29<00:05, 337.15it/s]\u001b[A\n"," 84% 9946/11873 [00:29<00:05, 328.78it/s]\u001b[A\n"," 84% 9979/11873 [00:29<00:05, 319.99it/s]\u001b[A\n"," 84% 10012/11873 [00:29<00:05, 318.19it/s]\u001b[A\n"," 85% 10044/11873 [00:29<00:05, 316.94it/s]\u001b[A\n"," 85% 10076/11873 [00:29<00:05, 315.82it/s]\u001b[A\n"," 85% 10112/11873 [00:30<00:05, 326.17it/s]\u001b[A\n"," 85% 10148/11873 [00:30<00:05, 334.24it/s]\u001b[A\n"," 86% 10184/11873 [00:30<00:04, 340.38it/s]\u001b[A\n"," 86% 10219/11873 [00:30<00:04, 341.81it/s]\u001b[A\n"," 86% 10254/11873 [00:30<00:04, 341.92it/s]\u001b[A\n"," 87% 10289/11873 [00:30<00:04, 327.92it/s]\u001b[A\n"," 87% 10322/11873 [00:30<00:04, 323.93it/s]\u001b[A\n"," 87% 10355/11873 [00:30<00:04, 321.94it/s]\u001b[A\n"," 87% 10388/11873 [00:30<00:04, 319.89it/s]\u001b[A\n"," 88% 10421/11873 [00:31<00:04, 317.17it/s]\u001b[A\n"," 88% 10453/11873 [00:31<00:04, 312.29it/s]\u001b[A\n"," 88% 10487/11873 [00:31<00:04, 319.15it/s]\u001b[A\n"," 89% 10523/11873 [00:31<00:04, 329.53it/s]\u001b[A\n"," 89% 10556/11873 [00:31<00:04, 327.53it/s]\u001b[A\n"," 89% 10591/11873 [00:31<00:03, 332.61it/s]\u001b[A\n"," 89% 10626/11873 [00:31<00:03, 335.08it/s]\u001b[A\n"," 90% 10660/11873 [00:31<00:03, 335.81it/s]\u001b[A\n"," 90% 10695/11873 [00:31<00:03, 338.61it/s]\u001b[A\n"," 90% 10730/11873 [00:31<00:03, 340.20it/s]\u001b[A\n"," 91% 10765/11873 [00:32<00:03, 330.87it/s]\u001b[A\n"," 91% 10799/11873 [00:32<00:03, 322.43it/s]\u001b[A\n"," 91% 10832/11873 [00:32<00:03, 296.46it/s]\u001b[A\n"," 92% 10868/11873 [00:32<00:03, 312.11it/s]\u001b[A\n"," 92% 10904/11873 [00:32<00:02, 324.43it/s]\u001b[A\n"," 92% 10937/11873 [00:32<00:02, 320.52it/s]\u001b[A\n"," 92% 10970/11873 [00:32<00:02, 316.17it/s]\u001b[A\n"," 93% 11002/11873 [00:32<00:02, 315.09it/s]\u001b[A\n"," 93% 11034/11873 [00:32<00:02, 315.56it/s]\u001b[A\n"," 93% 11068/11873 [00:33<00:02, 321.11it/s]\u001b[A\n"," 94% 11102/11873 [00:33<00:02, 325.01it/s]\u001b[A\n"," 94% 11135/11873 [00:33<00:02, 324.38it/s]\u001b[A\n"," 94% 11168/11873 [00:33<00:02, 320.56it/s]\u001b[A\n"," 94% 11201/11873 [00:33<00:02, 315.78it/s]\u001b[A\n"," 95% 11233/11873 [00:33<00:02, 314.60it/s]\u001b[A\n"," 95% 11265/11873 [00:33<00:01, 311.12it/s]\u001b[A\n"," 95% 11298/11873 [00:33<00:01, 314.14it/s]\u001b[A\n"," 95% 11330/11873 [00:33<00:01, 314.94it/s]\u001b[A\n"," 96% 11362/11873 [00:33<00:01, 314.52it/s]\u001b[A\n"," 96% 11396/11873 [00:34<00:01, 319.61it/s]\u001b[A\n"," 96% 11428/11873 [00:34<00:01, 319.34it/s]\u001b[A\n"," 97% 11462/11873 [00:34<00:01, 324.56it/s]\u001b[A\n"," 97% 11495/11873 [00:34<00:01, 320.54it/s]\u001b[A\n"," 97% 11528/11873 [00:34<00:01, 218.14it/s]\u001b[A\n"," 97% 11560/11873 [00:34<00:01, 240.18it/s]\u001b[A\n"," 98% 11595/11873 [00:34<00:01, 265.58it/s]\u001b[A\n"," 98% 11628/11873 [00:34<00:00, 281.37it/s]\u001b[A\n"," 98% 11661/11873 [00:35<00:00, 294.16it/s]\u001b[A\n"," 98% 11693/11873 [00:35<00:00, 297.84it/s]\u001b[A\n"," 99% 11727/11873 [00:35<00:00, 307.40it/s]\u001b[A\n"," 99% 11762/11873 [00:35<00:00, 317.04it/s]\u001b[A\n"," 99% 11797/11873 [00:35<00:00, 325.02it/s]\u001b[A\n","100% 11831/11873 [00:35<00:00, 325.40it/s]\u001b[A\n","100% 11873/11873 [00:35<00:00, 332.84it/s]\n","04/05/2022 20:53:46 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","04/05/2022 20:53:46 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","04/05/2022 20:53:48 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","04/05/2022 20:53:52 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1521/1521 [03:50<00:00,  6.59it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 78.5594\n","  eval_HasAns_f1         = 84.4821\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 81.3625\n","  eval_NoAns_f1          = 81.3625\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 79.9629\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 82.9201\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 79.9629\n","  eval_f1                = 82.9201\n","  eval_samples           =   12165\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-05 20:53:52,612 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path roberta-base \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1DUBdi1r44vJ","executionInfo":{"status":"ok","timestamp":1649208974553,"user_tz":240,"elapsed":10501845,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"3ce16c1a-b95e-409d-986f-e2362bdaa672"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["04/05/2022 22:41:17 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/05/2022 22:41:17 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Apr05_22-41-17_c282ee9c4a83,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/05/2022 22:41:18 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","04/05/2022 22:41:18 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/05/2022 22:41:18 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/05/2022 22:41:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/05/2022 22:41:18 - WARNING - datasets.builder - Reusing dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","04/05/2022 22:41:18 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","100% 2/2 [00:00<00:00, 534.17it/s]\n","[INFO|configuration_utils.py:654] 2022-04-05 22:41:18,500 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-05 22:41:18,503 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-05 22:41:18,604 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-05 22:41:18,706 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-05 22:41:18,707 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 22:41:19,414 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 22:41:19,414 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 22:41:19,414 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 22:41:19,414 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 22:41:19,414 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-05 22:41:19,414 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-05 22:41:19,527 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-05 22:41:19,527 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|modeling_utils.py:1772] 2022-04-05 22:41:19,793 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[WARNING|modeling_utils.py:2050] 2022-04-05 22:41:22,901 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-05 22:41:22,901 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","04/05/2022 22:41:22 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-f70d4265b8a69d5f.arrow\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/05/2022 22:41:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-0be1d39efeab877d.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:12<00:00,  6.04s/ba]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-05 22:42:39,848 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-05 22:42:39,848 >>   Num examples = 131823\n","[INFO|trainer.py:1292] 2022-04-05 22:42:39,848 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-05 22:42:39,848 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1294] 2022-04-05 22:42:39,848 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1295] 2022-04-05 22:42:39,848 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-05 22:42:39,848 >>   Total optimization steps = 10986\n","{'loss': 1.7704, 'learning_rate': 3.8179501183324234e-05, 'epoch': 0.09}\n","  5% 500/10986 [07:39<2:40:12,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 22:50:18,932 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-05 22:50:18,933 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 22:50:19,846 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 22:50:19,846 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 22:50:19,846 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.2195, 'learning_rate': 3.6359002366648465e-05, 'epoch': 0.18}\n","  9% 1000/10986 [15:21<2:33:03,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 22:58:01,750 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-05 22:58:01,751 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 22:58:02,672 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 22:58:02,679 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 22:58:02,679 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.1118, 'learning_rate': 3.4538503549972695e-05, 'epoch': 0.27}\n"," 14% 1500/10986 [23:04<2:25:34,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 23:05:44,508 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-05 23:05:44,509 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 23:05:45,428 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 23:05:45,435 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 23:05:45,435 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.0623, 'learning_rate': 3.2718004733296926e-05, 'epoch': 0.36}\n"," 18% 2000/10986 [30:47<2:17:48,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 23:13:27,636 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-05 23:13:27,637 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 23:13:28,554 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 23:13:28,561 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 23:13:28,562 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.0242, 'learning_rate': 3.0897505916621156e-05, 'epoch': 0.46}\n"," 23% 2500/10986 [38:30<2:09:37,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 23:21:09,948 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-05 23:21:09,950 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 23:21:10,804 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 23:21:10,804 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 23:21:10,810 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.0006, 'learning_rate': 2.9077007099945387e-05, 'epoch': 0.55}\n"," 27% 3000/10986 [46:12<2:01:52,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 23:28:51,874 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-05 23:28:51,875 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 23:28:52,749 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 23:28:52,751 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 23:28:52,751 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.9541, 'learning_rate': 2.7256508283269618e-05, 'epoch': 0.64}\n"," 32% 3500/10986 [53:53<1:54:11,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 23:36:33,757 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-05 23:36:33,758 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 23:36:34,616 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 23:36:34,622 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 23:36:34,622 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.9314, 'learning_rate': 2.543600946659385e-05, 'epoch': 0.73}\n"," 36% 4000/10986 [1:01:36<1:46:29,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 23:44:15,926 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-05 23:44:15,927 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 23:44:16,769 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 23:44:16,772 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 23:44:16,778 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.9063, 'learning_rate': 2.3615510649918082e-05, 'epoch': 0.82}\n"," 41% 4500/10986 [1:09:18<1:39:16,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 23:51:58,063 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-05 23:51:58,064 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 23:51:58,939 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 23:51:58,940 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 23:51:58,946 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.8766, 'learning_rate': 2.1795011833242313e-05, 'epoch': 0.91}\n"," 46% 5000/10986 [1:17:00<1:31:25,  1.09it/s][INFO|trainer.py:2166] 2022-04-05 23:59:40,100 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-05 23:59:40,101 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-05 23:59:40,972 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-05 23:59:40,975 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-05 23:59:40,975 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.855, 'learning_rate': 1.997451301656654e-05, 'epoch': 1.0}\n"," 50% 5500/10986 [1:24:42<1:23:23,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 00:07:22,557 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 00:07:22,558 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 00:07:23,436 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 00:07:23,438 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 00:07:23,444 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.6551, 'learning_rate': 1.8154014199890774e-05, 'epoch': 1.09}\n"," 55% 6000/10986 [1:32:26<1:16:24,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 00:15:06,011 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 00:15:06,012 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 00:15:06,896 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 00:15:06,897 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 00:15:06,903 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.6331, 'learning_rate': 1.6333515383215e-05, 'epoch': 1.18}\n"," 59% 6500/10986 [1:40:12<1:08:42,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 00:22:51,993 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 00:22:51,994 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 00:22:52,917 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 00:22:52,918 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 00:22:52,924 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.6471, 'learning_rate': 1.4513016566539234e-05, 'epoch': 1.27}\n"," 64% 7000/10986 [1:47:55<1:01:08,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 00:30:35,223 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 00:30:35,225 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 00:30:36,150 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 00:30:36,150 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 00:30:36,156 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.6385, 'learning_rate': 1.2692517749863464e-05, 'epoch': 1.37}\n"," 68% 7500/10986 [1:55:42<53:23,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 00:38:21,950 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 00:38:21,951 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 00:38:22,877 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 00:38:22,879 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 00:38:22,879 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.6326, 'learning_rate': 1.0872018933187693e-05, 'epoch': 1.46}\n"," 73% 8000/10986 [2:03:25<45:50,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 00:46:05,560 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 00:46:05,561 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 00:46:06,456 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 00:46:06,459 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 00:46:06,459 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.633, 'learning_rate': 9.051520116511924e-06, 'epoch': 1.55}\n"," 77% 8500/10986 [2:11:12<38:02,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 00:53:52,424 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 00:53:52,425 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 00:53:53,321 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 00:53:53,322 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 00:53:53,322 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.6191, 'learning_rate': 7.231021299836156e-06, 'epoch': 1.64}\n"," 82% 9000/10986 [2:18:55<30:26,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 01:01:34,957 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 01:01:34,958 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 01:01:35,810 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 01:01:35,811 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 01:01:35,818 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.5963, 'learning_rate': 5.410522483160386e-06, 'epoch': 1.73}\n"," 86% 9500/10986 [2:26:38<22:46,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 01:09:18,523 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 01:09:18,524 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 01:09:19,378 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 01:09:19,380 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 01:09:19,386 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.5962, 'learning_rate': 3.5900236664846172e-06, 'epoch': 1.82}\n"," 91% 10000/10986 [2:34:21<15:07,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 01:17:01,856 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 01:17:01,857 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 01:17:02,711 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 01:17:02,715 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 01:17:02,716 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6102, 'learning_rate': 1.7695248498088476e-06, 'epoch': 1.91}\n"," 96% 10500/10986 [2:42:05<07:24,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 01:24:45,134 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 01:24:45,135 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 01:24:45,962 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 01:24:45,964 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 01:24:45,970 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","100% 10986/10986 [2:49:35<00:00,  1.21it/s][INFO|trainer.py:1530] 2022-04-06 01:32:15,335 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10175.4867, 'train_samples_per_second': 25.91, 'train_steps_per_second': 1.08, 'train_loss': 0.8446246150107238, 'epoch': 2.0}\n","100% 10986/10986 [2:49:35<00:00,  1.08it/s]\n","[INFO|trainer.py:2166] 2022-04-06 01:32:15,337 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:441] 2022-04-06 01:32:15,338 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 01:32:16,326 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 01:32:16,327 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 01:32:16,332 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.8446\n","  train_runtime            = 2:49:35.48\n","  train_samples            =     131823\n","  train_samples_per_second =      25.91\n","  train_steps_per_second   =       1.08\n","04/06/2022 01:32:16 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 01:32:16,476 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 01:32:16,478 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 01:32:16,478 >>   Num examples = 12165\n","[INFO|trainer.py:2421] 2022-04-06 01:32:16,478 >>   Batch size = 8\n","100% 1520/1521 [02:56<00:00,  8.63it/s]04/06/2022 01:35:26 - INFO - utils_qa - Post-processing 11873 example predictions split into 12165 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 39/11873 [00:00<00:30, 388.27it/s]\u001b[A\n","  1% 78/11873 [00:00<00:33, 354.80it/s]\u001b[A\n","  1% 118/11873 [00:00<00:31, 371.55it/s]\u001b[A\n","  1% 161/11873 [00:00<00:29, 393.33it/s]\u001b[A\n","  2% 203/11873 [00:00<00:29, 402.23it/s]\u001b[A\n","  2% 245/11873 [00:00<00:28, 405.28it/s]\u001b[A\n","  2% 290/11873 [00:00<00:27, 417.39it/s]\u001b[A\n","  3% 332/11873 [00:00<00:28, 411.84it/s]\u001b[A\n","  3% 375/11873 [00:00<00:27, 416.23it/s]\u001b[A\n","  4% 418/11873 [00:01<00:27, 418.00it/s]\u001b[A\n","  4% 461/11873 [00:01<00:27, 420.34it/s]\u001b[A\n","  4% 504/11873 [00:01<00:27, 413.88it/s]\u001b[A\n","  5% 546/11873 [00:01<00:27, 409.32it/s]\u001b[A\n","  5% 588/11873 [00:01<00:27, 411.80it/s]\u001b[A\n","  5% 631/11873 [00:01<00:27, 415.07it/s]\u001b[A\n","  6% 673/11873 [00:01<00:27, 413.91it/s]\u001b[A\n","  6% 715/11873 [00:01<00:27, 410.50it/s]\u001b[A\n","  6% 757/11873 [00:01<00:27, 399.51it/s]\u001b[A\n","  7% 799/11873 [00:01<00:27, 404.66it/s]\u001b[A\n","  7% 841/11873 [00:02<00:27, 406.52it/s]\u001b[A\n","  7% 888/11873 [00:02<00:25, 422.80it/s]\u001b[A\n","  8% 932/11873 [00:02<00:25, 425.49it/s]\u001b[A\n","  8% 975/11873 [00:02<00:25, 426.05it/s]\u001b[A\n","  9% 1018/11873 [00:02<00:26, 403.09it/s]\u001b[A\n","  9% 1059/11873 [00:02<00:28, 377.34it/s]\u001b[A\n","  9% 1098/11873 [00:02<00:29, 362.93it/s]\u001b[A\n"," 10% 1135/11873 [00:02<00:30, 351.77it/s]\u001b[A\n"," 10% 1171/11873 [00:02<00:31, 344.57it/s]\u001b[A\n"," 10% 1206/11873 [00:03<00:31, 340.36it/s]\u001b[A\n"," 10% 1241/11873 [00:03<00:32, 330.49it/s]\u001b[A\n"," 11% 1275/11873 [00:03<00:32, 329.96it/s]\u001b[A\n"," 11% 1309/11873 [00:03<00:32, 327.75it/s]\u001b[A\n"," 11% 1343/11873 [00:03<00:32, 328.40it/s]\u001b[A\n"," 12% 1376/11873 [00:03<00:32, 324.85it/s]\u001b[A\n"," 12% 1409/11873 [00:03<00:32, 324.81it/s]\u001b[A\n"," 12% 1443/11873 [00:03<00:31, 326.42it/s]\u001b[A\n"," 12% 1476/11873 [00:03<00:32, 323.75it/s]\u001b[A\n"," 13% 1510/11873 [00:04<00:31, 326.00it/s]\u001b[A\n"," 13% 1543/11873 [00:04<00:31, 327.07it/s]\u001b[A\n"," 13% 1576/11873 [00:04<00:31, 325.32it/s]\u001b[A\n"," 14% 1610/11873 [00:04<00:31, 327.54it/s]\u001b[A\n"," 14% 1643/11873 [00:04<00:31, 324.09it/s]\u001b[A\n"," 14% 1676/11873 [00:04<00:31, 324.22it/s]\u001b[A\n"," 14% 1709/11873 [00:04<00:31, 317.95it/s]\u001b[A\n"," 15% 1743/11873 [00:04<00:31, 323.18it/s]\u001b[A\n"," 15% 1776/11873 [00:04<00:31, 319.48it/s]\u001b[A\n"," 15% 1808/11873 [00:04<00:31, 319.05it/s]\u001b[A\n"," 15% 1840/11873 [00:05<00:31, 319.24it/s]\u001b[A\n"," 16% 1873/11873 [00:05<00:31, 319.52it/s]\u001b[A\n"," 16% 1905/11873 [00:05<00:31, 319.30it/s]\u001b[A\n"," 16% 1938/11873 [00:05<00:30, 322.02it/s]\u001b[A\n"," 17% 1971/11873 [00:05<00:30, 321.05it/s]\u001b[A\n"," 17% 2004/11873 [00:05<00:30, 318.85it/s]\u001b[A\n"," 17% 2037/11873 [00:05<00:30, 322.03it/s]\u001b[A\n"," 17% 2070/11873 [00:05<00:30, 322.97it/s]\u001b[A\n"," 18% 2103/11873 [00:05<00:30, 321.79it/s]\u001b[A\n"," 18% 2136/11873 [00:05<00:30, 321.96it/s]\u001b[A\n"," 18% 2169/11873 [00:06<00:30, 319.45it/s]\u001b[A\n"," 19% 2202/11873 [00:06<00:30, 321.57it/s]\u001b[A\n"," 19% 2235/11873 [00:06<00:29, 321.63it/s]\u001b[A\n"," 19% 2268/11873 [00:06<00:29, 321.87it/s]\u001b[A\n"," 19% 2301/11873 [00:06<00:29, 322.12it/s]\u001b[A\n"," 20% 2334/11873 [00:06<00:29, 320.81it/s]\u001b[A\n"," 20% 2367/11873 [00:06<00:29, 322.30it/s]\u001b[A\n"," 20% 2400/11873 [00:06<00:29, 323.11it/s]\u001b[A\n"," 20% 2433/11873 [00:06<00:29, 319.09it/s]\u001b[A\n"," 21% 2465/11873 [00:06<00:29, 318.96it/s]\u001b[A\n"," 21% 2498/11873 [00:07<00:29, 321.27it/s]\u001b[A\n"," 21% 2532/11873 [00:07<00:28, 324.14it/s]\u001b[A\n"," 22% 2565/11873 [00:07<00:28, 324.69it/s]\u001b[A\n"," 22% 2599/11873 [00:07<00:28, 327.02it/s]\u001b[A\n"," 22% 2632/11873 [00:07<00:28, 324.69it/s]\u001b[A\n"," 22% 2665/11873 [00:07<00:28, 321.08it/s]\u001b[A\n"," 23% 2699/11873 [00:07<00:28, 323.96it/s]\u001b[A\n"," 23% 2732/11873 [00:07<00:28, 318.05it/s]\u001b[A\n"," 23% 2764/11873 [00:07<00:28, 317.10it/s]\u001b[A\n"," 24% 2796/11873 [00:08<00:28, 315.95it/s]\u001b[A\n"," 24% 2828/11873 [00:08<00:28, 315.48it/s]\u001b[A\n"," 24% 2861/11873 [00:08<00:28, 317.86it/s]\u001b[A\n"," 24% 2893/11873 [00:08<00:28, 311.10it/s]\u001b[A\n"," 25% 2926/11873 [00:08<00:28, 315.48it/s]\u001b[A\n"," 25% 2958/11873 [00:08<00:28, 315.37it/s]\u001b[A\n"," 25% 2990/11873 [00:08<00:28, 314.38it/s]\u001b[A\n"," 25% 3022/11873 [00:08<00:28, 309.50it/s]\u001b[A\n"," 26% 3053/11873 [00:08<00:28, 307.72it/s]\u001b[A\n"," 26% 3084/11873 [00:08<00:28, 305.50it/s]\u001b[A\n"," 26% 3115/11873 [00:09<00:31, 277.97it/s]\u001b[A\n"," 26% 3144/11873 [00:09<00:34, 252.35it/s]\u001b[A\n"," 27% 3170/11873 [00:09<00:35, 244.55it/s]\u001b[A\n"," 27% 3204/11873 [00:09<00:32, 268.03it/s]\u001b[A\n"," 27% 3236/11873 [00:09<00:30, 281.58it/s]\u001b[A\n"," 28% 3266/11873 [00:09<00:30, 285.08it/s]\u001b[A\n"," 28% 3295/11873 [00:09<00:41, 207.89it/s]\u001b[A\n"," 28% 3319/11873 [00:10<00:46, 185.10it/s]\u001b[A\n"," 28% 3340/11873 [00:10<00:45, 186.32it/s]\u001b[A\n"," 28% 3361/11873 [00:10<00:48, 175.28it/s]\u001b[A\n"," 29% 3388/11873 [00:10<00:43, 196.57it/s]\u001b[A\n"," 29% 3421/11873 [00:10<00:36, 228.82it/s]\u001b[A\n"," 29% 3453/11873 [00:10<00:33, 251.61it/s]\u001b[A\n"," 29% 3485/11873 [00:10<00:31, 268.42it/s]\u001b[A\n"," 30% 3517/11873 [00:10<00:29, 281.98it/s]\u001b[A\n"," 30% 3551/11873 [00:10<00:28, 296.18it/s]\u001b[A\n"," 30% 3585/11873 [00:11<00:27, 306.83it/s]\u001b[A\n"," 30% 3618/11873 [00:11<00:26, 313.49it/s]\u001b[A\n"," 31% 3652/11873 [00:11<00:25, 319.82it/s]\u001b[A\n"," 31% 3685/11873 [00:11<00:26, 310.26it/s]\u001b[A\n"," 31% 3717/11873 [00:11<00:26, 311.36it/s]\u001b[A\n"," 32% 3749/11873 [00:11<00:25, 313.24it/s]\u001b[A\n"," 32% 3781/11873 [00:11<00:25, 314.34it/s]\u001b[A\n"," 32% 3813/11873 [00:11<00:25, 314.99it/s]\u001b[A\n"," 32% 3845/11873 [00:11<00:27, 287.26it/s]\u001b[A\n"," 33% 3877/11873 [00:11<00:27, 294.62it/s]\u001b[A\n"," 33% 3910/11873 [00:12<00:26, 302.44it/s]\u001b[A\n"," 33% 3941/11873 [00:12<00:27, 284.39it/s]\u001b[A\n"," 33% 3973/11873 [00:12<00:26, 292.65it/s]\u001b[A\n"," 34% 4007/11873 [00:12<00:25, 304.08it/s]\u001b[A\n"," 34% 4038/11873 [00:12<00:25, 302.08it/s]\u001b[A\n"," 34% 4071/11873 [00:12<00:25, 309.33it/s]\u001b[A\n"," 35% 4105/11873 [00:12<00:24, 316.50it/s]\u001b[A\n"," 35% 4137/11873 [00:12<00:24, 315.49it/s]\u001b[A\n"," 35% 4169/11873 [00:12<00:26, 286.94it/s]\u001b[A\n"," 35% 4200/11873 [00:13<00:26, 291.14it/s]\u001b[A\n"," 36% 4233/11873 [00:13<00:25, 300.95it/s]\u001b[A\n"," 36% 4267/11873 [00:13<00:24, 311.35it/s]\u001b[A\n"," 36% 4300/11873 [00:13<00:24, 314.77it/s]\u001b[A\n"," 37% 4334/11873 [00:13<00:24, 312.37it/s]\u001b[A\n"," 37% 4368/11873 [00:13<00:23, 318.07it/s]\u001b[A\n"," 37% 4400/11873 [00:13<00:23, 317.82it/s]\u001b[A\n"," 37% 4432/11873 [00:13<00:29, 254.86it/s]\u001b[A\n"," 38% 4465/11873 [00:13<00:27, 273.64it/s]\u001b[A\n"," 38% 4498/11873 [00:14<00:25, 285.37it/s]\u001b[A\n"," 38% 4531/11873 [00:14<00:24, 296.91it/s]\u001b[A\n"," 38% 4564/11873 [00:14<00:24, 304.44it/s]\u001b[A\n"," 39% 4597/11873 [00:14<00:23, 311.52it/s]\u001b[A\n"," 39% 4630/11873 [00:14<00:22, 316.22it/s]\u001b[A\n"," 39% 4663/11873 [00:14<00:22, 318.88it/s]\u001b[A\n"," 40% 4696/11873 [00:14<00:22, 317.27it/s]\u001b[A\n"," 40% 4729/11873 [00:14<00:22, 320.03it/s]\u001b[A\n"," 40% 4762/11873 [00:14<00:22, 319.65it/s]\u001b[A\n"," 40% 4795/11873 [00:14<00:22, 315.75it/s]\u001b[A\n"," 41% 4827/11873 [00:15<00:22, 314.94it/s]\u001b[A\n"," 41% 4860/11873 [00:15<00:21, 318.78it/s]\u001b[A\n"," 41% 4893/11873 [00:15<00:21, 321.44it/s]\u001b[A\n"," 41% 4926/11873 [00:15<00:21, 323.80it/s]\u001b[A\n"," 42% 4959/11873 [00:15<00:21, 324.76it/s]\u001b[A\n"," 42% 4992/11873 [00:15<00:21, 323.15it/s]\u001b[A\n"," 42% 5025/11873 [00:15<00:21, 322.13it/s]\u001b[A\n"," 43% 5058/11873 [00:15<00:21, 321.80it/s]\u001b[A\n"," 43% 5092/11873 [00:15<00:20, 326.09it/s]\u001b[A\n"," 43% 5126/11873 [00:15<00:20, 328.29it/s]\u001b[A\n"," 43% 5159/11873 [00:16<00:20, 324.58it/s]\u001b[A\n"," 44% 5193/11873 [00:16<00:20, 327.30it/s]\u001b[A\n"," 44% 5227/11873 [00:16<00:20, 329.11it/s]\u001b[A\n"," 44% 5260/11873 [00:16<00:21, 302.02it/s]\u001b[A\n"," 45% 5292/11873 [00:16<00:21, 306.86it/s]\u001b[A\n"," 45% 5325/11873 [00:16<00:20, 312.74it/s]\u001b[A\n"," 45% 5358/11873 [00:16<00:20, 317.63it/s]\u001b[A\n"," 45% 5391/11873 [00:16<00:20, 319.96it/s]\u001b[A\n"," 46% 5424/11873 [00:16<00:20, 322.09it/s]\u001b[A\n"," 46% 5457/11873 [00:17<00:19, 322.92it/s]\u001b[A\n"," 46% 5490/11873 [00:17<00:19, 321.43it/s]\u001b[A\n"," 47% 5524/11873 [00:17<00:19, 325.57it/s]\u001b[A\n"," 47% 5557/11873 [00:17<00:19, 325.13it/s]\u001b[A\n"," 47% 5590/11873 [00:17<00:19, 325.24it/s]\u001b[A\n"," 47% 5623/11873 [00:17<00:19, 323.06it/s]\u001b[A\n"," 48% 5656/11873 [00:17<00:19, 321.30it/s]\u001b[A\n"," 48% 5689/11873 [00:17<00:19, 319.46it/s]\u001b[A\n"," 48% 5721/11873 [00:17<00:19, 317.22it/s]\u001b[A\n"," 48% 5754/11873 [00:17<00:19, 318.37it/s]\u001b[A\n"," 49% 5787/11873 [00:18<00:18, 320.76it/s]\u001b[A\n"," 49% 5821/11873 [00:18<00:18, 325.16it/s]\u001b[A\n"," 49% 5854/11873 [00:18<00:18, 321.82it/s]\u001b[A\n"," 50% 5887/11873 [00:18<00:18, 322.91it/s]\u001b[A\n"," 50% 5921/11873 [00:18<00:18, 326.29it/s]\u001b[A\n"," 50% 5954/11873 [00:18<00:18, 325.09it/s]\u001b[A\n"," 50% 5988/11873 [00:18<00:17, 328.09it/s]\u001b[A\n"," 51% 6021/11873 [00:18<00:17, 326.83it/s]\u001b[A\n"," 51% 6054/11873 [00:18<00:17, 325.12it/s]\u001b[A\n"," 51% 6087/11873 [00:18<00:17, 325.41it/s]\u001b[A\n"," 52% 6120/11873 [00:19<00:17, 323.13it/s]\u001b[A\n"," 52% 6154/11873 [00:19<00:17, 324.57it/s]\u001b[A\n"," 52% 6187/11873 [00:19<00:17, 323.42it/s]\u001b[A\n"," 52% 6220/11873 [00:19<00:17, 315.40it/s]\u001b[A\n"," 53% 6253/11873 [00:19<00:17, 317.51it/s]\u001b[A\n"," 53% 6286/11873 [00:19<00:17, 320.08it/s]\u001b[A\n"," 53% 6319/11873 [00:19<00:17, 321.35it/s]\u001b[A\n"," 53% 6352/11873 [00:19<00:17, 320.34it/s]\u001b[A\n"," 54% 6385/11873 [00:19<00:17, 319.76it/s]\u001b[A\n"," 54% 6417/11873 [00:20<00:17, 319.30it/s]\u001b[A\n"," 54% 6449/11873 [00:20<00:17, 317.26it/s]\u001b[A\n"," 55% 6482/11873 [00:20<00:16, 319.30it/s]\u001b[A\n"," 55% 6514/11873 [00:20<00:16, 317.71it/s]\u001b[A\n"," 55% 6546/11873 [00:20<00:16, 314.31it/s]\u001b[A\n"," 55% 6578/11873 [00:20<00:16, 315.54it/s]\u001b[A\n"," 56% 6611/11873 [00:20<00:16, 317.90it/s]\u001b[A\n"," 56% 6644/11873 [00:20<00:16, 318.69it/s]\u001b[A\n"," 56% 6676/11873 [00:20<00:16, 317.99it/s]\u001b[A\n"," 56% 6708/11873 [00:20<00:17, 299.27it/s]\u001b[A\n"," 57% 6739/11873 [00:21<00:17, 286.56it/s]\u001b[A\n"," 57% 6772/11873 [00:21<00:17, 297.56it/s]\u001b[A\n"," 57% 6805/11873 [00:21<00:16, 304.58it/s]\u001b[A\n"," 58% 6838/11873 [00:21<00:16, 311.53it/s]\u001b[A\n"," 58% 6870/11873 [00:21<00:16, 310.52it/s]\u001b[A\n"," 58% 6902/11873 [00:21<00:15, 311.31it/s]\u001b[A\n"," 58% 6934/11873 [00:21<00:15, 313.09it/s]\u001b[A\n"," 59% 6966/11873 [00:21<00:15, 313.39it/s]\u001b[A\n"," 59% 7000/11873 [00:21<00:15, 318.57it/s]\u001b[A\n"," 59% 7032/11873 [00:21<00:15, 317.67it/s]\u001b[A\n"," 60% 7065/11873 [00:22<00:15, 318.95it/s]\u001b[A\n"," 60% 7097/11873 [00:22<00:15, 315.59it/s]\u001b[A\n"," 60% 7131/11873 [00:22<00:14, 321.03it/s]\u001b[A\n"," 60% 7164/11873 [00:22<00:14, 322.39it/s]\u001b[A\n"," 61% 7197/11873 [00:22<00:14, 317.70it/s]\u001b[A\n"," 61% 7229/11873 [00:22<00:14, 317.41it/s]\u001b[A\n"," 61% 7261/11873 [00:22<00:14, 317.22it/s]\u001b[A\n"," 61% 7293/11873 [00:22<00:14, 317.02it/s]\u001b[A\n"," 62% 7325/11873 [00:22<00:14, 316.20it/s]\u001b[A\n"," 62% 7359/11873 [00:23<00:14, 320.62it/s]\u001b[A\n"," 62% 7392/11873 [00:23<00:14, 319.93it/s]\u001b[A\n"," 63% 7424/11873 [00:23<00:14, 302.52it/s]\u001b[A\n"," 63% 7457/11873 [00:23<00:14, 308.46it/s]\u001b[A\n"," 63% 7491/11873 [00:23<00:13, 315.19it/s]\u001b[A\n"," 63% 7524/11873 [00:23<00:13, 318.01it/s]\u001b[A\n"," 64% 7557/11873 [00:23<00:13, 320.80it/s]\u001b[A\n"," 64% 7590/11873 [00:23<00:13, 320.63it/s]\u001b[A\n"," 64% 7623/11873 [00:23<00:13, 316.21it/s]\u001b[A\n"," 64% 7655/11873 [00:23<00:13, 315.07it/s]\u001b[A\n"," 65% 7687/11873 [00:24<00:13, 314.79it/s]\u001b[A\n"," 65% 7719/11873 [00:24<00:14, 293.54it/s]\u001b[A\n"," 65% 7751/11873 [00:24<00:13, 299.45it/s]\u001b[A\n"," 66% 7783/11873 [00:24<00:13, 304.42it/s]\u001b[A\n"," 66% 7815/11873 [00:24<00:13, 307.54it/s]\u001b[A\n"," 66% 7846/11873 [00:24<00:13, 305.45it/s]\u001b[A\n"," 66% 7877/11873 [00:24<00:13, 287.31it/s]\u001b[A\n"," 67% 7909/11873 [00:24<00:13, 294.62it/s]\u001b[A\n"," 67% 7941/11873 [00:24<00:13, 299.87it/s]\u001b[A\n"," 67% 7972/11873 [00:25<00:12, 301.06it/s]\u001b[A\n"," 67% 8005/11873 [00:25<00:12, 307.55it/s]\u001b[A\n"," 68% 8039/11873 [00:25<00:12, 316.02it/s]\u001b[A\n"," 68% 8071/11873 [00:25<00:12, 311.46it/s]\u001b[A\n"," 68% 8103/11873 [00:25<00:12, 313.72it/s]\u001b[A\n"," 69% 8135/11873 [00:25<00:11, 313.72it/s]\u001b[A\n"," 69% 8169/11873 [00:25<00:11, 319.30it/s]\u001b[A\n"," 69% 8201/11873 [00:25<00:11, 314.91it/s]\u001b[A\n"," 69% 8233/11873 [00:25<00:11, 313.42it/s]\u001b[A\n"," 70% 8266/11873 [00:25<00:11, 318.22it/s]\u001b[A\n"," 70% 8299/11873 [00:26<00:11, 319.39it/s]\u001b[A\n"," 70% 8332/11873 [00:26<00:11, 321.57it/s]\u001b[A\n"," 70% 8365/11873 [00:26<00:10, 323.95it/s]\u001b[A\n"," 71% 8398/11873 [00:26<00:10, 324.46it/s]\u001b[A\n"," 71% 8431/11873 [00:26<00:10, 318.99it/s]\u001b[A\n"," 71% 8463/11873 [00:26<00:10, 313.98it/s]\u001b[A\n"," 72% 8496/11873 [00:26<00:10, 316.38it/s]\u001b[A\n"," 72% 8529/11873 [00:26<00:10, 318.23it/s]\u001b[A\n"," 72% 8561/11873 [00:26<00:10, 316.90it/s]\u001b[A\n"," 72% 8594/11873 [00:26<00:10, 319.57it/s]\u001b[A\n"," 73% 8626/11873 [00:27<00:10, 317.34it/s]\u001b[A\n"," 73% 8660/11873 [00:27<00:09, 322.80it/s]\u001b[A\n"," 73% 8694/11873 [00:27<00:09, 325.55it/s]\u001b[A\n"," 74% 8727/11873 [00:27<00:09, 325.12it/s]\u001b[A\n"," 74% 8760/11873 [00:27<00:09, 323.66it/s]\u001b[A\n"," 74% 8793/11873 [00:27<00:09, 320.41it/s]\u001b[A\n"," 74% 8826/11873 [00:27<00:09, 318.82it/s]\u001b[A\n"," 75% 8858/11873 [00:27<00:09, 318.07it/s]\u001b[A\n"," 75% 8890/11873 [00:27<00:09, 310.33it/s]\u001b[A\n"," 75% 8923/11873 [00:28<00:09, 315.28it/s]\u001b[A\n"," 75% 8955/11873 [00:28<00:09, 316.35it/s]\u001b[A\n"," 76% 8989/11873 [00:28<00:08, 322.54it/s]\u001b[A\n"," 76% 9022/11873 [00:28<00:08, 324.00it/s]\u001b[A\n"," 76% 9055/11873 [00:28<00:09, 312.32it/s]\u001b[A\n"," 77% 9087/11873 [00:28<00:08, 313.87it/s]\u001b[A\n"," 77% 9119/11873 [00:28<00:08, 314.29it/s]\u001b[A\n"," 77% 9151/11873 [00:28<00:08, 314.07it/s]\u001b[A\n"," 77% 9183/11873 [00:28<00:08, 313.78it/s]\u001b[A\n"," 78% 9216/11873 [00:28<00:08, 316.20it/s]\u001b[A\n"," 78% 9248/11873 [00:29<00:08, 317.18it/s]\u001b[A\n"," 78% 9281/11873 [00:29<00:08, 318.62it/s]\u001b[A\n"," 78% 9314/11873 [00:29<00:07, 319.89it/s]\u001b[A\n"," 79% 9347/11873 [00:29<00:07, 320.59it/s]\u001b[A\n"," 79% 9380/11873 [00:29<00:07, 320.38it/s]\u001b[A\n"," 79% 9413/11873 [00:29<00:07, 322.10it/s]\u001b[A\n"," 80% 9446/11873 [00:29<00:07, 322.28it/s]\u001b[A\n"," 80% 9479/11873 [00:29<00:07, 323.74it/s]\u001b[A\n"," 80% 9512/11873 [00:29<00:07, 323.14it/s]\u001b[A\n"," 80% 9545/11873 [00:29<00:07, 323.15it/s]\u001b[A\n"," 81% 9578/11873 [00:30<00:07, 317.83it/s]\u001b[A\n"," 81% 9610/11873 [00:30<00:07, 316.36it/s]\u001b[A\n"," 81% 9642/11873 [00:30<00:07, 316.48it/s]\u001b[A\n"," 81% 9674/11873 [00:30<00:06, 315.00it/s]\u001b[A\n"," 82% 9708/11873 [00:30<00:06, 320.28it/s]\u001b[A\n"," 82% 9741/11873 [00:30<00:06, 313.27it/s]\u001b[A\n"," 82% 9773/11873 [00:30<00:06, 314.17it/s]\u001b[A\n"," 83% 9805/11873 [00:30<00:06, 313.27it/s]\u001b[A\n"," 83% 9837/11873 [00:30<00:06, 313.69it/s]\u001b[A\n"," 83% 9869/11873 [00:30<00:06, 314.10it/s]\u001b[A\n"," 83% 9901/11873 [00:31<00:06, 308.55it/s]\u001b[A\n"," 84% 9934/11873 [00:31<00:06, 312.17it/s]\u001b[A\n"," 84% 9967/11873 [00:31<00:06, 316.06it/s]\u001b[A\n"," 84% 9999/11873 [00:31<00:05, 316.93it/s]\u001b[A\n"," 84% 10032/11873 [00:31<00:05, 319.72it/s]\u001b[A\n"," 85% 10065/11873 [00:31<00:05, 321.08it/s]\u001b[A\n"," 85% 10098/11873 [00:31<00:05, 318.40it/s]\u001b[A\n"," 85% 10130/11873 [00:31<00:05, 317.66it/s]\u001b[A\n"," 86% 10162/11873 [00:31<00:05, 318.27it/s]\u001b[A\n"," 86% 10194/11873 [00:32<00:05, 316.55it/s]\u001b[A\n"," 86% 10226/11873 [00:32<00:05, 314.95it/s]\u001b[A\n"," 86% 10258/11873 [00:32<00:05, 311.50it/s]\u001b[A\n"," 87% 10290/11873 [00:32<00:05, 309.66it/s]\u001b[A\n"," 87% 10322/11873 [00:32<00:04, 310.46it/s]\u001b[A\n"," 87% 10355/11873 [00:32<00:04, 315.63it/s]\u001b[A\n"," 87% 10387/11873 [00:32<00:04, 315.63it/s]\u001b[A\n"," 88% 10420/11873 [00:32<00:04, 315.95it/s]\u001b[A\n"," 88% 10452/11873 [00:32<00:04, 300.99it/s]\u001b[A\n"," 88% 10483/11873 [00:32<00:04, 298.55it/s]\u001b[A\n"," 89% 10515/11873 [00:33<00:04, 304.65it/s]\u001b[A\n"," 89% 10547/11873 [00:33<00:04, 308.51it/s]\u001b[A\n"," 89% 10578/11873 [00:33<00:04, 305.15it/s]\u001b[A\n"," 89% 10611/11873 [00:33<00:04, 310.49it/s]\u001b[A\n"," 90% 10643/11873 [00:33<00:03, 307.76it/s]\u001b[A\n"," 90% 10675/11873 [00:33<00:03, 310.76it/s]\u001b[A\n"," 90% 10707/11873 [00:33<00:03, 310.98it/s]\u001b[A\n"," 90% 10741/11873 [00:33<00:03, 316.53it/s]\u001b[A\n"," 91% 10773/11873 [00:33<00:03, 317.04it/s]\u001b[A\n"," 91% 10805/11873 [00:33<00:03, 310.09it/s]\u001b[A\n"," 91% 10837/11873 [00:34<00:03, 288.40it/s]\u001b[A\n"," 92% 10870/11873 [00:34<00:03, 298.77it/s]\u001b[A\n"," 92% 10902/11873 [00:34<00:03, 302.26it/s]\u001b[A\n"," 92% 10933/11873 [00:34<00:03, 300.14it/s]\u001b[A\n"," 92% 10965/11873 [00:34<00:02, 304.19it/s]\u001b[A\n"," 93% 10998/11873 [00:34<00:02, 310.63it/s]\u001b[A\n"," 93% 11031/11873 [00:34<00:02, 315.88it/s]\u001b[A\n"," 93% 11065/11873 [00:34<00:02, 320.96it/s]\u001b[A\n"," 93% 11098/11873 [00:34<00:02, 322.98it/s]\u001b[A\n"," 94% 11131/11873 [00:35<00:02, 322.26it/s]\u001b[A\n"," 94% 11164/11873 [00:35<00:02, 322.64it/s]\u001b[A\n"," 94% 11197/11873 [00:35<00:02, 324.03it/s]\u001b[A\n"," 95% 11230/11873 [00:35<00:01, 322.90it/s]\u001b[A\n"," 95% 11263/11873 [00:35<00:01, 314.37it/s]\u001b[A\n"," 95% 11296/11873 [00:35<00:01, 318.22it/s]\u001b[A\n"," 95% 11330/11873 [00:35<00:01, 322.05it/s]\u001b[A\n"," 96% 11363/11873 [00:35<00:01, 321.79it/s]\u001b[A\n"," 96% 11396/11873 [00:35<00:01, 322.01it/s]\u001b[A\n"," 96% 11429/11873 [00:35<00:01, 321.80it/s]\u001b[A\n"," 97% 11462/11873 [00:36<00:01, 318.33it/s]\u001b[A\n"," 97% 11494/11873 [00:36<00:01, 311.46it/s]\u001b[A\n"," 97% 11527/11873 [00:36<00:01, 315.77it/s]\u001b[A\n"," 97% 11559/11873 [00:36<00:01, 307.64it/s]\u001b[A\n"," 98% 11591/11873 [00:36<00:00, 310.91it/s]\u001b[A\n"," 98% 11623/11873 [00:36<00:00, 305.75it/s]\u001b[A\n"," 98% 11655/11873 [00:36<00:00, 308.15it/s]\u001b[A\n"," 98% 11687/11873 [00:36<00:00, 309.67it/s]\u001b[A\n"," 99% 11721/11873 [00:36<00:00, 316.37it/s]\u001b[A\n"," 99% 11755/11873 [00:37<00:00, 321.56it/s]\u001b[A\n"," 99% 11789/11873 [00:37<00:00, 324.74it/s]\u001b[A\n","100% 11823/11873 [00:37<00:00, 329.01it/s]\u001b[A\n","100% 11873/11873 [00:37<00:00, 317.82it/s]\n","04/06/2022 01:36:03 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","04/06/2022 01:36:03 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","04/06/2022 01:36:06 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","04/06/2022 01:36:10 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1521/1521 [03:54<00:00,  6.50it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 78.4244\n","  eval_HasAns_f1         = 84.6213\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 81.9008\n","  eval_NoAns_f1          = 81.9008\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 80.1651\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 83.2591\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 80.1651\n","  eval_f1                = 83.2591\n","  eval_samples           =   12165\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 01:36:10,884 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path roberta-base \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 3 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5dXCzCDmhD4e","executionInfo":{"status":"ok","timestamp":1649224454201,"user_tz":240,"elapsed":15441993,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"ef766107-7f8f-4c34-be91-3b31e9d3a165"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["04/06/2022 01:36:58 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 01:36:58 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Apr06_01-36-58_c282ee9c4a83,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 01:36:58 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","04/06/2022 01:36:58 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/06/2022 01:36:58 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/06/2022 01:36:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/06/2022 01:36:58 - WARNING - datasets.builder - Reusing dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","04/06/2022 01:36:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","100% 2/2 [00:00<00:00, 341.74it/s]\n","[INFO|configuration_utils.py:654] 2022-04-06 01:36:59,032 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-06 01:36:59,034 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-06 01:36:59,135 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-06 01:36:59,232 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-06 01:36:59,233 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 01:36:59,929 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 01:36:59,929 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 01:36:59,929 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 01:36:59,929 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 01:36:59,929 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 01:36:59,929 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-06 01:37:00,026 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-06 01:37:00,027 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|modeling_utils.py:1772] 2022-04-06 01:37:00,281 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[WARNING|modeling_utils.py:2050] 2022-04-06 01:37:03,405 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-06 01:37:03,405 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","04/06/2022 01:37:03 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-f70d4265b8a69d5f.arrow\n","04/06/2022 01:37:03 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-0be1d39efeab877d.arrow\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 01:37:07,914 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 01:37:07,914 >>   Num examples = 131823\n","[INFO|trainer.py:1292] 2022-04-06 01:37:07,914 >>   Num Epochs = 3\n","[INFO|trainer.py:1293] 2022-04-06 01:37:07,914 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1294] 2022-04-06 01:37:07,915 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1295] 2022-04-06 01:37:07,915 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 01:37:07,915 >>   Total optimization steps = 16479\n","{'loss': 1.7544, 'learning_rate': 3.878633412221616e-05, 'epoch': 0.09}\n","  3% 500/16479 [07:37<4:02:49,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 01:44:44,963 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 01:44:44,964 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 01:44:45,856 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 01:44:45,858 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 01:44:45,866 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.2247, 'learning_rate': 3.757266824443231e-05, 'epoch': 0.18}\n","  6% 1000/16479 [15:17<3:55:31,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 01:52:24,969 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 01:52:24,970 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 01:52:25,858 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 01:52:25,859 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 01:52:25,866 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.1181, 'learning_rate': 3.6359002366648465e-05, 'epoch': 0.27}\n","  9% 1500/16479 [22:57<3:48:26,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 02:00:05,272 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 02:00:05,273 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:00:06,179 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:00:06,186 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:00:06,186 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.0688, 'learning_rate': 3.514533648886462e-05, 'epoch': 0.36}\n"," 12% 2000/16479 [30:41<3:40:13,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 02:07:48,942 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 02:07:48,943 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:07:49,805 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:07:49,809 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:07:49,809 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.0313, 'learning_rate': 3.393167061108077e-05, 'epoch': 0.46}\n"," 15% 2500/16479 [38:20<3:32:41,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 02:15:28,750 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 02:15:28,751 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:15:29,630 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:15:29,638 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:15:29,638 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.003, 'learning_rate': 3.2718004733296926e-05, 'epoch': 0.55}\n"," 18% 3000/16479 [46:01<3:25:02,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 02:23:09,706 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 02:23:09,707 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:23:10,606 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:23:10,607 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:23:10,608 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 0.9619, 'learning_rate': 3.150433885551308e-05, 'epoch': 0.64}\n"," 21% 3500/16479 [53:41<3:17:08,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 02:30:49,769 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 02:30:49,770 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:30:50,657 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:30:50,657 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:30:50,663 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 0.9432, 'learning_rate': 3.0290672977729234e-05, 'epoch': 0.73}\n"," 24% 4000/16479 [1:01:21<3:09:50,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 02:38:29,781 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 02:38:29,782 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:38:30,637 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:38:30,640 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:38:30,640 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.9268, 'learning_rate': 2.9077007099945387e-05, 'epoch': 0.82}\n"," 27% 4500/16479 [1:09:02<3:02:09,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 02:46:10,096 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 02:46:10,097 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:46:10,950 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:46:10,951 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:46:10,951 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.8872, 'learning_rate': 2.786334122216154e-05, 'epoch': 0.91}\n"," 30% 5000/16479 [1:16:43<2:55:17,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 02:53:51,445 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 02:53:51,446 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:53:52,366 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:53:52,371 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:53:52,372 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.8759, 'learning_rate': 2.6649675344377696e-05, 'epoch': 1.0}\n"," 33% 5500/16479 [1:24:24<2:46:24,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 03:01:32,357 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:01:32,358 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:01:33,266 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:01:33,267 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:01:33,267 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.6612, 'learning_rate': 2.543600946659385e-05, 'epoch': 1.09}\n"," 36% 6000/16479 [1:32:05<2:40:00,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 03:09:13,491 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:09:13,492 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:09:14,406 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:09:14,413 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:09:14,414 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.6562, 'learning_rate': 2.422234358881e-05, 'epoch': 1.18}\n"," 39% 6500/16479 [1:39:46<2:32:35,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 03:16:54,636 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:16:54,637 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:16:55,552 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:16:55,553 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:16:55,559 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.6714, 'learning_rate': 2.3008677711026154e-05, 'epoch': 1.27}\n"," 42% 7000/16479 [1:47:28<2:24:26,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 03:24:35,956 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:24:35,957 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:24:36,860 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:24:36,860 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:24:36,867 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.6581, 'learning_rate': 2.1795011833242313e-05, 'epoch': 1.37}\n"," 46% 7500/16479 [1:55:12<2:17:19,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 03:32:20,034 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:32:20,035 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:32:20,948 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:32:20,951 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:32:20,951 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.6636, 'learning_rate': 2.0581345955458466e-05, 'epoch': 1.46}\n"," 49% 8000/16479 [2:02:53<2:09:37,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 03:40:01,438 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:40:01,439 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:40:02,370 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:40:02,375 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:40:02,381 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.6671, 'learning_rate': 1.9367680077674618e-05, 'epoch': 1.55}\n"," 52% 8500/16479 [2:10:34<2:01:40,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 03:47:42,717 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:47:42,718 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:47:43,597 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:47:43,598 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:47:43,598 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.6513, 'learning_rate': 1.8154014199890774e-05, 'epoch': 1.64}\n"," 55% 9000/16479 [2:18:16<1:54:05,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 03:55:24,047 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:55:24,048 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:55:24,907 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:55:24,913 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:55:24,914 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.6309, 'learning_rate': 1.6940348322106927e-05, 'epoch': 1.73}\n"," 58% 9500/16479 [2:25:57<1:46:12,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 04:03:05,294 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:03:05,295 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:03:06,150 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:03:06,154 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:03:06,154 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.6293, 'learning_rate': 1.572668244432308e-05, 'epoch': 1.82}\n"," 61% 10000/16479 [2:33:38<1:38:52,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 04:10:46,618 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:10:46,619 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:10:47,487 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:10:47,494 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:10:47,494 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6501, 'learning_rate': 1.4513016566539234e-05, 'epoch': 1.91}\n"," 64% 10500/16479 [2:41:19<1:31:19,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 04:18:27,590 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:18:27,591 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:18:28,461 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:18:28,462 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:18:28,463 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.6268, 'learning_rate': 1.3299350688755386e-05, 'epoch': 2.0}\n"," 67% 11000/16479 [2:49:00<1:23:27,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 04:26:08,668 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:26:08,669 >> Configuration saved in /tmp/debug_squad/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:26:09,529 >> Model weights saved in /tmp/debug_squad/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:26:09,530 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:26:09,530 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.4553, 'learning_rate': 1.208568481097154e-05, 'epoch': 2.09}\n"," 70% 11500/16479 [2:56:41<1:16:02,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 04:33:49,433 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:33:49,434 >> Configuration saved in /tmp/debug_squad/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:33:50,292 >> Model weights saved in /tmp/debug_squad/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:33:50,293 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:33:50,293 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.4728, 'learning_rate': 1.0872018933187693e-05, 'epoch': 2.18}\n"," 73% 12000/16479 [3:04:22<1:08:26,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 04:41:30,207 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:41:30,208 >> Configuration saved in /tmp/debug_squad/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:41:31,073 >> Model weights saved in /tmp/debug_squad/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:41:31,073 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:41:31,074 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.4656, 'learning_rate': 9.658353055403848e-06, 'epoch': 2.28}\n"," 76% 12500/16479 [3:12:03<1:00:35,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 04:49:11,254 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:49:11,255 >> Configuration saved in /tmp/debug_squad/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:49:12,113 >> Model weights saved in /tmp/debug_squad/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:49:12,114 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:49:12,114 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.4556, 'learning_rate': 8.444687177620002e-06, 'epoch': 2.37}\n"," 79% 13000/16479 [3:19:44<53:00,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 04:56:52,212 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:56:52,213 >> Configuration saved in /tmp/debug_squad/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:56:53,071 >> Model weights saved in /tmp/debug_squad/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:56:53,072 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:56:53,072 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.4551, 'learning_rate': 7.231021299836156e-06, 'epoch': 2.46}\n"," 82% 13500/16479 [3:27:25<45:23,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 05:04:32,975 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:04:32,976 >> Configuration saved in /tmp/debug_squad/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:04:33,846 >> Model weights saved in /tmp/debug_squad/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:04:33,847 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:04:33,848 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.4496, 'learning_rate': 6.017355422052309e-06, 'epoch': 2.55}\n"," 85% 14000/16479 [3:35:05<37:48,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 05:12:13,764 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:12:13,765 >> Configuration saved in /tmp/debug_squad/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:12:14,618 >> Model weights saved in /tmp/debug_squad/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:12:14,618 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:12:14,619 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.4245, 'learning_rate': 4.803689544268463e-06, 'epoch': 2.64}\n"," 88% 14500/16479 [3:42:46<30:03,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 05:19:54,210 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:19:54,211 >> Configuration saved in /tmp/debug_squad/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:19:55,015 >> Model weights saved in /tmp/debug_squad/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:19:55,016 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:19:55,016 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.4402, 'learning_rate': 3.5900236664846172e-06, 'epoch': 2.73}\n"," 91% 15000/16479 [3:50:26<22:30,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 05:27:34,011 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:27:34,012 >> Configuration saved in /tmp/debug_squad/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:27:34,833 >> Model weights saved in /tmp/debug_squad/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:27:34,834 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:27:34,834 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.419, 'learning_rate': 2.3763577887007707e-06, 'epoch': 2.82}\n"," 94% 15500/16479 [3:58:05<14:54,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 05:35:13,908 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:35:13,909 >> Configuration saved in /tmp/debug_squad/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:35:14,730 >> Model weights saved in /tmp/debug_squad/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:35:14,730 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:35:14,731 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.4095, 'learning_rate': 1.1626919109169246e-06, 'epoch': 2.91}\n"," 97% 16000/16479 [4:05:45<07:18,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 05:42:53,772 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-16000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:42:53,773 >> Configuration saved in /tmp/debug_squad/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:42:54,743 >> Model weights saved in /tmp/debug_squad/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:42:54,743 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:42:54,744 >> Special tokens file saved in /tmp/debug_squad/checkpoint-16000/special_tokens_map.json\n","100% 16479/16479 [4:13:06<00:00,  1.21it/s][INFO|trainer.py:1530] 2022-04-06 05:50:14,272 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 15186.3574, 'train_samples_per_second': 26.041, 'train_steps_per_second': 1.085, 'train_loss': 0.7226726236371741, 'epoch': 3.0}\n","100% 16479/16479 [4:13:06<00:00,  1.09it/s]\n","[INFO|trainer.py:2166] 2022-04-06 05:50:14,274 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:441] 2022-04-06 05:50:14,276 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:50:15,097 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:50:15,102 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:50:15,103 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =     0.7227\n","  train_runtime            = 4:13:06.35\n","  train_samples            =     131823\n","  train_samples_per_second =     26.041\n","  train_steps_per_second   =      1.085\n","04/06/2022 05:50:15 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 05:50:15,237 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 05:50:15,255 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 05:50:15,255 >>   Num examples = 12165\n","[INFO|trainer.py:2421] 2022-04-06 05:50:15,255 >>   Batch size = 8\n","100% 1520/1521 [02:56<00:00,  8.61it/s]04/06/2022 05:53:25 - INFO - utils_qa - Post-processing 11873 example predictions split into 12165 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 35/11873 [00:00<00:34, 343.64it/s]\u001b[A\n","  1% 70/11873 [00:00<00:36, 327.44it/s]\u001b[A\n","  1% 109/11873 [00:00<00:33, 353.15it/s]\u001b[A\n","  1% 151/11873 [00:00<00:31, 377.76it/s]\u001b[A\n","  2% 190/11873 [00:00<00:30, 381.96it/s]\u001b[A\n","  2% 231/11873 [00:00<00:29, 390.70it/s]\u001b[A\n","  2% 271/11873 [00:00<00:29, 388.86it/s]\u001b[A\n","  3% 311/11873 [00:00<00:29, 391.58it/s]\u001b[A\n","  3% 351/11873 [00:00<00:29, 389.95it/s]\u001b[A\n","  3% 392/11873 [00:01<00:29, 395.66it/s]\u001b[A\n","  4% 432/11873 [00:01<00:28, 395.02it/s]\u001b[A\n","  4% 473/11873 [00:01<00:28, 398.78it/s]\u001b[A\n","  4% 513/11873 [00:01<00:28, 398.70it/s]\u001b[A\n","  5% 553/11873 [00:01<00:28, 392.32it/s]\u001b[A\n","  5% 593/11873 [00:01<00:28, 393.41it/s]\u001b[A\n","  5% 635/11873 [00:01<00:28, 400.48it/s]\u001b[A\n","  6% 677/11873 [00:01<00:27, 404.92it/s]\u001b[A\n","  6% 718/11873 [00:01<00:27, 399.17it/s]\u001b[A\n","  6% 758/11873 [00:01<00:28, 393.50it/s]\u001b[A\n","  7% 799/11873 [00:02<00:27, 397.80it/s]\u001b[A\n","  7% 839/11873 [00:02<00:27, 397.50it/s]\u001b[A\n","  7% 883/11873 [00:02<00:26, 408.67it/s]\u001b[A\n","  8% 925/11873 [00:02<00:26, 411.52it/s]\u001b[A\n","  8% 969/11873 [00:02<00:26, 417.16it/s]\u001b[A\n","  9% 1011/11873 [00:02<00:27, 401.65it/s]\u001b[A\n","  9% 1052/11873 [00:02<00:29, 372.34it/s]\u001b[A\n","  9% 1090/11873 [00:02<00:30, 355.31it/s]\u001b[A\n","  9% 1126/11873 [00:02<00:31, 342.09it/s]\u001b[A\n"," 10% 1161/11873 [00:03<00:31, 335.50it/s]\u001b[A\n"," 10% 1195/11873 [00:03<00:32, 328.12it/s]\u001b[A\n"," 10% 1228/11873 [00:03<00:32, 324.52it/s]\u001b[A\n"," 11% 1261/11873 [00:03<00:33, 320.79it/s]\u001b[A\n"," 11% 1294/11873 [00:03<00:33, 319.62it/s]\u001b[A\n"," 11% 1327/11873 [00:03<00:32, 319.73it/s]\u001b[A\n"," 11% 1359/11873 [00:03<00:32, 318.89it/s]\u001b[A\n"," 12% 1391/11873 [00:03<00:32, 318.55it/s]\u001b[A\n"," 12% 1424/11873 [00:03<00:32, 319.42it/s]\u001b[A\n"," 12% 1457/11873 [00:03<00:32, 321.30it/s]\u001b[A\n"," 13% 1490/11873 [00:04<00:32, 317.90it/s]\u001b[A\n"," 13% 1522/11873 [00:04<00:32, 317.95it/s]\u001b[A\n"," 13% 1554/11873 [00:04<00:32, 318.54it/s]\u001b[A\n"," 13% 1586/11873 [00:04<00:32, 318.03it/s]\u001b[A\n"," 14% 1618/11873 [00:04<00:32, 315.41it/s]\u001b[A\n"," 14% 1650/11873 [00:04<00:32, 315.07it/s]\u001b[A\n"," 14% 1682/11873 [00:04<00:32, 314.45it/s]\u001b[A\n"," 14% 1714/11873 [00:04<00:32, 312.89it/s]\u001b[A\n"," 15% 1746/11873 [00:04<00:32, 311.78it/s]\u001b[A\n"," 15% 1778/11873 [00:04<00:32, 313.91it/s]\u001b[A\n"," 15% 1810/11873 [00:05<00:32, 313.65it/s]\u001b[A\n"," 16% 1842/11873 [00:05<00:31, 313.93it/s]\u001b[A\n"," 16% 1875/11873 [00:05<00:31, 317.88it/s]\u001b[A\n"," 16% 1907/11873 [00:05<00:31, 317.89it/s]\u001b[A\n"," 16% 1940/11873 [00:05<00:31, 318.58it/s]\u001b[A\n"," 17% 1972/11873 [00:05<00:31, 318.87it/s]\u001b[A\n"," 17% 2004/11873 [00:05<00:30, 318.51it/s]\u001b[A\n"," 17% 2036/11873 [00:05<00:30, 317.94it/s]\u001b[A\n"," 17% 2069/11873 [00:05<00:30, 319.35it/s]\u001b[A\n"," 18% 2102/11873 [00:06<00:30, 322.00it/s]\u001b[A\n"," 18% 2135/11873 [00:06<00:30, 318.37it/s]\u001b[A\n"," 18% 2167/11873 [00:06<00:30, 314.56it/s]\u001b[A\n"," 19% 2199/11873 [00:06<00:30, 315.39it/s]\u001b[A\n"," 19% 2232/11873 [00:06<00:30, 319.13it/s]\u001b[A\n"," 19% 2265/11873 [00:06<00:29, 321.22it/s]\u001b[A\n"," 19% 2298/11873 [00:06<00:30, 319.08it/s]\u001b[A\n"," 20% 2330/11873 [00:06<00:29, 318.72it/s]\u001b[A\n"," 20% 2362/11873 [00:06<00:29, 318.86it/s]\u001b[A\n"," 20% 2395/11873 [00:06<00:29, 320.88it/s]\u001b[A\n"," 20% 2428/11873 [00:07<00:29, 317.08it/s]\u001b[A\n"," 21% 2460/11873 [00:07<00:29, 316.79it/s]\u001b[A\n"," 21% 2492/11873 [00:07<00:30, 311.27it/s]\u001b[A\n"," 21% 2525/11873 [00:07<00:29, 315.39it/s]\u001b[A\n"," 22% 2558/11873 [00:07<00:29, 318.50it/s]\u001b[A\n"," 22% 2590/11873 [00:07<00:29, 316.81it/s]\u001b[A\n"," 22% 2623/11873 [00:07<00:28, 319.51it/s]\u001b[A\n"," 22% 2656/11873 [00:07<00:28, 321.83it/s]\u001b[A\n"," 23% 2689/11873 [00:07<00:28, 320.17it/s]\u001b[A\n"," 23% 2722/11873 [00:07<00:28, 319.85it/s]\u001b[A\n"," 23% 2754/11873 [00:08<00:28, 317.34it/s]\u001b[A\n"," 23% 2786/11873 [00:08<00:28, 313.65it/s]\u001b[A\n"," 24% 2818/11873 [00:08<00:28, 313.80it/s]\u001b[A\n"," 24% 2850/11873 [00:08<00:29, 308.33it/s]\u001b[A\n"," 24% 2881/11873 [00:08<00:29, 308.47it/s]\u001b[A\n"," 25% 2913/11873 [00:08<00:28, 309.94it/s]\u001b[A\n"," 25% 2945/11873 [00:08<00:28, 312.19it/s]\u001b[A\n"," 25% 2978/11873 [00:08<00:28, 317.11it/s]\u001b[A\n"," 25% 3010/11873 [00:08<00:28, 308.42it/s]\u001b[A\n"," 26% 3041/11873 [00:08<00:28, 307.59it/s]\u001b[A\n"," 26% 3072/11873 [00:09<00:29, 302.67it/s]\u001b[A\n"," 26% 3103/11873 [00:09<00:29, 299.91it/s]\u001b[A\n"," 26% 3134/11873 [00:09<00:34, 253.35it/s]\u001b[A\n"," 27% 3161/11873 [00:09<00:37, 233.05it/s]\u001b[A\n"," 27% 3192/11873 [00:09<00:34, 251.53it/s]\u001b[A\n"," 27% 3224/11873 [00:09<00:32, 267.40it/s]\u001b[A\n"," 27% 3255/11873 [00:09<00:30, 278.39it/s]\u001b[A\n"," 28% 3284/11873 [00:10<00:37, 227.89it/s]\u001b[A\n"," 28% 3309/11873 [00:10<00:44, 193.40it/s]\u001b[A\n"," 28% 3331/11873 [00:10<00:47, 180.60it/s]\u001b[A\n"," 28% 3351/11873 [00:10<00:47, 179.66it/s]\u001b[A\n"," 28% 3370/11873 [00:10<00:49, 171.62it/s]\u001b[A\n"," 29% 3401/11873 [00:10<00:41, 203.70it/s]\u001b[A\n"," 29% 3434/11873 [00:10<00:35, 235.63it/s]\u001b[A\n"," 29% 3466/11873 [00:10<00:32, 257.65it/s]\u001b[A\n"," 29% 3497/11873 [00:10<00:30, 271.35it/s]\u001b[A\n"," 30% 3528/11873 [00:11<00:29, 281.94it/s]\u001b[A\n"," 30% 3560/11873 [00:11<00:28, 292.61it/s]\u001b[A\n"," 30% 3590/11873 [00:11<00:28, 290.78it/s]\u001b[A\n"," 31% 3623/11873 [00:11<00:27, 301.23it/s]\u001b[A\n"," 31% 3655/11873 [00:11<00:27, 304.29it/s]\u001b[A\n"," 31% 3686/11873 [00:11<00:27, 303.14it/s]\u001b[A\n"," 31% 3717/11873 [00:11<00:26, 303.91it/s]\u001b[A\n"," 32% 3749/11873 [00:11<00:26, 306.64it/s]\u001b[A\n"," 32% 3781/11873 [00:11<00:26, 310.26it/s]\u001b[A\n"," 32% 3813/11873 [00:11<00:26, 309.91it/s]\u001b[A\n"," 32% 3845/11873 [00:12<00:28, 284.37it/s]\u001b[A\n"," 33% 3877/11873 [00:12<00:27, 292.91it/s]\u001b[A\n"," 33% 3909/11873 [00:12<00:26, 300.40it/s]\u001b[A\n"," 33% 3940/11873 [00:12<00:28, 275.43it/s]\u001b[A\n"," 33% 3969/11873 [00:12<00:28, 277.71it/s]\u001b[A\n"," 34% 4000/11873 [00:12<00:27, 285.22it/s]\u001b[A\n"," 34% 4033/11873 [00:12<00:26, 296.82it/s]\u001b[A\n"," 34% 4065/11873 [00:12<00:25, 301.75it/s]\u001b[A\n"," 34% 4096/11873 [00:12<00:25, 304.07it/s]\u001b[A\n"," 35% 4127/11873 [00:13<00:25, 304.46it/s]\u001b[A\n"," 35% 4158/11873 [00:13<00:27, 279.70it/s]\u001b[A\n"," 35% 4188/11873 [00:13<00:27, 284.27it/s]\u001b[A\n"," 36% 4220/11873 [00:13<00:26, 293.15it/s]\u001b[A\n"," 36% 4252/11873 [00:13<00:25, 299.84it/s]\u001b[A\n"," 36% 4284/11873 [00:13<00:24, 305.10it/s]\u001b[A\n"," 36% 4317/11873 [00:13<00:24, 310.45it/s]\u001b[A\n"," 37% 4349/11873 [00:13<00:24, 312.71it/s]\u001b[A\n"," 37% 4381/11873 [00:13<00:24, 308.85it/s]\u001b[A\n"," 37% 4412/11873 [00:14<00:25, 298.17it/s]\u001b[A\n"," 37% 4442/11873 [00:14<00:29, 248.57it/s]\u001b[A\n"," 38% 4475/11873 [00:14<00:27, 268.78it/s]\u001b[A\n"," 38% 4505/11873 [00:14<00:26, 276.39it/s]\u001b[A\n"," 38% 4536/11873 [00:14<00:26, 280.50it/s]\u001b[A\n"," 38% 4568/11873 [00:14<00:25, 290.98it/s]\u001b[A\n"," 39% 4600/11873 [00:14<00:24, 297.45it/s]\u001b[A\n"," 39% 4631/11873 [00:14<00:24, 299.35it/s]\u001b[A\n"," 39% 4662/11873 [00:14<00:23, 302.33it/s]\u001b[A\n"," 40% 4693/11873 [00:15<00:23, 302.53it/s]\u001b[A\n"," 40% 4724/11873 [00:15<00:23, 301.97it/s]\u001b[A\n"," 40% 4756/11873 [00:15<00:23, 304.88it/s]\u001b[A\n"," 40% 4787/11873 [00:15<00:23, 299.10it/s]\u001b[A\n"," 41% 4818/11873 [00:15<00:23, 298.79it/s]\u001b[A\n"," 41% 4850/11873 [00:15<00:23, 303.38it/s]\u001b[A\n"," 41% 4881/11873 [00:15<00:22, 305.03it/s]\u001b[A\n"," 41% 4913/11873 [00:15<00:22, 306.81it/s]\u001b[A\n"," 42% 4945/11873 [00:15<00:22, 310.30it/s]\u001b[A\n"," 42% 4977/11873 [00:15<00:22, 311.16it/s]\u001b[A\n"," 42% 5009/11873 [00:16<00:21, 312.14it/s]\u001b[A\n"," 42% 5041/11873 [00:16<00:22, 303.27it/s]\u001b[A\n"," 43% 5074/11873 [00:16<00:21, 310.83it/s]\u001b[A\n"," 43% 5107/11873 [00:16<00:21, 315.14it/s]\u001b[A\n"," 43% 5139/11873 [00:16<00:31, 216.77it/s]\u001b[A\n"," 44% 5171/11873 [00:16<00:27, 239.75it/s]\u001b[A\n"," 44% 5205/11873 [00:16<00:25, 262.10it/s]\u001b[A\n"," 44% 5237/11873 [00:16<00:24, 275.18it/s]\u001b[A\n"," 44% 5267/11873 [00:17<00:24, 264.94it/s]\u001b[A\n"," 45% 5300/11873 [00:17<00:23, 280.72it/s]\u001b[A\n"," 45% 5333/11873 [00:17<00:22, 293.96it/s]\u001b[A\n"," 45% 5365/11873 [00:17<00:21, 300.26it/s]\u001b[A\n"," 45% 5398/11873 [00:17<00:21, 307.85it/s]\u001b[A\n"," 46% 5431/11873 [00:17<00:20, 312.19it/s]\u001b[A\n"," 46% 5463/11873 [00:17<00:20, 310.37it/s]\u001b[A\n"," 46% 5495/11873 [00:17<00:20, 311.31it/s]\u001b[A\n"," 47% 5529/11873 [00:17<00:19, 317.34it/s]\u001b[A\n"," 47% 5561/11873 [00:17<00:19, 316.57it/s]\u001b[A\n"," 47% 5593/11873 [00:18<00:19, 317.48it/s]\u001b[A\n"," 47% 5625/11873 [00:18<00:19, 317.27it/s]\u001b[A\n"," 48% 5657/11873 [00:18<00:19, 313.90it/s]\u001b[A\n"," 48% 5689/11873 [00:18<00:19, 311.49it/s]\u001b[A\n"," 48% 5721/11873 [00:18<00:19, 312.19it/s]\u001b[A\n"," 48% 5753/11873 [00:18<00:19, 313.82it/s]\u001b[A\n"," 49% 5785/11873 [00:18<00:19, 313.48it/s]\u001b[A\n"," 49% 5818/11873 [00:18<00:19, 316.27it/s]\u001b[A\n"," 49% 5850/11873 [00:18<00:18, 317.23it/s]\u001b[A\n"," 50% 5882/11873 [00:18<00:18, 317.30it/s]\u001b[A\n"," 50% 5914/11873 [00:19<00:18, 317.35it/s]\u001b[A\n"," 50% 5946/11873 [00:19<00:18, 316.77it/s]\u001b[A\n"," 50% 5978/11873 [00:19<00:18, 317.54it/s]\u001b[A\n"," 51% 6010/11873 [00:19<00:18, 315.62it/s]\u001b[A\n"," 51% 6042/11873 [00:19<00:18, 316.14it/s]\u001b[A\n"," 51% 6074/11873 [00:19<00:18, 313.14it/s]\u001b[A\n"," 51% 6106/11873 [00:19<00:18, 313.27it/s]\u001b[A\n"," 52% 6138/11873 [00:19<00:18, 313.08it/s]\u001b[A\n"," 52% 6170/11873 [00:19<00:18, 311.77it/s]\u001b[A\n"," 52% 6202/11873 [00:20<00:18, 305.60it/s]\u001b[A\n"," 53% 6234/11873 [00:20<00:18, 307.63it/s]\u001b[A\n"," 53% 6266/11873 [00:20<00:18, 310.64it/s]\u001b[A\n"," 53% 6299/11873 [00:20<00:17, 315.05it/s]\u001b[A\n"," 53% 6331/11873 [00:20<00:17, 314.62it/s]\u001b[A\n"," 54% 6363/11873 [00:20<00:17, 314.64it/s]\u001b[A\n"," 54% 6395/11873 [00:20<00:17, 312.70it/s]\u001b[A\n"," 54% 6427/11873 [00:20<00:17, 310.83it/s]\u001b[A\n"," 54% 6459/11873 [00:20<00:17, 310.64it/s]\u001b[A\n"," 55% 6491/11873 [00:20<00:17, 309.90it/s]\u001b[A\n"," 55% 6522/11873 [00:21<00:17, 308.43it/s]\u001b[A\n"," 55% 6553/11873 [00:21<00:17, 307.25it/s]\u001b[A\n"," 55% 6584/11873 [00:21<00:17, 307.03it/s]\u001b[A\n"," 56% 6615/11873 [00:21<00:17, 305.74it/s]\u001b[A\n"," 56% 6646/11873 [00:21<00:17, 306.08it/s]\u001b[A\n"," 56% 6678/11873 [00:21<00:16, 308.31it/s]\u001b[A\n"," 57% 6709/11873 [00:21<00:17, 288.62it/s]\u001b[A\n"," 57% 6739/11873 [00:21<00:18, 277.56it/s]\u001b[A\n"," 57% 6771/11873 [00:21<00:17, 289.22it/s]\u001b[A\n"," 57% 6802/11873 [00:21<00:17, 294.68it/s]\u001b[A\n"," 58% 6834/11873 [00:22<00:16, 299.50it/s]\u001b[A\n"," 58% 6865/11873 [00:22<00:16, 301.14it/s]\u001b[A\n"," 58% 6897/11873 [00:22<00:16, 304.63it/s]\u001b[A\n"," 58% 6929/11873 [00:22<00:16, 307.16it/s]\u001b[A\n"," 59% 6960/11873 [00:22<00:16, 306.45it/s]\u001b[A\n"," 59% 6993/11873 [00:22<00:15, 311.91it/s]\u001b[A\n"," 59% 7025/11873 [00:22<00:15, 314.28it/s]\u001b[A\n"," 59% 7057/11873 [00:22<00:15, 313.67it/s]\u001b[A\n"," 60% 7089/11873 [00:22<00:15, 313.44it/s]\u001b[A\n"," 60% 7121/11873 [00:23<00:15, 314.60it/s]\u001b[A\n"," 60% 7153/11873 [00:23<00:15, 312.18it/s]\u001b[A\n"," 61% 7185/11873 [00:23<00:15, 310.69it/s]\u001b[A\n"," 61% 7217/11873 [00:23<00:14, 312.38it/s]\u001b[A\n"," 61% 7249/11873 [00:23<00:14, 310.06it/s]\u001b[A\n"," 61% 7281/11873 [00:23<00:14, 310.48it/s]\u001b[A\n"," 62% 7313/11873 [00:23<00:14, 311.70it/s]\u001b[A\n"," 62% 7345/11873 [00:23<00:14, 308.94it/s]\u001b[A\n"," 62% 7377/11873 [00:23<00:14, 311.23it/s]\u001b[A\n"," 62% 7409/11873 [00:23<00:14, 307.88it/s]\u001b[A\n"," 63% 7440/11873 [00:24<00:15, 291.86it/s]\u001b[A\n"," 63% 7472/11873 [00:24<00:14, 299.12it/s]\u001b[A\n"," 63% 7505/11873 [00:24<00:14, 306.22it/s]\u001b[A\n"," 63% 7537/11873 [00:24<00:14, 308.44it/s]\u001b[A\n"," 64% 7569/11873 [00:24<00:13, 310.95it/s]\u001b[A\n"," 64% 7601/11873 [00:24<00:13, 310.46it/s]\u001b[A\n"," 64% 7633/11873 [00:24<00:13, 308.35it/s]\u001b[A\n"," 65% 7665/11873 [00:24<00:13, 310.93it/s]\u001b[A\n"," 65% 7697/11873 [00:24<00:13, 310.07it/s]\u001b[A\n"," 65% 7729/11873 [00:25<00:14, 288.02it/s]\u001b[A\n"," 65% 7760/11873 [00:25<00:14, 292.57it/s]\u001b[A\n"," 66% 7792/11873 [00:25<00:13, 299.61it/s]\u001b[A\n"," 66% 7823/11873 [00:25<00:13, 301.56it/s]\u001b[A\n"," 66% 7854/11873 [00:25<00:13, 302.00it/s]\u001b[A\n"," 66% 7885/11873 [00:25<00:14, 283.50it/s]\u001b[A\n"," 67% 7917/11873 [00:25<00:13, 292.87it/s]\u001b[A\n"," 67% 7949/11873 [00:25<00:13, 300.41it/s]\u001b[A\n"," 67% 7980/11873 [00:25<00:12, 301.02it/s]\u001b[A\n"," 67% 8013/11873 [00:25<00:12, 308.22it/s]\u001b[A\n"," 68% 8045/11873 [00:26<00:12, 311.12it/s]\u001b[A\n"," 68% 8078/11873 [00:26<00:12, 314.34it/s]\u001b[A\n"," 68% 8111/11873 [00:26<00:11, 317.72it/s]\u001b[A\n"," 69% 8143/11873 [00:26<00:11, 317.75it/s]\u001b[A\n"," 69% 8175/11873 [00:26<00:11, 312.28it/s]\u001b[A\n"," 69% 8207/11873 [00:26<00:11, 309.13it/s]\u001b[A\n"," 69% 8239/11873 [00:26<00:11, 312.06it/s]\u001b[A\n"," 70% 8272/11873 [00:26<00:11, 317.16it/s]\u001b[A\n"," 70% 8304/11873 [00:26<00:11, 316.18it/s]\u001b[A\n"," 70% 8336/11873 [00:26<00:11, 317.07it/s]\u001b[A\n"," 70% 8369/11873 [00:27<00:11, 318.38it/s]\u001b[A\n"," 71% 8402/11873 [00:27<00:10, 319.31it/s]\u001b[A\n"," 71% 8434/11873 [00:27<00:10, 316.04it/s]\u001b[A\n"," 71% 8466/11873 [00:27<00:10, 312.57it/s]\u001b[A\n"," 72% 8499/11873 [00:27<00:10, 315.91it/s]\u001b[A\n"," 72% 8531/11873 [00:27<00:10, 314.33it/s]\u001b[A\n"," 72% 8563/11873 [00:27<00:10, 315.73it/s]\u001b[A\n"," 72% 8597/11873 [00:27<00:10, 322.36it/s]\u001b[A\n"," 73% 8630/11873 [00:27<00:10, 315.95it/s]\u001b[A\n"," 73% 8662/11873 [00:27<00:10, 316.30it/s]\u001b[A\n"," 73% 8695/11873 [00:28<00:09, 318.55it/s]\u001b[A\n"," 74% 8727/11873 [00:28<00:10, 313.79it/s]\u001b[A\n"," 74% 8759/11873 [00:28<00:09, 315.58it/s]\u001b[A\n"," 74% 8792/11873 [00:28<00:09, 318.97it/s]\u001b[A\n"," 74% 8824/11873 [00:28<00:09, 316.59it/s]\u001b[A\n"," 75% 8856/11873 [00:28<00:09, 316.36it/s]\u001b[A\n"," 75% 8889/11873 [00:28<00:09, 320.36it/s]\u001b[A\n"," 75% 8922/11873 [00:28<00:09, 322.82it/s]\u001b[A\n"," 75% 8955/11873 [00:28<00:09, 318.11it/s]\u001b[A\n"," 76% 8987/11873 [00:29<00:09, 317.11it/s]\u001b[A\n"," 76% 9019/11873 [00:29<00:09, 315.18it/s]\u001b[A\n"," 76% 9051/11873 [00:29<00:08, 315.27it/s]\u001b[A\n"," 77% 9084/11873 [00:29<00:08, 317.32it/s]\u001b[A\n"," 77% 9116/11873 [00:29<00:08, 317.47it/s]\u001b[A\n"," 77% 9148/11873 [00:29<00:08, 315.68it/s]\u001b[A\n"," 77% 9181/11873 [00:29<00:08, 318.49it/s]\u001b[A\n"," 78% 9214/11873 [00:29<00:08, 320.32it/s]\u001b[A\n"," 78% 9247/11873 [00:29<00:08, 321.08it/s]\u001b[A\n"," 78% 9280/11873 [00:29<00:08, 322.45it/s]\u001b[A\n"," 78% 9313/11873 [00:30<00:07, 321.46it/s]\u001b[A\n"," 79% 9346/11873 [00:30<00:07, 319.15it/s]\u001b[A\n"," 79% 9379/11873 [00:30<00:07, 321.13it/s]\u001b[A\n"," 79% 9412/11873 [00:30<00:07, 316.80it/s]\u001b[A\n"," 80% 9445/11873 [00:30<00:07, 320.27it/s]\u001b[A\n"," 80% 9478/11873 [00:30<00:07, 320.57it/s]\u001b[A\n"," 80% 9511/11873 [00:30<00:07, 322.46it/s]\u001b[A\n"," 80% 9545/11873 [00:30<00:07, 326.93it/s]\u001b[A\n"," 81% 9578/11873 [00:30<00:07, 327.11it/s]\u001b[A\n"," 81% 9611/11873 [00:30<00:07, 313.23it/s]\u001b[A\n"," 81% 9644/11873 [00:31<00:07, 316.51it/s]\u001b[A\n"," 82% 9677/11873 [00:31<00:06, 318.07it/s]\u001b[A\n"," 82% 9711/11873 [00:31<00:06, 322.25it/s]\u001b[A\n"," 82% 9744/11873 [00:31<00:06, 323.79it/s]\u001b[A\n"," 82% 9778/11873 [00:31<00:06, 327.01it/s]\u001b[A\n"," 83% 9811/11873 [00:31<00:06, 319.58it/s]\u001b[A\n"," 83% 9844/11873 [00:31<00:06, 320.39it/s]\u001b[A\n"," 83% 9877/11873 [00:31<00:06, 320.50it/s]\u001b[A\n"," 83% 9910/11873 [00:31<00:06, 319.68it/s]\u001b[A\n"," 84% 9942/11873 [00:31<00:06, 318.97it/s]\u001b[A\n"," 84% 9975/11873 [00:32<00:05, 319.35it/s]\u001b[A\n"," 84% 10008/11873 [00:32<00:05, 320.44it/s]\u001b[A\n"," 85% 10041/11873 [00:32<00:05, 320.00it/s]\u001b[A\n"," 85% 10074/11873 [00:32<00:05, 318.75it/s]\u001b[A\n"," 85% 10106/11873 [00:32<00:05, 318.87it/s]\u001b[A\n"," 85% 10139/11873 [00:32<00:05, 319.98it/s]\u001b[A\n"," 86% 10172/11873 [00:32<00:05, 319.59it/s]\u001b[A\n"," 86% 10204/11873 [00:32<00:05, 316.26it/s]\u001b[A\n"," 86% 10236/11873 [00:32<00:05, 312.64it/s]\u001b[A\n"," 86% 10268/11873 [00:33<00:05, 313.12it/s]\u001b[A\n"," 87% 10300/11873 [00:33<00:05, 307.54it/s]\u001b[A\n"," 87% 10333/11873 [00:33<00:04, 311.72it/s]\u001b[A\n"," 87% 10365/11873 [00:33<00:04, 313.01it/s]\u001b[A\n"," 88% 10397/11873 [00:33<00:04, 313.02it/s]\u001b[A\n"," 88% 10429/11873 [00:33<00:04, 295.23it/s]\u001b[A\n"," 88% 10461/11873 [00:33<00:04, 301.67it/s]\u001b[A\n"," 88% 10494/11873 [00:33<00:04, 307.50it/s]\u001b[A\n"," 89% 10525/11873 [00:33<00:04, 303.04it/s]\u001b[A\n"," 89% 10556/11873 [00:33<00:04, 301.74it/s]\u001b[A\n"," 89% 10588/11873 [00:34<00:04, 304.97it/s]\u001b[A\n"," 89% 10620/11873 [00:34<00:04, 307.58it/s]\u001b[A\n"," 90% 10651/11873 [00:34<00:04, 304.62it/s]\u001b[A\n"," 90% 10683/11873 [00:34<00:03, 307.66it/s]\u001b[A\n"," 90% 10715/11873 [00:34<00:03, 308.89it/s]\u001b[A\n"," 91% 10747/11873 [00:34<00:03, 310.59it/s]\u001b[A\n"," 91% 10779/11873 [00:34<00:03, 307.88it/s]\u001b[A\n"," 91% 10811/11873 [00:34<00:03, 309.13it/s]\u001b[A\n"," 91% 10842/11873 [00:34<00:03, 284.45it/s]\u001b[A\n"," 92% 10874/11873 [00:35<00:03, 293.97it/s]\u001b[A\n"," 92% 10906/11873 [00:35<00:03, 299.40it/s]\u001b[A\n"," 92% 10937/11873 [00:35<00:03, 298.96it/s]\u001b[A\n"," 92% 10968/11873 [00:35<00:03, 301.13it/s]\u001b[A\n"," 93% 10999/11873 [00:35<00:02, 299.88it/s]\u001b[A\n"," 93% 11031/11873 [00:35<00:02, 305.04it/s]\u001b[A\n"," 93% 11063/11873 [00:35<00:02, 309.14it/s]\u001b[A\n"," 93% 11096/11873 [00:35<00:02, 314.39it/s]\u001b[A\n"," 94% 11129/11873 [00:35<00:02, 316.08it/s]\u001b[A\n"," 94% 11161/11873 [00:35<00:02, 308.75it/s]\u001b[A\n"," 94% 11193/11873 [00:36<00:02, 309.63it/s]\u001b[A\n"," 95% 11225/11873 [00:36<00:02, 310.14it/s]\u001b[A\n"," 95% 11257/11873 [00:36<00:01, 308.87it/s]\u001b[A\n"," 95% 11289/11873 [00:36<00:01, 309.47it/s]\u001b[A\n"," 95% 11321/11873 [00:36<00:01, 311.71it/s]\u001b[A\n"," 96% 11353/11873 [00:36<00:01, 312.19it/s]\u001b[A\n"," 96% 11386/11873 [00:36<00:01, 314.88it/s]\u001b[A\n"," 96% 11418/11873 [00:36<00:01, 314.97it/s]\u001b[A\n"," 96% 11450/11873 [00:36<00:01, 315.32it/s]\u001b[A\n"," 97% 11482/11873 [00:36<00:01, 314.98it/s]\u001b[A\n"," 97% 11515/11873 [00:37<00:01, 318.34it/s]\u001b[A\n"," 97% 11547/11873 [00:37<00:01, 316.13it/s]\u001b[A\n"," 98% 11579/11873 [00:37<00:00, 312.72it/s]\u001b[A\n"," 98% 11611/11873 [00:37<00:00, 311.68it/s]\u001b[A\n"," 98% 11643/11873 [00:37<00:00, 313.98it/s]\u001b[A\n"," 98% 11675/11873 [00:37<00:00, 313.27it/s]\u001b[A\n"," 99% 11707/11873 [00:37<00:00, 314.02it/s]\u001b[A\n"," 99% 11740/11873 [00:37<00:00, 317.81it/s]\u001b[A\n"," 99% 11773/11873 [00:37<00:00, 319.17it/s]\u001b[A\n"," 99% 11806/11873 [00:38<00:00, 321.76it/s]\u001b[A\n","100% 11839/11873 [00:38<00:00, 323.28it/s]\u001b[A\n","100% 11873/11873 [00:38<00:00, 310.72it/s]\n","04/06/2022 05:54:03 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","04/06/2022 05:54:03 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","04/06/2022 05:54:06 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","04/06/2022 05:54:10 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1521/1521 [03:54<00:00,  6.48it/s]\n","***** eval metrics *****\n","  epoch                  =     3.0\n","  eval_HasAns_exact      =  77.969\n","  eval_HasAns_f1         = 84.7523\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 81.4298\n","  eval_NoAns_f1          = 81.4298\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 79.7018\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 83.0886\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 79.7018\n","  eval_f1                = 83.0886\n","  eval_samples           =   12165\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 05:54:10,485 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]}]}