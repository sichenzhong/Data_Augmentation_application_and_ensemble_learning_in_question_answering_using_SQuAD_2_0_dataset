{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Back Translation augmented possible only.ipynb","provenance":[],"machine_shape":"hm","mount_file_id":"1FRJvTJOJnCCC05umt-328dIdL2Zp24WC","authorship_tag":"ABX9TyOrPCzlOrtWqgdfeRJ3Jlue"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"81fa025b51244f1a8bf1a8aa860b4111":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3fc5395c42e47f7b623a4a0fceed3b6","IPY_MODEL_af32d380ed7e48b1bfe25823d053c6e0","IPY_MODEL_f2dd1c2ffb8642ffba83fba026d19e00"],"layout":"IPY_MODEL_6711740bc1c946ff968b4d97cb2fd230"}},"e3fc5395c42e47f7b623a4a0fceed3b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64a959d813f9497ea38b6f9fef51fc3b","placeholder":"​","style":"IPY_MODEL_f05537facf284a9196c10f8099d4ced7","value":"Downloading data files: 100%"}},"af32d380ed7e48b1bfe25823d053c6e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7be54ae811b45feab7a75715aaed4b5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa2a9ae3e0a54171b0761d79811ac791","value":2}},"f2dd1c2ffb8642ffba83fba026d19e00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61ed8e378f1d4fce8d3f122d7939c239","placeholder":"​","style":"IPY_MODEL_130dc502f13c49819b81f5e3e711b7f5","value":" 2/2 [00:00&lt;00:00, 66.88it/s]"}},"6711740bc1c946ff968b4d97cb2fd230":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64a959d813f9497ea38b6f9fef51fc3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f05537facf284a9196c10f8099d4ced7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7be54ae811b45feab7a75715aaed4b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa2a9ae3e0a54171b0761d79811ac791":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61ed8e378f1d4fce8d3f122d7939c239":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"130dc502f13c49819b81f5e3e711b7f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4c409595e644ba98d829295d0d275ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_672944ad9eff4c12be78e793db5c8841","IPY_MODEL_e6a4b78eb2804984ae0fcb74e20ac05b","IPY_MODEL_f042034767794af2b71c8b67e06d1444"],"layout":"IPY_MODEL_7a00a37e7ddd4962be6978ecce60da6b"}},"672944ad9eff4c12be78e793db5c8841":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f476f98367814e2fa212427ad528f872","placeholder":"​","style":"IPY_MODEL_1dec37cdf9124768bcc87c6afa787ce7","value":"Extracting data files: 100%"}},"e6a4b78eb2804984ae0fcb74e20ac05b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db1dbe7e48394a0fa8da98f0043d9ac3","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9475ce2d083645ffa03ff74c401f547b","value":2}},"f042034767794af2b71c8b67e06d1444":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1bf2e8409b22440f9bdc0d43c2f16862","placeholder":"​","style":"IPY_MODEL_7b15eb11a5964716bf7c54662e7bf06e","value":" 2/2 [00:00&lt;00:00, 35.72it/s]"}},"7a00a37e7ddd4962be6978ecce60da6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f476f98367814e2fa212427ad528f872":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dec37cdf9124768bcc87c6afa787ce7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db1dbe7e48394a0fa8da98f0043d9ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9475ce2d083645ffa03ff74c401f547b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1bf2e8409b22440f9bdc0d43c2f16862":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b15eb11a5964716bf7c54662e7bf06e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df61da473eb54c788c45685d3b04db01":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07a3e7eabfc147d8be9f4770310f8c3c","IPY_MODEL_5bd50d2e41a944998ad3a5acb65e5824","IPY_MODEL_c9a2847d4ac446c6a4a94a6a4623e8e1"],"layout":"IPY_MODEL_c5dc4874ee5b451687d4b75d4e3cd78f"}},"07a3e7eabfc147d8be9f4770310f8c3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b796598363a4e2785276a58f7c29cbc","placeholder":"​","style":"IPY_MODEL_d63eaf892b044df3aae0dbb606305d1d","value":"Generating train split: "}},"5bd50d2e41a944998ad3a5acb65e5824":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0ee1078d60f4170944f01f70875b13f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4bf09c1f914c43918aca15bdcb2efe44","value":1}},"c9a2847d4ac446c6a4a94a6a4623e8e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24a78ef024b840cf8122516a327d6551","placeholder":"​","style":"IPY_MODEL_b107d6caf2fe4cdb821d647296303551","value":" 129449/0 [00:12&lt;00:00, 12286.63 examples/s]"}},"c5dc4874ee5b451687d4b75d4e3cd78f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b796598363a4e2785276a58f7c29cbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d63eaf892b044df3aae0dbb606305d1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0ee1078d60f4170944f01f70875b13f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"4bf09c1f914c43918aca15bdcb2efe44":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"24a78ef024b840cf8122516a327d6551":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b107d6caf2fe4cdb821d647296303551":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f6c5b9d21fb4a39949c79d5636cb74a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32d36241bdad465588fc421df31bfcfa","IPY_MODEL_09bb9c5969ee45e5b607cb7896e02b26","IPY_MODEL_f079426ff1d2445d9355e28890eb7004"],"layout":"IPY_MODEL_8cffd5034421402ab2dc4ac3d9a3729a"}},"32d36241bdad465588fc421df31bfcfa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0abea93ce6b1476999acd946c552ce68","placeholder":"​","style":"IPY_MODEL_f1f40e68a0394990989ee09b4ad9c052","value":"Generating validation split: "}},"09bb9c5969ee45e5b607cb7896e02b26":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_c84a427993614d88851994f5be438a44","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_17736704b17c4e76867ddcbd64f95492","value":1}},"f079426ff1d2445d9355e28890eb7004":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0908d66b9e9b4f8d9af73a9617b3b840","placeholder":"​","style":"IPY_MODEL_e9336dbaffc646dd8a5b4810d80f3db8","value":" 11826/0 [00:01&lt;00:00, 11457.88 examples/s]"}},"8cffd5034421402ab2dc4ac3d9a3729a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0abea93ce6b1476999acd946c552ce68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1f40e68a0394990989ee09b4ad9c052":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c84a427993614d88851994f5be438a44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"17736704b17c4e76867ddcbd64f95492":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0908d66b9e9b4f8d9af73a9617b3b840":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9336dbaffc646dd8a5b4810d80f3db8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"086c46d8474e450daa87324ec1c18227":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce84c1f6bcf54111892922effa1885b2","IPY_MODEL_66a94f361aba4e3395481103f4ef79d3","IPY_MODEL_7f39e66497a94a61bdeebabb214dabb8"],"layout":"IPY_MODEL_1100b9d90dc7450aabf2b1df1a88adbd"}},"ce84c1f6bcf54111892922effa1885b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6dbe48b553b44489ad4fe90f3a997c8","placeholder":"​","style":"IPY_MODEL_2157c251b722405abb1fae06ea760858","value":"100%"}},"66a94f361aba4e3395481103f4ef79d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02ea6c2f440c41228f5b5e8044c00a5b","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_038b6cbf444c4deb94413be8f10eed54","value":2}},"7f39e66497a94a61bdeebabb214dabb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f016798956e449e195e2a3bafc6539e8","placeholder":"​","style":"IPY_MODEL_33df565b439b4ed28c754ff1dcc07cae","value":" 2/2 [00:00&lt;00:00, 49.90it/s]"}},"1100b9d90dc7450aabf2b1df1a88adbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6dbe48b553b44489ad4fe90f3a997c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2157c251b722405abb1fae06ea760858":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02ea6c2f440c41228f5b5e8044c00a5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"038b6cbf444c4deb94413be8f10eed54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f016798956e449e195e2a3bafc6539e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33df565b439b4ed28c754ff1dcc07cae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47d714d123a94195ac986f07ac1b83be":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_50d89cce8cb64e599f9ace256bf60f5c","IPY_MODEL_247a07fa870e4086987616c8265200c8","IPY_MODEL_c33ad1a8398c4c2089a0db690563cfcf"],"layout":"IPY_MODEL_6a3504de49484aebb2b12380385f4a27"}},"50d89cce8cb64e599f9ace256bf60f5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c4366d30a164cf5aeed495838472396","placeholder":"​","style":"IPY_MODEL_1133d337c83e44b1b01b6d30d4796ae3","value":"Pushing dataset shards to the dataset hub: 100%"}},"247a07fa870e4086987616c8265200c8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bc27000f28c4fac91f3acbfd2856996","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_553c128e3a9b4080bf91597e47b457d7","value":1}},"c33ad1a8398c4c2089a0db690563cfcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c46823e3609b4bbd94dbba7d8db6cbbf","placeholder":"​","style":"IPY_MODEL_d4ffe71829e945cab4ec10ca8ea89918","value":" 1/1 [00:06&lt;00:00,  6.19s/it]"}},"6a3504de49484aebb2b12380385f4a27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c4366d30a164cf5aeed495838472396":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1133d337c83e44b1b01b6d30d4796ae3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7bc27000f28c4fac91f3acbfd2856996":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"553c128e3a9b4080bf91597e47b457d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c46823e3609b4bbd94dbba7d8db6cbbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4ffe71829e945cab4ec10ca8ea89918":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06213f2c46e643f5a465b767c32f1afa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_752d9c076a5d409fadddec1f418ef5ce","IPY_MODEL_fe503fa3a9214e15b8ab719bb8037e55","IPY_MODEL_e5bf9385c4404885b616092be1433727"],"layout":"IPY_MODEL_78885faa59f5447981eff093fbf4aa80"}},"752d9c076a5d409fadddec1f418ef5ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52e54902aa0540b28f9fa2a19a831296","placeholder":"​","style":"IPY_MODEL_3ff8dfae3d4946f88ff4428c403ff81f","value":"Pushing dataset shards to the dataset hub: 100%"}},"fe503fa3a9214e15b8ab719bb8037e55":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16eefa7a53934c6688c9e1cd9510079e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa6415c789334cd0ae4c52c25080370c","value":1}},"e5bf9385c4404885b616092be1433727":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a87b298d5e184a8b8e9a0a5af21db086","placeholder":"​","style":"IPY_MODEL_8f1f4359063e4ecebaf73d7d14510de7","value":" 1/1 [00:01&lt;00:00,  1.88s/it]"}},"78885faa59f5447981eff093fbf4aa80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52e54902aa0540b28f9fa2a19a831296":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ff8dfae3d4946f88ff4428c403ff81f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16eefa7a53934c6688c9e1cd9510079e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa6415c789334cd0ae4c52c25080370c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a87b298d5e184a8b8e9a0a5af21db086":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f1f4359063e4ecebaf73d7d14510de7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70oLi9mZP6oK","executionInfo":{"status":"ok","timestamp":1649303492183,"user_tz":240,"elapsed":225,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"2e2c26ef-bdcf-45d3-cfc0-c5202d9bad37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Apr  7 03:51:31 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KPK78kqCH-mS","executionInfo":{"status":"ok","timestamp":1649303511528,"user_tz":240,"elapsed":15601,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"8d8ca023-e759-4073-c649-ea702b440d22"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGqcN-zXTvvo","executionInfo":{"status":"ok","timestamp":1649303519683,"user_tz":240,"elapsed":8159,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"7ad2abab-fed1-432e-cb58-258a326219e6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n","\u001b[K     |████████████████████████████████| 325 kB 14.7 MB/s \n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 81.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 7.4 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 81.5 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 77.7 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 81.9 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 87.9 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.5 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 76.3 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.1 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tI_RBT1FSotu","outputId":"81fa4421-f7e5-4945-dee3-c47ff895ed3c","executionInfo":{"status":"ok","timestamp":1649303547165,"user_tz":240,"elapsed":27489,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-mwcmjinj\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-mwcmjinj\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.11.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (3.6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.63.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 13.8 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 65.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 77.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (1.21.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0.dev0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.0.dev0) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.0.dev0) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2021.10.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.15.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.19.0.dev0-py3-none-any.whl size=3965509 sha256=5ec96cf2e9c9e8630122a2bd104291059996bb6ba09913dbffa39c1f9d3547ab\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9qq_7t5v/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.19.0.dev0\n"]}]},{"cell_type":"code","source":["import torch\n","import datetime\n","import json\n","import os\n","import time\n","import datasets\n","import pprint\n","import random\n","import string\n","import sys\n","import transformers\n","from datasets import load_dataset\n","from datasets.tasks import QuestionAnsweringExtractive"],"metadata":{"id":"DZ3Ma-pCRJDJ","executionInfo":{"status":"ok","timestamp":1649303555777,"user_tz":240,"elapsed":8618,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available()\n","                      else 'cpu')"],"metadata":{"id":"HNMUVyBpRGw8","executionInfo":{"status":"ok","timestamp":1649303555778,"user_tz":240,"elapsed":5,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/huggingface/transformers.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUVkgX-IQIiR","executionInfo":{"status":"ok","timestamp":1649303563756,"user_tz":240,"elapsed":7982,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"67747202-f401-4ce4-b85e-cfec1572f661"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 108786, done.\u001b[K\n","remote: Total 108786 (delta 0), reused 0 (delta 0), pack-reused 108786\u001b[K\n","Receiving objects: 100% (108786/108786), 95.51 MiB | 24.89 MiB/s, done.\n","Resolving deltas: 100% (79274/79274), done.\n"]}]},{"cell_type":"code","source":["dataset = load_dataset('/content/drive/MyDrive/QA/squad_v2_back_trans_possib_aug.py')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":148,"referenced_widgets":["81fa025b51244f1a8bf1a8aa860b4111","e3fc5395c42e47f7b623a4a0fceed3b6","af32d380ed7e48b1bfe25823d053c6e0","f2dd1c2ffb8642ffba83fba026d19e00","6711740bc1c946ff968b4d97cb2fd230","64a959d813f9497ea38b6f9fef51fc3b","f05537facf284a9196c10f8099d4ced7","f7be54ae811b45feab7a75715aaed4b5","fa2a9ae3e0a54171b0761d79811ac791","61ed8e378f1d4fce8d3f122d7939c239","130dc502f13c49819b81f5e3e711b7f5","e4c409595e644ba98d829295d0d275ac","672944ad9eff4c12be78e793db5c8841","e6a4b78eb2804984ae0fcb74e20ac05b","f042034767794af2b71c8b67e06d1444","7a00a37e7ddd4962be6978ecce60da6b","f476f98367814e2fa212427ad528f872","1dec37cdf9124768bcc87c6afa787ce7","db1dbe7e48394a0fa8da98f0043d9ac3","9475ce2d083645ffa03ff74c401f547b","1bf2e8409b22440f9bdc0d43c2f16862","7b15eb11a5964716bf7c54662e7bf06e","df61da473eb54c788c45685d3b04db01","07a3e7eabfc147d8be9f4770310f8c3c","5bd50d2e41a944998ad3a5acb65e5824","c9a2847d4ac446c6a4a94a6a4623e8e1","c5dc4874ee5b451687d4b75d4e3cd78f","8b796598363a4e2785276a58f7c29cbc","d63eaf892b044df3aae0dbb606305d1d","e0ee1078d60f4170944f01f70875b13f","4bf09c1f914c43918aca15bdcb2efe44","24a78ef024b840cf8122516a327d6551","b107d6caf2fe4cdb821d647296303551","9f6c5b9d21fb4a39949c79d5636cb74a","32d36241bdad465588fc421df31bfcfa","09bb9c5969ee45e5b607cb7896e02b26","f079426ff1d2445d9355e28890eb7004","8cffd5034421402ab2dc4ac3d9a3729a","0abea93ce6b1476999acd946c552ce68","f1f40e68a0394990989ee09b4ad9c052","c84a427993614d88851994f5be438a44","17736704b17c4e76867ddcbd64f95492","0908d66b9e9b4f8d9af73a9617b3b840","e9336dbaffc646dd8a5b4810d80f3db8","086c46d8474e450daa87324ec1c18227","ce84c1f6bcf54111892922effa1885b2","66a94f361aba4e3395481103f4ef79d3","7f39e66497a94a61bdeebabb214dabb8","1100b9d90dc7450aabf2b1df1a88adbd","d6dbe48b553b44489ad4fe90f3a997c8","2157c251b722405abb1fae06ea760858","02ea6c2f440c41228f5b5e8044c00a5b","038b6cbf444c4deb94413be8f10eed54","f016798956e449e195e2a3bafc6539e8","33df565b439b4ed28c754ff1dcc07cae"]},"id":"A2gdlPTvcmfL","executionInfo":{"status":"ok","timestamp":1648783087292,"user_tz":240,"elapsed":16650,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"c0f960c0-d79f-4e57-87cf-903833cfdcd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset squad_v2_aug/squad_v2 to /root/.cache/huggingface/datasets/squad_v2_aug/squad_v2/2.0.0/0c014370f57c5ba7a3c9d6cf6dae4a6d48d3d2cf9c230fd3ad3c5c31ba5a0929...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81fa025b51244f1a8bf1a8aa860b4111"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4c409595e644ba98d829295d0d275ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df61da473eb54c788c45685d3b04db01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f6c5b9d21fb4a39949c79d5636cb74a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset squad_v2_aug downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2_aug/squad_v2/2.0.0/0c014370f57c5ba7a3c9d6cf6dae4a6d48d3d2cf9c230fd3ad3c5c31ba5a0929. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"086c46d8474e450daa87324ec1c18227"}},"metadata":{}}]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttpAfwxBWdR0","executionInfo":{"status":"ok","timestamp":1648783092969,"user_tz":240,"elapsed":5681,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"dee51448-d331-4b9c-eaca-acd43287d250"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/token.\n","        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n","        \n","Token: \n","Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}]},{"cell_type":"code","source":["dataset.push_to_hub(\"sichenzhong/squad_v2_back_trans_possib_aug\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":150,"referenced_widgets":["47d714d123a94195ac986f07ac1b83be","50d89cce8cb64e599f9ace256bf60f5c","247a07fa870e4086987616c8265200c8","c33ad1a8398c4c2089a0db690563cfcf","6a3504de49484aebb2b12380385f4a27","1c4366d30a164cf5aeed495838472396","1133d337c83e44b1b01b6d30d4796ae3","7bc27000f28c4fac91f3acbfd2856996","553c128e3a9b4080bf91597e47b457d7","c46823e3609b4bbd94dbba7d8db6cbbf","d4ffe71829e945cab4ec10ca8ea89918","06213f2c46e643f5a465b767c32f1afa","752d9c076a5d409fadddec1f418ef5ce","fe503fa3a9214e15b8ab719bb8037e55","e5bf9385c4404885b616092be1433727","78885faa59f5447981eff093fbf4aa80","52e54902aa0540b28f9fa2a19a831296","3ff8dfae3d4946f88ff4428c403ff81f","16eefa7a53934c6688c9e1cd9510079e","fa6415c789334cd0ae4c52c25080370c","a87b298d5e184a8b8e9a0a5af21db086","8f1f4359063e4ecebaf73d7d14510de7"]},"id":"WGAeWENJWR_F","executionInfo":{"status":"ok","timestamp":1648783103790,"user_tz":240,"elapsed":10823,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"821bfa9d-47da-49e9-d448-ddaf7b5c561b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Pushing split train to the Hub.\n","The repository already exists: the `private` keyword argument will be ignored.\n"]},{"output_type":"display_data","data":{"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47d714d123a94195ac986f07ac1b83be"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Pushing split validation to the Hub.\n","The repository already exists: the `private` keyword argument will be ignored.\n"]},{"output_type":"display_data","data":{"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06213f2c46e643f5a465b767c32f1afa"}},"metadata":{}}]},{"cell_type":"code","source":["%cd /content/transformers/examples/pytorch/question-answering/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uLaizsXQrk9","executionInfo":{"status":"ok","timestamp":1649303563758,"user_tz":240,"elapsed":14,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"4a48abc5-bde1-4c18-cbee-8e86f6df89d6"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/transformers/examples/pytorch/question-answering\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path bert-base-cased \\\n","  --dataset_name sichenzhong/squad_v2_back_trans_possib_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 3 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aI-ipSbaHqF6","executionInfo":{"status":"ok","timestamp":1649272202248,"user_tz":240,"elapsed":16272761,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"ab6dacfb-fdb1-41b9-fbd3-9d7ccc808d07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["04/06/2022 14:38:53 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 14:38:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/runs/Apr06_14-38-53_c8c80d020252,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 14:38:54 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/sichenzhong/squad_v2_back_trans_possib_aug/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpi2_5gdkg\n","Downloading: 100% 2.17k/2.17k [00:00<00:00, 2.50MB/s]\n","04/06/2022 14:38:54 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/sichenzhong/squad_v2_back_trans_possib_aug/resolve/main/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/0636147c4e1182e4fd841482998e71e8d31f9f965bc306e23f0aa0785e1bf582.d853b616491ed83628665a533ccc2d19760ceca61149e66b633684fac7eb47b4\n","04/06/2022 14:38:54 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/0636147c4e1182e4fd841482998e71e8d31f9f965bc306e23f0aa0785e1bf582.d853b616491ed83628665a533ccc2d19760ceca61149e66b633684fac7eb47b4\n","04/06/2022 14:38:54 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef\n","04/06/2022 14:38:54 - INFO - datasets.builder - Generating dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","Downloading and preparing dataset squad_v2_aug/squad_v2 (download: 19.09 MiB, generated: 123.01 MiB, post-processed: Unknown size, total: 142.10 MiB) to /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n","04/06/2022 14:38:54 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]04/06/2022 14:38:55 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/sichenzhong/squad_v2_back_trans_possib_aug/resolve/9503eb59cefcf2169f4db081eaac7c6d4e1f6ad3/data/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp6xfo9t0g\n","\n","Downloading data:   0% 0.00/1.26M [00:00<?, ?B/s]\u001b[A\n","Downloading data:   3% 43.0k/1.26M [00:00<00:05, 242kB/s]\u001b[A\n","Downloading data:  19% 234k/1.26M [00:00<00:01, 985kB/s] \u001b[A\n","Downloading data: 100% 1.26M/1.26M [00:00<00:00, 2.32MB/s]\n","04/06/2022 14:38:56 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/sichenzhong/squad_v2_back_trans_possib_aug/resolve/9503eb59cefcf2169f4db081eaac7c6d4e1f6ad3/data/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/97ddd4e63e5009c19295ffa1814da799e5c7534e76c33bceda883bb9f5bc68a0\n","04/06/2022 14:38:56 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/97ddd4e63e5009c19295ffa1814da799e5c7534e76c33bceda883bb9f5bc68a0\n","Downloading data files:  50% 1/2 [00:01<00:01,  1.96s/it]04/06/2022 14:38:57 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/sichenzhong/squad_v2_back_trans_possib_aug/resolve/9503eb59cefcf2169f4db081eaac7c6d4e1f6ad3/data/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpnmyyun4a\n","\n","Downloading data:   0% 0.00/18.8M [00:00<?, ?B/s]\u001b[A\n","Downloading data:   0% 43.0k/18.8M [00:00<01:17, 241kB/s]\u001b[A\n","Downloading data:   1% 206k/18.8M [00:00<00:21, 854kB/s] \u001b[A\n","Downloading data:   3% 578k/18.8M [00:00<00:09, 1.96MB/s]\u001b[A\n","Downloading data:   7% 1.24M/18.8M [00:00<00:05, 3.06MB/s]\u001b[A\n","Downloading data:  15% 2.74M/18.8M [00:00<00:02, 6.61MB/s]\u001b[A\n","Downloading data:  33% 6.17M/18.8M [00:00<00:00, 14.9MB/s]\u001b[A\n","Downloading data:  51% 9.52M/18.8M [00:00<00:00, 20.5MB/s]\u001b[A\n","Downloading data:  70% 13.1M/18.8M [00:00<00:00, 21.9MB/s]\u001b[A\n","Downloading data: 100% 18.8M/18.8M [00:01<00:00, 16.1MB/s]\n","04/06/2022 14:38:59 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/sichenzhong/squad_v2_back_trans_possib_aug/resolve/9503eb59cefcf2169f4db081eaac7c6d4e1f6ad3/data/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/61eaf29c4a7cceec98a6e5d583d88eb04c50c89921a02deb073f8315b2ec43ab\n","04/06/2022 14:38:59 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/61eaf29c4a7cceec98a6e5d583d88eb04c50c89921a02deb073f8315b2ec43ab\n","Downloading data files: 100% 2/2 [00:04<00:00,  2.45s/it]\n","04/06/2022 14:38:59 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","04/06/2022 14:38:59 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1369.57it/s]\n","04/06/2022 14:38:59 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n","04/06/2022 14:38:59 - INFO - datasets.builder - Generating validation split\n","04/06/2022 14:38:59 - INFO - datasets.builder - Generating train split\n","04/06/2022 14:39:00 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 343.40it/s]\n","[INFO|hub.py:583] 2022-04-06 14:39:00,533 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmfm72yzo\n","Downloading: 100% 570/570 [00:00<00:00, 452kB/s]\n","[INFO|hub.py:587] 2022-04-06 14:39:00,893 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|hub.py:595] 2022-04-06 14:39:00,893 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:654] 2022-04-06 14:39:00,894 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 14:39:00,894 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 14:39:01,251 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmjoa54fj\n","Downloading: 100% 29.0/29.0 [00:00<00:00, 25.1kB/s]\n","[INFO|hub.py:587] 2022-04-06 14:39:01,617 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|hub.py:595] 2022-04-06 14:39:01,617 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:654] 2022-04-06 14:39:01,973 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 14:39:01,974 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 14:39:02,689 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpyrs9kvpp\n","Downloading: 100% 208k/208k [00:00<00:00, 634kB/s]\n","[INFO|hub.py:587] 2022-04-06 14:39:03,383 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:595] 2022-04-06 14:39:03,383 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:583] 2022-04-06 14:39:03,741 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphgpjb9h4\n","Downloading: 100% 426k/426k [00:00<00:00, 1.04MB/s]\n","[INFO|hub.py:587] 2022-04-06 14:39:04,521 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|hub.py:595] 2022-04-06 14:39:04,521 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:39:05,595 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:39:05,595 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:39:05,595 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:39:05,595 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 14:39:05,595 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:654] 2022-04-06 14:39:05,952 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 14:39:05,953 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 14:39:06,357 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpr92w6h4r\n","Downloading: 100% 416M/416M [00:05<00:00, 74.2MB/s]\n","[INFO|hub.py:587] 2022-04-06 14:39:12,296 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|hub.py:595] 2022-04-06 14:39:12,296 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|modeling_utils.py:1772] 2022-04-06 14:39:12,296 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:2049] 2022-04-06 14:39:13,748 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-04-06 14:39:13,748 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/06/2022 14:39:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-893f072bac1fb154.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:42<00:00,  3.12ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/06/2022 14:39:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-fbbe23a02a0043a8.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:03<00:00,  5.29s/ba]\n","04/06/2022 14:40:59 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpr29d5ac2\n","Downloading builder script: 6.46kB [00:00, 5.75MB/s]       \n","04/06/2022 14:40:59 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/06/2022 14:40:59 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/06/2022 14:41:00 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpvlu3fka5\n","Downloading extra modules: 11.3kB [00:00, 11.2MB/s]       \n","04/06/2022 14:41:00 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","04/06/2022 14:41:00 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 14:41:09,921 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 14:41:09,921 >>   Num examples = 132074\n","[INFO|trainer.py:1292] 2022-04-06 14:41:09,921 >>   Num Epochs = 3\n","[INFO|trainer.py:1293] 2022-04-06 14:41:09,921 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1294] 2022-04-06 14:41:09,921 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1295] 2022-04-06 14:41:09,922 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 14:41:09,922 >>   Total optimization steps = 24765\n","{'loss': 2.2635, 'learning_rate': 3.9192408641227546e-05, 'epoch': 0.06}\n","  2% 500/24765 [05:16<4:15:29,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 14:46:26,450 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 14:46:26,456 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:46:27,660 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:46:27,663 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:46:27,666 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 1.6454, 'learning_rate': 3.838481728245508e-05, 'epoch': 0.12}\n","  4% 1000/24765 [10:39<4:10:07,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 14:51:49,120 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 14:51:49,125 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:51:50,284 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:51:50,288 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:51:50,290 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.5367, 'learning_rate': 3.7577225923682624e-05, 'epoch': 0.18}\n","  6% 1500/24765 [16:02<4:05:39,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 14:57:12,361 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 14:57:12,366 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:57:13,569 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:57:13,572 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:57:13,575 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.446, 'learning_rate': 3.676963456491016e-05, 'epoch': 0.24}\n","  8% 2000/24765 [21:22<3:59:56,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:02:32,770 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:02:32,775 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:02:33,960 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:02:33,963 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:02:33,966 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.3455, 'learning_rate': 3.5962043206137695e-05, 'epoch': 0.3}\n"," 10% 2500/24765 [26:45<3:55:08,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:07:55,901 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:07:55,906 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:07:57,101 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:07:57,105 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:07:57,107 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.3424, 'learning_rate': 3.515445184736523e-05, 'epoch': 0.36}\n"," 12% 3000/24765 [32:09<3:49:07,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:13:19,139 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:13:19,144 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:13:20,355 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:13:20,359 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:13:20,362 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.2486, 'learning_rate': 3.4346860488592774e-05, 'epoch': 0.42}\n"," 14% 3500/24765 [37:29<3:44:14,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:18:39,402 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:18:39,407 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:18:40,621 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:18:40,625 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:18:40,628 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.246, 'learning_rate': 3.3539269129820316e-05, 'epoch': 0.48}\n"," 16% 4000/24765 [42:52<3:38:41,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:24:02,593 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:24:02,598 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:24:03,811 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:24:03,814 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:24:03,817 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.2273, 'learning_rate': 3.273167777104785e-05, 'epoch': 0.55}\n"," 18% 4500/24765 [48:15<3:32:57,  1.59it/s][INFO|trainer.py:2166] 2022-04-06 15:29:25,115 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:29:25,119 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:29:26,269 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:29:26,273 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:29:26,275 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.1886, 'learning_rate': 3.1924086412275394e-05, 'epoch': 0.61}\n"," 20% 5000/24765 [53:34<3:28:07,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:34:44,831 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:34:44,836 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:34:46,057 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:34:46,061 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:34:46,063 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.1984, 'learning_rate': 3.111649505350293e-05, 'epoch': 0.67}\n"," 22% 5500/24765 [58:58<3:23:39,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:40:08,008 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:40:08,013 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:40:09,230 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:40:09,233 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:40:09,236 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.1417, 'learning_rate': 3.030890369473047e-05, 'epoch': 0.73}\n"," 24% 6000/24765 [1:04:18<3:17:46,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:45:28,337 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:45:28,342 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:45:29,567 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:45:29,571 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:45:29,573 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.1537, 'learning_rate': 2.9501312335958005e-05, 'epoch': 0.79}\n"," 26% 6500/24765 [1:09:39<3:12:23,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:50:48,962 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 15:50:48,967 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:50:50,185 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:50:50,189 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:50:50,192 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.0985, 'learning_rate': 2.8693720977185547e-05, 'epoch': 0.85}\n"," 28% 7000/24765 [1:14:59<3:07:22,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 15:56:09,138 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 15:56:09,143 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 15:56:10,401 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 15:56:10,405 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 15:56:10,408 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.1008, 'learning_rate': 2.7886129618413086e-05, 'epoch': 0.91}\n"," 30% 7500/24765 [1:20:19<3:01:57,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:01:29,852 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:01:29,857 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:01:31,069 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:01:31,072 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:01:31,075 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 1.1008, 'learning_rate': 2.7078538259640622e-05, 'epoch': 0.97}\n"," 32% 8000/24765 [1:25:40<2:56:26,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:06:50,091 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:06:50,097 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:06:51,298 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:06:51,302 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:06:51,304 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.9531, 'learning_rate': 2.6270946900868165e-05, 'epoch': 1.03}\n"," 34% 8500/24765 [1:31:00<2:51:22,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:12:10,041 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:12:10,046 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:12:11,240 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:12:11,252 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:12:11,257 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.7791, 'learning_rate': 2.54633555420957e-05, 'epoch': 1.09}\n"," 36% 9000/24765 [1:36:23<2:45:57,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:17:33,102 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:17:33,107 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:17:34,405 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:17:34,409 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:17:34,411 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.818, 'learning_rate': 2.465576418332324e-05, 'epoch': 1.15}\n"," 38% 9500/24765 [1:41:43<2:40:44,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:22:53,919 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:22:53,924 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:22:55,214 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:22:55,218 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:22:55,221 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.8163, 'learning_rate': 2.384817282455078e-05, 'epoch': 1.21}\n"," 40% 10000/24765 [1:47:04<2:35:26,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:28:14,293 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:28:14,297 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:28:15,569 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:28:15,573 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:28:15,576 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.7929, 'learning_rate': 2.3040581465778318e-05, 'epoch': 1.27}\n"," 42% 10500/24765 [1:52:24<2:30:09,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:33:34,666 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:33:34,671 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:33:35,886 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:33:35,890 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:33:35,893 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.7715, 'learning_rate': 2.223299010700586e-05, 'epoch': 1.33}\n"," 44% 11000/24765 [1:57:44<2:24:50,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:38:54,943 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:38:54,948 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:38:56,264 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:38:56,267 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:38:56,270 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.7902, 'learning_rate': 2.1425398748233396e-05, 'epoch': 1.39}\n"," 46% 11500/24765 [2:03:05<2:19:43,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:44:15,075 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-11500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:44:15,081 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:44:16,304 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:44:16,308 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:44:16,311 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.8048, 'learning_rate': 2.0617807389460935e-05, 'epoch': 1.45}\n"," 48% 12000/24765 [2:08:25<2:14:25,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:49:35,225 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-12000\n","[INFO|configuration_utils.py:441] 2022-04-06 16:49:35,230 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:49:36,467 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:49:36,471 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:49:36,473 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.7852, 'learning_rate': 1.9810216030688474e-05, 'epoch': 1.51}\n"," 50% 12500/24765 [2:13:45<2:09:19,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 16:54:55,618 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-12500\n","[INFO|configuration_utils.py:441] 2022-04-06 16:54:55,624 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 16:54:56,869 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 16:54:56,872 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 16:54:56,874 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.7623, 'learning_rate': 1.9002624671916013e-05, 'epoch': 1.57}\n"," 52% 13000/24765 [2:19:05<2:04:06,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:00:15,646 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-13000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:00:15,651 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:00:16,959 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:00:16,963 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:00:16,966 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.7948, 'learning_rate': 1.8195033313143552e-05, 'epoch': 1.64}\n"," 55% 13500/24765 [2:24:26<1:58:31,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:05:36,103 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-13500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:05:36,108 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:05:37,370 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:05:37,374 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:05:37,377 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.7644, 'learning_rate': 1.7387441954371088e-05, 'epoch': 1.7}\n"," 57% 14000/24765 [2:29:46<1:53:29,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:10:56,483 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-14000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:10:56,488 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:10:57,688 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:10:57,692 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:10:57,695 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.7864, 'learning_rate': 1.6579850595598627e-05, 'epoch': 1.76}\n"," 59% 14500/24765 [2:35:06<1:48:16,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:16:16,805 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-14500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:16:16,810 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:16:18,119 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:16:18,124 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:16:18,126 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.7926, 'learning_rate': 1.5772259236826166e-05, 'epoch': 1.82}\n"," 61% 15000/24765 [2:40:27<1:42:59,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:21:37,080 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-15000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:21:37,086 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:21:38,356 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:21:38,359 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:21:38,362 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.7714, 'learning_rate': 1.4964667878053707e-05, 'epoch': 1.88}\n"," 63% 15500/24765 [2:45:47<1:37:30,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:26:57,764 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-15500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:26:57,769 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:26:59,047 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:26:59,051 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:26:59,054 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.7681, 'learning_rate': 1.4157076519281246e-05, 'epoch': 1.94}\n"," 65% 16000/24765 [2:51:08<1:32:20,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:32:18,237 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-16000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:32:18,242 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:32:19,584 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:32:19,588 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:32:19,590 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-16000/special_tokens_map.json\n","{'loss': 0.759, 'learning_rate': 1.3349485160508783e-05, 'epoch': 2.0}\n"," 67% 16500/24765 [2:56:29<1:27:05,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:37:39,175 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-16500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:37:39,180 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-16500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:37:40,536 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-16500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:37:40,539 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-16500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:37:40,542 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-16500/special_tokens_map.json\n","{'loss': 0.4854, 'learning_rate': 1.2541893801736322e-05, 'epoch': 2.06}\n"," 69% 17000/24765 [3:01:49<1:21:47,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:42:59,877 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-17000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:42:59,882 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-17000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:43:01,287 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-17000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:43:01,291 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-17000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:43:01,294 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-17000/special_tokens_map.json\n","{'loss': 0.4938, 'learning_rate': 1.1734302442963861e-05, 'epoch': 2.12}\n"," 71% 17500/24765 [3:07:10<1:16:33,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:48:20,779 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-17500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:48:20,784 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-17500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:48:22,134 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-17500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:48:22,137 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-17500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:48:22,153 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-17500/special_tokens_map.json\n","{'loss': 0.4774, 'learning_rate': 1.0926711084191399e-05, 'epoch': 2.18}\n"," 73% 18000/24765 [3:12:30<1:11:19,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:53:40,928 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-18000\n","[INFO|configuration_utils.py:441] 2022-04-06 17:53:40,933 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-18000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:53:42,355 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-18000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:53:42,358 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-18000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:53:42,833 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-18000/special_tokens_map.json\n","{'loss': 0.4812, 'learning_rate': 1.011911972541894e-05, 'epoch': 2.24}\n"," 75% 18500/24765 [3:17:52<1:06:02,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 17:59:02,478 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-18500\n","[INFO|configuration_utils.py:441] 2022-04-06 17:59:02,484 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-18500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 17:59:03,943 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-18500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 17:59:03,947 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-18500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 17:59:03,949 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-18500/special_tokens_map.json\n","{'loss': 0.4881, 'learning_rate': 9.311528366646477e-06, 'epoch': 2.3}\n"," 77% 19000/24765 [3:23:13<1:00:40,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:04:23,133 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-19000\n","[INFO|configuration_utils.py:441] 2022-04-06 18:04:23,138 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-19000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:04:24,422 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-19000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:04:24,426 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-19000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:04:24,428 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-19000/special_tokens_map.json\n","{'loss': 0.4687, 'learning_rate': 8.503937007874016e-06, 'epoch': 2.36}\n"," 79% 19500/24765 [3:28:35<55:27,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:09:45,950 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-19500\n","[INFO|configuration_utils.py:441] 2022-04-06 18:09:45,954 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-19500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:09:47,215 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-19500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:09:47,219 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-19500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:09:47,221 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-19500/special_tokens_map.json\n","{'loss': 0.4871, 'learning_rate': 7.696345649101555e-06, 'epoch': 2.42}\n"," 81% 20000/24765 [3:33:58<50:11,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:15:08,445 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-20000\n","[INFO|configuration_utils.py:441] 2022-04-06 18:15:08,450 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-20000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:15:09,702 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-20000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:15:09,706 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-20000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:15:09,709 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-20000/special_tokens_map.json\n","{'loss': 0.4789, 'learning_rate': 6.8887542903290935e-06, 'epoch': 2.48}\n"," 83% 20500/24765 [3:39:21<44:54,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:20:31,495 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-20500\n","[INFO|configuration_utils.py:441] 2022-04-06 18:20:31,500 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-20500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:20:32,749 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-20500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:20:32,752 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-20500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:20:32,755 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-20500/special_tokens_map.json\n","{'loss': 0.4811, 'learning_rate': 6.0811629315566326e-06, 'epoch': 2.54}\n"," 85% 21000/24765 [3:44:41<39:38,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:25:51,802 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-21000\n","[INFO|configuration_utils.py:441] 2022-04-06 18:25:51,807 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-21000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:25:52,987 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-21000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:25:52,991 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-21000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:25:52,994 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-21000/special_tokens_map.json\n","{'loss': 0.4613, 'learning_rate': 5.273571572784172e-06, 'epoch': 2.6}\n"," 87% 21500/24765 [3:50:02<34:23,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:31:12,048 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-21500\n","[INFO|configuration_utils.py:441] 2022-04-06 18:31:12,054 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-21500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:31:13,323 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-21500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:31:13,327 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-21500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:31:13,330 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-21500/special_tokens_map.json\n","{'loss': 0.4672, 'learning_rate': 4.46598021401171e-06, 'epoch': 2.67}\n"," 89% 22000/24765 [3:55:22<29:06,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:36:32,453 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-22000\n","[INFO|configuration_utils.py:441] 2022-04-06 18:36:32,458 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-22000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:36:33,639 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-22000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:36:33,643 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-22000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:36:33,646 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-22000/special_tokens_map.json\n","{'loss': 0.457, 'learning_rate': 3.6583888552392494e-06, 'epoch': 2.73}\n"," 91% 22500/24765 [4:00:42<23:52,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:41:52,587 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-22500\n","[INFO|configuration_utils.py:441] 2022-04-06 18:41:52,591 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-22500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:41:53,878 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-22500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:41:53,881 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-22500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:41:53,884 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-22500/special_tokens_map.json\n","{'loss': 0.4872, 'learning_rate': 2.8507974964667877e-06, 'epoch': 2.79}\n"," 93% 23000/24765 [4:06:03<18:36,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:47:13,012 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-23000\n","[INFO|configuration_utils.py:441] 2022-04-06 18:47:13,028 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-23000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:47:14,296 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-23000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:47:14,300 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-23000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:47:14,303 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-23000/special_tokens_map.json\n","{'loss': 0.4747, 'learning_rate': 2.0432061376943268e-06, 'epoch': 2.85}\n"," 95% 23500/24765 [4:11:23<13:19,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:52:33,587 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-23500\n","[INFO|configuration_utils.py:441] 2022-04-06 18:52:33,591 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-23500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:52:34,848 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-23500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:52:34,852 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-23500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:52:34,869 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-23500/special_tokens_map.json\n","{'loss': 0.4475, 'learning_rate': 1.2356147789218657e-06, 'epoch': 2.91}\n"," 97% 24000/24765 [4:16:44<08:03,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 18:57:54,144 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-24000\n","[INFO|configuration_utils.py:441] 2022-04-06 18:57:54,149 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-24000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 18:57:55,416 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-24000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 18:57:55,420 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-24000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 18:57:55,423 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-24000/special_tokens_map.json\n","{'loss': 0.4787, 'learning_rate': 4.280234201494044e-07, 'epoch': 2.97}\n"," 99% 24500/24765 [4:22:04<02:47,  1.58it/s][INFO|trainer.py:2166] 2022-04-06 19:03:14,345 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-24500\n","[INFO|configuration_utils.py:441] 2022-04-06 19:03:14,350 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-24500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 19:03:15,560 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-24500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 19:03:15,564 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-24500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 19:03:15,566 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/checkpoint-24500/special_tokens_map.json\n","100% 24765/24765 [4:24:58<00:00,  1.75it/s][INFO|trainer.py:1530] 2022-04-06 19:06:08,600 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 15898.6803, 'train_samples_per_second': 24.922, 'train_steps_per_second': 1.558, 'train_loss': 0.8613087146373174, 'epoch': 3.0}\n","100% 24765/24765 [4:24:58<00:00,  1.56it/s]\n","[INFO|trainer.py:2166] 2022-04-06 19:06:08,607 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug\n","[INFO|configuration_utils.py:441] 2022-04-06 19:06:08,612 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 19:06:09,916 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 19:06:09,920 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 19:06:09,922 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =     0.8613\n","  train_runtime            = 4:24:58.68\n","  train_samples            =     132074\n","  train_samples_per_second =     24.922\n","  train_steps_per_second   =      1.558\n","04/06/2022 19:06:09 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 19:06:09,996 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 19:06:10,005 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 19:06:10,005 >>   Num examples = 12199\n","[INFO|trainer.py:2421] 2022-04-06 19:06:10,005 >>   Batch size = 8\n","100% 1525/1525 [02:57<00:00,  8.77it/s]04/06/2022 19:09:19 - INFO - utils_qa - Post-processing 11873 example predictions split into 12199 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 37/11873 [00:00<00:32, 367.55it/s]\u001b[A\n","  1% 76/11873 [00:00<00:31, 377.66it/s]\u001b[A\n","  1% 123/11873 [00:00<00:28, 415.23it/s]\u001b[A\n","  1% 171/11873 [00:00<00:26, 437.42it/s]\u001b[A\n","  2% 215/11873 [00:00<00:26, 432.45it/s]\u001b[A\n","  2% 259/11873 [00:00<00:26, 432.44it/s]\u001b[A\n","  3% 306/11873 [00:00<00:26, 443.10it/s]\u001b[A\n","  3% 351/11873 [00:00<00:26, 437.63it/s]\u001b[A\n","  3% 399/11873 [00:00<00:25, 447.86it/s]\u001b[A\n","  4% 449/11873 [00:01<00:24, 461.05it/s]\u001b[A\n","  4% 496/11873 [00:01<00:24, 461.19it/s]\u001b[A\n","  5% 543/11873 [00:01<00:24, 459.49it/s]\u001b[A\n","  5% 589/11873 [00:01<00:24, 456.46it/s]\u001b[A\n","  5% 635/11873 [00:01<00:24, 454.32it/s]\u001b[A\n","  6% 681/11873 [00:01<00:25, 444.58it/s]\u001b[A\n","  6% 726/11873 [00:01<00:26, 427.30it/s]\u001b[A\n","  6% 769/11873 [00:01<00:26, 413.80it/s]\u001b[A\n","  7% 812/11873 [00:01<00:26, 418.01it/s]\u001b[A\n","  7% 860/11873 [00:01<00:25, 434.22it/s]\u001b[A\n","  8% 911/11873 [00:02<00:24, 456.17it/s]\u001b[A\n","  8% 961/11873 [00:02<00:23, 468.91it/s]\u001b[A\n","  8% 1009/11873 [00:02<00:23, 458.36it/s]\u001b[A\n","  9% 1055/11873 [00:02<00:25, 425.10it/s]\u001b[A\n","  9% 1099/11873 [00:02<00:27, 389.83it/s]\u001b[A\n"," 10% 1139/11873 [00:02<00:27, 383.94it/s]\u001b[A\n"," 10% 1178/11873 [00:02<00:28, 381.64it/s]\u001b[A\n"," 10% 1217/11873 [00:02<00:28, 376.01it/s]\u001b[A\n"," 11% 1255/11873 [00:02<00:29, 365.07it/s]\u001b[A\n"," 11% 1293/11873 [00:03<00:28, 366.61it/s]\u001b[A\n"," 11% 1330/11873 [00:03<00:28, 364.67it/s]\u001b[A\n"," 12% 1368/11873 [00:03<00:28, 368.79it/s]\u001b[A\n"," 12% 1405/11873 [00:03<00:28, 365.49it/s]\u001b[A\n"," 12% 1443/11873 [00:03<00:28, 367.44it/s]\u001b[A\n"," 12% 1480/11873 [00:03<00:28, 365.93it/s]\u001b[A\n"," 13% 1518/11873 [00:03<00:28, 368.47it/s]\u001b[A\n"," 13% 1555/11873 [00:03<00:28, 367.30it/s]\u001b[A\n"," 13% 1592/11873 [00:03<00:28, 364.99it/s]\u001b[A\n"," 14% 1629/11873 [00:03<00:28, 365.76it/s]\u001b[A\n"," 14% 1666/11873 [00:04<00:27, 365.87it/s]\u001b[A\n"," 14% 1703/11873 [00:04<00:27, 366.06it/s]\u001b[A\n"," 15% 1742/11873 [00:04<00:27, 370.64it/s]\u001b[A\n"," 15% 1780/11873 [00:04<00:27, 363.91it/s]\u001b[A\n"," 15% 1817/11873 [00:04<00:28, 354.25it/s]\u001b[A\n"," 16% 1853/11873 [00:04<00:28, 355.89it/s]\u001b[A\n"," 16% 1891/11873 [00:04<00:27, 362.32it/s]\u001b[A\n"," 16% 1928/11873 [00:04<00:28, 353.17it/s]\u001b[A\n"," 17% 1966/11873 [00:04<00:27, 359.05it/s]\u001b[A\n"," 17% 2002/11873 [00:05<00:27, 358.67it/s]\u001b[A\n"," 17% 2041/11873 [00:05<00:26, 365.49it/s]\u001b[A\n"," 18% 2078/11873 [00:05<00:26, 362.91it/s]\u001b[A\n"," 18% 2116/11873 [00:05<00:26, 366.95it/s]\u001b[A\n"," 18% 2153/11873 [00:05<00:26, 365.25it/s]\u001b[A\n"," 18% 2190/11873 [00:05<00:26, 366.20it/s]\u001b[A\n"," 19% 2228/11873 [00:05<00:26, 369.11it/s]\u001b[A\n"," 19% 2265/11873 [00:05<00:26, 368.59it/s]\u001b[A\n"," 19% 2302/11873 [00:05<00:26, 360.79it/s]\u001b[A\n"," 20% 2339/11873 [00:05<00:26, 360.16it/s]\u001b[A\n"," 20% 2376/11873 [00:06<00:27, 350.77it/s]\u001b[A\n"," 20% 2412/11873 [00:06<00:27, 342.27it/s]\u001b[A\n"," 21% 2447/11873 [00:06<00:28, 335.93it/s]\u001b[A\n"," 21% 2481/11873 [00:06<00:28, 333.19it/s]\u001b[A\n"," 21% 2515/11873 [00:06<00:28, 332.76it/s]\u001b[A\n"," 21% 2550/11873 [00:06<00:27, 337.49it/s]\u001b[A\n"," 22% 2585/11873 [00:06<00:27, 339.10it/s]\u001b[A\n"," 22% 2622/11873 [00:06<00:26, 346.44it/s]\u001b[A\n"," 22% 2659/11873 [00:06<00:26, 353.28it/s]\u001b[A\n"," 23% 2695/11873 [00:07<00:25, 353.08it/s]\u001b[A\n"," 23% 2731/11873 [00:07<00:26, 350.15it/s]\u001b[A\n"," 23% 2768/11873 [00:07<00:25, 355.82it/s]\u001b[A\n"," 24% 2805/11873 [00:07<00:25, 358.98it/s]\u001b[A\n"," 24% 2844/11873 [00:07<00:24, 366.90it/s]\u001b[A\n"," 24% 2882/11873 [00:07<00:24, 369.39it/s]\u001b[A\n"," 25% 2919/11873 [00:07<00:24, 369.15it/s]\u001b[A\n"," 25% 2958/11873 [00:07<00:23, 372.72it/s]\u001b[A\n"," 25% 2996/11873 [00:07<00:25, 350.21it/s]\u001b[A\n"," 26% 3032/11873 [00:07<00:26, 336.39it/s]\u001b[A\n"," 26% 3066/11873 [00:08<00:27, 324.87it/s]\u001b[A\n"," 26% 3099/11873 [00:08<00:27, 314.43it/s]\u001b[A\n"," 26% 3131/11873 [00:08<00:31, 273.91it/s]\u001b[A\n"," 27% 3160/11873 [00:08<00:33, 259.46it/s]\u001b[A\n"," 27% 3196/11873 [00:08<00:30, 283.64it/s]\u001b[A\n"," 27% 3234/11873 [00:08<00:28, 307.32it/s]\u001b[A\n"," 28% 3267/11873 [00:08<00:27, 310.29it/s]\u001b[A\n"," 28% 3299/11873 [00:08<00:36, 233.42it/s]\u001b[A\n"," 28% 3326/11873 [00:09<00:39, 215.53it/s]\u001b[A\n"," 28% 3350/11873 [00:09<00:39, 217.02it/s]\u001b[A\n"," 28% 3374/11873 [00:09<00:40, 209.24it/s]\u001b[A\n"," 29% 3410/11873 [00:09<00:34, 245.89it/s]\u001b[A\n"," 29% 3443/11873 [00:09<00:31, 266.02it/s]\u001b[A\n"," 29% 3476/11873 [00:09<00:29, 283.02it/s]\u001b[A\n"," 30% 3508/11873 [00:09<00:28, 292.17it/s]\u001b[A\n"," 30% 3541/11873 [00:09<00:27, 301.05it/s]\u001b[A\n"," 30% 3573/11873 [00:09<00:27, 305.10it/s]\u001b[A\n"," 30% 3607/11873 [00:10<00:26, 313.58it/s]\u001b[A\n"," 31% 3642/11873 [00:10<00:25, 321.58it/s]\u001b[A\n"," 31% 3676/11873 [00:10<00:25, 324.85it/s]\u001b[A\n"," 31% 3710/11873 [00:10<00:25, 325.82it/s]\u001b[A\n"," 32% 3743/11873 [00:10<00:24, 325.67it/s]\u001b[A\n"," 32% 3777/11873 [00:10<00:24, 328.21it/s]\u001b[A\n"," 32% 3813/11873 [00:10<00:23, 336.77it/s]\u001b[A\n"," 32% 3847/11873 [00:10<00:25, 316.15it/s]\u001b[A\n"," 33% 3883/11873 [00:10<00:24, 327.77it/s]\u001b[A\n"," 33% 3917/11873 [00:11<00:25, 315.41it/s]\u001b[A\n"," 33% 3951/11873 [00:11<00:24, 322.17it/s]\u001b[A\n"," 34% 3984/11873 [00:11<00:24, 320.12it/s]\u001b[A\n"," 34% 4023/11873 [00:11<00:23, 338.20it/s]\u001b[A\n"," 34% 4062/11873 [00:11<00:22, 351.27it/s]\u001b[A\n"," 35% 4101/11873 [00:11<00:21, 360.97it/s]\u001b[A\n"," 35% 4138/11873 [00:11<00:21, 361.09it/s]\u001b[A\n"," 35% 4175/11873 [00:11<00:22, 337.36it/s]\u001b[A\n"," 35% 4211/11873 [00:11<00:22, 342.84it/s]\u001b[A\n"," 36% 4248/11873 [00:11<00:21, 348.05it/s]\u001b[A\n"," 36% 4284/11873 [00:12<00:22, 342.81it/s]\u001b[A\n"," 36% 4323/11873 [00:12<00:21, 354.41it/s]\u001b[A\n"," 37% 4361/11873 [00:12<00:20, 359.84it/s]\u001b[A\n"," 37% 4399/11873 [00:12<00:20, 362.22it/s]\u001b[A\n"," 37% 4436/11873 [00:12<00:25, 293.57it/s]\u001b[A\n"," 38% 4474/11873 [00:12<00:23, 314.63it/s]\u001b[A\n"," 38% 4510/11873 [00:12<00:22, 325.09it/s]\u001b[A\n"," 38% 4547/11873 [00:12<00:21, 336.99it/s]\u001b[A\n"," 39% 4585/11873 [00:12<00:20, 347.80it/s]\u001b[A\n"," 39% 4621/11873 [00:13<00:21, 343.32it/s]\u001b[A\n"," 39% 4657/11873 [00:13<00:20, 347.86it/s]\u001b[A\n"," 40% 4694/11873 [00:13<00:20, 353.11it/s]\u001b[A\n"," 40% 4731/11873 [00:13<00:20, 355.91it/s]\u001b[A\n"," 40% 4769/11873 [00:13<00:19, 360.17it/s]\u001b[A\n"," 40% 4806/11873 [00:13<00:20, 351.80it/s]\u001b[A\n"," 41% 4842/11873 [00:13<00:20, 349.54it/s]\u001b[A\n"," 41% 4878/11873 [00:13<00:20, 349.48it/s]\u001b[A\n"," 41% 4914/11873 [00:13<00:20, 347.25it/s]\u001b[A\n"," 42% 4950/11873 [00:14<00:19, 348.94it/s]\u001b[A\n"," 42% 4985/11873 [00:14<00:20, 340.12it/s]\u001b[A\n"," 42% 5020/11873 [00:14<00:20, 334.66it/s]\u001b[A\n"," 43% 5054/11873 [00:14<00:20, 333.11it/s]\u001b[A\n"," 43% 5093/11873 [00:14<00:19, 347.21it/s]\u001b[A\n"," 43% 5130/11873 [00:14<00:19, 353.75it/s]\u001b[A\n"," 44% 5166/11873 [00:14<00:18, 355.37it/s]\u001b[A\n"," 44% 5203/11873 [00:14<00:18, 358.91it/s]\u001b[A\n"," 44% 5241/11873 [00:14<00:18, 363.12it/s]\u001b[A\n"," 44% 5278/11873 [00:14<00:19, 332.81it/s]\u001b[A\n"," 45% 5312/11873 [00:15<00:19, 331.60it/s]\u001b[A\n"," 45% 5349/11873 [00:15<00:19, 342.38it/s]\u001b[A\n"," 45% 5387/11873 [00:15<00:18, 351.67it/s]\u001b[A\n"," 46% 5424/11873 [00:15<00:18, 355.78it/s]\u001b[A\n"," 46% 5462/11873 [00:15<00:17, 358.24it/s]\u001b[A\n"," 46% 5498/11873 [00:15<00:18, 346.14it/s]\u001b[A\n"," 47% 5533/11873 [00:15<00:18, 339.64it/s]\u001b[A\n"," 47% 5568/11873 [00:15<00:18, 333.01it/s]\u001b[A\n"," 47% 5602/11873 [00:15<00:19, 326.55it/s]\u001b[A\n"," 47% 5635/11873 [00:16<00:19, 326.62it/s]\u001b[A\n"," 48% 5672/11873 [00:16<00:18, 338.99it/s]\u001b[A\n"," 48% 5709/11873 [00:16<00:17, 347.36it/s]\u001b[A\n"," 48% 5747/11873 [00:16<00:17, 355.19it/s]\u001b[A\n"," 49% 5783/11873 [00:16<00:17, 356.06it/s]\u001b[A\n"," 49% 5822/11873 [00:16<00:16, 364.98it/s]\u001b[A\n"," 49% 5860/11873 [00:16<00:16, 368.31it/s]\u001b[A\n"," 50% 5898/11873 [00:16<00:16, 369.21it/s]\u001b[A\n"," 50% 5936/11873 [00:16<00:15, 372.16it/s]\u001b[A\n"," 50% 5974/11873 [00:16<00:15, 373.09it/s]\u001b[A\n"," 51% 6012/11873 [00:17<00:16, 364.01it/s]\u001b[A\n"," 51% 6049/11873 [00:17<00:15, 364.81it/s]\u001b[A\n"," 51% 6086/11873 [00:17<00:16, 360.19it/s]\u001b[A\n"," 52% 6123/11873 [00:17<00:16, 359.25it/s]\u001b[A\n"," 52% 6159/11873 [00:17<00:15, 357.71it/s]\u001b[A\n"," 52% 6195/11873 [00:17<00:15, 355.41it/s]\u001b[A\n"," 52% 6231/11873 [00:17<00:16, 343.63it/s]\u001b[A\n"," 53% 6266/11873 [00:17<00:16, 340.71it/s]\u001b[A\n"," 53% 6301/11873 [00:17<00:16, 337.56it/s]\u001b[A\n"," 53% 6335/11873 [00:18<00:16, 336.17it/s]\u001b[A\n"," 54% 6372/11873 [00:18<00:15, 345.50it/s]\u001b[A\n"," 54% 6409/11873 [00:18<00:15, 351.98it/s]\u001b[A\n"," 54% 6446/11873 [00:18<00:15, 355.18it/s]\u001b[A\n"," 55% 6484/11873 [00:18<00:14, 361.06it/s]\u001b[A\n"," 55% 6521/11873 [00:18<00:14, 360.93it/s]\u001b[A\n"," 55% 6558/11873 [00:18<00:14, 358.00it/s]\u001b[A\n"," 56% 6594/11873 [00:18<00:15, 347.00it/s]\u001b[A\n"," 56% 6629/11873 [00:18<00:15, 336.20it/s]\u001b[A\n"," 56% 6663/11873 [00:18<00:15, 331.10it/s]\u001b[A\n"," 56% 6697/11873 [00:19<00:15, 325.61it/s]\u001b[A\n"," 57% 6730/11873 [00:19<00:16, 303.43it/s]\u001b[A\n"," 57% 6768/11873 [00:19<00:15, 323.57it/s]\u001b[A\n"," 57% 6804/11873 [00:19<00:15, 332.42it/s]\u001b[A\n"," 58% 6843/11873 [00:19<00:14, 348.14it/s]\u001b[A\n"," 58% 6879/11873 [00:19<00:14, 349.49it/s]\u001b[A\n"," 58% 6915/11873 [00:19<00:14, 333.82it/s]\u001b[A\n"," 59% 6949/11873 [00:19<00:14, 334.35it/s]\u001b[A\n"," 59% 6983/11873 [00:19<00:14, 332.09it/s]\u001b[A\n"," 59% 7017/11873 [00:20<00:14, 328.33it/s]\u001b[A\n"," 59% 7054/11873 [00:20<00:14, 339.08it/s]\u001b[A\n"," 60% 7092/11873 [00:20<00:13, 348.30it/s]\u001b[A\n"," 60% 7130/11873 [00:20<00:13, 355.41it/s]\u001b[A\n"," 60% 7167/11873 [00:20<00:13, 358.98it/s]\u001b[A\n"," 61% 7204/11873 [00:20<00:12, 361.73it/s]\u001b[A\n"," 61% 7242/11873 [00:20<00:12, 366.93it/s]\u001b[A\n"," 61% 7279/11873 [00:20<00:12, 366.38it/s]\u001b[A\n"," 62% 7317/11873 [00:20<00:12, 370.01it/s]\u001b[A\n"," 62% 7355/11873 [00:20<00:12, 370.24it/s]\u001b[A\n"," 62% 7393/11873 [00:21<00:12, 362.64it/s]\u001b[A\n"," 63% 7430/11873 [00:21<00:12, 345.99it/s]\u001b[A\n"," 63% 7468/11873 [00:21<00:12, 354.03it/s]\u001b[A\n"," 63% 7506/11873 [00:21<00:12, 361.42it/s]\u001b[A\n"," 64% 7543/11873 [00:21<00:12, 360.39it/s]\u001b[A\n"," 64% 7580/11873 [00:21<00:11, 362.90it/s]\u001b[A\n"," 64% 7617/11873 [00:21<00:12, 339.64it/s]\u001b[A\n"," 64% 7652/11873 [00:21<00:12, 328.48it/s]\u001b[A\n"," 65% 7686/11873 [00:21<00:12, 326.07it/s]\u001b[A\n"," 65% 7719/11873 [00:22<00:13, 300.12it/s]\u001b[A\n"," 65% 7753/11873 [00:22<00:13, 308.87it/s]\u001b[A\n"," 66% 7790/11873 [00:22<00:12, 324.43it/s]\u001b[A\n"," 66% 7825/11873 [00:22<00:12, 331.63it/s]\u001b[A\n"," 66% 7862/11873 [00:22<00:11, 340.39it/s]\u001b[A\n"," 67% 7897/11873 [00:22<00:12, 326.49it/s]\u001b[A\n"," 67% 7934/11873 [00:22<00:11, 336.36it/s]\u001b[A\n"," 67% 7970/11873 [00:22<00:11, 342.06it/s]\u001b[A\n"," 67% 8007/11873 [00:22<00:11, 349.21it/s]\u001b[A\n"," 68% 8043/11873 [00:22<00:10, 348.23it/s]\u001b[A\n"," 68% 8080/11873 [00:23<00:10, 351.79it/s]\u001b[A\n"," 68% 8117/11873 [00:23<00:10, 355.30it/s]\u001b[A\n"," 69% 8153/11873 [00:23<00:10, 354.09it/s]\u001b[A\n"," 69% 8189/11873 [00:23<00:10, 354.00it/s]\u001b[A\n"," 69% 8225/11873 [00:23<00:10, 349.68it/s]\u001b[A\n"," 70% 8264/11873 [00:23<00:10, 358.68it/s]\u001b[A\n"," 70% 8302/11873 [00:23<00:09, 362.41it/s]\u001b[A\n"," 70% 8340/11873 [00:23<00:09, 364.72it/s]\u001b[A\n"," 71% 8377/11873 [00:23<00:09, 366.22it/s]\u001b[A\n"," 71% 8414/11873 [00:24<00:09, 361.42it/s]\u001b[A\n"," 71% 8451/11873 [00:24<00:09, 359.59it/s]\u001b[A\n"," 71% 8488/11873 [00:24<00:09, 361.72it/s]\u001b[A\n"," 72% 8527/11873 [00:24<00:09, 367.55it/s]\u001b[A\n"," 72% 8565/11873 [00:24<00:08, 369.53it/s]\u001b[A\n"," 72% 8603/11873 [00:24<00:08, 372.09it/s]\u001b[A\n"," 73% 8641/11873 [00:24<00:08, 369.77it/s]\u001b[A\n"," 73% 8678/11873 [00:24<00:08, 363.20it/s]\u001b[A\n"," 73% 8715/11873 [00:24<00:08, 360.70it/s]\u001b[A\n"," 74% 8752/11873 [00:24<00:08, 359.60it/s]\u001b[A\n"," 74% 8788/11873 [00:25<00:08, 356.10it/s]\u001b[A\n"," 74% 8824/11873 [00:25<00:08, 356.15it/s]\u001b[A\n"," 75% 8860/11873 [00:25<00:08, 353.74it/s]\u001b[A\n"," 75% 8898/11873 [00:25<00:08, 360.95it/s]\u001b[A\n"," 75% 8936/11873 [00:25<00:08, 364.09it/s]\u001b[A\n"," 76% 8973/11873 [00:25<00:07, 365.82it/s]\u001b[A\n"," 76% 9012/11873 [00:25<00:07, 371.90it/s]\u001b[A\n"," 76% 9050/11873 [00:25<00:07, 370.45it/s]\u001b[A\n"," 77% 9088/11873 [00:25<00:07, 369.43it/s]\u001b[A\n"," 77% 9126/11873 [00:25<00:07, 371.13it/s]\u001b[A\n"," 77% 9164/11873 [00:26<00:07, 372.62it/s]\u001b[A\n"," 78% 9202/11873 [00:26<00:07, 371.66it/s]\u001b[A\n"," 78% 9240/11873 [00:26<00:07, 372.24it/s]\u001b[A\n"," 78% 9278/11873 [00:26<00:06, 374.27it/s]\u001b[A\n"," 78% 9316/11873 [00:26<00:06, 369.46it/s]\u001b[A\n"," 79% 9354/11873 [00:26<00:06, 370.86it/s]\u001b[A\n"," 79% 9392/11873 [00:26<00:06, 371.64it/s]\u001b[A\n"," 79% 9430/11873 [00:26<00:06, 370.93it/s]\u001b[A\n"," 80% 9468/11873 [00:26<00:06, 370.50it/s]\u001b[A\n"," 80% 9506/11873 [00:26<00:06, 372.25it/s]\u001b[A\n"," 80% 9544/11873 [00:27<00:06, 373.26it/s]\u001b[A\n"," 81% 9582/11873 [00:27<00:06, 373.52it/s]\u001b[A\n"," 81% 9620/11873 [00:27<00:06, 373.33it/s]\u001b[A\n"," 81% 9658/11873 [00:27<00:05, 373.78it/s]\u001b[A\n"," 82% 9696/11873 [00:27<00:05, 370.93it/s]\u001b[A\n"," 82% 9735/11873 [00:27<00:05, 374.31it/s]\u001b[A\n"," 82% 9773/11873 [00:27<00:05, 371.74it/s]\u001b[A\n"," 83% 9811/11873 [00:27<00:05, 358.85it/s]\u001b[A\n"," 83% 9847/11873 [00:27<00:05, 353.14it/s]\u001b[A\n"," 83% 9883/11873 [00:28<00:05, 347.58it/s]\u001b[A\n"," 84% 9918/11873 [00:28<00:05, 343.97it/s]\u001b[A\n"," 84% 9953/11873 [00:28<00:05, 344.74it/s]\u001b[A\n"," 84% 9988/11873 [00:28<00:05, 337.30it/s]\u001b[A\n"," 84% 10022/11873 [00:28<00:05, 331.26it/s]\u001b[A\n"," 85% 10056/11873 [00:28<00:05, 330.57it/s]\u001b[A\n"," 85% 10092/11873 [00:28<00:05, 338.86it/s]\u001b[A\n"," 85% 10126/11873 [00:28<00:05, 333.70it/s]\u001b[A\n"," 86% 10160/11873 [00:28<00:05, 333.41it/s]\u001b[A\n"," 86% 10194/11873 [00:28<00:05, 329.19it/s]\u001b[A\n"," 86% 10227/11873 [00:29<00:05, 327.35it/s]\u001b[A\n"," 86% 10262/11873 [00:29<00:04, 333.71it/s]\u001b[A\n"," 87% 10296/11873 [00:29<00:04, 323.07it/s]\u001b[A\n"," 87% 10330/11873 [00:29<00:04, 326.05it/s]\u001b[A\n"," 87% 10363/11873 [00:29<00:04, 327.10it/s]\u001b[A\n"," 88% 10396/11873 [00:29<00:04, 326.74it/s]\u001b[A\n"," 88% 10429/11873 [00:29<00:04, 305.06it/s]\u001b[A\n"," 88% 10463/11873 [00:29<00:04, 313.29it/s]\u001b[A\n"," 88% 10495/11873 [00:29<00:04, 313.66it/s]\u001b[A\n"," 89% 10528/11873 [00:30<00:04, 317.98it/s]\u001b[A\n"," 89% 10560/11873 [00:30<00:04, 297.36it/s]\u001b[A\n"," 89% 10594/11873 [00:30<00:04, 307.71it/s]\u001b[A\n"," 90% 10631/11873 [00:30<00:03, 323.36it/s]\u001b[A\n"," 90% 10667/11873 [00:30<00:03, 332.72it/s]\u001b[A\n"," 90% 10705/11873 [00:30<00:03, 344.46it/s]\u001b[A\n"," 90% 10743/11873 [00:30<00:03, 353.72it/s]\u001b[A\n"," 91% 10779/11873 [00:30<00:03, 351.81it/s]\u001b[A\n"," 91% 10816/11873 [00:30<00:02, 354.69it/s]\u001b[A\n"," 91% 10852/11873 [00:30<00:03, 329.02it/s]\u001b[A\n"," 92% 10888/11873 [00:31<00:02, 335.84it/s]\u001b[A\n"," 92% 10923/11873 [00:31<00:02, 338.01it/s]\u001b[A\n"," 92% 10959/11873 [00:31<00:02, 342.19it/s]\u001b[A\n"," 93% 10996/11873 [00:31<00:02, 350.21it/s]\u001b[A\n"," 93% 11033/11873 [00:31<00:02, 355.66it/s]\u001b[A\n"," 93% 11070/11873 [00:31<00:02, 357.54it/s]\u001b[A\n"," 94% 11108/11873 [00:31<00:02, 362.00it/s]\u001b[A\n"," 94% 11145/11873 [00:31<00:02, 353.99it/s]\u001b[A\n"," 94% 11181/11873 [00:31<00:01, 347.47it/s]\u001b[A\n"," 94% 11216/11873 [00:32<00:01, 342.58it/s]\u001b[A\n"," 95% 11251/11873 [00:32<00:01, 337.83it/s]\u001b[A\n"," 95% 11289/11873 [00:32<00:01, 347.90it/s]\u001b[A\n"," 95% 11324/11873 [00:32<00:01, 347.72it/s]\u001b[A\n"," 96% 11362/11873 [00:32<00:01, 355.45it/s]\u001b[A\n"," 96% 11400/11873 [00:32<00:01, 360.16it/s]\u001b[A\n"," 96% 11437/11873 [00:32<00:01, 356.49it/s]\u001b[A\n"," 97% 11474/11873 [00:32<00:01, 358.89it/s]\u001b[A\n"," 97% 11511/11873 [00:32<00:01, 360.39it/s]\u001b[A\n"," 97% 11548/11873 [00:32<00:00, 362.12it/s]\u001b[A\n"," 98% 11585/11873 [00:33<00:00, 363.62it/s]\u001b[A\n"," 98% 11622/11873 [00:33<00:00, 362.60it/s]\u001b[A\n"," 98% 11659/11873 [00:33<00:00, 361.31it/s]\u001b[A\n"," 99% 11696/11873 [00:33<00:00, 360.26it/s]\u001b[A\n"," 99% 11734/11873 [00:33<00:00, 364.40it/s]\u001b[A\n"," 99% 11772/11873 [00:33<00:00, 368.45it/s]\u001b[A\n"," 99% 11809/11873 [00:33<00:00, 367.90it/s]\u001b[A\n","100% 11873/11873 [00:33<00:00, 351.10it/s]\n","04/06/2022 19:09:53 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/eval_predictions.json.\n","04/06/2022 19:09:53 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/eval_nbest_predictions.json.\n","04/06/2022 19:09:56 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-possible-aug/eval_null_odds.json.\n","04/06/2022 19:09:59 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1525/1525 [03:49<00:00,  6.66it/s]\n","***** eval metrics *****\n","  epoch                  =     3.0\n","  eval_HasAns_exact      = 71.9636\n","  eval_HasAns_f1         = 78.7745\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 68.9151\n","  eval_NoAns_f1          = 68.9151\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 70.4371\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 73.8377\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 70.4371\n","  eval_f1                = 73.8377\n","  eval_samples           =   12199\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 19:09:59,567 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_back_trans_possib_aug', 'type': 'sichenzhong/squad_v2_back_trans_possib_aug', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path albert-base-v2 \\\n","  --dataset_name sichenzhong/squad_v2_back_trans_possib_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 512 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-possible-aug"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oBLhvX5ILHHT","executionInfo":{"status":"ok","timestamp":1648840131164,"user_tz":240,"elapsed":22751976,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"423b88a7-489b-4e83-e9b4-21bf0cf86128"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["04/01/2022 12:49:39 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/01/2022 12:49:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Apr01_12-49-39_178ef9e0203c,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/01/2022 12:49:41 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef\n","04/01/2022 12:49:41 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/01/2022 12:49:41 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","04/01/2022 12:49:41 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","04/01/2022 12:49:41 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","100% 2/2 [00:00<00:00, 510.13it/s]\n","[INFO|configuration_utils.py:653] 2022-04-01 12:49:41,678 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-04-01 12:49:41,680 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-01 12:49:42,027 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:653] 2022-04-01 12:49:42,374 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-04-01 12:49:42,376 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_utils_base.py:1778] 2022-04-01 12:49:44,468 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1778] 2022-04-01 12:49:44,468 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-04-01 12:49:44,468 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-01 12:49:44,468 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-01 12:49:44,468 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:653] 2022-04-01 12:49:44,816 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:689] 2022-04-01 12:49:44,817 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|modeling_utils.py:1771] 2022-04-01 12:49:45,262 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[WARNING|modeling_utils.py:2049] 2022-04-01 12:49:45,404 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']\n","- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-04-01 12:49:45,404 >> Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","04/01/2022 12:49:45 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-a2bb98440a2c3b83.arrow\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/01/2022 12:49:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-944e5416f9406523.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:26<00:00,  7.24s/ba]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1289] 2022-04-01 12:51:16,811 >> ***** Running training *****\n","[INFO|trainer.py:1290] 2022-04-01 12:51:16,811 >>   Num examples = 130546\n","[INFO|trainer.py:1291] 2022-04-01 12:51:16,811 >>   Num Epochs = 3\n","[INFO|trainer.py:1292] 2022-04-01 12:51:16,811 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1293] 2022-04-01 12:51:16,811 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1294] 2022-04-01 12:51:16,811 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1295] 2022-04-01 12:51:16,811 >>   Total optimization steps = 24480\n","{'loss': 1.7647, 'learning_rate': 1.9591503267973857e-05, 'epoch': 0.06}\n","  2% 500/24480 [07:34<6:03:13,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 12:58:51,013 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:440] 2022-04-01 12:58:51,015 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 12:58:51,099 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 12:58:51,099 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 12:58:51,099 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.326, 'learning_rate': 1.9183006535947716e-05, 'epoch': 0.12}\n","  4% 1000/24480 [15:08<5:56:09,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 13:06:25,725 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:440] 2022-04-01 13:06:25,726 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 13:06:25,814 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 13:06:25,815 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 13:06:25,815 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.2167, 'learning_rate': 1.877450980392157e-05, 'epoch': 0.18}\n","  6% 1500/24480 [22:43<5:47:55,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 13:14:00,763 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:440] 2022-04-01 13:14:00,764 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 13:14:00,849 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 13:14:00,850 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 13:14:00,850 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.1433, 'learning_rate': 1.8366013071895427e-05, 'epoch': 0.25}\n","  8% 2000/24480 [30:18<5:40:27,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 13:21:35,773 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:440] 2022-04-01 13:21:35,774 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 13:21:35,859 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 13:21:35,860 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 13:21:35,860 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.1063, 'learning_rate': 1.7957516339869282e-05, 'epoch': 0.31}\n"," 10% 2500/24480 [37:54<5:33:35,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 13:29:11,082 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:440] 2022-04-01 13:29:11,083 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 13:29:11,165 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 13:29:11,165 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 13:29:11,166 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.0946, 'learning_rate': 1.7549019607843138e-05, 'epoch': 0.37}\n"," 12% 3000/24480 [45:29<5:25:46,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 13:36:46,393 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:440] 2022-04-01 13:36:46,394 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 13:36:46,476 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 13:36:46,476 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 13:36:46,477 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.0551, 'learning_rate': 1.7140522875816993e-05, 'epoch': 0.43}\n"," 14% 3500/24480 [53:05<5:18:21,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 13:44:21,996 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:440] 2022-04-01 13:44:21,997 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 13:44:22,082 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 13:44:22,082 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 13:44:22,083 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.0571, 'learning_rate': 1.6732026143790852e-05, 'epoch': 0.49}\n"," 16% 4000/24480 [1:00:40<5:11:23,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 13:51:57,598 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:440] 2022-04-01 13:51:57,600 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 13:51:57,686 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 13:51:57,687 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 13:51:57,687 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.0326, 'learning_rate': 1.6323529411764708e-05, 'epoch': 0.55}\n"," 18% 4500/24480 [1:08:16<5:03:02,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 13:59:33,536 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:440] 2022-04-01 13:59:33,537 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 13:59:33,621 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 13:59:33,622 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 13:59:33,622 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.9944, 'learning_rate': 1.5915032679738563e-05, 'epoch': 0.61}\n"," 20% 5000/24480 [1:15:52<4:55:37,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 14:07:09,507 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:440] 2022-04-01 14:07:09,508 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 14:07:09,591 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 14:07:09,591 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 14:07:09,592 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.0182, 'learning_rate': 1.550653594771242e-05, 'epoch': 0.67}\n"," 22% 5500/24480 [1:23:28<4:48:06,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 14:14:45,614 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:440] 2022-04-01 14:14:45,615 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 14:14:45,700 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 14:14:45,701 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 14:14:45,701 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.0136, 'learning_rate': 1.5098039215686276e-05, 'epoch': 0.74}\n"," 25% 6000/24480 [1:31:05<4:40:59,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 14:22:21,869 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:440] 2022-04-01 14:22:21,871 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 14:22:21,953 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 14:22:21,954 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 14:22:21,954 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.977, 'learning_rate': 1.4689542483660133e-05, 'epoch': 0.8}\n"," 27% 6500/24480 [1:38:41<4:32:46,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 14:29:57,961 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:440] 2022-04-01 14:29:57,963 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 14:29:58,049 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 14:29:58,050 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 14:29:58,050 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.9724, 'learning_rate': 1.4281045751633989e-05, 'epoch': 0.86}\n"," 29% 7000/24480 [1:46:17<4:25:06,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 14:37:33,933 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:440] 2022-04-01 14:37:33,934 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 14:37:34,021 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 14:37:34,022 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 14:37:34,022 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.9635, 'learning_rate': 1.3872549019607844e-05, 'epoch': 0.92}\n"," 31% 7500/24480 [1:53:53<4:17:37,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 14:45:10,049 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:440] 2022-04-01 14:45:10,050 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 14:45:10,135 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 14:45:10,135 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 14:45:10,136 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.9601, 'learning_rate': 1.3464052287581701e-05, 'epoch': 0.98}\n"," 33% 8000/24480 [2:01:29<4:10:20,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 14:52:46,194 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:440] 2022-04-01 14:52:46,195 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 14:52:46,282 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 14:52:46,283 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 14:52:46,283 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.7953, 'learning_rate': 1.3055555555555557e-05, 'epoch': 1.04}\n"," 35% 8500/24480 [2:09:04<4:02:49,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 15:00:21,522 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:440] 2022-04-01 15:00:21,523 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 15:00:21,606 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 15:00:21,607 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 15:00:21,607 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.7508, 'learning_rate': 1.2647058823529412e-05, 'epoch': 1.1}\n"," 37% 9000/24480 [2:16:40<3:54:44,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 15:07:57,576 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:440] 2022-04-01 15:07:57,577 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 15:07:57,663 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 15:07:57,663 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 15:07:57,664 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.7641, 'learning_rate': 1.223856209150327e-05, 'epoch': 1.16}\n"," 39% 9500/24480 [2:24:16<3:48:06,  1.09it/s][INFO|trainer.py:2165] 2022-04-01 15:15:33,689 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:440] 2022-04-01 15:15:33,690 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 15:15:33,773 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 15:15:33,774 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 15:15:33,774 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.7322, 'learning_rate': 1.1830065359477125e-05, 'epoch': 1.23}\n"," 41% 10000/24480 [2:31:52<3:39:57,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 15:23:09,787 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:440] 2022-04-01 15:23:09,789 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 15:23:09,876 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 15:23:09,876 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 15:23:09,877 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.7656, 'learning_rate': 1.142156862745098e-05, 'epoch': 1.29}\n"," 43% 10500/24480 [2:39:29<3:32:39,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 15:30:45,930 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:440] 2022-04-01 15:30:45,931 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 15:30:46,017 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 15:30:46,018 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 15:30:46,018 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.7457, 'learning_rate': 1.1013071895424838e-05, 'epoch': 1.35}\n"," 45% 11000/24480 [2:47:05<3:24:43,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 15:38:21,913 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11000\n","[INFO|configuration_utils.py:440] 2022-04-01 15:38:21,914 >> Configuration saved in /tmp/debug_squad/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 15:38:22,001 >> Model weights saved in /tmp/debug_squad/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 15:38:22,002 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 15:38:22,002 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.7356, 'learning_rate': 1.0604575163398693e-05, 'epoch': 1.41}\n"," 47% 11500/24480 [2:54:41<3:17:17,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 15:45:58,114 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11500\n","[INFO|configuration_utils.py:440] 2022-04-01 15:45:58,115 >> Configuration saved in /tmp/debug_squad/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 15:45:58,199 >> Model weights saved in /tmp/debug_squad/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 15:45:58,200 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 15:45:58,200 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.746, 'learning_rate': 1.0196078431372549e-05, 'epoch': 1.47}\n"," 49% 12000/24480 [3:02:17<3:09:13,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 15:53:34,071 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12000\n","[INFO|configuration_utils.py:440] 2022-04-01 15:53:34,072 >> Configuration saved in /tmp/debug_squad/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 15:53:34,154 >> Model weights saved in /tmp/debug_squad/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 15:53:34,154 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 15:53:34,155 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.7417, 'learning_rate': 9.787581699346406e-06, 'epoch': 1.53}\n"," 51% 12500/24480 [3:09:53<3:01:56,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 16:01:10,099 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12500\n","[INFO|configuration_utils.py:440] 2022-04-01 16:01:10,101 >> Configuration saved in /tmp/debug_squad/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 16:01:10,184 >> Model weights saved in /tmp/debug_squad/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 16:01:10,185 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 16:01:10,185 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.744, 'learning_rate': 9.379084967320261e-06, 'epoch': 1.59}\n"," 53% 13000/24480 [3:17:29<2:54:11,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 16:08:45,847 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13000\n","[INFO|configuration_utils.py:440] 2022-04-01 16:08:45,848 >> Configuration saved in /tmp/debug_squad/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 16:08:45,929 >> Model weights saved in /tmp/debug_squad/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 16:08:45,929 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 16:08:45,929 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.7563, 'learning_rate': 8.970588235294119e-06, 'epoch': 1.65}\n"," 55% 13500/24480 [3:25:04<2:47:02,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 16:16:21,807 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13500\n","[INFO|configuration_utils.py:440] 2022-04-01 16:16:21,808 >> Configuration saved in /tmp/debug_squad/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 16:16:21,892 >> Model weights saved in /tmp/debug_squad/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 16:16:21,893 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 16:16:21,893 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.7279, 'learning_rate': 8.562091503267974e-06, 'epoch': 1.72}\n"," 57% 14000/24480 [3:32:40<2:38:49,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 16:23:57,772 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14000\n","[INFO|configuration_utils.py:440] 2022-04-01 16:23:57,773 >> Configuration saved in /tmp/debug_squad/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 16:23:57,855 >> Model weights saved in /tmp/debug_squad/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 16:23:57,855 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 16:23:57,856 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.7136, 'learning_rate': 8.153594771241831e-06, 'epoch': 1.78}\n"," 59% 14500/24480 [3:40:16<2:31:25,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 16:31:33,648 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14500\n","[INFO|configuration_utils.py:440] 2022-04-01 16:31:33,650 >> Configuration saved in /tmp/debug_squad/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 16:31:33,731 >> Model weights saved in /tmp/debug_squad/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 16:31:33,732 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 16:31:33,733 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.7108, 'learning_rate': 7.745098039215687e-06, 'epoch': 1.84}\n"," 61% 15000/24480 [3:47:52<2:24:08,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 16:39:09,405 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15000\n","[INFO|configuration_utils.py:440] 2022-04-01 16:39:09,406 >> Configuration saved in /tmp/debug_squad/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 16:39:09,488 >> Model weights saved in /tmp/debug_squad/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 16:39:09,489 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 16:39:09,489 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.7169, 'learning_rate': 7.336601307189542e-06, 'epoch': 1.9}\n"," 63% 15500/24480 [3:55:28<2:16:27,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 16:46:45,256 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15500\n","[INFO|configuration_utils.py:440] 2022-04-01 16:46:45,257 >> Configuration saved in /tmp/debug_squad/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 16:46:45,338 >> Model weights saved in /tmp/debug_squad/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 16:46:45,339 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 16:46:45,339 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.7116, 'learning_rate': 6.928104575163399e-06, 'epoch': 1.96}\n"," 65% 16000/24480 [4:03:04<2:08:51,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 16:54:20,877 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-16000\n","[INFO|configuration_utils.py:440] 2022-04-01 16:54:20,879 >> Configuration saved in /tmp/debug_squad/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 16:54:20,959 >> Model weights saved in /tmp/debug_squad/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 16:54:20,960 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 16:54:20,961 >> Special tokens file saved in /tmp/debug_squad/checkpoint-16000/special_tokens_map.json\n","{'loss': 0.6445, 'learning_rate': 6.519607843137256e-06, 'epoch': 2.02}\n"," 67% 16500/24480 [4:10:39<2:00:58,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 17:01:55,846 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-16500\n","[INFO|configuration_utils.py:440] 2022-04-01 17:01:55,847 >> Configuration saved in /tmp/debug_squad/checkpoint-16500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 17:01:55,934 >> Model weights saved in /tmp/debug_squad/checkpoint-16500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 17:01:55,935 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-16500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 17:01:55,935 >> Special tokens file saved in /tmp/debug_squad/checkpoint-16500/special_tokens_map.json\n","{'loss': 0.5212, 'learning_rate': 6.111111111111112e-06, 'epoch': 2.08}\n"," 69% 17000/24480 [4:18:14<1:53:22,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 17:09:31,551 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-17000\n","[INFO|configuration_utils.py:440] 2022-04-01 17:09:31,552 >> Configuration saved in /tmp/debug_squad/checkpoint-17000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 17:09:31,635 >> Model weights saved in /tmp/debug_squad/checkpoint-17000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 17:09:31,636 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-17000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 17:09:31,636 >> Special tokens file saved in /tmp/debug_squad/checkpoint-17000/special_tokens_map.json\n","{'loss': 0.532, 'learning_rate': 5.7026143790849676e-06, 'epoch': 2.14}\n"," 71% 17500/24480 [4:25:50<1:45:44,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 17:17:07,354 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-17500\n","[INFO|configuration_utils.py:440] 2022-04-01 17:17:07,355 >> Configuration saved in /tmp/debug_squad/checkpoint-17500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 17:17:07,444 >> Model weights saved in /tmp/debug_squad/checkpoint-17500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 17:17:07,444 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-17500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 17:17:07,445 >> Special tokens file saved in /tmp/debug_squad/checkpoint-17500/special_tokens_map.json\n","{'loss': 0.5236, 'learning_rate': 5.294117647058824e-06, 'epoch': 2.21}\n"," 74% 18000/24480 [4:33:26<1:38:35,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 17:24:43,213 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-18000\n","[INFO|configuration_utils.py:440] 2022-04-01 17:24:43,214 >> Configuration saved in /tmp/debug_squad/checkpoint-18000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 17:24:43,299 >> Model weights saved in /tmp/debug_squad/checkpoint-18000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 17:24:43,299 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-18000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 17:24:43,300 >> Special tokens file saved in /tmp/debug_squad/checkpoint-18000/special_tokens_map.json\n","{'loss': 0.5283, 'learning_rate': 4.88562091503268e-06, 'epoch': 2.27}\n"," 76% 18500/24480 [4:41:02<1:30:44,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 17:32:18,962 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-18500\n","[INFO|configuration_utils.py:440] 2022-04-01 17:32:18,963 >> Configuration saved in /tmp/debug_squad/checkpoint-18500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 17:32:19,046 >> Model weights saved in /tmp/debug_squad/checkpoint-18500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 17:32:19,047 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-18500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 17:32:19,047 >> Special tokens file saved in /tmp/debug_squad/checkpoint-18500/special_tokens_map.json\n","{'loss': 0.4897, 'learning_rate': 4.477124183006537e-06, 'epoch': 2.33}\n"," 78% 19000/24480 [4:48:37<1:23:10,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 17:39:54,682 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-19000\n","[INFO|configuration_utils.py:440] 2022-04-01 17:39:54,683 >> Configuration saved in /tmp/debug_squad/checkpoint-19000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 17:39:54,769 >> Model weights saved in /tmp/debug_squad/checkpoint-19000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 17:39:54,769 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-19000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 17:39:54,770 >> Special tokens file saved in /tmp/debug_squad/checkpoint-19000/special_tokens_map.json\n","{'loss': 0.5346, 'learning_rate': 4.068627450980392e-06, 'epoch': 2.39}\n"," 80% 19500/24480 [4:56:13<1:15:45,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 17:47:30,500 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-19500\n","[INFO|configuration_utils.py:440] 2022-04-01 17:47:30,501 >> Configuration saved in /tmp/debug_squad/checkpoint-19500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 17:47:30,595 >> Model weights saved in /tmp/debug_squad/checkpoint-19500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 17:47:30,596 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-19500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 17:47:30,596 >> Special tokens file saved in /tmp/debug_squad/checkpoint-19500/special_tokens_map.json\n","{'loss': 0.5308, 'learning_rate': 3.6601307189542484e-06, 'epoch': 2.45}\n"," 82% 20000/24480 [5:03:49<1:07:56,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 17:55:06,498 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-20000\n","[INFO|configuration_utils.py:440] 2022-04-01 17:55:06,499 >> Configuration saved in /tmp/debug_squad/checkpoint-20000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 17:55:06,629 >> Model weights saved in /tmp/debug_squad/checkpoint-20000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 17:55:06,630 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-20000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 17:55:06,630 >> Special tokens file saved in /tmp/debug_squad/checkpoint-20000/special_tokens_map.json\n","{'loss': 0.5343, 'learning_rate': 3.2516339869281048e-06, 'epoch': 2.51}\n"," 84% 20500/24480 [5:11:25<1:00:30,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 18:02:42,440 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-20500\n","[INFO|configuration_utils.py:440] 2022-04-01 18:02:42,442 >> Configuration saved in /tmp/debug_squad/checkpoint-20500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 18:02:42,570 >> Model weights saved in /tmp/debug_squad/checkpoint-20500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 18:02:42,571 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-20500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 18:02:42,571 >> Special tokens file saved in /tmp/debug_squad/checkpoint-20500/special_tokens_map.json\n","{'loss': 0.5016, 'learning_rate': 2.843137254901961e-06, 'epoch': 2.57}\n"," 86% 21000/24480 [5:19:01<52:46,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 18:10:18,359 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-21000\n","[INFO|configuration_utils.py:440] 2022-04-01 18:10:18,361 >> Configuration saved in /tmp/debug_squad/checkpoint-21000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 18:10:18,491 >> Model weights saved in /tmp/debug_squad/checkpoint-21000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 18:10:18,492 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-21000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 18:10:18,492 >> Special tokens file saved in /tmp/debug_squad/checkpoint-21000/special_tokens_map.json\n","{'loss': 0.4992, 'learning_rate': 2.434640522875817e-06, 'epoch': 2.63}\n"," 88% 21500/24480 [5:26:37<45:23,  1.09it/s][INFO|trainer.py:2165] 2022-04-01 18:17:54,372 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-21500\n","[INFO|configuration_utils.py:440] 2022-04-01 18:17:54,373 >> Configuration saved in /tmp/debug_squad/checkpoint-21500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 18:17:54,501 >> Model weights saved in /tmp/debug_squad/checkpoint-21500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 18:17:54,501 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-21500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 18:17:54,502 >> Special tokens file saved in /tmp/debug_squad/checkpoint-21500/special_tokens_map.json\n","{'loss': 0.5033, 'learning_rate': 2.0261437908496734e-06, 'epoch': 2.7}\n"," 90% 22000/24480 [5:34:13<37:38,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 18:25:30,403 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-22000\n","[INFO|configuration_utils.py:440] 2022-04-01 18:25:30,404 >> Configuration saved in /tmp/debug_squad/checkpoint-22000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 18:25:30,531 >> Model weights saved in /tmp/debug_squad/checkpoint-22000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 18:25:30,532 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-22000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 18:25:30,532 >> Special tokens file saved in /tmp/debug_squad/checkpoint-22000/special_tokens_map.json\n","{'loss': 0.5114, 'learning_rate': 1.6176470588235297e-06, 'epoch': 2.76}\n"," 92% 22500/24480 [5:41:49<30:03,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 18:33:06,228 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-22500\n","[INFO|configuration_utils.py:440] 2022-04-01 18:33:06,230 >> Configuration saved in /tmp/debug_squad/checkpoint-22500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 18:33:06,359 >> Model weights saved in /tmp/debug_squad/checkpoint-22500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 18:33:06,360 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-22500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 18:33:06,360 >> Special tokens file saved in /tmp/debug_squad/checkpoint-22500/special_tokens_map.json\n","{'loss': 0.4934, 'learning_rate': 1.2091503267973858e-06, 'epoch': 2.82}\n"," 94% 23000/24480 [5:49:25<22:27,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 18:40:42,172 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-23000\n","[INFO|configuration_utils.py:440] 2022-04-01 18:40:42,173 >> Configuration saved in /tmp/debug_squad/checkpoint-23000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 18:40:42,306 >> Model weights saved in /tmp/debug_squad/checkpoint-23000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 18:40:42,307 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-23000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 18:40:42,307 >> Special tokens file saved in /tmp/debug_squad/checkpoint-23000/special_tokens_map.json\n","{'loss': 0.4993, 'learning_rate': 8.006535947712418e-07, 'epoch': 2.88}\n"," 96% 23500/24480 [5:57:01<14:52,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 18:48:18,069 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-23500\n","[INFO|configuration_utils.py:440] 2022-04-01 18:48:18,070 >> Configuration saved in /tmp/debug_squad/checkpoint-23500/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 18:48:18,199 >> Model weights saved in /tmp/debug_squad/checkpoint-23500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 18:48:18,199 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-23500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 18:48:18,199 >> Special tokens file saved in /tmp/debug_squad/checkpoint-23500/special_tokens_map.json\n","{'loss': 0.4968, 'learning_rate': 3.921568627450981e-07, 'epoch': 2.94}\n"," 98% 24000/24480 [6:04:37<07:16,  1.10it/s][INFO|trainer.py:2165] 2022-04-01 18:55:54,129 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-24000\n","[INFO|configuration_utils.py:440] 2022-04-01 18:55:54,131 >> Configuration saved in /tmp/debug_squad/checkpoint-24000/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 18:55:54,258 >> Model weights saved in /tmp/debug_squad/checkpoint-24000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 18:55:54,259 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-24000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 18:55:54,259 >> Special tokens file saved in /tmp/debug_squad/checkpoint-24000/special_tokens_map.json\n","100% 24480/24480 [6:11:54<00:00,  1.47it/s][INFO|trainer.py:1529] 2022-04-01 19:03:11,026 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 22314.2146, 'train_samples_per_second': 17.551, 'train_steps_per_second': 1.097, 'train_loss': 0.7836291531332178, 'epoch': 3.0}\n","100% 24480/24480 [6:11:54<00:00,  1.10it/s]\n","[INFO|trainer.py:2165] 2022-04-01 19:03:11,028 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:440] 2022-04-01 19:03:11,029 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1377] 2022-04-01 19:03:11,113 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-01 19:03:11,114 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-01 19:03:11,115 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =     0.7836\n","  train_runtime            = 6:11:54.21\n","  train_samples            =     130546\n","  train_samples_per_second =     17.551\n","  train_steps_per_second   =      1.097\n","04/01/2022 19:03:11 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-01 19:03:11,125 >> The following columns in the evaluation set  don't have a corresponding argument in `AlbertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `AlbertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2415] 2022-04-01 19:03:11,127 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2417] 2022-04-01 19:03:11,127 >>   Num examples = 11968\n","[INFO|trainer.py:2420] 2022-04-01 19:03:11,127 >>   Batch size = 8\n","100% 1496/1496 [04:20<00:00,  5.71it/s]04/01/2022 19:07:50 - INFO - utils_qa - Post-processing 11873 example predictions split into 11968 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 31/11873 [00:00<00:38, 307.07it/s]\u001b[A\n","  1% 63/11873 [00:00<00:37, 314.43it/s]\u001b[A\n","  1% 95/11873 [00:00<00:38, 308.99it/s]\u001b[A\n","  1% 127/11873 [00:00<00:37, 312.58it/s]\u001b[A\n","  1% 160/11873 [00:00<00:36, 316.82it/s]\u001b[A\n","  2% 194/11873 [00:00<00:36, 323.66it/s]\u001b[A\n","  2% 228/11873 [00:00<00:35, 326.39it/s]\u001b[A\n","  2% 261/11873 [00:00<00:35, 324.21it/s]\u001b[A\n","  2% 294/11873 [00:00<00:35, 322.54it/s]\u001b[A\n","  3% 327/11873 [00:01<00:36, 315.93it/s]\u001b[A\n","  3% 360/11873 [00:01<00:36, 319.42it/s]\u001b[A\n","  3% 395/11873 [00:01<00:35, 325.45it/s]\u001b[A\n","  4% 430/11873 [00:01<00:34, 332.00it/s]\u001b[A\n","  4% 464/11873 [00:01<00:34, 330.79it/s]\u001b[A\n","  4% 498/11873 [00:01<00:34, 331.46it/s]\u001b[A\n","  4% 532/11873 [00:01<00:34, 328.49it/s]\u001b[A\n","  5% 566/11873 [00:01<00:34, 330.48it/s]\u001b[A\n","  5% 600/11873 [00:01<00:34, 327.93it/s]\u001b[A\n","  5% 634/11873 [00:01<00:34, 329.66it/s]\u001b[A\n","  6% 669/11873 [00:02<00:33, 333.54it/s]\u001b[A\n","  6% 703/11873 [00:02<00:33, 330.88it/s]\u001b[A\n","  6% 737/11873 [00:02<00:33, 329.78it/s]\u001b[A\n","  6% 770/11873 [00:02<00:33, 328.57it/s]\u001b[A\n","  7% 806/11873 [00:02<00:32, 335.64it/s]\u001b[A\n","  7% 840/11873 [00:02<00:32, 336.40it/s]\u001b[A\n","  7% 877/11873 [00:02<00:31, 345.53it/s]\u001b[A\n","  8% 912/11873 [00:02<00:31, 342.65it/s]\u001b[A\n","  8% 947/11873 [00:02<00:31, 344.15it/s]\u001b[A\n","  8% 982/11873 [00:02<00:32, 336.70it/s]\u001b[A\n","  9% 1016/11873 [00:03<00:34, 318.89it/s]\u001b[A\n","  9% 1049/11873 [00:03<00:36, 296.49it/s]\u001b[A\n","  9% 1080/11873 [00:03<00:37, 288.58it/s]\u001b[A\n","  9% 1110/11873 [00:03<00:38, 276.68it/s]\u001b[A\n"," 10% 1138/11873 [00:03<00:39, 272.21it/s]\u001b[A\n"," 10% 1166/11873 [00:03<00:40, 265.63it/s]\u001b[A\n"," 10% 1193/11873 [00:03<00:40, 262.83it/s]\u001b[A\n"," 10% 1220/11873 [00:03<00:41, 259.15it/s]\u001b[A\n"," 10% 1246/11873 [00:04<00:41, 253.10it/s]\u001b[A\n"," 11% 1272/11873 [00:04<00:42, 247.99it/s]\u001b[A\n"," 11% 1297/11873 [00:04<00:42, 247.30it/s]\u001b[A\n"," 11% 1322/11873 [00:04<00:42, 247.36it/s]\u001b[A\n"," 11% 1348/11873 [00:04<00:42, 248.21it/s]\u001b[A\n"," 12% 1374/11873 [00:04<00:42, 246.93it/s]\u001b[A\n"," 12% 1400/11873 [00:04<00:42, 248.61it/s]\u001b[A\n"," 12% 1425/11873 [00:04<00:43, 239.19it/s]\u001b[A\n"," 12% 1449/11873 [00:04<00:43, 237.38it/s]\u001b[A\n"," 12% 1473/11873 [00:04<00:43, 238.00it/s]\u001b[A\n"," 13% 1499/11873 [00:05<00:42, 241.73it/s]\u001b[A\n"," 13% 1524/11873 [00:05<00:43, 237.82it/s]\u001b[A\n"," 13% 1548/11873 [00:05<00:43, 236.62it/s]\u001b[A\n"," 13% 1572/11873 [00:05<00:43, 236.62it/s]\u001b[A\n"," 13% 1596/11873 [00:05<00:43, 234.04it/s]\u001b[A\n"," 14% 1621/11873 [00:05<00:43, 237.70it/s]\u001b[A\n"," 14% 1648/11873 [00:05<00:41, 244.65it/s]\u001b[A\n"," 14% 1673/11873 [00:05<00:42, 242.66it/s]\u001b[A\n"," 14% 1698/11873 [00:05<00:41, 243.51it/s]\u001b[A\n"," 15% 1723/11873 [00:05<00:41, 242.39it/s]\u001b[A\n"," 15% 1749/11873 [00:06<00:40, 247.31it/s]\u001b[A\n"," 15% 1774/11873 [00:06<00:40, 247.14it/s]\u001b[A\n"," 15% 1799/11873 [00:06<00:40, 245.95it/s]\u001b[A\n"," 15% 1824/11873 [00:06<00:41, 240.67it/s]\u001b[A\n"," 16% 1851/11873 [00:06<00:40, 247.43it/s]\u001b[A\n"," 16% 1877/11873 [00:06<00:39, 250.08it/s]\u001b[A\n"," 16% 1904/11873 [00:06<00:39, 253.48it/s]\u001b[A\n"," 16% 1930/11873 [00:06<00:39, 254.81it/s]\u001b[A\n"," 16% 1956/11873 [00:06<00:38, 256.09it/s]\u001b[A\n"," 17% 1982/11873 [00:07<00:38, 254.94it/s]\u001b[A\n"," 17% 2008/11873 [00:07<00:38, 256.11it/s]\u001b[A\n"," 17% 2034/11873 [00:07<00:38, 254.90it/s]\u001b[A\n"," 17% 2061/11873 [00:07<00:38, 256.59it/s]\u001b[A\n"," 18% 2087/11873 [00:07<00:38, 254.73it/s]\u001b[A\n"," 18% 2113/11873 [00:07<00:38, 252.11it/s]\u001b[A\n"," 18% 2139/11873 [00:07<00:38, 253.66it/s]\u001b[A\n"," 18% 2165/11873 [00:07<00:38, 249.84it/s]\u001b[A\n"," 18% 2191/11873 [00:07<00:38, 249.69it/s]\u001b[A\n"," 19% 2217/11873 [00:07<00:38, 252.09it/s]\u001b[A\n"," 19% 2243/11873 [00:08<00:38, 252.53it/s]\u001b[A\n"," 19% 2269/11873 [00:08<00:38, 251.90it/s]\u001b[A\n"," 19% 2295/11873 [00:08<00:38, 250.38it/s]\u001b[A\n"," 20% 2322/11873 [00:08<00:37, 253.76it/s]\u001b[A\n"," 20% 2348/11873 [00:08<00:37, 255.15it/s]\u001b[A\n"," 20% 2374/11873 [00:08<00:38, 247.61it/s]\u001b[A\n"," 20% 2399/11873 [00:08<00:38, 244.67it/s]\u001b[A\n"," 20% 2424/11873 [00:08<00:39, 240.83it/s]\u001b[A\n"," 21% 2449/11873 [00:08<00:40, 234.20it/s]\u001b[A\n"," 21% 2474/11873 [00:09<00:39, 238.46it/s]\u001b[A\n"," 21% 2499/11873 [00:09<00:39, 238.76it/s]\u001b[A\n"," 21% 2524/11873 [00:09<00:38, 241.68it/s]\u001b[A\n"," 21% 2549/11873 [00:09<00:38, 240.30it/s]\u001b[A\n"," 22% 2574/11873 [00:09<00:38, 241.74it/s]\u001b[A\n"," 22% 2600/11873 [00:09<00:37, 245.05it/s]\u001b[A\n"," 22% 2625/11873 [00:09<00:37, 244.76it/s]\u001b[A\n"," 22% 2650/11873 [00:09<00:38, 238.27it/s]\u001b[A\n"," 23% 2674/11873 [00:09<00:38, 237.23it/s]\u001b[A\n"," 23% 2699/11873 [00:09<00:38, 240.71it/s]\u001b[A\n"," 23% 2724/11873 [00:10<00:38, 240.10it/s]\u001b[A\n"," 23% 2749/11873 [00:10<00:38, 238.66it/s]\u001b[A\n"," 23% 2774/11873 [00:10<00:38, 239.35it/s]\u001b[A\n"," 24% 2799/11873 [00:10<00:37, 239.87it/s]\u001b[A\n"," 24% 2825/11873 [00:10<00:36, 245.02it/s]\u001b[A\n"," 24% 2851/11873 [00:10<00:36, 247.61it/s]\u001b[A\n"," 24% 2877/11873 [00:10<00:35, 250.50it/s]\u001b[A\n"," 24% 2903/11873 [00:10<00:36, 246.96it/s]\u001b[A\n"," 25% 2929/11873 [00:10<00:36, 247.48it/s]\u001b[A\n"," 25% 2955/11873 [00:10<00:35, 248.97it/s]\u001b[A\n"," 25% 2981/11873 [00:11<00:35, 251.60it/s]\u001b[A\n"," 25% 3007/11873 [00:11<00:36, 242.69it/s]\u001b[A\n"," 26% 3032/11873 [00:11<00:36, 242.04it/s]\u001b[A\n"," 26% 3057/11873 [00:11<00:36, 241.87it/s]\u001b[A\n"," 26% 3082/11873 [00:11<00:36, 242.51it/s]\u001b[A\n"," 26% 3107/11873 [00:11<00:37, 235.03it/s]\u001b[A\n"," 26% 3131/11873 [00:11<00:40, 213.80it/s]\u001b[A\n"," 27% 3153/11873 [00:11<00:41, 209.64it/s]\u001b[A\n"," 27% 3175/11873 [00:11<00:42, 205.52it/s]\u001b[A\n"," 27% 3202/11873 [00:12<00:39, 221.48it/s]\u001b[A\n"," 27% 3229/11873 [00:12<00:37, 233.26it/s]\u001b[A\n"," 27% 3255/11873 [00:12<00:35, 239.86it/s]\u001b[A\n"," 28% 3280/11873 [00:12<00:38, 221.51it/s]\u001b[A\n"," 28% 3303/11873 [00:12<00:45, 190.25it/s]\u001b[A\n"," 28% 3323/11873 [00:12<00:48, 177.65it/s]\u001b[A\n"," 28% 3348/11873 [00:12<00:44, 192.40it/s]\u001b[A\n"," 28% 3368/11873 [00:12<00:51, 166.51it/s]\u001b[A\n"," 29% 3394/11873 [00:13<00:45, 187.73it/s]\u001b[A\n"," 29% 3419/11873 [00:13<00:41, 203.06it/s]\u001b[A\n"," 29% 3444/11873 [00:13<00:39, 215.48it/s]\u001b[A\n"," 29% 3469/11873 [00:13<00:37, 223.34it/s]\u001b[A\n"," 29% 3494/11873 [00:13<00:36, 230.39it/s]\u001b[A\n"," 30% 3520/11873 [00:13<00:35, 236.28it/s]\u001b[A\n"," 30% 3545/11873 [00:13<00:35, 234.93it/s]\u001b[A\n"," 30% 3569/11873 [00:13<00:35, 234.72it/s]\u001b[A\n"," 30% 3594/11873 [00:13<00:34, 237.90it/s]\u001b[A\n"," 30% 3618/11873 [00:14<00:35, 230.75it/s]\u001b[A\n"," 31% 3643/11873 [00:14<00:35, 234.17it/s]\u001b[A\n"," 31% 3667/11873 [00:14<00:35, 230.37it/s]\u001b[A\n"," 31% 3691/11873 [00:14<00:35, 228.18it/s]\u001b[A\n"," 31% 3715/11873 [00:14<00:35, 230.12it/s]\u001b[A\n"," 32% 3741/11873 [00:14<00:34, 236.77it/s]\u001b[A\n"," 32% 3767/11873 [00:14<00:33, 240.45it/s]\u001b[A\n"," 32% 3792/11873 [00:14<00:34, 233.97it/s]\u001b[A\n"," 32% 3818/11873 [00:14<00:33, 239.37it/s]\u001b[A\n"," 32% 3844/11873 [00:14<00:32, 243.66it/s]\u001b[A\n"," 33% 3870/11873 [00:15<00:32, 246.98it/s]\u001b[A\n"," 33% 3896/11873 [00:15<00:31, 249.68it/s]\u001b[A\n"," 33% 3922/11873 [00:15<00:32, 245.22it/s]\u001b[A\n"," 33% 3948/11873 [00:15<00:32, 246.95it/s]\u001b[A\n"," 33% 3973/11873 [00:15<00:32, 245.36it/s]\u001b[A\n"," 34% 3999/11873 [00:15<00:31, 249.39it/s]\u001b[A\n"," 34% 4024/11873 [00:15<00:32, 244.98it/s]\u001b[A\n"," 34% 4050/11873 [00:15<00:31, 246.68it/s]\u001b[A\n"," 34% 4076/11873 [00:15<00:31, 248.09it/s]\u001b[A\n"," 35% 4102/11873 [00:16<00:30, 251.51it/s]\u001b[A\n"," 35% 4128/11873 [00:16<00:30, 250.49it/s]\u001b[A\n"," 35% 4154/11873 [00:16<00:31, 247.16it/s]\u001b[A\n"," 35% 4179/11873 [00:16<00:31, 246.44it/s]\u001b[A\n"," 35% 4204/11873 [00:16<00:31, 246.02it/s]\u001b[A\n"," 36% 4229/11873 [00:16<00:31, 244.28it/s]\u001b[A\n"," 36% 4254/11873 [00:16<00:30, 245.94it/s]\u001b[A\n"," 36% 4279/11873 [00:16<00:31, 240.45it/s]\u001b[A\n"," 36% 4304/11873 [00:16<00:31, 240.73it/s]\u001b[A\n"," 36% 4329/11873 [00:16<00:31, 242.48it/s]\u001b[A\n"," 37% 4354/11873 [00:17<00:31, 242.25it/s]\u001b[A\n"," 37% 4379/11873 [00:17<00:30, 243.70it/s]\u001b[A\n"," 37% 4404/11873 [00:17<00:30, 243.26it/s]\u001b[A\n"," 37% 4429/11873 [00:17<00:35, 207.01it/s]\u001b[A\n"," 38% 4456/11873 [00:17<00:33, 222.16it/s]\u001b[A\n"," 38% 4483/11873 [00:17<00:31, 232.41it/s]\u001b[A\n"," 38% 4507/11873 [00:17<00:31, 232.08it/s]\u001b[A\n"," 38% 4532/11873 [00:17<00:31, 235.08it/s]\u001b[A\n"," 38% 4557/11873 [00:17<00:30, 238.44it/s]\u001b[A\n"," 39% 4583/11873 [00:18<00:30, 242.98it/s]\u001b[A\n"," 39% 4609/11873 [00:18<00:29, 245.76it/s]\u001b[A\n"," 39% 4634/11873 [00:18<00:29, 246.74it/s]\u001b[A\n"," 39% 4659/11873 [00:18<00:29, 244.87it/s]\u001b[A\n"," 39% 4684/11873 [00:18<00:29, 244.54it/s]\u001b[A\n"," 40% 4710/11873 [00:18<00:28, 247.66it/s]\u001b[A\n"," 40% 4735/11873 [00:18<00:28, 247.05it/s]\u001b[A\n"," 40% 4760/11873 [00:18<00:29, 243.30it/s]\u001b[A\n"," 40% 4785/11873 [00:18<00:29, 242.08it/s]\u001b[A\n"," 41% 4810/11873 [00:18<00:28, 244.24it/s]\u001b[A\n"," 41% 4835/11873 [00:19<00:28, 245.52it/s]\u001b[A\n"," 41% 4860/11873 [00:19<00:28, 245.27it/s]\u001b[A\n"," 41% 4885/11873 [00:19<00:28, 244.16it/s]\u001b[A\n"," 41% 4910/11873 [00:19<00:28, 243.84it/s]\u001b[A\n"," 42% 4935/11873 [00:19<00:28, 244.92it/s]\u001b[A\n"," 42% 4960/11873 [00:19<00:28, 239.38it/s]\u001b[A\n"," 42% 4984/11873 [00:19<00:28, 239.48it/s]\u001b[A\n"," 42% 5009/11873 [00:19<00:28, 240.35it/s]\u001b[A\n"," 42% 5034/11873 [00:19<00:28, 237.79it/s]\u001b[A\n"," 43% 5059/11873 [00:19<00:28, 239.57it/s]\u001b[A\n"," 43% 5084/11873 [00:20<00:28, 241.92it/s]\u001b[A\n"," 43% 5110/11873 [00:20<00:27, 246.02it/s]\u001b[A\n"," 43% 5137/11873 [00:20<00:26, 251.03it/s]\u001b[A\n"," 43% 5164/11873 [00:20<00:26, 254.24it/s]\u001b[A\n"," 44% 5190/11873 [00:20<00:26, 253.78it/s]\u001b[A\n"," 44% 5216/11873 [00:20<00:26, 253.59it/s]\u001b[A\n"," 44% 5242/11873 [00:20<00:26, 251.96it/s]\u001b[A\n"," 44% 5268/11873 [00:20<00:29, 223.48it/s]\u001b[A\n"," 45% 5292/11873 [00:20<00:29, 226.46it/s]\u001b[A\n"," 45% 5317/11873 [00:21<00:28, 231.60it/s]\u001b[A\n"," 45% 5341/11873 [00:21<00:28, 231.92it/s]\u001b[A\n"," 45% 5366/11873 [00:21<00:27, 235.61it/s]\u001b[A\n"," 45% 5391/11873 [00:21<00:27, 238.89it/s]\u001b[A\n"," 46% 5417/11873 [00:21<00:26, 244.66it/s]\u001b[A\n"," 46% 5443/11873 [00:21<00:26, 246.98it/s]\u001b[A\n"," 46% 5468/11873 [00:21<00:25, 247.81it/s]\u001b[A\n"," 46% 5493/11873 [00:21<00:26, 244.78it/s]\u001b[A\n"," 46% 5518/11873 [00:21<00:26, 243.39it/s]\u001b[A\n"," 47% 5543/11873 [00:21<00:25, 244.27it/s]\u001b[A\n"," 47% 5568/11873 [00:22<00:25, 244.15it/s]\u001b[A\n"," 47% 5593/11873 [00:22<00:26, 241.39it/s]\u001b[A\n"," 47% 5618/11873 [00:22<00:26, 239.63it/s]\u001b[A\n"," 48% 5642/11873 [00:22<00:26, 236.47it/s]\u001b[A\n"," 48% 5668/11873 [00:22<00:25, 242.42it/s]\u001b[A\n"," 48% 5693/11873 [00:22<00:25, 241.45it/s]\u001b[A\n"," 48% 5719/11873 [00:22<00:25, 244.70it/s]\u001b[A\n"," 48% 5744/11873 [00:22<00:25, 244.41it/s]\u001b[A\n"," 49% 5769/11873 [00:22<00:25, 242.60it/s]\u001b[A\n"," 49% 5796/11873 [00:23<00:24, 249.23it/s]\u001b[A\n"," 49% 5822/11873 [00:23<00:23, 252.21it/s]\u001b[A\n"," 49% 5848/11873 [00:23<00:23, 253.87it/s]\u001b[A\n"," 49% 5874/11873 [00:23<00:23, 254.70it/s]\u001b[A\n"," 50% 5900/11873 [00:23<00:23, 250.99it/s]\u001b[A\n"," 50% 5926/11873 [00:23<00:23, 251.25it/s]\u001b[A\n"," 50% 5952/11873 [00:23<00:23, 251.99it/s]\u001b[A\n"," 50% 5978/11873 [00:23<00:23, 252.03it/s]\u001b[A\n"," 51% 6004/11873 [00:23<00:23, 253.80it/s]\u001b[A\n"," 51% 6030/11873 [00:23<00:23, 251.52it/s]\u001b[A\n"," 51% 6056/11873 [00:24<00:22, 253.25it/s]\u001b[A\n"," 51% 6082/11873 [00:24<00:23, 251.25it/s]\u001b[A\n"," 51% 6108/11873 [00:24<00:22, 252.95it/s]\u001b[A\n"," 52% 6134/11873 [00:24<00:22, 253.27it/s]\u001b[A\n"," 52% 6160/11873 [00:24<00:22, 249.50it/s]\u001b[A\n"," 52% 6187/11873 [00:24<00:22, 253.59it/s]\u001b[A\n"," 52% 6214/11873 [00:24<00:22, 255.33it/s]\u001b[A\n"," 53% 6240/11873 [00:24<00:22, 251.03it/s]\u001b[A\n"," 53% 6266/11873 [00:24<00:22, 251.17it/s]\u001b[A\n"," 53% 6292/11873 [00:24<00:22, 251.00it/s]\u001b[A\n"," 53% 6318/11873 [00:25<00:22, 249.79it/s]\u001b[A\n"," 53% 6343/11873 [00:25<00:22, 246.60it/s]\u001b[A\n"," 54% 6369/11873 [00:25<00:22, 249.33it/s]\u001b[A\n"," 54% 6396/11873 [00:25<00:21, 253.11it/s]\u001b[A\n"," 54% 6423/11873 [00:25<00:21, 255.70it/s]\u001b[A\n"," 54% 6449/11873 [00:25<00:21, 255.86it/s]\u001b[A\n"," 55% 6475/11873 [00:25<00:21, 249.63it/s]\u001b[A\n"," 55% 6500/11873 [00:25<00:21, 248.74it/s]\u001b[A\n"," 55% 6525/11873 [00:25<00:21, 244.94it/s]\u001b[A\n"," 55% 6551/11873 [00:26<00:21, 247.68it/s]\u001b[A\n"," 55% 6576/11873 [00:26<00:21, 248.26it/s]\u001b[A\n"," 56% 6602/11873 [00:26<00:21, 249.01it/s]\u001b[A\n"," 56% 6628/11873 [00:26<00:20, 250.04it/s]\u001b[A\n"," 56% 6654/11873 [00:26<00:20, 249.97it/s]\u001b[A\n"," 56% 6680/11873 [00:26<00:20, 249.35it/s]\u001b[A\n"," 56% 6705/11873 [00:26<00:20, 246.89it/s]\u001b[A\n"," 57% 6730/11873 [00:26<00:20, 245.13it/s]\u001b[A\n"," 57% 6755/11873 [00:26<00:20, 244.43it/s]\u001b[A\n"," 57% 6780/11873 [00:26<00:20, 244.67it/s]\u001b[A\n"," 57% 6807/11873 [00:27<00:20, 249.27it/s]\u001b[A\n"," 58% 6833/11873 [00:27<00:20, 251.55it/s]\u001b[A\n"," 58% 6859/11873 [00:27<00:19, 252.90it/s]\u001b[A\n"," 58% 6885/11873 [00:27<00:19, 249.63it/s]\u001b[A\n"," 58% 6910/11873 [00:27<00:19, 248.25it/s]\u001b[A\n"," 58% 6935/11873 [00:27<00:20, 245.52it/s]\u001b[A\n"," 59% 6961/11873 [00:27<00:19, 248.02it/s]\u001b[A\n"," 59% 6987/11873 [00:27<00:19, 250.10it/s]\u001b[A\n"," 59% 7013/11873 [00:27<00:19, 252.58it/s]\u001b[A\n"," 59% 7040/11873 [00:27<00:18, 254.94it/s]\u001b[A\n"," 60% 7067/11873 [00:28<00:18, 259.20it/s]\u001b[A\n"," 60% 7093/11873 [00:28<00:18, 255.69it/s]\u001b[A\n"," 60% 7120/11873 [00:28<00:18, 257.48it/s]\u001b[A\n"," 60% 7148/11873 [00:28<00:18, 261.79it/s]\u001b[A\n"," 60% 7175/11873 [00:28<00:18, 257.05it/s]\u001b[A\n"," 61% 7202/11873 [00:28<00:17, 259.66it/s]\u001b[A\n"," 61% 7228/11873 [00:28<00:18, 254.96it/s]\u001b[A\n"," 61% 7254/11873 [00:28<00:18, 248.55it/s]\u001b[A\n"," 61% 7279/11873 [00:28<00:18, 248.82it/s]\u001b[A\n"," 62% 7304/11873 [00:29<00:18, 248.46it/s]\u001b[A\n"," 62% 7330/11873 [00:29<00:18, 251.20it/s]\u001b[A\n"," 62% 7357/11873 [00:29<00:17, 254.93it/s]\u001b[A\n"," 62% 7383/11873 [00:29<00:17, 253.84it/s]\u001b[A\n"," 62% 7409/11873 [00:29<00:17, 252.73it/s]\u001b[A\n"," 63% 7435/11873 [00:29<00:17, 247.81it/s]\u001b[A\n"," 63% 7460/11873 [00:29<00:18, 245.14it/s]\u001b[A\n"," 63% 7486/11873 [00:29<00:17, 247.19it/s]\u001b[A\n"," 63% 7511/11873 [00:29<00:17, 246.78it/s]\u001b[A\n"," 63% 7536/11873 [00:29<00:17, 245.84it/s]\u001b[A\n"," 64% 7562/11873 [00:30<00:17, 248.04it/s]\u001b[A\n"," 64% 7587/11873 [00:30<00:17, 244.03it/s]\u001b[A\n"," 64% 7612/11873 [00:30<00:17, 242.10it/s]\u001b[A\n"," 64% 7637/11873 [00:30<00:17, 239.55it/s]\u001b[A\n"," 65% 7663/11873 [00:30<00:17, 243.52it/s]\u001b[A\n"," 65% 7688/11873 [00:30<00:17, 244.34it/s]\u001b[A\n"," 65% 7714/11873 [00:30<00:16, 247.03it/s]\u001b[A\n"," 65% 7740/11873 [00:30<00:16, 248.21it/s]\u001b[A\n"," 65% 7765/11873 [00:30<00:16, 246.84it/s]\u001b[A\n"," 66% 7791/11873 [00:30<00:16, 249.25it/s]\u001b[A\n"," 66% 7816/11873 [00:31<00:16, 248.80it/s]\u001b[A\n"," 66% 7841/11873 [00:31<00:16, 243.07it/s]\u001b[A\n"," 66% 7866/11873 [00:31<00:16, 244.99it/s]\u001b[A\n"," 66% 7891/11873 [00:31<00:16, 242.99it/s]\u001b[A\n"," 67% 7916/11873 [00:31<00:16, 240.09it/s]\u001b[A\n"," 67% 7941/11873 [00:31<00:16, 241.90it/s]\u001b[A\n"," 67% 7966/11873 [00:31<00:16, 242.64it/s]\u001b[A\n"," 67% 7991/11873 [00:31<00:16, 241.92it/s]\u001b[A\n"," 68% 8016/11873 [00:31<00:15, 242.18it/s]\u001b[A\n"," 68% 8042/11873 [00:32<00:15, 246.91it/s]\u001b[A\n"," 68% 8067/11873 [00:32<00:15, 247.47it/s]\u001b[A\n"," 68% 8092/11873 [00:32<00:15, 244.42it/s]\u001b[A\n"," 68% 8117/11873 [00:32<00:15, 244.14it/s]\u001b[A\n"," 69% 8142/11873 [00:32<00:15, 243.88it/s]\u001b[A\n"," 69% 8167/11873 [00:32<00:15, 240.19it/s]\u001b[A\n"," 69% 8192/11873 [00:32<00:15, 238.93it/s]\u001b[A\n"," 69% 8216/11873 [00:32<00:15, 236.81it/s]\u001b[A\n"," 69% 8240/11873 [00:32<00:15, 236.61it/s]\u001b[A\n"," 70% 8266/11873 [00:32<00:14, 240.83it/s]\u001b[A\n"," 70% 8291/11873 [00:33<00:14, 239.32it/s]\u001b[A\n"," 70% 8315/11873 [00:33<00:14, 239.36it/s]\u001b[A\n"," 70% 8340/11873 [00:33<00:14, 241.84it/s]\u001b[A\n"," 70% 8365/11873 [00:33<00:14, 243.25it/s]\u001b[A\n"," 71% 8390/11873 [00:33<00:14, 244.09it/s]\u001b[A\n"," 71% 8415/11873 [00:33<00:14, 245.75it/s]\u001b[A\n"," 71% 8440/11873 [00:33<00:14, 244.71it/s]\u001b[A\n"," 71% 8465/11873 [00:33<00:14, 242.34it/s]\u001b[A\n"," 72% 8490/11873 [00:33<00:13, 243.87it/s]\u001b[A\n"," 72% 8515/11873 [00:33<00:14, 235.27it/s]\u001b[A\n"," 72% 8539/11873 [00:34<00:14, 235.54it/s]\u001b[A\n"," 72% 8564/11873 [00:34<00:13, 237.21it/s]\u001b[A\n"," 72% 8589/11873 [00:34<00:13, 240.31it/s]\u001b[A\n"," 73% 8614/11873 [00:34<00:13, 239.09it/s]\u001b[A\n"," 73% 8639/11873 [00:34<00:13, 241.07it/s]\u001b[A\n"," 73% 8665/11873 [00:34<00:13, 245.91it/s]\u001b[A\n"," 73% 8691/11873 [00:34<00:12, 249.84it/s]\u001b[A\n"," 73% 8717/11873 [00:34<00:12, 251.58it/s]\u001b[A\n"," 74% 8743/11873 [00:34<00:12, 249.37it/s]\u001b[A\n"," 74% 8769/11873 [00:35<00:12, 251.73it/s]\u001b[A\n"," 74% 8795/11873 [00:35<00:12, 248.27it/s]\u001b[A\n"," 74% 8820/11873 [00:35<00:12, 248.68it/s]\u001b[A\n"," 75% 8846/11873 [00:35<00:12, 250.37it/s]\u001b[A\n"," 75% 8872/11873 [00:35<00:11, 251.40it/s]\u001b[A\n"," 75% 8898/11873 [00:35<00:11, 251.02it/s]\u001b[A\n"," 75% 8924/11873 [00:35<00:11, 252.26it/s]\u001b[A\n"," 75% 8950/11873 [00:35<00:11, 248.67it/s]\u001b[A\n"," 76% 8975/11873 [00:35<00:11, 248.67it/s]\u001b[A\n"," 76% 9001/11873 [00:35<00:11, 251.69it/s]\u001b[A\n"," 76% 9027/11873 [00:36<00:11, 251.35it/s]\u001b[A\n"," 76% 9053/11873 [00:36<00:11, 253.76it/s]\u001b[A\n"," 76% 9079/11873 [00:36<00:10, 254.85it/s]\u001b[A\n"," 77% 9105/11873 [00:36<00:11, 251.29it/s]\u001b[A\n"," 77% 9131/11873 [00:36<00:11, 244.31it/s]\u001b[A\n"," 77% 9156/11873 [00:36<00:11, 245.70it/s]\u001b[A\n"," 77% 9181/11873 [00:36<00:11, 243.43it/s]\u001b[A\n"," 78% 9206/11873 [00:36<00:10, 244.13it/s]\u001b[A\n"," 78% 9231/11873 [00:36<00:10, 242.49it/s]\u001b[A\n"," 78% 9256/11873 [00:36<00:10, 243.09it/s]\u001b[A\n"," 78% 9283/11873 [00:37<00:10, 249.43it/s]\u001b[A\n"," 78% 9310/11873 [00:37<00:10, 253.19it/s]\u001b[A\n"," 79% 9336/11873 [00:37<00:10, 252.24it/s]\u001b[A\n"," 79% 9362/11873 [00:37<00:09, 253.74it/s]\u001b[A\n"," 79% 9388/11873 [00:37<00:10, 243.07it/s]\u001b[A\n"," 79% 9414/11873 [00:37<00:09, 247.64it/s]\u001b[A\n"," 80% 9440/11873 [00:37<00:09, 249.50it/s]\u001b[A\n"," 80% 9466/11873 [00:37<00:09, 247.17it/s]\u001b[A\n"," 80% 9492/11873 [00:37<00:09, 250.73it/s]\u001b[A\n"," 80% 9519/11873 [00:38<00:09, 255.07it/s]\u001b[A\n"," 80% 9545/11873 [00:38<00:09, 254.97it/s]\u001b[A\n"," 81% 9572/11873 [00:38<00:08, 258.43it/s]\u001b[A\n"," 81% 9598/11873 [00:38<00:08, 256.19it/s]\u001b[A\n"," 81% 9624/11873 [00:38<00:08, 256.78it/s]\u001b[A\n"," 81% 9651/11873 [00:38<00:08, 258.25it/s]\u001b[A\n"," 82% 9678/11873 [00:38<00:08, 261.52it/s]\u001b[A\n"," 82% 9705/11873 [00:38<00:08, 262.61it/s]\u001b[A\n"," 82% 9732/11873 [00:38<00:08, 259.09it/s]\u001b[A\n"," 82% 9758/11873 [00:38<00:08, 257.80it/s]\u001b[A\n"," 82% 9785/11873 [00:39<00:08, 259.67it/s]\u001b[A\n"," 83% 9811/11873 [00:39<00:07, 258.01it/s]\u001b[A\n"," 83% 9838/11873 [00:39<00:07, 260.33it/s]\u001b[A\n"," 83% 9865/11873 [00:39<00:07, 256.53it/s]\u001b[A\n"," 83% 9892/11873 [00:39<00:07, 258.77it/s]\u001b[A\n"," 84% 9918/11873 [00:39<00:07, 258.12it/s]\u001b[A\n"," 84% 9944/11873 [00:39<00:07, 251.56it/s]\u001b[A\n"," 84% 9971/11873 [00:39<00:07, 255.27it/s]\u001b[A\n"," 84% 9997/11873 [00:39<00:07, 254.54it/s]\u001b[A\n"," 84% 10024/11873 [00:39<00:07, 256.97it/s]\u001b[A\n"," 85% 10051/11873 [00:40<00:07, 260.11it/s]\u001b[A\n"," 85% 10078/11873 [00:40<00:06, 257.18it/s]\u001b[A\n"," 85% 10105/11873 [00:40<00:06, 258.75it/s]\u001b[A\n"," 85% 10131/11873 [00:40<00:06, 256.79it/s]\u001b[A\n"," 86% 10157/11873 [00:40<00:06, 252.40it/s]\u001b[A\n"," 86% 10184/11873 [00:40<00:06, 253.02it/s]\u001b[A\n"," 86% 10211/11873 [00:40<00:06, 255.37it/s]\u001b[A\n"," 86% 10237/11873 [00:40<00:06, 254.35it/s]\u001b[A\n"," 86% 10263/11873 [00:40<00:06, 255.67it/s]\u001b[A\n"," 87% 10289/11873 [00:41<00:06, 248.09it/s]\u001b[A\n"," 87% 10315/11873 [00:41<00:06, 249.13it/s]\u001b[A\n"," 87% 10342/11873 [00:41<00:06, 253.60it/s]\u001b[A\n"," 87% 10368/11873 [00:41<00:05, 251.62it/s]\u001b[A\n"," 88% 10394/11873 [00:41<00:05, 251.09it/s]\u001b[A\n"," 88% 10421/11873 [00:41<00:05, 256.42it/s]\u001b[A\n"," 88% 10447/11873 [00:41<00:05, 254.05it/s]\u001b[A\n"," 88% 10473/11873 [00:41<00:05, 254.64it/s]\u001b[A\n"," 88% 10499/11873 [00:41<00:05, 255.04it/s]\u001b[A\n"," 89% 10525/11873 [00:41<00:05, 252.67it/s]\u001b[A\n"," 89% 10551/11873 [00:42<00:05, 243.87it/s]\u001b[A\n"," 89% 10577/11873 [00:42<00:05, 246.09it/s]\u001b[A\n"," 89% 10603/11873 [00:42<00:05, 249.15it/s]\u001b[A\n"," 90% 10629/11873 [00:42<00:04, 251.18it/s]\u001b[A\n"," 90% 10655/11873 [00:42<00:04, 248.52it/s]\u001b[A\n"," 90% 10681/11873 [00:42<00:04, 251.47it/s]\u001b[A\n"," 90% 10707/11873 [00:42<00:04, 253.44it/s]\u001b[A\n"," 90% 10733/11873 [00:42<00:04, 255.34it/s]\u001b[A\n"," 91% 10759/11873 [00:42<00:04, 254.19it/s]\u001b[A\n"," 91% 10785/11873 [00:42<00:04, 250.13it/s]\u001b[A\n"," 91% 10812/11873 [00:43<00:04, 253.31it/s]\u001b[A\n"," 91% 10838/11873 [00:43<00:04, 254.09it/s]\u001b[A\n"," 92% 10864/11873 [00:43<00:03, 254.62it/s]\u001b[A\n"," 92% 10890/11873 [00:43<00:03, 253.80it/s]\u001b[A\n"," 92% 10916/11873 [00:43<00:03, 252.25it/s]\u001b[A\n"," 92% 10942/11873 [00:43<00:03, 247.55it/s]\u001b[A\n"," 92% 10968/11873 [00:43<00:03, 250.76it/s]\u001b[A\n"," 93% 10995/11873 [00:43<00:03, 254.04it/s]\u001b[A\n"," 93% 11022/11873 [00:43<00:03, 256.47it/s]\u001b[A\n"," 93% 11048/11873 [00:44<00:03, 256.75it/s]\u001b[A\n"," 93% 11075/11873 [00:44<00:03, 258.43it/s]\u001b[A\n"," 93% 11101/11873 [00:44<00:04, 170.53it/s]\u001b[A\n"," 94% 11127/11873 [00:44<00:03, 188.96it/s]\u001b[A\n"," 94% 11154/11873 [00:44<00:03, 207.15it/s]\u001b[A\n"," 94% 11180/11873 [00:44<00:03, 218.97it/s]\u001b[A\n"," 94% 11206/11873 [00:44<00:02, 228.70it/s]\u001b[A\n"," 95% 11231/11873 [00:44<00:02, 234.05it/s]\u001b[A\n"," 95% 11256/11873 [00:45<00:02, 238.40it/s]\u001b[A\n"," 95% 11281/11873 [00:45<00:02, 240.63it/s]\u001b[A\n"," 95% 11307/11873 [00:45<00:02, 244.14it/s]\u001b[A\n"," 95% 11332/11873 [00:45<00:02, 245.53it/s]\u001b[A\n"," 96% 11357/11873 [00:45<00:02, 242.83it/s]\u001b[A\n"," 96% 11383/11873 [00:45<00:01, 247.38it/s]\u001b[A\n"," 96% 11408/11873 [00:45<00:01, 246.16it/s]\u001b[A\n"," 96% 11433/11873 [00:45<00:01, 241.23it/s]\u001b[A\n"," 97% 11458/11873 [00:45<00:01, 241.72it/s]\u001b[A\n"," 97% 11483/11873 [00:45<00:01, 241.65it/s]\u001b[A\n"," 97% 11508/11873 [00:46<00:01, 241.15it/s]\u001b[A\n"," 97% 11534/11873 [00:46<00:01, 245.37it/s]\u001b[A\n"," 97% 11559/11873 [00:46<00:01, 241.06it/s]\u001b[A\n"," 98% 11585/11873 [00:46<00:01, 244.76it/s]\u001b[A\n"," 98% 11610/11873 [00:46<00:01, 243.15it/s]\u001b[A\n"," 98% 11635/11873 [00:46<00:00, 243.98it/s]\u001b[A\n"," 98% 11661/11873 [00:46<00:00, 246.20it/s]\u001b[A\n"," 98% 11686/11873 [00:46<00:00, 243.76it/s]\u001b[A\n"," 99% 11711/11873 [00:46<00:00, 240.09it/s]\u001b[A\n"," 99% 11736/11873 [00:46<00:00, 237.78it/s]\u001b[A\n"," 99% 11761/11873 [00:47<00:00, 241.16it/s]\u001b[A\n"," 99% 11787/11873 [00:47<00:00, 244.96it/s]\u001b[A\n"," 99% 11813/11873 [00:47<00:00, 248.49it/s]\u001b[A\n","100% 11838/11873 [00:47<00:00, 238.94it/s]\u001b[A\n","100% 11873/11873 [00:47<00:00, 249.73it/s]\n","04/01/2022 19:08:38 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","04/01/2022 19:08:38 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","04/01/2022 19:08:41 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","04/01/2022 19:08:45 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1496/1496 [05:33<00:00,  4.48it/s]\n","***** eval metrics *****\n","  epoch                  =     3.0\n","  eval_HasAns_exact      =  75.776\n","  eval_HasAns_f1         = 82.1131\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 79.6131\n","  eval_NoAns_f1          = 79.6131\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 77.7057\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 80.8698\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 77.6973\n","  eval_f1                = 80.8613\n","  eval_samples           =   11968\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-01 19:08:45,691 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_back_trans_possib_aug', 'type': 'sichenzhong/squad_v2_back_trans_possib_aug', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path roberta-base \\\n","  --dataset_name sichenzhong/squad_v2_back_trans_possib_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug"],"metadata":{"id":"ilFZFajKUPsU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649310083551,"user_tz":240,"elapsed":6195165,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"7ffcadad-8fdc-4f9f-b85c-b26d687ecfd1"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["04/07/2022 03:58:11 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/07/2022 03:58:11 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/runs/Apr07_03-58-11_fab11cd6bda9,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/07/2022 03:58:13 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef\n","04/07/2022 03:58:13 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/07/2022 03:58:13 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","04/07/2022 03:58:13 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","04/07/2022 03:58:13 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","100% 2/2 [00:00<00:00, 654.75it/s]\n","[INFO|configuration_utils.py:654] 2022-04-07 03:58:13,638 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-07 03:58:13,639 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-07 03:58:14,000 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-07 03:58:14,361 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-07 03:58:14,361 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 03:58:16,894 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 03:58:16,894 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 03:58:16,894 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 03:58:16,894 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 03:58:16,894 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 03:58:16,894 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-07 03:58:17,254 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-07 03:58:17,255 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|modeling_utils.py:1772] 2022-04-07 03:58:17,715 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[WARNING|modeling_utils.py:2049] 2022-04-07 03:58:19,030 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-04-07 03:58:19,030 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/07/2022 03:58:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-d39c4aa9fa4b0368.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:43<00:00,  3.02ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/07/2022 03:59:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_possib_aug-1020e03d86b006ef/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-5655de31895ded30.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:08<00:00,  5.72s/ba]\n","04/07/2022 04:00:11 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpbja7697p\n","Downloading builder script: 6.46kB [00:00, 4.38MB/s]       \n","04/07/2022 04:00:11 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/07/2022 04:00:11 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/07/2022 04:00:11 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpqwl29h91\n","Downloading extra modules: 11.3kB [00:00, 9.62MB/s]       \n","04/07/2022 04:00:11 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","04/07/2022 04:00:11 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-07 04:00:24,558 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-07 04:00:24,558 >>   Num examples = 131814\n","[INFO|trainer.py:1292] 2022-04-07 04:00:24,558 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-07 04:00:24,558 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1294] 2022-04-07 04:00:24,558 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1295] 2022-04-07 04:00:24,558 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-07 04:00:24,558 >>   Total optimization steps = 10986\n","{'loss': 1.8667, 'learning_rate': 3.8179501183324234e-05, 'epoch': 0.09}\n","  5% 500/10986 [04:23<1:32:14,  1.89it/s][INFO|trainer.py:2166] 2022-04-07 04:04:47,798 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:04:47,804 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:04:49,186 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:04:49,191 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:04:49,194 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 1.3299, 'learning_rate': 3.6359002366648465e-05, 'epoch': 0.18}\n","  9% 1000/10986 [08:54<1:28:03,  1.89it/s][INFO|trainer.py:2166] 2022-04-07 04:09:18,659 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-07 04:09:18,664 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:09:20,027 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:09:20,031 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:09:20,035 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.2061, 'learning_rate': 3.4538503549972695e-05, 'epoch': 0.27}\n"," 14% 1500/10986 [13:25<1:23:20,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:13:50,062 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:13:50,067 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:13:51,411 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:13:51,415 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:13:51,419 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.1408, 'learning_rate': 3.2718004733296926e-05, 'epoch': 0.36}\n"," 18% 2000/10986 [17:56<1:18:42,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:18:20,875 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-07 04:18:20,882 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:18:22,220 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:18:22,224 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:18:22,228 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.0852, 'learning_rate': 3.0897505916621156e-05, 'epoch': 0.46}\n"," 23% 2500/10986 [22:27<1:14:37,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:22:52,058 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:22:52,063 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:22:53,412 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:22:53,417 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:22:53,420 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.0711, 'learning_rate': 2.9077007099945387e-05, 'epoch': 0.55}\n"," 27% 3000/10986 [26:56<1:10:06,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:27:21,076 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-07 04:27:21,081 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:27:22,420 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:27:22,424 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:27:22,427 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.0339, 'learning_rate': 2.7256508283269618e-05, 'epoch': 0.64}\n"," 32% 3500/10986 [31:25<1:05:44,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:31:50,077 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:31:50,083 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:31:51,417 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:31:51,421 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:31:51,425 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.0056, 'learning_rate': 2.543600946659385e-05, 'epoch': 0.73}\n"," 36% 4000/10986 [35:54<1:01:10,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:36:18,677 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-07 04:36:18,682 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:36:20,013 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:36:20,862 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:36:21,030 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 0.9929, 'learning_rate': 2.3615510649918082e-05, 'epoch': 0.82}\n"," 41% 4500/10986 [40:22<56:56,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:40:47,345 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:40:47,368 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:40:48,685 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:40:48,689 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:40:48,692 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.9608, 'learning_rate': 2.1795011833242313e-05, 'epoch': 0.91}\n"," 46% 5000/10986 [44:49<52:29,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:45:14,490 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-07 04:45:14,496 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:45:15,804 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:45:15,809 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:45:15,812 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.9635, 'learning_rate': 1.997451301656654e-05, 'epoch': 1.0}\n"," 50% 5500/10986 [49:17<47:17,  1.93it/s][INFO|trainer.py:2166] 2022-04-07 04:49:41,794 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:49:41,799 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:49:43,130 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:49:43,134 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:49:43,137 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.7485, 'learning_rate': 1.8154014199890774e-05, 'epoch': 1.09}\n"," 55% 6000/10986 [53:44<43:38,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:54:09,494 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-07 04:54:09,499 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:54:10,798 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:54:10,802 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:54:10,805 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.7247, 'learning_rate': 1.6333515383215e-05, 'epoch': 1.18}\n"," 59% 6500/10986 [58:12<39:17,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 04:58:36,949 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-07 04:58:36,954 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 04:58:38,275 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 04:58:38,279 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 04:58:38,283 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.7222, 'learning_rate': 1.4513016566539234e-05, 'epoch': 1.27}\n"," 64% 7000/10986 [1:02:42<34:57,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 05:03:07,445 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-07 05:03:07,450 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:03:08,798 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:03:08,802 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:03:08,806 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.7235, 'learning_rate': 1.2692517749863464e-05, 'epoch': 1.37}\n"," 68% 7500/10986 [1:07:10<30:33,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 05:07:35,528 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-07 05:07:35,534 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:07:36,952 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:07:36,958 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:07:36,965 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.6907, 'learning_rate': 1.0872018933187693e-05, 'epoch': 1.46}\n"," 73% 8000/10986 [1:11:41<26:09,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 05:12:06,094 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-07 05:12:06,101 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:12:07,390 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:12:07,395 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:12:07,399 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.691, 'learning_rate': 9.051520116511924e-06, 'epoch': 1.55}\n"," 77% 8500/10986 [1:16:11<21:52,  1.89it/s][INFO|trainer.py:2166] 2022-04-07 05:16:36,275 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-07 05:16:36,281 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:16:37,561 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:16:37,565 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:16:37,569 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.6877, 'learning_rate': 7.231021299836156e-06, 'epoch': 1.64}\n"," 82% 9000/10986 [1:20:41<17:21,  1.91it/s][INFO|trainer.py:2166] 2022-04-07 05:21:06,256 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-07 05:21:06,261 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:21:07,520 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:21:07,524 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:21:07,528 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.6805, 'learning_rate': 5.410522483160386e-06, 'epoch': 1.73}\n"," 86% 9500/10986 [1:25:10<13:01,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 05:25:35,152 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-07 05:25:35,157 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:25:36,437 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:25:36,442 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:25:36,446 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.6883, 'learning_rate': 3.5900236664846172e-06, 'epoch': 1.82}\n"," 91% 10000/10986 [1:29:39<08:40,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 05:30:03,621 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-07 05:30:03,626 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:30:04,914 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:30:04,918 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:30:04,922 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6756, 'learning_rate': 1.7695248498088476e-06, 'epoch': 1.91}\n"," 96% 10500/10986 [1:34:08<04:15,  1.90it/s][INFO|trainer.py:2166] 2022-04-07 05:34:32,797 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-07 05:34:32,803 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:34:34,058 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:34:34,063 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:34:34,066 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/checkpoint-10500/special_tokens_map.json\n","100% 10986/10986 [1:38:28<00:00,  2.41it/s][INFO|trainer.py:1530] 2022-04-07 05:38:53,431 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 5908.8728, 'train_samples_per_second': 44.616, 'train_steps_per_second': 1.859, 'train_loss': 0.9258325848665346, 'epoch': 2.0}\n","100% 10986/10986 [1:38:28<00:00,  1.86it/s]\n","[INFO|trainer.py:2166] 2022-04-07 05:38:53,436 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug\n","[INFO|configuration_utils.py:441] 2022-04-07 05:38:53,441 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 05:38:54,730 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 05:38:55,676 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 05:38:55,680 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.9258\n","  train_runtime            = 1:38:28.87\n","  train_samples            =     131814\n","  train_samples_per_second =     44.616\n","  train_steps_per_second   =      1.859\n","04/07/2022 05:38:55 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-07 05:38:55,806 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-07 05:38:55,809 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-07 05:38:55,809 >>   Num examples = 12165\n","[INFO|trainer.py:2421] 2022-04-07 05:38:55,809 >>   Batch size = 8\n","100% 1520/1521 [01:34<00:00, 16.13it/s]04/07/2022 05:40:41 - INFO - utils_qa - Post-processing 11873 example predictions split into 12165 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 43/11873 [00:00<00:27, 424.45it/s]\u001b[A\n","  1% 86/11873 [00:00<00:29, 400.93it/s]\u001b[A\n","  1% 130/11873 [00:00<00:28, 415.64it/s]\u001b[A\n","  2% 179/11873 [00:00<00:26, 442.64it/s]\u001b[A\n","  2% 226/11873 [00:00<00:25, 451.62it/s]\u001b[A\n","  2% 273/11873 [00:00<00:25, 454.87it/s]\u001b[A\n","  3% 321/11873 [00:00<00:25, 460.57it/s]\u001b[A\n","  3% 368/11873 [00:00<00:25, 459.17it/s]\u001b[A\n","  4% 419/11873 [00:00<00:24, 471.71it/s]\u001b[A\n","  4% 467/11873 [00:01<00:24, 473.16it/s]\u001b[A\n","  4% 515/11873 [00:01<00:24, 470.64it/s]\u001b[A\n","  5% 563/11873 [00:01<00:24, 461.46it/s]\u001b[A\n","  5% 610/11873 [00:01<00:24, 463.22it/s]\u001b[A\n","  6% 657/11873 [00:01<00:24, 463.25it/s]\u001b[A\n","  6% 705/11873 [00:01<00:23, 466.96it/s]\u001b[A\n","  6% 752/11873 [00:01<00:24, 463.00it/s]\u001b[A\n","  7% 799/11873 [00:01<00:24, 460.03it/s]\u001b[A\n","  7% 846/11873 [00:01<00:24, 457.96it/s]\u001b[A\n","  8% 896/11873 [00:01<00:23, 470.20it/s]\u001b[A\n","  8% 945/11873 [00:02<00:22, 475.61it/s]\u001b[A\n","  8% 993/11873 [00:02<00:23, 462.27it/s]\u001b[A\n","  9% 1040/11873 [00:02<00:25, 426.21it/s]\u001b[A\n","  9% 1084/11873 [00:02<00:26, 401.49it/s]\u001b[A\n","  9% 1125/11873 [00:02<00:27, 388.48it/s]\u001b[A\n"," 10% 1165/11873 [00:02<00:28, 376.55it/s]\u001b[A\n"," 10% 1203/11873 [00:02<00:28, 374.91it/s]\u001b[A\n","100% 1521/1521 [01:48<00:00, 16.13it/s]\n"," 11% 1278/11873 [00:02<00:28, 366.98it/s]\u001b[A\n"," 11% 1316/11873 [00:03<00:28, 368.28it/s]\u001b[A\n"," 11% 1353/11873 [00:03<00:28, 366.12it/s]\u001b[A\n"," 12% 1390/11873 [00:03<00:28, 366.87it/s]\u001b[A\n"," 12% 1427/11873 [00:03<00:28, 363.73it/s]\u001b[A\n"," 12% 1464/11873 [00:03<00:29, 353.65it/s]\u001b[A\n"," 13% 1500/11873 [00:03<00:29, 355.33it/s]\u001b[A\n"," 13% 1536/11873 [00:03<00:28, 356.59it/s]\u001b[A\n"," 13% 1572/11873 [00:03<00:29, 354.66it/s]\u001b[A\n"," 14% 1608/11873 [00:03<00:28, 354.45it/s]\u001b[A\n"," 14% 1644/11873 [00:03<00:28, 354.80it/s]\u001b[A\n"," 14% 1681/11873 [00:04<00:28, 356.94it/s]\u001b[A\n"," 14% 1717/11873 [00:04<00:28, 356.01it/s]\u001b[A\n"," 15% 1753/11873 [00:04<00:28, 356.90it/s]\u001b[A\n"," 15% 1789/11873 [00:04<00:28, 353.28it/s]\u001b[A\n"," 15% 1825/11873 [00:04<00:28, 352.18it/s]\u001b[A\n"," 16% 1862/11873 [00:04<00:28, 355.63it/s]\u001b[A\n"," 16% 1899/11873 [00:04<00:27, 358.01it/s]\u001b[A\n"," 16% 1936/11873 [00:04<00:27, 360.97it/s]\u001b[A\n"," 17% 1973/11873 [00:04<00:27, 361.53it/s]\u001b[A\n"," 17% 2011/11873 [00:05<00:27, 365.25it/s]\u001b[A\n"," 17% 2048/11873 [00:05<00:26, 364.77it/s]\u001b[A\n"," 18% 2085/11873 [00:05<00:26, 363.27it/s]\u001b[A\n"," 18% 2122/11873 [00:05<00:26, 362.08it/s]\u001b[A\n"," 18% 2159/11873 [00:05<00:26, 362.78it/s]\u001b[A\n"," 18% 2196/11873 [00:05<00:27, 354.31it/s]\u001b[A\n"," 19% 2233/11873 [00:05<00:27, 356.40it/s]\u001b[A\n"," 19% 2271/11873 [00:05<00:26, 360.29it/s]\u001b[A\n"," 19% 2308/11873 [00:05<00:26, 360.23it/s]\u001b[A\n"," 20% 2345/11873 [00:05<00:26, 358.70it/s]\u001b[A\n"," 20% 2383/11873 [00:06<00:26, 363.17it/s]\u001b[A\n"," 20% 2420/11873 [00:06<00:26, 360.61it/s]\u001b[A\n"," 21% 2457/11873 [00:06<00:26, 355.10it/s]\u001b[A\n"," 21% 2493/11873 [00:06<00:26, 353.32it/s]\u001b[A\n"," 21% 2529/11873 [00:06<00:26, 349.76it/s]\u001b[A\n"," 22% 2565/11873 [00:06<00:26, 352.08it/s]\u001b[A\n"," 22% 2602/11873 [00:06<00:26, 354.87it/s]\u001b[A\n"," 22% 2639/11873 [00:06<00:25, 358.83it/s]\u001b[A\n"," 23% 2675/11873 [00:06<00:25, 357.98it/s]\u001b[A\n"," 23% 2712/11873 [00:06<00:25, 359.21it/s]\u001b[A\n"," 23% 2748/11873 [00:07<00:25, 358.08it/s]\u001b[A\n"," 23% 2784/11873 [00:07<00:25, 355.11it/s]\u001b[A\n"," 24% 2822/11873 [00:07<00:24, 362.28it/s]\u001b[A\n"," 24% 2859/11873 [00:07<00:24, 362.21it/s]\u001b[A\n"," 24% 2896/11873 [00:07<00:25, 357.07it/s]\u001b[A\n"," 25% 2933/11873 [00:07<00:24, 358.79it/s]\u001b[A\n"," 25% 2969/11873 [00:07<00:24, 357.48it/s]\u001b[A\n"," 25% 3005/11873 [00:07<00:25, 353.99it/s]\u001b[A\n"," 26% 3041/11873 [00:07<00:25, 350.28it/s]\u001b[A\n"," 26% 3077/11873 [00:07<00:25, 347.20it/s]\u001b[A\n"," 26% 3112/11873 [00:08<00:26, 324.81it/s]\u001b[A\n"," 26% 3145/11873 [00:08<00:30, 284.65it/s]\u001b[A\n"," 27% 3175/11873 [00:08<00:31, 277.65it/s]\u001b[A\n"," 27% 3210/11873 [00:08<00:29, 295.79it/s]\u001b[A\n"," 27% 3246/11873 [00:08<00:27, 311.57it/s]\u001b[A\n"," 28% 3278/11873 [00:08<00:30, 285.46it/s]\u001b[A\n"," 28% 3308/11873 [00:08<00:37, 225.84it/s]\u001b[A\n"," 28% 3333/11873 [00:09<00:40, 208.32it/s]\u001b[A\n"," 28% 3356/11873 [00:09<00:43, 195.98it/s]\u001b[A\n"," 28% 3381/11873 [00:09<00:40, 208.33it/s]\u001b[A\n"," 29% 3417/11873 [00:09<00:34, 245.34it/s]\u001b[A\n"," 29% 3452/11873 [00:09<00:30, 272.34it/s]\u001b[A\n"," 29% 3486/11873 [00:09<00:28, 290.04it/s]\u001b[A\n"," 30% 3522/11873 [00:09<00:26, 309.37it/s]\u001b[A\n"," 30% 3560/11873 [00:09<00:25, 327.86it/s]\u001b[A\n"," 30% 3595/11873 [00:09<00:24, 334.20it/s]\u001b[A\n"," 31% 3633/11873 [00:10<00:23, 345.58it/s]\u001b[A\n"," 31% 3668/11873 [00:10<00:23, 345.68it/s]\u001b[A\n"," 31% 3703/11873 [00:10<00:23, 344.07it/s]\u001b[A\n"," 31% 3739/11873 [00:10<00:23, 346.58it/s]\u001b[A\n"," 32% 3774/11873 [00:10<00:23, 343.65it/s]\u001b[A\n"," 32% 3810/11873 [00:10<00:23, 346.73it/s]\u001b[A\n"," 32% 3845/11873 [00:10<00:24, 325.10it/s]\u001b[A\n"," 33% 3881/11873 [00:10<00:24, 332.28it/s]\u001b[A\n"," 33% 3915/11873 [00:10<00:24, 325.58it/s]\u001b[A\n"," 33% 3948/11873 [00:10<00:24, 320.30it/s]\u001b[A\n"," 34% 3982/11873 [00:11<00:24, 325.73it/s]\u001b[A\n"," 34% 4019/11873 [00:11<00:23, 336.53it/s]\u001b[A\n"," 34% 4055/11873 [00:11<00:22, 343.23it/s]\u001b[A\n"," 34% 4090/11873 [00:11<00:22, 345.14it/s]\u001b[A\n"," 35% 4126/11873 [00:11<00:22, 349.43it/s]\u001b[A\n"," 35% 4161/11873 [00:11<00:23, 322.25it/s]\u001b[A\n"," 35% 4195/11873 [00:11<00:23, 325.05it/s]\u001b[A\n"," 36% 4230/11873 [00:11<00:23, 330.59it/s]\u001b[A\n"," 36% 4266/11873 [00:11<00:22, 337.16it/s]\u001b[A\n"," 36% 4302/11873 [00:12<00:22, 341.08it/s]\u001b[A\n"," 37% 4339/11873 [00:12<00:21, 348.56it/s]\u001b[A\n"," 37% 4375/11873 [00:12<00:21, 351.81it/s]\u001b[A\n"," 37% 4411/11873 [00:12<00:21, 342.95it/s]\u001b[A\n"," 37% 4446/11873 [00:12<00:25, 287.03it/s]\u001b[A\n"," 38% 4483/11873 [00:12<00:24, 307.70it/s]\u001b[A\n"," 38% 4518/11873 [00:12<00:23, 317.86it/s]\u001b[A\n"," 38% 4554/11873 [00:12<00:22, 328.11it/s]\u001b[A\n"," 39% 4591/11873 [00:12<00:21, 338.11it/s]\u001b[A\n"," 39% 4627/11873 [00:13<00:21, 341.60it/s]\u001b[A\n"," 39% 4662/11873 [00:13<00:21, 340.03it/s]\u001b[A\n"," 40% 4698/11873 [00:13<00:20, 344.92it/s]\u001b[A\n"," 40% 4733/11873 [00:13<00:20, 345.29it/s]\u001b[A\n"," 40% 4768/11873 [00:13<00:20, 344.15it/s]\u001b[A\n"," 40% 4803/11873 [00:13<00:20, 338.46it/s]\u001b[A\n"," 41% 4837/11873 [00:13<00:20, 336.75it/s]\u001b[A\n"," 41% 4872/11873 [00:13<00:20, 339.78it/s]\u001b[A\n"," 41% 4907/11873 [00:13<00:20, 340.08it/s]\u001b[A\n"," 42% 4943/11873 [00:13<00:20, 345.39it/s]\u001b[A\n"," 42% 4979/11873 [00:14<00:19, 349.09it/s]\u001b[A\n"," 42% 5014/11873 [00:14<00:19, 348.20it/s]\u001b[A\n"," 43% 5049/11873 [00:14<00:19, 348.15it/s]\u001b[A\n"," 43% 5084/11873 [00:14<00:19, 347.70it/s]\u001b[A\n"," 43% 5120/11873 [00:14<00:19, 348.72it/s]\u001b[A\n"," 43% 5155/11873 [00:14<00:19, 344.80it/s]\u001b[A\n"," 44% 5191/11873 [00:14<00:19, 347.35it/s]\u001b[A\n"," 44% 5228/11873 [00:14<00:18, 353.22it/s]\u001b[A\n"," 44% 5264/11873 [00:14<00:20, 326.17it/s]\u001b[A\n"," 45% 5299/11873 [00:14<00:19, 332.54it/s]\u001b[A\n"," 45% 5334/11873 [00:15<00:19, 336.31it/s]\u001b[A\n"," 45% 5371/11873 [00:15<00:18, 345.07it/s]\u001b[A\n"," 46% 5408/11873 [00:15<00:18, 349.86it/s]\u001b[A\n"," 46% 5444/11873 [00:15<00:18, 351.61it/s]\u001b[A\n"," 46% 5480/11873 [00:15<00:18, 346.18it/s]\u001b[A\n"," 46% 5517/11873 [00:15<00:18, 352.22it/s]\u001b[A\n"," 47% 5553/11873 [00:15<00:18, 350.44it/s]\u001b[A\n"," 47% 5590/11873 [00:15<00:17, 355.10it/s]\u001b[A\n"," 47% 5626/11873 [00:15<00:17, 354.80it/s]\u001b[A\n"," 48% 5662/11873 [00:16<00:17, 346.64it/s]\u001b[A\n"," 48% 5697/11873 [00:16<00:17, 346.57it/s]\u001b[A\n"," 48% 5732/11873 [00:16<00:17, 343.84it/s]\u001b[A\n"," 49% 5767/11873 [00:16<00:17, 343.53it/s]\u001b[A\n"," 49% 5804/11873 [00:16<00:17, 349.02it/s]\u001b[A\n"," 49% 5840/11873 [00:16<00:17, 351.94it/s]\u001b[A\n"," 50% 5878/11873 [00:16<00:16, 358.11it/s]\u001b[A\n"," 50% 5915/11873 [00:16<00:16, 360.57it/s]\u001b[A\n"," 50% 5952/11873 [00:16<00:16, 361.67it/s]\u001b[A\n"," 50% 5989/11873 [00:16<00:16, 361.53it/s]\u001b[A\n"," 51% 6026/11873 [00:17<00:16, 360.48it/s]\u001b[A\n"," 51% 6063/11873 [00:17<00:16, 359.91it/s]\u001b[A\n"," 51% 6099/11873 [00:17<00:16, 356.82it/s]\u001b[A\n"," 52% 6135/11873 [00:17<00:16, 356.90it/s]\u001b[A\n"," 52% 6171/11873 [00:17<00:16, 354.55it/s]\u001b[A\n"," 52% 6207/11873 [00:17<00:16, 352.96it/s]\u001b[A\n"," 53% 6243/11873 [00:17<00:15, 355.00it/s]\u001b[A\n"," 53% 6280/11873 [00:17<00:15, 358.16it/s]\u001b[A\n"," 53% 6317/11873 [00:17<00:15, 358.76it/s]\u001b[A\n"," 54% 6353/11873 [00:17<00:15, 356.14it/s]\u001b[A\n"," 54% 6389/11873 [00:18<00:15, 356.92it/s]\u001b[A\n"," 54% 6425/11873 [00:18<00:15, 355.65it/s]\u001b[A\n"," 54% 6461/11873 [00:18<00:15, 352.99it/s]\u001b[A\n"," 55% 6497/11873 [00:18<00:15, 344.78it/s]\u001b[A\n"," 55% 6532/11873 [00:18<00:15, 345.62it/s]\u001b[A\n"," 55% 6567/11873 [00:18<00:15, 343.95it/s]\u001b[A\n"," 56% 6603/11873 [00:18<00:15, 346.81it/s]\u001b[A\n"," 56% 6640/11873 [00:18<00:14, 352.16it/s]\u001b[A\n"," 56% 6677/11873 [00:18<00:14, 356.97it/s]\u001b[A\n"," 57% 6713/11873 [00:19<00:15, 323.90it/s]\u001b[A\n"," 57% 6748/11873 [00:19<00:15, 330.71it/s]\u001b[A\n"," 57% 6784/11873 [00:19<00:15, 338.11it/s]\u001b[A\n"," 57% 6819/11873 [00:19<00:14, 341.11it/s]\u001b[A\n"," 58% 6855/11873 [00:19<00:14, 343.90it/s]\u001b[A\n"," 58% 6891/11873 [00:19<00:14, 346.33it/s]\u001b[A\n"," 58% 6927/11873 [00:19<00:14, 349.94it/s]\u001b[A\n"," 59% 6964/11873 [00:19<00:13, 353.86it/s]\u001b[A\n"," 59% 7000/11873 [00:19<00:13, 354.05it/s]\u001b[A\n"," 59% 7036/11873 [00:19<00:13, 352.97it/s]\u001b[A\n"," 60% 7072/11873 [00:20<00:13, 353.19it/s]\u001b[A\n"," 60% 7109/11873 [00:20<00:13, 355.45it/s]\u001b[A\n"," 60% 7145/11873 [00:20<00:13, 354.91it/s]\u001b[A\n"," 60% 7181/11873 [00:20<00:13, 354.16it/s]\u001b[A\n"," 61% 7218/11873 [00:20<00:13, 356.06it/s]\u001b[A\n"," 61% 7254/11873 [00:20<00:13, 350.83it/s]\u001b[A\n"," 61% 7290/11873 [00:20<00:13, 352.05it/s]\u001b[A\n"," 62% 7326/11873 [00:20<00:12, 354.08it/s]\u001b[A\n"," 62% 7363/11873 [00:20<00:12, 356.68it/s]\u001b[A\n"," 62% 7399/11873 [00:20<00:12, 356.63it/s]\u001b[A\n"," 63% 7435/11873 [00:21<00:13, 336.91it/s]\u001b[A\n"," 63% 7472/11873 [00:21<00:12, 344.00it/s]\u001b[A\n"," 63% 7508/11873 [00:21<00:12, 348.05it/s]\u001b[A\n"," 64% 7544/11873 [00:21<00:12, 351.26it/s]\u001b[A\n"," 64% 7580/11873 [00:21<00:12, 348.54it/s]\u001b[A\n"," 64% 7615/11873 [00:21<00:12, 345.18it/s]\u001b[A\n"," 64% 7651/11873 [00:21<00:12, 348.06it/s]\u001b[A\n"," 65% 7687/11873 [00:21<00:11, 349.40it/s]\u001b[A\n"," 65% 7722/11873 [00:21<00:12, 326.20it/s]\u001b[A\n"," 65% 7757/11873 [00:22<00:12, 332.56it/s]\u001b[A\n"," 66% 7792/11873 [00:22<00:12, 335.10it/s]\u001b[A\n"," 66% 7828/11873 [00:22<00:11, 339.84it/s]\u001b[A\n"," 66% 7864/11873 [00:22<00:11, 342.96it/s]\u001b[A\n"," 67% 7899/11873 [00:22<00:12, 324.28it/s]\u001b[A\n"," 67% 7935/11873 [00:22<00:11, 334.19it/s]\u001b[A\n"," 67% 7971/11873 [00:22<00:11, 338.79it/s]\u001b[A\n"," 67% 8009/11873 [00:22<00:11, 348.59it/s]\u001b[A\n"," 68% 8045/11873 [00:22<00:10, 349.65it/s]\u001b[A\n"," 68% 8082/11873 [00:22<00:10, 353.84it/s]\u001b[A\n"," 68% 8119/11873 [00:23<00:10, 354.14it/s]\u001b[A\n"," 69% 8155/11873 [00:23<00:10, 354.36it/s]\u001b[A\n"," 69% 8191/11873 [00:23<00:10, 352.21it/s]\u001b[A\n"," 69% 8227/11873 [00:23<00:10, 352.13it/s]\u001b[A\n"," 70% 8265/11873 [00:23<00:10, 357.81it/s]\u001b[A\n"," 70% 8301/11873 [00:23<00:10, 354.94it/s]\u001b[A\n"," 70% 8337/11873 [00:23<00:09, 356.30it/s]\u001b[A\n"," 71% 8375/11873 [00:23<00:09, 360.71it/s]\u001b[A\n"," 71% 8412/11873 [00:23<00:09, 359.98it/s]\u001b[A\n"," 71% 8449/11873 [00:23<00:09, 356.38it/s]\u001b[A\n"," 71% 8485/11873 [00:24<00:09, 353.19it/s]\u001b[A\n"," 72% 8522/11873 [00:24<00:09, 357.65it/s]\u001b[A\n"," 72% 8558/11873 [00:24<00:09, 356.51it/s]\u001b[A\n"," 72% 8594/11873 [00:24<00:09, 357.24it/s]\u001b[A\n"," 73% 8630/11873 [00:24<00:09, 356.51it/s]\u001b[A\n"," 73% 8666/11873 [00:24<00:08, 357.08it/s]\u001b[A\n"," 73% 8703/11873 [00:24<00:08, 360.45it/s]\u001b[A\n"," 74% 8740/11873 [00:24<00:08, 361.81it/s]\u001b[A\n"," 74% 8777/11873 [00:24<00:08, 358.30it/s]\u001b[A\n"," 74% 8813/11873 [00:24<00:08, 355.88it/s]\u001b[A\n"," 75% 8849/11873 [00:25<00:08, 355.16it/s]\u001b[A\n"," 75% 8886/11873 [00:25<00:08, 356.64it/s]\u001b[A\n"," 75% 8923/11873 [00:25<00:08, 358.20it/s]\u001b[A\n"," 75% 8960/11873 [00:25<00:08, 358.58it/s]\u001b[A\n"," 76% 8997/11873 [00:25<00:07, 360.64it/s]\u001b[A\n"," 76% 9034/11873 [00:25<00:07, 356.36it/s]\u001b[A\n"," 76% 9071/11873 [00:25<00:07, 358.31it/s]\u001b[A\n"," 77% 9108/11873 [00:25<00:07, 358.94it/s]\u001b[A\n"," 77% 9144/11873 [00:25<00:07, 358.92it/s]\u001b[A\n"," 77% 9181/11873 [00:26<00:07, 361.49it/s]\u001b[A\n"," 78% 9218/11873 [00:26<00:07, 360.82it/s]\u001b[A\n"," 78% 9255/11873 [00:26<00:07, 356.23it/s]\u001b[A\n"," 78% 9291/11873 [00:26<00:07, 355.70it/s]\u001b[A\n"," 79% 9327/11873 [00:26<00:07, 356.88it/s]\u001b[A\n"," 79% 9363/11873 [00:26<00:07, 354.72it/s]\u001b[A\n"," 79% 9399/11873 [00:26<00:06, 356.13it/s]\u001b[A\n"," 79% 9436/11873 [00:26<00:06, 357.87it/s]\u001b[A\n"," 80% 9474/11873 [00:26<00:06, 362.77it/s]\u001b[A\n"," 80% 9511/11873 [00:26<00:06, 357.96it/s]\u001b[A\n"," 80% 9548/11873 [00:27<00:06, 360.05it/s]\u001b[A\n"," 81% 9585/11873 [00:27<00:06, 361.04it/s]\u001b[A\n"," 81% 9622/11873 [00:27<00:06, 361.47it/s]\u001b[A\n"," 81% 9659/11873 [00:27<00:06, 359.85it/s]\u001b[A\n"," 82% 9697/11873 [00:27<00:05, 363.29it/s]\u001b[A\n"," 82% 9734/11873 [00:27<00:05, 364.23it/s]\u001b[A\n"," 82% 9771/11873 [00:27<00:05, 365.19it/s]\u001b[A\n"," 83% 9808/11873 [00:27<00:05, 362.39it/s]\u001b[A\n"," 83% 9845/11873 [00:27<00:05, 364.30it/s]\u001b[A\n"," 83% 9882/11873 [00:27<00:05, 355.83it/s]\u001b[A\n"," 84% 9918/11873 [00:28<00:05, 353.87it/s]\u001b[A\n"," 84% 9955/11873 [00:28<00:05, 356.14it/s]\u001b[A\n"," 84% 9992/11873 [00:28<00:05, 360.10it/s]\u001b[A\n"," 84% 10030/11873 [00:28<00:05, 361.60it/s]\u001b[A\n"," 85% 10067/11873 [00:28<00:05, 356.32it/s]\u001b[A\n"," 85% 10103/11873 [00:28<00:04, 356.77it/s]\u001b[A\n"," 85% 10139/11873 [00:28<00:04, 355.60it/s]\u001b[A\n"," 86% 10176/11873 [00:28<00:04, 356.65it/s]\u001b[A\n"," 86% 10212/11873 [00:28<00:04, 353.09it/s]\u001b[A\n"," 86% 10248/11873 [00:28<00:04, 354.91it/s]\u001b[A\n"," 87% 10284/11873 [00:29<00:04, 354.22it/s]\u001b[A\n"," 87% 10321/11873 [00:29<00:04, 356.25it/s]\u001b[A\n"," 87% 10359/11873 [00:29<00:04, 361.66it/s]\u001b[A\n"," 88% 10396/11873 [00:29<00:04, 361.76it/s]\u001b[A\n"," 88% 10433/11873 [00:29<00:04, 341.98it/s]\u001b[A\n"," 88% 10469/11873 [00:29<00:04, 345.14it/s]\u001b[A\n"," 88% 10505/11873 [00:29<00:03, 347.23it/s]\u001b[A\n"," 89% 10542/11873 [00:29<00:03, 352.38it/s]\u001b[A\n"," 89% 10578/11873 [00:29<00:03, 353.25it/s]\u001b[A\n"," 89% 10614/11873 [00:30<00:03, 349.91it/s]\u001b[A\n"," 90% 10650/11873 [00:30<00:03, 342.02it/s]\u001b[A\n"," 90% 10686/11873 [00:30<00:03, 346.16it/s]\u001b[A\n"," 90% 10721/11873 [00:30<00:03, 346.76it/s]\u001b[A\n"," 91% 10756/11873 [00:30<00:03, 346.13it/s]\u001b[A\n"," 91% 10792/11873 [00:30<00:03, 347.67it/s]\u001b[A\n"," 91% 10827/11873 [00:30<00:03, 324.24it/s]\u001b[A\n"," 91% 10863/11873 [00:30<00:03, 333.90it/s]\u001b[A\n"," 92% 10900/11873 [00:30<00:02, 342.49it/s]\u001b[A\n"," 92% 10935/11873 [00:30<00:02, 337.90it/s]\u001b[A\n"," 92% 10971/11873 [00:31<00:02, 343.28it/s]\u001b[A\n"," 93% 11008/11873 [00:31<00:02, 348.12it/s]\u001b[A\n"," 93% 11046/11873 [00:31<00:02, 353.61it/s]\u001b[A\n"," 93% 11084/11873 [00:31<00:02, 359.87it/s]\u001b[A\n"," 94% 11121/11873 [00:31<00:02, 358.86it/s]\u001b[A\n"," 94% 11157/11873 [00:31<00:02, 353.43it/s]\u001b[A\n"," 94% 11194/11873 [00:31<00:01, 355.96it/s]\u001b[A\n"," 95% 11230/11873 [00:31<00:01, 356.84it/s]\u001b[A\n"," 95% 11266/11873 [00:31<00:01, 354.81it/s]\u001b[A\n"," 95% 11303/11873 [00:32<00:01, 358.94it/s]\u001b[A\n"," 96% 11339/11873 [00:32<00:01, 359.04it/s]\u001b[A\n"," 96% 11375/11873 [00:32<00:01, 358.27it/s]\u001b[A\n"," 96% 11412/11873 [00:32<00:01, 359.67it/s]\u001b[A\n"," 96% 11448/11873 [00:32<00:01, 358.32it/s]\u001b[A\n"," 97% 11484/11873 [00:32<00:01, 353.08it/s]\u001b[A\n"," 97% 11520/11873 [00:32<00:01, 352.67it/s]\u001b[A\n"," 97% 11556/11873 [00:32<00:00, 351.05it/s]\u001b[A\n"," 98% 11594/11873 [00:32<00:00, 357.16it/s]\u001b[A\n"," 98% 11631/11873 [00:32<00:00, 358.70it/s]\u001b[A\n"," 98% 11668/11873 [00:33<00:00, 359.84it/s]\u001b[A\n"," 99% 11704/11873 [00:33<00:00, 354.20it/s]\u001b[A\n"," 99% 11740/11873 [00:33<00:00, 351.83it/s]\u001b[A\n"," 99% 11776/11873 [00:33<00:00, 352.19it/s]\u001b[A\n"," 99% 11812/11873 [00:33<00:00, 348.82it/s]\u001b[A\n","100% 11873/11873 [00:33<00:00, 353.20it/s]\n","04/07/2022 05:41:15 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/eval_predictions.json.\n","04/07/2022 05:41:15 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/eval_nbest_predictions.json.\n","04/07/2022 05:41:17 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-possible-aug/eval_null_odds.json.\n","04/07/2022 05:41:20 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1521/1521 [02:24<00:00, 10.49it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 78.2389\n","  eval_HasAns_f1         = 84.1001\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 80.4542\n","  eval_NoAns_f1          = 80.4542\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 79.3481\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 82.2745\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 79.3481\n","  eval_f1                = 82.2745\n","  eval_samples           =   12165\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-07 05:41:21,238 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_back_trans_possib_aug', 'type': 'sichenzhong/squad_v2_back_trans_possib_aug', 'args': 'squad_v2'}}\n"]}]}]}