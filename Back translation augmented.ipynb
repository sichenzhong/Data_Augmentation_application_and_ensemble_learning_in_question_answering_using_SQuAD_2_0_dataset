{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Back translation augmented.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bdce4b15c55c4ef7990d49663f6245e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_925a51281335405091a76b9e235ec662","IPY_MODEL_ffe0e194b477416f8da4747245d1bb3e","IPY_MODEL_cb5c673b408b4c2bb48795987f081148"],"layout":"IPY_MODEL_e5f02bf5f6d74f4dac6d33ac8a0719f9"}},"925a51281335405091a76b9e235ec662":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee766e6c4cdf43288020f546248f024c","placeholder":"​","style":"IPY_MODEL_8aa6f13b13d54cd3852e624e2cf114e4","value":"Downloading data files: 100%"}},"ffe0e194b477416f8da4747245d1bb3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7fd570470c24528a946980e9f9925c9","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9814bef10a94ecb93f64b832abb92f7","value":2}},"cb5c673b408b4c2bb48795987f081148":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04f6886921864b79ab7eee8ad81a28e8","placeholder":"​","style":"IPY_MODEL_723e37520a2d4dd499a87c0f8b8902ce","value":" 2/2 [00:00&lt;00:00, 65.33it/s]"}},"e5f02bf5f6d74f4dac6d33ac8a0719f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee766e6c4cdf43288020f546248f024c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8aa6f13b13d54cd3852e624e2cf114e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7fd570470c24528a946980e9f9925c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9814bef10a94ecb93f64b832abb92f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"04f6886921864b79ab7eee8ad81a28e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"723e37520a2d4dd499a87c0f8b8902ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20ec39a1876a4824b4210c6c393e8ad6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_09cff206da554a48a8f76efec85b6902","IPY_MODEL_7b17ba924b9d45a6a4c19c14892b3c4e","IPY_MODEL_92bee9b3d7e14117937497cad677d19c"],"layout":"IPY_MODEL_ef1d4a64cdfd47128a377030ab1602a3"}},"09cff206da554a48a8f76efec85b6902":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_998fe1650b614a3ba1d98d427d1f9970","placeholder":"​","style":"IPY_MODEL_25fdccbf85b643f090aec1c87fff9a71","value":"Extracting data files: 100%"}},"7b17ba924b9d45a6a4c19c14892b3c4e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9805029f35a2407a9eac71e6f7f21ba5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_152881a472db44dea1efe8775d4f3c37","value":2}},"92bee9b3d7e14117937497cad677d19c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc74df9fb6f94357a7d069719660959e","placeholder":"​","style":"IPY_MODEL_63e34e436b6d424d94f43cbb180aa6af","value":" 2/2 [00:00&lt;00:00, 35.19it/s]"}},"ef1d4a64cdfd47128a377030ab1602a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"998fe1650b614a3ba1d98d427d1f9970":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25fdccbf85b643f090aec1c87fff9a71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9805029f35a2407a9eac71e6f7f21ba5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"152881a472db44dea1efe8775d4f3c37":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc74df9fb6f94357a7d069719660959e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63e34e436b6d424d94f43cbb180aa6af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28c841f1499d468584824002614c6222":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2ed83ca8abbf41dcaca904e4e901f7ef","IPY_MODEL_fc7dc23f3e2b407bae75350957245032","IPY_MODEL_2a557f85e76f4d418af230439b53b105"],"layout":"IPY_MODEL_c8326e1b37f948fa9dd6bfaea97182ca"}},"2ed83ca8abbf41dcaca904e4e901f7ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6901a93ec3974ae7be1ac9a9ca1eceb5","placeholder":"​","style":"IPY_MODEL_f3fad66a43d548ecbd6692fd9ff10c31","value":"Generating train split: "}},"fc7dc23f3e2b407bae75350957245032":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_00fa726ca2124054baad0ddaa583736c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4aa5b9817d94735940fd6439cf08132","value":1}},"2a557f85e76f4d418af230439b53b105":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bba6e3d20ef14893b446cc8e2032c83f","placeholder":"​","style":"IPY_MODEL_447457a2787447ff930e0f7275e879b1","value":" 129122/0 [00:12&lt;00:00, 12368.81 examples/s]"}},"c8326e1b37f948fa9dd6bfaea97182ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6901a93ec3974ae7be1ac9a9ca1eceb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3fad66a43d548ecbd6692fd9ff10c31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00fa726ca2124054baad0ddaa583736c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"b4aa5b9817d94735940fd6439cf08132":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bba6e3d20ef14893b446cc8e2032c83f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447457a2787447ff930e0f7275e879b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbaa38f8050247a8911cc92e693c3eb5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ffebccb358c46dca5fe8d28c2badfa7","IPY_MODEL_1b36a51f9f3b4e4d8c84349f96f30ab0","IPY_MODEL_95ab54c9041846088045b45a19f6be43"],"layout":"IPY_MODEL_d922cd51605e4d69967679de64a281c6"}},"6ffebccb358c46dca5fe8d28c2badfa7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e74a8658f254a8d9b38c4d06941400e","placeholder":"​","style":"IPY_MODEL_da85d7d0a75144d48c8bbde80a17a52b","value":"Generating validation split: "}},"1b36a51f9f3b4e4d8c84349f96f30ab0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b89e56165324aaf81a9aa81d7404a4a","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb7d10d5ca1640f7ba97db91b6edeef5","value":1}},"95ab54c9041846088045b45a19f6be43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b761db2dd416461b8ee92221fa952dac","placeholder":"​","style":"IPY_MODEL_55d9ff8f5b1a4516b1b5de31d3ae00a2","value":" 11189/0 [00:01&lt;00:00, 10177.30 examples/s]"}},"d922cd51605e4d69967679de64a281c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e74a8658f254a8d9b38c4d06941400e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da85d7d0a75144d48c8bbde80a17a52b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b89e56165324aaf81a9aa81d7404a4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"fb7d10d5ca1640f7ba97db91b6edeef5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b761db2dd416461b8ee92221fa952dac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55d9ff8f5b1a4516b1b5de31d3ae00a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"770f3af7e8454966ad0a34e4257519c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1f1b8c0fb7a41d3b29e060d2eca2362","IPY_MODEL_ed8c505317f24c5092f1aa04d7adec33","IPY_MODEL_414c42cc9cea47c6b7fc92f115372b42"],"layout":"IPY_MODEL_45b89215ed11407a817a771675c1b63c"}},"c1f1b8c0fb7a41d3b29e060d2eca2362":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9ce4efece1743d39b768a014aa86f3d","placeholder":"​","style":"IPY_MODEL_d1e8c0793eb54bdba3a222e4601d4cc4","value":"100%"}},"ed8c505317f24c5092f1aa04d7adec33":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_18ed2833eca24058b026d8ade90de6de","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d6a5419e219d4e97a3642a5d38cb12a2","value":2}},"414c42cc9cea47c6b7fc92f115372b42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a3c0401549c4d4aa71d3880c795dddc","placeholder":"​","style":"IPY_MODEL_9a590335497d464b8fcd18fb4b95e184","value":" 2/2 [00:00&lt;00:00, 50.76it/s]"}},"45b89215ed11407a817a771675c1b63c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9ce4efece1743d39b768a014aa86f3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1e8c0793eb54bdba3a222e4601d4cc4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18ed2833eca24058b026d8ade90de6de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6a5419e219d4e97a3642a5d38cb12a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a3c0401549c4d4aa71d3880c795dddc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a590335497d464b8fcd18fb4b95e184":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"093786d54dae4ad9a271abdbd87509cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f82bbe5da0424c4ea65c909b1d7cb5e4","IPY_MODEL_8f6cd6a8caa44142bab12daad93ad604","IPY_MODEL_ceeb2639ddda48fe917522ddd95179b1"],"layout":"IPY_MODEL_dd3d0fbe32da45d49799ad1b180bbd97"}},"f82bbe5da0424c4ea65c909b1d7cb5e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53ba3f7953024a39815af6d207814514","placeholder":"​","style":"IPY_MODEL_00b8ce66116444cb827f066bc7142872","value":"Pushing dataset shards to the dataset hub: 100%"}},"8f6cd6a8caa44142bab12daad93ad604":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3dcbc3f77b5453daa40c11df5027736","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97bf3efbee634098a302375df70c4ed4","value":1}},"ceeb2639ddda48fe917522ddd95179b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_daf4e45c2a1647f4a98b62bb89cd3953","placeholder":"​","style":"IPY_MODEL_6e39e61dad3f4361ac12121108a7bb99","value":" 1/1 [00:06&lt;00:00,  6.83s/it]"}},"dd3d0fbe32da45d49799ad1b180bbd97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53ba3f7953024a39815af6d207814514":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00b8ce66116444cb827f066bc7142872":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3dcbc3f77b5453daa40c11df5027736":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97bf3efbee634098a302375df70c4ed4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"daf4e45c2a1647f4a98b62bb89cd3953":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e39e61dad3f4361ac12121108a7bb99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3c4b10b79e84b8aaac753cbdf54c1d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8ef39611c5ed47fa8deca510127698b6","IPY_MODEL_0e31eb50abed4701ae0bc0032075509c","IPY_MODEL_d696913f32364de48b27ab41db93ffd2"],"layout":"IPY_MODEL_acc91e54583c4298b287c07390c67086"}},"8ef39611c5ed47fa8deca510127698b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08dd93ca68e24e76bad0a9b345851cba","placeholder":"​","style":"IPY_MODEL_7d4793c995e6401a83df39784634adc7","value":"Pushing dataset shards to the dataset hub: 100%"}},"0e31eb50abed4701ae0bc0032075509c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1aa17b436184cf4a6995c8d8066fe27","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f661a4b6a8c4bf3b3a369fd24524ff4","value":1}},"d696913f32364de48b27ab41db93ffd2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6da14aa0eae14a578edda3fcd4359543","placeholder":"​","style":"IPY_MODEL_69f134e1bd334499bd700f4ca05f40f0","value":" 1/1 [00:01&lt;00:00,  1.78s/it]"}},"acc91e54583c4298b287c07390c67086":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08dd93ca68e24e76bad0a9b345851cba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d4793c995e6401a83df39784634adc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1aa17b436184cf4a6995c8d8066fe27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f661a4b6a8c4bf3b3a369fd24524ff4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6da14aa0eae14a578edda3fcd4359543":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69f134e1bd334499bd700f4ca05f40f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70oLi9mZP6oK","executionInfo":{"status":"ok","timestamp":1649211362381,"user_tz":240,"elapsed":11,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"415afef3-fe0f-400a-b454-ae8de4a761ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Apr  6 02:16:01 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGqcN-zXTvvo","executionInfo":{"status":"ok","timestamp":1649211372367,"user_tz":240,"elapsed":9990,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"fde06f46-efb6-4aa8-bc41-ec4f73fa8bf5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n","\u001b[K     |████████████████████████████████| 325 kB 11.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.0-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[K     |████████████████████████████████| 136 kB 69.8 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 35.8 MB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 56.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 60.0 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 47.7 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 58.2 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tI_RBT1FSotu","outputId":"ebe2d6b2-6758-4b6d-d986-467c91419de4","executionInfo":{"status":"ok","timestamp":1649211399859,"user_tz":240,"elapsed":27498,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-s67clow5\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-s67clow5\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.63.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 13.0 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 61.6 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 57.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (3.6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.11.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (1.21.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2.23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (0.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0.dev0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0.dev0) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0.dev0) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (1.25.11)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.1.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.18.0.dev0-py3-none-any.whl size=3958915 sha256=ebbd975c48b48962e3459d5c71c00d212dd2202dde9b0c1f53516ba8d80ca4b5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-59ny54mp/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0.dev0\n"]}]},{"cell_type":"code","source":["import torch\n","import datetime\n","import json\n","import os\n","import time\n","import datasets\n","import pprint\n","import random\n","import string\n","import sys\n","import transformers\n","from datasets import load_dataset\n","from datasets.tasks import QuestionAnsweringExtractive"],"metadata":{"id":"DZ3Ma-pCRJDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available()\n","                      else 'cpu')"],"metadata":{"id":"HNMUVyBpRGw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/huggingface/transformers.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUVkgX-IQIiR","executionInfo":{"status":"ok","timestamp":1649211418965,"user_tz":240,"elapsed":9291,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"288ab573-68d0-444a-e090-638d66084656"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 108469, done.\u001b[K\n","remote: Counting objects: 100% (8/8), done.\u001b[K\n","remote: Compressing objects: 100% (8/8), done.\u001b[K\n","remote: Total 108469 (delta 0), reused 4 (delta 0), pack-reused 108461\u001b[K\n","Receiving objects: 100% (108469/108469), 95.00 MiB | 18.11 MiB/s, done.\n","Resolving deltas: 100% (79037/79037), done.\n"]}]},{"cell_type":"code","source":["dataset = load_dataset('/content/drive/MyDrive/QA/squad_v2_back_trans_aug.py')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":148,"referenced_widgets":["bdce4b15c55c4ef7990d49663f6245e8","925a51281335405091a76b9e235ec662","ffe0e194b477416f8da4747245d1bb3e","cb5c673b408b4c2bb48795987f081148","e5f02bf5f6d74f4dac6d33ac8a0719f9","ee766e6c4cdf43288020f546248f024c","8aa6f13b13d54cd3852e624e2cf114e4","e7fd570470c24528a946980e9f9925c9","f9814bef10a94ecb93f64b832abb92f7","04f6886921864b79ab7eee8ad81a28e8","723e37520a2d4dd499a87c0f8b8902ce","20ec39a1876a4824b4210c6c393e8ad6","09cff206da554a48a8f76efec85b6902","7b17ba924b9d45a6a4c19c14892b3c4e","92bee9b3d7e14117937497cad677d19c","ef1d4a64cdfd47128a377030ab1602a3","998fe1650b614a3ba1d98d427d1f9970","25fdccbf85b643f090aec1c87fff9a71","9805029f35a2407a9eac71e6f7f21ba5","152881a472db44dea1efe8775d4f3c37","fc74df9fb6f94357a7d069719660959e","63e34e436b6d424d94f43cbb180aa6af","28c841f1499d468584824002614c6222","2ed83ca8abbf41dcaca904e4e901f7ef","fc7dc23f3e2b407bae75350957245032","2a557f85e76f4d418af230439b53b105","c8326e1b37f948fa9dd6bfaea97182ca","6901a93ec3974ae7be1ac9a9ca1eceb5","f3fad66a43d548ecbd6692fd9ff10c31","00fa726ca2124054baad0ddaa583736c","b4aa5b9817d94735940fd6439cf08132","bba6e3d20ef14893b446cc8e2032c83f","447457a2787447ff930e0f7275e879b1","fbaa38f8050247a8911cc92e693c3eb5","6ffebccb358c46dca5fe8d28c2badfa7","1b36a51f9f3b4e4d8c84349f96f30ab0","95ab54c9041846088045b45a19f6be43","d922cd51605e4d69967679de64a281c6","7e74a8658f254a8d9b38c4d06941400e","da85d7d0a75144d48c8bbde80a17a52b","9b89e56165324aaf81a9aa81d7404a4a","fb7d10d5ca1640f7ba97db91b6edeef5","b761db2dd416461b8ee92221fa952dac","55d9ff8f5b1a4516b1b5de31d3ae00a2","770f3af7e8454966ad0a34e4257519c9","c1f1b8c0fb7a41d3b29e060d2eca2362","ed8c505317f24c5092f1aa04d7adec33","414c42cc9cea47c6b7fc92f115372b42","45b89215ed11407a817a771675c1b63c","f9ce4efece1743d39b768a014aa86f3d","d1e8c0793eb54bdba3a222e4601d4cc4","18ed2833eca24058b026d8ade90de6de","d6a5419e219d4e97a3642a5d38cb12a2","2a3c0401549c4d4aa71d3880c795dddc","9a590335497d464b8fcd18fb4b95e184"]},"id":"A2gdlPTvcmfL","executionInfo":{"status":"ok","timestamp":1648601858179,"user_tz":240,"elapsed":16812,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"0aac15ed-07db-4bf1-b096-1d1e64b62426"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset squad_v2_aug/squad_v2 to /root/.cache/huggingface/datasets/squad_v2_aug/squad_v2/2.0.0/34078ce9141fc6b4b0bce4f1d7219fccb4b8d2457c063dcd02c1202e8ab358cd...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdce4b15c55c4ef7990d49663f6245e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20ec39a1876a4824b4210c6c393e8ad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28c841f1499d468584824002614c6222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbaa38f8050247a8911cc92e693c3eb5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset squad_v2_aug downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2_aug/squad_v2/2.0.0/34078ce9141fc6b4b0bce4f1d7219fccb4b8d2457c063dcd02c1202e8ab358cd. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"770f3af7e8454966ad0a34e4257519c9"}},"metadata":{}}]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttpAfwxBWdR0","executionInfo":{"status":"ok","timestamp":1648601921638,"user_tz":240,"elapsed":63463,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"948915be-f9a7-423a-9fe4-c66bd5bb506a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","\n","        To login, `huggingface_hub` now requires a token generated from https://huggingface.co/settings/token.\n","        (Deprecated, will be removed in v0.3.0) To login with username and password instead, interrupt with Ctrl+C.\n","        \n","Token: \n","Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}]},{"cell_type":"code","source":["dataset.push_to_hub(\"sichenzhong/squad_v2_back_trans_aug\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":150,"referenced_widgets":["093786d54dae4ad9a271abdbd87509cc","f82bbe5da0424c4ea65c909b1d7cb5e4","8f6cd6a8caa44142bab12daad93ad604","ceeb2639ddda48fe917522ddd95179b1","dd3d0fbe32da45d49799ad1b180bbd97","53ba3f7953024a39815af6d207814514","00b8ce66116444cb827f066bc7142872","d3dcbc3f77b5453daa40c11df5027736","97bf3efbee634098a302375df70c4ed4","daf4e45c2a1647f4a98b62bb89cd3953","6e39e61dad3f4361ac12121108a7bb99","b3c4b10b79e84b8aaac753cbdf54c1d6","8ef39611c5ed47fa8deca510127698b6","0e31eb50abed4701ae0bc0032075509c","d696913f32364de48b27ab41db93ffd2","acc91e54583c4298b287c07390c67086","08dd93ca68e24e76bad0a9b345851cba","7d4793c995e6401a83df39784634adc7","c1aa17b436184cf4a6995c8d8066fe27","4f661a4b6a8c4bf3b3a369fd24524ff4","6da14aa0eae14a578edda3fcd4359543","69f134e1bd334499bd700f4ca05f40f0"]},"id":"WGAeWENJWR_F","executionInfo":{"status":"ok","timestamp":1648601973103,"user_tz":240,"elapsed":11431,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"38322aa7-70ca-47d2-b66a-7d2f25290c81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Pushing split train to the Hub.\n","The repository already exists: the `private` keyword argument will be ignored.\n"]},{"output_type":"display_data","data":{"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"093786d54dae4ad9a271abdbd87509cc"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Pushing split validation to the Hub.\n","The repository already exists: the `private` keyword argument will be ignored.\n"]},{"output_type":"display_data","data":{"text/plain":["Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3c4b10b79e84b8aaac753cbdf54c1d6"}},"metadata":{}}]},{"cell_type":"code","source":["%cd /content/transformers/examples/pytorch/question-answering/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uLaizsXQrk9","executionInfo":{"status":"ok","timestamp":1649211418966,"user_tz":240,"elapsed":29,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"7b12b6f9-6e26-4033-b188-2da3082193d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/transformers/examples/pytorch/question-answering\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8UqQTDOrqI3u","executionInfo":{"status":"ok","timestamp":1649211437958,"user_tz":240,"elapsed":19016,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"f23917fc-f6ba-449d-d916-d27a22183172"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path bert-base-cased \\\n","  --dataset_name sichenzhong/squad_v2_back_trans_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 3 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aI-ipSbaHqF6","executionInfo":{"status":"ok","timestamp":1649228189829,"user_tz":240,"elapsed":16608954,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"e08cfe10-9445-455f-9325-820e3022e30e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["04/06/2022 02:19:44 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 02:19:44 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/runs/Apr06_02-19-44_2439d11b3fff,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 02:19:45 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b\n","04/06/2022 02:19:45 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/06/2022 02:19:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","04/06/2022 02:19:45 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","04/06/2022 02:19:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","100% 2/2 [00:00<00:00, 537.46it/s]\n","[INFO|configuration_utils.py:654] 2022-04-06 02:19:46,075 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 02:19:46,076 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 02:19:46,434 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpc3ojasbb\n","Downloading: 100% 29.0/29.0 [00:00<00:00, 28.0kB/s]\n","[INFO|hub.py:587] 2022-04-06 02:19:46,780 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|hub.py:595] 2022-04-06 02:19:46,780 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:654] 2022-04-06 02:19:47,128 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 02:19:47,129 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 02:19:47,821 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpckj3r1za\n","Downloading: 100% 208k/208k [00:00<00:00, 659kB/s]\n","[INFO|hub.py:587] 2022-04-06 02:19:48,495 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:595] 2022-04-06 02:19:48,495 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:583] 2022-04-06 02:19:48,844 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpthuceln0\n","Downloading: 100% 426k/426k [00:00<00:00, 1.34MB/s]\n","[INFO|hub.py:587] 2022-04-06 02:19:49,599 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|hub.py:595] 2022-04-06 02:19:49,599 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 02:19:50,679 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 02:19:50,679 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 02:19:50,679 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 02:19:50,679 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 02:19:50,679 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:654] 2022-04-06 02:19:51,026 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-06 02:19:51,027 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 02:19:51,432 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzya0x0u0\n","Downloading: 100% 416M/416M [00:08<00:00, 50.7MB/s]\n","[INFO|hub.py:587] 2022-04-06 02:20:00,086 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|hub.py:595] 2022-04-06 02:20:00,086 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|modeling_utils.py:1772] 2022-04-06 02:20:00,086 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:2050] 2022-04-06 02:20:02,050 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-06 02:20:02,050 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/06/2022 02:20:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-a9718c9810ec3406.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:50<00:00,  2.59ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/06/2022 02:20:53 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-fd1e0e4375789ed6.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:14<00:00,  6.23s/ba]\n","04/06/2022 02:22:08 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp62zeuc88\n","Downloading builder script: 6.46kB [00:00, 6.25MB/s]       \n","04/06/2022 02:22:08 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/06/2022 02:22:08 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/06/2022 02:22:08 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp9ef0z4d_\n","Downloading extra modules: 11.3kB [00:00, 9.55MB/s]       \n","04/06/2022 02:22:08 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","04/06/2022 02:22:08 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 02:22:19,689 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 02:22:19,689 >>   Num examples = 132074\n","[INFO|trainer.py:1292] 2022-04-06 02:22:19,690 >>   Num Epochs = 3\n","[INFO|trainer.py:1293] 2022-04-06 02:22:19,690 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1294] 2022-04-06 02:22:19,690 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1295] 2022-04-06 02:22:19,690 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 02:22:19,690 >>   Total optimization steps = 24765\n","{'loss': 2.2635, 'learning_rate': 3.9192408641227546e-05, 'epoch': 0.06}\n","  2% 500/24765 [05:22<4:22:01,  1.54it/s][INFO|trainer.py:2166] 2022-04-06 02:27:42,614 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 02:27:42,620 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:27:43,836 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:27:43,841 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:27:43,844 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 1.6454, 'learning_rate': 3.838481728245508e-05, 'epoch': 0.12}\n","  4% 1000/24765 [10:49<4:15:43,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 02:33:09,373 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 02:33:09,379 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:33:10,537 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:33:10,542 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:33:10,547 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.5367, 'learning_rate': 3.7577225923682624e-05, 'epoch': 0.18}\n","  6% 1500/24765 [16:16<4:09:54,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 02:38:35,899 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 02:38:35,906 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:38:37,065 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:38:37,070 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:38:37,074 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.446, 'learning_rate': 3.676963456491016e-05, 'epoch': 0.24}\n","  8% 2000/24765 [21:42<4:04:51,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 02:44:02,540 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 02:44:02,546 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:44:03,706 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:44:03,711 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:44:03,714 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.3455, 'learning_rate': 3.5962043206137695e-05, 'epoch': 0.3}\n"," 10% 2500/24765 [27:09<3:59:29,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 02:49:29,154 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 02:49:29,160 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:49:30,308 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:49:30,313 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:49:30,316 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.3424, 'learning_rate': 3.515445184736523e-05, 'epoch': 0.36}\n"," 12% 3000/24765 [32:35<3:54:19,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 02:54:55,474 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 02:54:55,479 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 02:54:56,637 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 02:54:56,642 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 02:54:56,646 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.2486, 'learning_rate': 3.4346860488592774e-05, 'epoch': 0.42}\n"," 14% 3500/24765 [38:02<3:48:34,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:00:22,324 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:00:22,347 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:00:23,484 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:00:23,489 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:00:23,493 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.246, 'learning_rate': 3.3539269129820316e-05, 'epoch': 0.48}\n"," 16% 4000/24765 [43:29<3:43:00,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:05:49,311 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:05:49,319 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:05:50,470 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:05:50,494 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:05:50,498 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.2273, 'learning_rate': 3.273167777104785e-05, 'epoch': 0.55}\n"," 18% 4500/24765 [48:55<3:37:19,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:11:15,710 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:11:15,716 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:11:16,877 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:11:16,882 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:11:16,905 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.1886, 'learning_rate': 3.1924086412275394e-05, 'epoch': 0.61}\n"," 20% 5000/24765 [54:22<3:32:22,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:16:42,205 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:16:42,210 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:16:43,401 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:16:43,406 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:16:43,409 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.1984, 'learning_rate': 3.111649505350293e-05, 'epoch': 0.67}\n"," 22% 5500/24765 [59:49<3:27:14,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:22:09,321 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:22:09,327 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:22:10,521 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:22:10,526 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:22:10,529 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.1417, 'learning_rate': 3.030890369473047e-05, 'epoch': 0.73}\n"," 24% 6000/24765 [1:05:16<3:21:58,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:27:35,856 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:27:35,862 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:27:37,032 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:27:37,037 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:27:37,040 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.1537, 'learning_rate': 2.9501312335958005e-05, 'epoch': 0.79}\n"," 26% 6500/24765 [1:10:42<3:16:22,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:33:02,467 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:33:02,474 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:33:03,650 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:33:03,654 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:33:03,657 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.0985, 'learning_rate': 2.8693720977185547e-05, 'epoch': 0.85}\n"," 28% 7000/24765 [1:16:12<3:11:22,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:38:32,242 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:38:32,248 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:38:33,425 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:38:33,430 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:38:33,433 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.1008, 'learning_rate': 2.7886129618413086e-05, 'epoch': 0.91}\n"," 30% 7500/24765 [1:21:42<3:05:42,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:44:02,147 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:44:02,153 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:44:03,317 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:44:03,321 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:44:03,325 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 1.1008, 'learning_rate': 2.7078538259640622e-05, 'epoch': 0.97}\n"," 32% 8000/24765 [1:27:11<3:00:28,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:49:31,552 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 03:49:31,558 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:49:32,755 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:49:32,760 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:49:32,763 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.9531, 'learning_rate': 2.6270946900868165e-05, 'epoch': 1.03}\n"," 34% 8500/24765 [1:32:38<2:54:53,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 03:54:58,082 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 03:54:58,089 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 03:54:59,256 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 03:54:59,261 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 03:54:59,265 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.7791, 'learning_rate': 2.54633555420957e-05, 'epoch': 1.09}\n"," 36% 9000/24765 [1:38:05<2:49:38,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 04:00:24,843 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:00:24,849 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:00:26,064 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:00:26,069 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:00:26,072 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.818, 'learning_rate': 2.465576418332324e-05, 'epoch': 1.15}\n"," 38% 9500/24765 [1:43:32<2:44:08,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 04:05:52,312 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:05:52,319 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:05:53,500 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:05:53,505 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:05:53,509 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.8163, 'learning_rate': 2.384817282455078e-05, 'epoch': 1.21}\n"," 40% 10000/24765 [1:48:59<2:38:52,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 04:11:19,051 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:11:19,057 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:11:20,240 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:11:20,246 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:11:20,250 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.7929, 'learning_rate': 2.3040581465778318e-05, 'epoch': 1.27}\n"," 42% 10500/24765 [1:54:26<2:33:37,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 04:16:45,803 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:16:45,808 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:16:46,983 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:16:46,988 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:16:46,991 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.7715, 'learning_rate': 2.223299010700586e-05, 'epoch': 1.33}\n"," 44% 11000/24765 [1:59:52<2:28:14,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 04:22:12,399 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:22:12,405 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:22:13,564 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:22:13,569 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:22:13,573 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.7902, 'learning_rate': 2.1425398748233396e-05, 'epoch': 1.39}\n"," 46% 11500/24765 [2:05:19<2:22:46,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 04:27:39,209 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-11500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:27:39,215 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:27:40,383 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:27:40,389 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:27:40,394 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.8048, 'learning_rate': 2.0617807389460935e-05, 'epoch': 1.45}\n"," 48% 12000/24765 [2:10:46<2:17:23,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 04:33:05,912 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-12000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:33:05,919 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:33:07,084 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:33:07,090 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:33:07,093 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.7852, 'learning_rate': 1.9810216030688474e-05, 'epoch': 1.51}\n"," 50% 12500/24765 [2:16:12<2:11:58,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 04:38:32,737 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-12500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:38:32,743 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:38:33,932 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:38:33,938 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:38:33,942 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.7623, 'learning_rate': 1.9002624671916013e-05, 'epoch': 1.57}\n"," 52% 13000/24765 [2:21:39<2:06:37,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 04:43:59,521 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-13000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:43:59,527 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:44:00,730 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:44:00,735 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:44:00,738 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.7948, 'learning_rate': 1.8195033313143552e-05, 'epoch': 1.64}\n"," 55% 13500/24765 [2:27:06<2:01:13,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 04:49:26,298 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-13500\n","[INFO|configuration_utils.py:441] 2022-04-06 04:49:26,304 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:49:27,500 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:49:27,504 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:49:27,508 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.7644, 'learning_rate': 1.7387441954371088e-05, 'epoch': 1.7}\n"," 57% 14000/24765 [2:32:33<1:55:41,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 04:54:53,265 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-14000\n","[INFO|configuration_utils.py:441] 2022-04-06 04:54:53,271 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 04:54:54,463 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 04:54:54,469 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 04:54:54,472 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.7864, 'learning_rate': 1.6579850595598627e-05, 'epoch': 1.76}\n"," 59% 14500/24765 [2:38:00<1:50:23,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 05:00:19,983 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-14500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:00:19,990 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:00:21,164 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:00:21,186 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:00:21,189 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.7926, 'learning_rate': 1.5772259236826166e-05, 'epoch': 1.82}\n"," 61% 15000/24765 [2:43:27<1:44:59,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 05:05:46,773 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-15000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:05:46,779 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:05:47,962 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:05:47,986 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:05:47,990 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.7714, 'learning_rate': 1.4964667878053707e-05, 'epoch': 1.88}\n"," 63% 15500/24765 [2:48:53<1:39:39,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 05:11:13,555 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-15500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:11:13,562 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:11:14,742 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:11:14,746 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:11:14,750 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.7681, 'learning_rate': 1.4157076519281246e-05, 'epoch': 1.94}\n"," 65% 16000/24765 [2:54:20<1:34:06,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 05:16:40,164 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-16000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:16:40,170 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:16:41,353 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:16:41,358 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:16:41,362 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-16000/special_tokens_map.json\n","{'loss': 0.759, 'learning_rate': 1.3349485160508783e-05, 'epoch': 2.0}\n"," 67% 16500/24765 [2:59:46<1:28:42,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 05:22:06,515 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-16500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:22:06,521 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-16500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:22:07,703 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-16500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:22:07,708 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-16500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:22:07,713 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-16500/special_tokens_map.json\n","{'loss': 0.4854, 'learning_rate': 1.2541893801736322e-05, 'epoch': 2.06}\n"," 69% 17000/24765 [3:05:13<1:23:30,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 05:27:33,191 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-17000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:27:33,197 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-17000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:27:34,391 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-17000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:27:34,395 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-17000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:27:34,399 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-17000/special_tokens_map.json\n","{'loss': 0.4938, 'learning_rate': 1.1734302442963861e-05, 'epoch': 2.12}\n"," 71% 17500/24765 [3:10:40<1:18:12,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 05:32:59,974 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-17500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:32:59,981 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-17500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:33:01,173 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-17500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:33:01,177 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-17500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:33:01,180 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-17500/special_tokens_map.json\n","{'loss': 0.4774, 'learning_rate': 1.0926711084191399e-05, 'epoch': 2.18}\n"," 73% 18000/24765 [3:16:06<1:12:49,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 05:38:26,728 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-18000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:38:26,734 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-18000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:38:27,955 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-18000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:38:27,959 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-18000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:38:27,963 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-18000/special_tokens_map.json\n","{'loss': 0.4812, 'learning_rate': 1.011911972541894e-05, 'epoch': 2.24}\n"," 75% 18500/24765 [3:21:33<1:07:14,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 05:43:53,663 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-18500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:43:53,669 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-18500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:43:54,850 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-18500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:43:54,855 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-18500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:43:54,859 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-18500/special_tokens_map.json\n","{'loss': 0.4881, 'learning_rate': 9.311528366646477e-06, 'epoch': 2.3}\n"," 77% 19000/24765 [3:27:03<1:01:55,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 05:49:23,239 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-19000\n","[INFO|configuration_utils.py:441] 2022-04-06 05:49:23,245 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-19000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:49:24,420 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-19000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:49:24,425 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-19000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:49:24,428 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-19000/special_tokens_map.json\n","{'loss': 0.4687, 'learning_rate': 8.503937007874016e-06, 'epoch': 2.36}\n"," 79% 19500/24765 [3:32:31<56:39,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 05:54:51,705 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-19500\n","[INFO|configuration_utils.py:441] 2022-04-06 05:54:51,712 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-19500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 05:54:52,872 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-19500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 05:54:52,877 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-19500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 05:54:52,882 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-19500/special_tokens_map.json\n","{'loss': 0.4871, 'learning_rate': 7.696345649101555e-06, 'epoch': 2.42}\n"," 81% 20000/24765 [3:38:01<51:14,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 06:00:21,351 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-20000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:00:21,357 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-20000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:00:22,545 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-20000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:00:22,550 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-20000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:00:22,554 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-20000/special_tokens_map.json\n","{'loss': 0.4789, 'learning_rate': 6.8887542903290935e-06, 'epoch': 2.48}\n"," 83% 20500/24765 [3:43:28<45:45,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 06:05:47,813 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-20500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:05:47,820 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-20500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:05:48,985 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-20500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:05:48,990 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-20500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:05:48,994 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-20500/special_tokens_map.json\n","{'loss': 0.4811, 'learning_rate': 6.0811629315566326e-06, 'epoch': 2.54}\n"," 85% 21000/24765 [3:48:54<40:27,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 06:11:14,380 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-21000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:11:14,386 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-21000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:11:15,562 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-21000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:11:15,567 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-21000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:11:15,571 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-21000/special_tokens_map.json\n","{'loss': 0.4613, 'learning_rate': 5.273571572784172e-06, 'epoch': 2.6}\n"," 87% 21500/24765 [3:54:24<35:09,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 06:16:44,053 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-21500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:16:44,060 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-21500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:16:45,259 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-21500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:16:45,265 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-21500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:16:45,268 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-21500/special_tokens_map.json\n","{'loss': 0.4672, 'learning_rate': 4.46598021401171e-06, 'epoch': 2.67}\n"," 89% 22000/24765 [3:59:51<29:47,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 06:22:10,878 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-22000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:22:10,884 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-22000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:22:12,080 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-22000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:22:12,085 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-22000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:22:12,089 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-22000/special_tokens_map.json\n","{'loss': 0.457, 'learning_rate': 3.6583888552392494e-06, 'epoch': 2.73}\n"," 91% 22500/24765 [4:05:18<24:22,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 06:27:37,758 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-22500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:27:37,764 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-22500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:27:38,971 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-22500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:27:38,975 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-22500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:27:38,980 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-22500/special_tokens_map.json\n","{'loss': 0.4872, 'learning_rate': 2.8507974964667877e-06, 'epoch': 2.79}\n"," 93% 23000/24765 [4:10:44<18:56,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 06:33:04,503 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-23000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:33:04,509 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-23000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:33:05,688 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-23000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:33:05,891 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-23000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:33:05,895 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-23000/special_tokens_map.json\n","{'loss': 0.4747, 'learning_rate': 2.0432061376943268e-06, 'epoch': 2.85}\n"," 95% 23500/24765 [4:16:11<13:35,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 06:38:31,381 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-23500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:38:31,387 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-23500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:38:32,578 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-23500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:38:32,600 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-23500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:38:32,604 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-23500/special_tokens_map.json\n","{'loss': 0.4475, 'learning_rate': 1.2356147789218657e-06, 'epoch': 2.91}\n"," 97% 24000/24765 [4:21:38<08:13,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 06:43:58,279 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-24000\n","[INFO|configuration_utils.py:441] 2022-04-06 06:43:58,286 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-24000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:43:59,489 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-24000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:43:59,495 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-24000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:43:59,498 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-24000/special_tokens_map.json\n","{'loss': 0.4787, 'learning_rate': 4.280234201494044e-07, 'epoch': 2.97}\n"," 99% 24500/24765 [4:27:05<02:51,  1.55it/s][INFO|trainer.py:2166] 2022-04-06 06:49:25,177 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-24500\n","[INFO|configuration_utils.py:441] 2022-04-06 06:49:25,184 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-24500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:49:26,397 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-24500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:49:26,402 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-24500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:49:26,405 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/checkpoint-24500/special_tokens_map.json\n","100% 24765/24765 [4:30:00<00:00,  1.72it/s][INFO|trainer.py:1530] 2022-04-06 06:52:20,074 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 16200.3845, 'train_samples_per_second': 24.458, 'train_steps_per_second': 1.529, 'train_loss': 0.8613087146373174, 'epoch': 3.0}\n","100% 24765/24765 [4:30:00<00:00,  1.53it/s]\n","[INFO|trainer.py:2166] 2022-04-06 06:52:20,080 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug\n","[INFO|configuration_utils.py:441] 2022-04-06 06:52:20,086 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 06:52:21,424 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 06:52:21,429 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 06:52:21,433 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =     0.8613\n","  train_runtime            = 4:30:00.38\n","  train_samples            =     132074\n","  train_samples_per_second =     24.458\n","  train_steps_per_second   =      1.529\n","04/06/2022 06:52:21 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 06:52:21,495 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 06:52:21,511 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 06:52:21,511 >>   Num examples = 12199\n","[INFO|trainer.py:2421] 2022-04-06 06:52:21,511 >>   Batch size = 8\n","100% 1525/1525 [02:59<00:00,  8.66it/s]04/06/2022 06:55:36 - INFO - utils_qa - Post-processing 11873 example predictions split into 12199 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 25/11873 [00:00<00:47, 249.95it/s]\u001b[A\n","  1% 61/11873 [00:00<00:37, 313.93it/s]\u001b[A\n","  1% 93/11873 [00:00<00:38, 302.31it/s]\u001b[A\n","  1% 129/11873 [00:00<00:36, 321.49it/s]\u001b[A\n","  1% 166/11873 [00:00<00:34, 335.99it/s]\u001b[A\n","  2% 203/11873 [00:00<00:33, 345.86it/s]\u001b[A\n","  2% 240/11873 [00:00<00:33, 350.83it/s]\u001b[A\n","  2% 277/11873 [00:00<00:32, 354.78it/s]\u001b[A\n","  3% 315/11873 [00:00<00:32, 361.02it/s]\u001b[A\n","  3% 352/11873 [00:01<00:32, 358.49it/s]\u001b[A\n","  3% 389/11873 [00:01<00:31, 360.39it/s]\u001b[A\n","  4% 426/11873 [00:01<00:31, 360.86it/s]\u001b[A\n","  4% 465/11873 [00:01<00:31, 367.16it/s]\u001b[A\n","  4% 502/11873 [00:01<00:31, 361.59it/s]\u001b[A\n","  5% 539/11873 [00:01<00:31, 355.95it/s]\u001b[A\n","  5% 575/11873 [00:01<00:32, 352.49it/s]\u001b[A\n","  5% 612/11873 [00:01<00:31, 357.03it/s]\u001b[A\n","  5% 649/11873 [00:01<00:31, 358.97it/s]\u001b[A\n","  6% 687/11873 [00:01<00:30, 364.82it/s]\u001b[A\n","  6% 724/11873 [00:02<00:31, 359.00it/s]\u001b[A\n","  6% 760/11873 [00:02<00:31, 350.60it/s]\u001b[A\n","  7% 798/11873 [00:02<00:30, 357.90it/s]\u001b[A\n","  7% 836/11873 [00:02<00:30, 361.45it/s]\u001b[A\n","  7% 875/11873 [00:02<00:29, 369.18it/s]\u001b[A\n","  8% 913/11873 [00:02<00:29, 371.60it/s]\u001b[A\n","  8% 952/11873 [00:02<00:29, 374.25it/s]\u001b[A\n","  8% 990/11873 [00:02<00:29, 364.99it/s]\u001b[A\n","  9% 1027/11873 [00:02<00:31, 345.61it/s]\u001b[A\n","  9% 1062/11873 [00:03<00:32, 329.53it/s]\u001b[A\n","  9% 1096/11873 [00:03<00:33, 317.57it/s]\u001b[A\n"," 10% 1128/11873 [00:03<00:34, 309.80it/s]\u001b[A\n"," 10% 1160/11873 [00:03<00:35, 304.76it/s]\u001b[A\n"," 10% 1191/11873 [00:03<00:35, 301.95it/s]\u001b[A\n"," 10% 1222/11873 [00:03<00:35, 298.92it/s]\u001b[A\n"," 11% 1252/11873 [00:03<00:35, 297.77it/s]\u001b[A\n"," 11% 1282/11873 [00:03<00:36, 293.33it/s]\u001b[A\n"," 11% 1312/11873 [00:03<00:36, 288.80it/s]\u001b[A\n"," 11% 1342/11873 [00:03<00:36, 291.43it/s]\u001b[A\n"," 12% 1372/11873 [00:04<00:35, 291.73it/s]\u001b[A\n"," 12% 1402/11873 [00:04<00:35, 291.03it/s]\u001b[A\n"," 12% 1432/11873 [00:04<00:35, 292.01it/s]\u001b[A\n"," 12% 1462/11873 [00:04<00:35, 291.49it/s]\u001b[A\n"," 13% 1492/11873 [00:04<00:35, 289.65it/s]\u001b[A\n"," 13% 1521/11873 [00:04<00:35, 289.68it/s]\u001b[A\n"," 13% 1550/11873 [00:04<00:36, 282.07it/s]\u001b[A\n"," 13% 1579/11873 [00:04<00:36, 284.32it/s]\u001b[A\n"," 14% 1609/11873 [00:04<00:35, 286.01it/s]\u001b[A\n"," 14% 1638/11873 [00:05<00:35, 286.83it/s]\u001b[A\n"," 14% 1668/11873 [00:05<00:35, 290.27it/s]\u001b[A\n"," 14% 1698/11873 [00:05<00:35, 289.94it/s]\u001b[A\n"," 15% 1728/11873 [00:05<00:34, 292.18it/s]\u001b[A\n"," 15% 1758/11873 [00:05<00:34, 292.52it/s]\u001b[A\n"," 15% 1788/11873 [00:05<00:35, 285.48it/s]\u001b[A\n"," 15% 1817/11873 [00:05<00:35, 285.64it/s]\u001b[A\n"," 16% 1847/11873 [00:05<00:34, 286.96it/s]\u001b[A\n"," 16% 1876/11873 [00:05<00:34, 287.79it/s]\u001b[A\n"," 16% 1906/11873 [00:05<00:34, 290.13it/s]\u001b[A\n"," 16% 1936/11873 [00:06<00:34, 287.72it/s]\u001b[A\n"," 17% 1967/11873 [00:06<00:33, 291.65it/s]\u001b[A\n"," 17% 1998/11873 [00:06<00:33, 294.93it/s]\u001b[A\n"," 17% 2028/11873 [00:06<00:33, 291.34it/s]\u001b[A\n"," 17% 2058/11873 [00:06<00:33, 292.85it/s]\u001b[A\n"," 18% 2088/11873 [00:06<00:33, 293.77it/s]\u001b[A\n"," 18% 2118/11873 [00:06<00:33, 292.36it/s]\u001b[A\n"," 18% 2148/11873 [00:06<00:33, 291.40it/s]\u001b[A\n"," 18% 2178/11873 [00:06<00:33, 291.53it/s]\u001b[A\n"," 19% 2208/11873 [00:06<00:33, 290.63it/s]\u001b[A\n"," 19% 2239/11873 [00:07<00:32, 294.96it/s]\u001b[A\n"," 19% 2269/11873 [00:07<00:32, 294.81it/s]\u001b[A\n"," 19% 2299/11873 [00:07<00:32, 294.51it/s]\u001b[A\n"," 20% 2329/11873 [00:07<00:32, 295.98it/s]\u001b[A\n"," 20% 2359/11873 [00:07<00:32, 291.25it/s]\u001b[A\n"," 20% 2390/11873 [00:07<00:32, 294.66it/s]\u001b[A\n"," 20% 2420/11873 [00:07<00:32, 290.91it/s]\u001b[A\n"," 21% 2450/11873 [00:07<00:32, 287.95it/s]\u001b[A\n"," 21% 2480/11873 [00:07<00:32, 288.95it/s]\u001b[A\n"," 21% 2510/11873 [00:08<00:32, 289.27it/s]\u001b[A\n"," 21% 2539/11873 [00:08<00:32, 287.21it/s]\u001b[A\n"," 22% 2570/11873 [00:08<00:31, 292.19it/s]\u001b[A\n"," 22% 2600/11873 [00:08<00:31, 294.04it/s]\u001b[A\n"," 22% 2630/11873 [00:08<00:31, 294.16it/s]\u001b[A\n"," 22% 2660/11873 [00:08<00:31, 292.35it/s]\u001b[A\n"," 23% 2690/11873 [00:08<00:31, 291.56it/s]\u001b[A\n"," 23% 2720/11873 [00:08<00:31, 292.19it/s]\u001b[A\n"," 23% 2750/11873 [00:08<00:31, 291.77it/s]\u001b[A\n"," 23% 2780/11873 [00:08<00:31, 286.45it/s]\u001b[A\n"," 24% 2810/11873 [00:09<00:31, 288.86it/s]\u001b[A\n"," 24% 2840/11873 [00:09<00:31, 291.01it/s]\u001b[A\n"," 24% 2870/11873 [00:09<00:30, 291.65it/s]\u001b[A\n"," 24% 2900/11873 [00:09<00:30, 292.40it/s]\u001b[A\n"," 25% 2930/11873 [00:09<00:30, 292.53it/s]\u001b[A\n"," 25% 2960/11873 [00:09<00:30, 293.71it/s]\u001b[A\n"," 25% 2990/11873 [00:09<00:30, 288.16it/s]\u001b[A\n"," 25% 3019/11873 [00:09<00:34, 257.89it/s]\u001b[A\n"," 26% 3047/11873 [00:09<00:33, 263.11it/s]\u001b[A\n"," 26% 3074/11873 [00:10<00:33, 264.94it/s]\u001b[A\n"," 26% 3101/11873 [00:10<00:33, 261.33it/s]\u001b[A\n"," 26% 3128/11873 [00:10<00:40, 218.25it/s]\u001b[A\n"," 27% 3152/11873 [00:10<00:40, 214.22it/s]\u001b[A\n"," 27% 3175/11873 [00:10<00:40, 215.21it/s]\u001b[A\n"," 27% 3204/11873 [00:10<00:36, 234.96it/s]\u001b[A\n"," 27% 3232/11873 [00:10<00:34, 247.00it/s]\u001b[A\n"," 27% 3261/11873 [00:10<00:33, 257.07it/s]\u001b[A\n"," 28% 3288/11873 [00:11<00:44, 193.14it/s]\u001b[A\n"," 28% 3310/11873 [00:11<00:49, 171.42it/s]\u001b[A\n"," 28% 3330/11873 [00:11<00:52, 161.38it/s]\u001b[A\n"," 28% 3349/11873 [00:11<00:51, 165.13it/s]\u001b[A\n"," 28% 3367/11873 [00:11<00:55, 152.80it/s]\u001b[A\n"," 29% 3396/11873 [00:11<00:45, 185.04it/s]\u001b[A\n"," 29% 3427/11873 [00:11<00:39, 214.69it/s]\u001b[A\n"," 29% 3456/11873 [00:11<00:35, 234.22it/s]\u001b[A\n"," 29% 3486/11873 [00:12<00:33, 250.18it/s]\u001b[A\n"," 30% 3515/11873 [00:12<00:32, 260.38it/s]\u001b[A\n"," 30% 3545/11873 [00:12<00:30, 270.11it/s]\u001b[A\n"," 30% 3575/11873 [00:12<00:29, 277.19it/s]\u001b[A\n"," 30% 3606/11873 [00:12<00:29, 284.79it/s]\u001b[A\n"," 31% 3636/11873 [00:12<00:28, 287.20it/s]\u001b[A\n"," 31% 3666/11873 [00:12<00:28, 288.42it/s]\u001b[A\n"," 31% 3696/11873 [00:12<00:29, 280.13it/s]\u001b[A\n"," 31% 3725/11873 [00:12<00:28, 282.23it/s]\u001b[A\n"," 32% 3755/11873 [00:12<00:28, 285.80it/s]\u001b[A\n"," 32% 3784/11873 [00:13<00:28, 283.38it/s]\u001b[A\n"," 32% 3814/11873 [00:13<00:28, 283.64it/s]\u001b[A\n"," 32% 3843/11873 [00:13<00:30, 264.84it/s]\u001b[A\n"," 33% 3873/11873 [00:13<00:29, 273.25it/s]\u001b[A\n"," 33% 3902/11873 [00:13<00:28, 276.49it/s]\u001b[A\n"," 33% 3930/11873 [00:13<00:31, 254.01it/s]\u001b[A\n"," 33% 3958/11873 [00:13<00:30, 257.56it/s]\u001b[A\n"," 34% 3985/11873 [00:13<00:31, 250.79it/s]\u001b[A\n"," 34% 4015/11873 [00:13<00:29, 264.25it/s]\u001b[A\n"," 34% 4046/11873 [00:14<00:28, 274.86it/s]\u001b[A\n"," 34% 4074/11873 [00:14<00:28, 273.36it/s]\u001b[A\n"," 35% 4104/11873 [00:14<00:27, 279.20it/s]\u001b[A\n"," 35% 4133/11873 [00:14<00:28, 276.03it/s]\u001b[A\n"," 35% 4161/11873 [00:14<00:30, 255.63it/s]\u001b[A\n"," 35% 4190/11873 [00:14<00:29, 262.87it/s]\u001b[A\n"," 36% 4219/11873 [00:14<00:28, 268.71it/s]\u001b[A\n"," 36% 4250/11873 [00:14<00:27, 278.48it/s]\u001b[A\n"," 36% 4279/11873 [00:14<00:27, 279.93it/s]\u001b[A\n"," 36% 4309/11873 [00:14<00:26, 285.39it/s]\u001b[A\n"," 37% 4340/11873 [00:15<00:25, 291.47it/s]\u001b[A\n"," 37% 4370/11873 [00:15<00:25, 293.54it/s]\u001b[A\n"," 37% 4400/11873 [00:15<00:25, 290.74it/s]\u001b[A\n"," 37% 4430/11873 [00:15<00:32, 226.84it/s]\u001b[A\n"," 38% 4461/11873 [00:15<00:30, 245.49it/s]\u001b[A\n"," 38% 4491/11873 [00:15<00:28, 257.85it/s]\u001b[A\n"," 38% 4521/11873 [00:15<00:27, 267.17it/s]\u001b[A\n"," 38% 4551/11873 [00:15<00:26, 274.09it/s]\u001b[A\n"," 39% 4582/11873 [00:16<00:25, 282.04it/s]\u001b[A\n"," 39% 4612/11873 [00:16<00:25, 285.30it/s]\u001b[A\n"," 39% 4641/11873 [00:16<00:25, 285.48it/s]\u001b[A\n"," 39% 4671/11873 [00:16<00:25, 287.96it/s]\u001b[A\n"," 40% 4701/11873 [00:16<00:24, 287.04it/s]\u001b[A\n"," 40% 4731/11873 [00:16<00:24, 289.17it/s]\u001b[A\n"," 40% 4761/11873 [00:16<00:24, 291.43it/s]\u001b[A\n"," 40% 4791/11873 [00:16<00:24, 283.96it/s]\u001b[A\n"," 41% 4821/11873 [00:16<00:24, 287.22it/s]\u001b[A\n"," 41% 4851/11873 [00:16<00:24, 288.94it/s]\u001b[A\n"," 41% 4880/11873 [00:17<00:24, 287.59it/s]\u001b[A\n"," 41% 4910/11873 [00:17<00:24, 288.96it/s]\u001b[A\n"," 42% 4940/11873 [00:17<00:23, 291.05it/s]\u001b[A\n"," 42% 4970/11873 [00:17<00:23, 290.81it/s]\u001b[A\n"," 42% 5000/11873 [00:17<00:23, 289.16it/s]\u001b[A\n"," 42% 5029/11873 [00:17<00:23, 288.53it/s]\u001b[A\n"," 43% 5058/11873 [00:17<00:23, 288.91it/s]\u001b[A\n"," 43% 5087/11873 [00:17<00:23, 288.86it/s]\u001b[A\n"," 43% 5116/11873 [00:17<00:23, 288.61it/s]\u001b[A\n"," 43% 5146/11873 [00:17<00:23, 289.81it/s]\u001b[A\n"," 44% 5175/11873 [00:18<00:23, 288.94it/s]\u001b[A\n"," 44% 5205/11873 [00:18<00:23, 289.72it/s]\u001b[A\n"," 44% 5234/11873 [00:18<00:22, 288.92it/s]\u001b[A\n"," 44% 5263/11873 [00:18<00:25, 262.21it/s]\u001b[A\n"," 45% 5293/11873 [00:18<00:24, 271.08it/s]\u001b[A\n"," 45% 5323/11873 [00:18<00:23, 277.44it/s]\u001b[A\n"," 45% 5353/11873 [00:18<00:23, 282.20it/s]\u001b[A\n"," 45% 5383/11873 [00:18<00:22, 286.90it/s]\u001b[A\n"," 46% 5412/11873 [00:18<00:22, 284.92it/s]\u001b[A\n"," 46% 5441/11873 [00:19<00:22, 284.79it/s]\u001b[A\n"," 46% 5470/11873 [00:19<00:22, 285.38it/s]\u001b[A\n"," 46% 5500/11873 [00:19<00:22, 287.46it/s]\u001b[A\n"," 47% 5531/11873 [00:19<00:21, 291.56it/s]\u001b[A\n"," 47% 5561/11873 [00:19<00:21, 293.17it/s]\u001b[A\n"," 47% 5591/11873 [00:19<00:21, 291.15it/s]\u001b[A\n"," 47% 5621/11873 [00:19<00:21, 290.17it/s]\u001b[A\n"," 48% 5651/11873 [00:19<00:21, 288.85it/s]\u001b[A\n"," 48% 5681/11873 [00:19<00:21, 289.50it/s]\u001b[A\n"," 48% 5710/11873 [00:19<00:21, 289.57it/s]\u001b[A\n"," 48% 5740/11873 [00:20<00:21, 290.74it/s]\u001b[A\n"," 49% 5770/11873 [00:20<00:21, 288.88it/s]\u001b[A\n"," 49% 5800/11873 [00:20<00:20, 290.94it/s]\u001b[A\n"," 49% 5830/11873 [00:20<00:20, 292.79it/s]\u001b[A\n"," 49% 5860/11873 [00:20<00:20, 291.74it/s]\u001b[A\n"," 50% 5890/11873 [00:20<00:21, 284.88it/s]\u001b[A\n"," 50% 5920/11873 [00:20<00:20, 287.51it/s]\u001b[A\n"," 50% 5949/11873 [00:20<00:20, 284.58it/s]\u001b[A\n"," 50% 5979/11873 [00:20<00:20, 288.46it/s]\u001b[A\n"," 51% 6008/11873 [00:20<00:20, 287.74it/s]\u001b[A\n"," 51% 6037/11873 [00:21<00:20, 281.46it/s]\u001b[A\n"," 51% 6066/11873 [00:21<00:20, 283.93it/s]\u001b[A\n"," 51% 6096/11873 [00:21<00:20, 285.80it/s]\u001b[A\n"," 52% 6125/11873 [00:21<00:20, 286.71it/s]\u001b[A\n"," 52% 6154/11873 [00:21<00:20, 285.34it/s]\u001b[A\n"," 52% 6184/11873 [00:21<00:19, 288.49it/s]\u001b[A\n"," 52% 6213/11873 [00:21<00:20, 282.45it/s]\u001b[A\n"," 53% 6243/11873 [00:21<00:19, 286.22it/s]\u001b[A\n"," 53% 6273/11873 [00:21<00:19, 289.08it/s]\u001b[A\n"," 53% 6303/11873 [00:21<00:19, 290.33it/s]\u001b[A\n"," 53% 6333/11873 [00:22<00:19, 291.44it/s]\u001b[A\n"," 54% 6363/11873 [00:22<00:18, 291.42it/s]\u001b[A\n"," 54% 6393/11873 [00:22<00:18, 292.81it/s]\u001b[A\n"," 54% 6423/11873 [00:22<00:18, 291.01it/s]\u001b[A\n"," 54% 6453/11873 [00:22<00:18, 290.07it/s]\u001b[A\n"," 55% 6484/11873 [00:22<00:18, 293.25it/s]\u001b[A\n"," 55% 6514/11873 [00:22<00:18, 290.69it/s]\u001b[A\n"," 55% 6544/11873 [00:22<00:18, 285.07it/s]\u001b[A\n"," 55% 6574/11873 [00:22<00:18, 286.56it/s]\u001b[A\n"," 56% 6603/11873 [00:23<00:18, 286.78it/s]\u001b[A\n"," 56% 6632/11873 [00:23<00:18, 284.97it/s]\u001b[A\n"," 56% 6662/11873 [00:23<00:18, 287.69it/s]\u001b[A\n"," 56% 6692/11873 [00:23<00:17, 288.46it/s]\u001b[A\n"," 57% 6721/11873 [00:23<00:20, 248.22it/s]\u001b[A\n"," 57% 6752/11873 [00:23<00:19, 263.61it/s]\u001b[A\n"," 57% 6781/11873 [00:23<00:18, 270.37it/s]\u001b[A\n"," 57% 6809/11873 [00:23<00:18, 270.96it/s]\u001b[A\n"," 58% 6840/11873 [00:23<00:17, 279.95it/s]\u001b[A\n"," 58% 6870/11873 [00:24<00:17, 283.53it/s]\u001b[A\n"," 58% 6899/11873 [00:24<00:17, 284.12it/s]\u001b[A\n"," 58% 6929/11873 [00:24<00:17, 288.24it/s]\u001b[A\n"," 59% 6959/11873 [00:24<00:16, 289.95it/s]\u001b[A\n"," 59% 6989/11873 [00:24<00:17, 285.90it/s]\u001b[A\n"," 59% 7019/11873 [00:24<00:16, 287.23it/s]\u001b[A\n"," 59% 7048/11873 [00:24<00:16, 285.10it/s]\u001b[A\n"," 60% 7077/11873 [00:24<00:16, 285.41it/s]\u001b[A\n"," 60% 7107/11873 [00:24<00:16, 287.45it/s]\u001b[A\n"," 60% 7136/11873 [00:24<00:16, 286.88it/s]\u001b[A\n"," 60% 7165/11873 [00:25<00:16, 286.23it/s]\u001b[A\n"," 61% 7194/11873 [00:25<00:16, 286.63it/s]\u001b[A\n"," 61% 7223/11873 [00:25<00:16, 287.24it/s]\u001b[A\n"," 61% 7252/11873 [00:25<00:16, 287.40it/s]\u001b[A\n"," 61% 7281/11873 [00:25<00:16, 286.07it/s]\u001b[A\n"," 62% 7310/11873 [00:25<00:16, 285.13it/s]\u001b[A\n"," 62% 7339/11873 [00:25<00:16, 283.08it/s]\u001b[A\n"," 62% 7369/11873 [00:25<00:15, 287.44it/s]\u001b[A\n"," 62% 7398/11873 [00:25<00:15, 287.38it/s]\u001b[A\n"," 63% 7427/11873 [00:25<00:16, 268.97it/s]\u001b[A\n"," 63% 7456/11873 [00:26<00:16, 273.07it/s]\u001b[A\n"," 63% 7486/11873 [00:26<00:15, 279.47it/s]\u001b[A\n"," 63% 7516/11873 [00:26<00:15, 282.97it/s]\u001b[A\n"," 64% 7546/11873 [00:26<00:15, 284.91it/s]\u001b[A\n"," 64% 7575/11873 [00:26<00:15, 285.82it/s]\u001b[A\n"," 64% 7604/11873 [00:26<00:15, 279.12it/s]\u001b[A\n"," 64% 7632/11873 [00:26<00:15, 278.37it/s]\u001b[A\n"," 65% 7661/11873 [00:26<00:14, 280.90it/s]\u001b[A\n"," 65% 7690/11873 [00:26<00:14, 282.67it/s]\u001b[A\n"," 65% 7719/11873 [00:27<00:15, 263.55it/s]\u001b[A\n"," 65% 7748/11873 [00:27<00:15, 269.54it/s]\u001b[A\n"," 66% 7777/11873 [00:27<00:14, 275.11it/s]\u001b[A\n"," 66% 7806/11873 [00:27<00:14, 278.41it/s]\u001b[A\n"," 66% 7835/11873 [00:27<00:14, 279.53it/s]\u001b[A\n"," 66% 7864/11873 [00:27<00:14, 281.20it/s]\u001b[A\n"," 66% 7893/11873 [00:27<00:15, 262.41it/s]\u001b[A\n"," 67% 7923/11873 [00:27<00:14, 271.13it/s]\u001b[A\n"," 67% 7954/11873 [00:27<00:13, 280.18it/s]\u001b[A\n"," 67% 7983/11873 [00:27<00:14, 276.41it/s]\u001b[A\n"," 67% 8014/11873 [00:28<00:13, 284.77it/s]\u001b[A\n"," 68% 8043/11873 [00:28<00:13, 286.18it/s]\u001b[A\n"," 68% 8072/11873 [00:28<00:13, 286.57it/s]\u001b[A\n"," 68% 8101/11873 [00:28<00:13, 286.39it/s]\u001b[A\n"," 68% 8130/11873 [00:28<00:13, 286.35it/s]\u001b[A\n"," 69% 8159/11873 [00:28<00:13, 285.34it/s]\u001b[A\n"," 69% 8188/11873 [00:28<00:12, 285.65it/s]\u001b[A\n"," 69% 8217/11873 [00:28<00:12, 283.74it/s]\u001b[A\n"," 69% 8247/11873 [00:28<00:12, 288.41it/s]\u001b[A\n"," 70% 8277/11873 [00:28<00:12, 290.31it/s]\u001b[A\n"," 70% 8307/11873 [00:29<00:12, 290.87it/s]\u001b[A\n"," 70% 8337/11873 [00:29<00:12, 289.12it/s]\u001b[A\n"," 70% 8366/11873 [00:29<00:12, 283.95it/s]\u001b[A\n"," 71% 8395/11873 [00:29<00:12, 283.63it/s]\u001b[A\n"," 71% 8424/11873 [00:29<00:12, 283.47it/s]\u001b[A\n"," 71% 8453/11873 [00:29<00:12, 283.28it/s]\u001b[A\n"," 71% 8482/11873 [00:29<00:12, 282.40it/s]\u001b[A\n"," 72% 8513/11873 [00:29<00:11, 288.44it/s]\u001b[A\n"," 72% 8542/11873 [00:29<00:11, 288.77it/s]\u001b[A\n"," 72% 8572/11873 [00:30<00:11, 290.99it/s]\u001b[A\n"," 72% 8602/11873 [00:30<00:11, 290.98it/s]\u001b[A\n"," 73% 8632/11873 [00:30<00:11, 286.94it/s]\u001b[A\n"," 73% 8662/11873 [00:30<00:11, 290.48it/s]\u001b[A\n"," 73% 8692/11873 [00:30<00:10, 291.43it/s]\u001b[A\n"," 73% 8722/11873 [00:30<00:10, 290.26it/s]\u001b[A\n"," 74% 8752/11873 [00:30<00:10, 290.75it/s]\u001b[A\n"," 74% 8782/11873 [00:30<00:10, 290.51it/s]\u001b[A\n"," 74% 8812/11873 [00:30<00:10, 289.73it/s]\u001b[A\n"," 74% 8842/11873 [00:30<00:10, 290.40it/s]\u001b[A\n"," 75% 8872/11873 [00:31<00:10, 292.70it/s]\u001b[A\n"," 75% 8902/11873 [00:31<00:10, 290.69it/s]\u001b[A\n"," 75% 8932/11873 [00:31<00:10, 287.23it/s]\u001b[A\n"," 75% 8961/11873 [00:31<00:10, 283.35it/s]\u001b[A\n"," 76% 8991/11873 [00:31<00:10, 286.02it/s]\u001b[A\n"," 76% 9020/11873 [00:31<00:09, 286.74it/s]\u001b[A\n"," 76% 9049/11873 [00:31<00:09, 286.27it/s]\u001b[A\n"," 76% 9078/11873 [00:31<00:09, 285.65it/s]\u001b[A\n"," 77% 9107/11873 [00:31<00:09, 285.73it/s]\u001b[A\n"," 77% 9136/11873 [00:31<00:09, 285.64it/s]\u001b[A\n"," 77% 9166/11873 [00:32<00:09, 289.70it/s]\u001b[A\n"," 77% 9195/11873 [00:32<00:09, 289.57it/s]\u001b[A\n"," 78% 9224/11873 [00:32<00:09, 288.88it/s]\u001b[A\n"," 78% 9253/11873 [00:32<00:09, 288.09it/s]\u001b[A\n"," 78% 9283/11873 [00:32<00:08, 290.27it/s]\u001b[A\n"," 78% 9313/11873 [00:32<00:08, 291.57it/s]\u001b[A\n"," 79% 9343/11873 [00:32<00:08, 289.42it/s]\u001b[A\n"," 79% 9372/11873 [00:32<00:08, 288.26it/s]\u001b[A\n"," 79% 9401/11873 [00:32<00:08, 285.56it/s]\u001b[A\n"," 79% 9430/11873 [00:32<00:08, 285.80it/s]\u001b[A\n"," 80% 9460/11873 [00:33<00:08, 287.33it/s]\u001b[A\n"," 80% 9490/11873 [00:33<00:08, 288.62it/s]\u001b[A\n"," 80% 9520/11873 [00:33<00:08, 291.40it/s]\u001b[A\n"," 80% 9550/11873 [00:33<00:07, 292.74it/s]\u001b[A\n"," 81% 9580/11873 [00:33<00:07, 294.36it/s]\u001b[A\n"," 81% 9610/11873 [00:33<00:07, 291.33it/s]\u001b[A\n"," 81% 9640/11873 [00:33<00:07, 290.58it/s]\u001b[A\n"," 81% 9670/11873 [00:33<00:07, 291.35it/s]\u001b[A\n"," 82% 9700/11873 [00:33<00:07, 293.43it/s]\u001b[A\n"," 82% 9731/11873 [00:34<00:07, 295.55it/s]\u001b[A\n"," 82% 9762/11873 [00:34<00:07, 296.05it/s]\u001b[A\n"," 82% 9792/11873 [00:34<00:07, 294.39it/s]\u001b[A\n"," 83% 9822/11873 [00:34<00:06, 293.38it/s]\u001b[A\n"," 83% 9852/11873 [00:34<00:06, 294.07it/s]\u001b[A\n"," 83% 9882/11873 [00:34<00:06, 295.28it/s]\u001b[A\n"," 83% 9912/11873 [00:34<00:06, 293.03it/s]\u001b[A\n"," 84% 9942/11873 [00:34<00:06, 292.27it/s]\u001b[A\n"," 84% 9972/11873 [00:34<00:06, 291.74it/s]\u001b[A\n"," 84% 10002/11873 [00:34<00:06, 290.16it/s]\u001b[A\n"," 85% 10033/11873 [00:35<00:06, 293.67it/s]\u001b[A\n"," 85% 10063/11873 [00:35<00:06, 295.20it/s]\u001b[A\n"," 85% 10093/11873 [00:35<00:06, 295.25it/s]\u001b[A\n"," 85% 10123/11873 [00:35<00:05, 296.51it/s]\u001b[A\n"," 86% 10153/11873 [00:35<00:05, 295.41it/s]\u001b[A\n"," 86% 10183/11873 [00:35<00:05, 296.49it/s]\u001b[A\n"," 86% 10213/11873 [00:35<00:05, 292.05it/s]\u001b[A\n"," 86% 10243/11873 [00:35<00:05, 292.52it/s]\u001b[A\n"," 87% 10273/11873 [00:35<00:05, 290.52it/s]\u001b[A\n"," 87% 10303/11873 [00:35<00:05, 286.44it/s]\u001b[A\n"," 87% 10333/11873 [00:36<00:05, 289.96it/s]\u001b[A\n"," 87% 10363/11873 [00:36<00:05, 289.85it/s]\u001b[A\n"," 88% 10393/11873 [00:36<00:05, 290.26it/s]\u001b[A\n"," 88% 10423/11873 [00:36<00:05, 279.25it/s]\u001b[A\n"," 88% 10452/11873 [00:36<00:05, 269.87it/s]\u001b[A\n"," 88% 10482/11873 [00:36<00:05, 277.27it/s]\u001b[A\n"," 89% 10511/11873 [00:36<00:04, 279.11it/s]\u001b[A\n"," 89% 10540/11873 [00:36<00:04, 281.71it/s]\u001b[A\n"," 89% 10569/11873 [00:36<00:04, 264.20it/s]\u001b[A\n"," 89% 10598/11873 [00:37<00:04, 270.98it/s]\u001b[A\n"," 90% 10628/11873 [00:37<00:04, 276.72it/s]\u001b[A\n"," 90% 10656/11873 [00:37<00:04, 276.98it/s]\u001b[A\n"," 90% 10686/11873 [00:37<00:04, 282.70it/s]\u001b[A\n"," 90% 10716/11873 [00:37<00:04, 286.44it/s]\u001b[A\n"," 90% 10745/11873 [00:37<00:03, 286.76it/s]\u001b[A\n"," 91% 10774/11873 [00:37<00:03, 287.63it/s]\u001b[A\n"," 91% 10803/11873 [00:37<00:03, 284.08it/s]\u001b[A\n"," 91% 10832/11873 [00:37<00:04, 258.69it/s]\u001b[A\n"," 91% 10861/11873 [00:38<00:03, 265.72it/s]\u001b[A\n"," 92% 10890/11873 [00:38<00:03, 270.42it/s]\u001b[A\n"," 92% 10919/11873 [00:38<00:03, 275.75it/s]\u001b[A\n"," 92% 10947/11873 [00:38<00:03, 273.37it/s]\u001b[A\n"," 92% 10976/11873 [00:38<00:03, 277.45it/s]\u001b[A\n"," 93% 11006/11873 [00:38<00:03, 283.61it/s]\u001b[A\n"," 93% 11036/11873 [00:38<00:02, 286.67it/s]\u001b[A\n"," 93% 11065/11873 [00:38<00:02, 287.54it/s]\u001b[A\n"," 93% 11095/11873 [00:38<00:02, 291.03it/s]\u001b[A\n"," 94% 11125/11873 [00:38<00:02, 289.54it/s]\u001b[A\n"," 94% 11155/11873 [00:39<00:02, 290.84it/s]\u001b[A\n"," 94% 11185/11873 [00:39<00:02, 290.95it/s]\u001b[A\n"," 94% 11215/11873 [00:39<00:02, 292.57it/s]\u001b[A\n"," 95% 11245/11873 [00:39<00:02, 287.93it/s]\u001b[A\n"," 95% 11275/11873 [00:39<00:02, 290.88it/s]\u001b[A\n"," 95% 11306/11873 [00:39<00:01, 293.91it/s]\u001b[A\n"," 95% 11336/11873 [00:39<00:01, 288.27it/s]\u001b[A\n"," 96% 11365/11873 [00:39<00:01, 285.97it/s]\u001b[A\n"," 96% 11395/11873 [00:39<00:01, 288.33it/s]\u001b[A\n"," 96% 11424/11873 [00:39<00:01, 287.92it/s]\u001b[A\n"," 96% 11455/11873 [00:40<00:01, 292.42it/s]\u001b[A\n"," 97% 11485/11873 [00:40<00:01, 291.21it/s]\u001b[A\n"," 97% 11515/11873 [00:40<00:01, 291.40it/s]\u001b[A\n"," 97% 11545/11873 [00:40<00:01, 289.25it/s]\u001b[A\n"," 97% 11575/11873 [00:40<00:01, 290.76it/s]\u001b[A\n"," 98% 11605/11873 [00:40<00:00, 289.58it/s]\u001b[A\n"," 98% 11635/11873 [00:40<00:00, 289.74it/s]\u001b[A\n"," 98% 11665/11873 [00:40<00:00, 290.73it/s]\u001b[A\n"," 99% 11695/11873 [00:40<00:00, 286.48it/s]\u001b[A\n"," 99% 11725/11873 [00:40<00:00, 289.26it/s]\u001b[A\n"," 99% 11754/11873 [00:41<00:00, 288.39it/s]\u001b[A\n"," 99% 11783/11873 [00:41<00:00, 288.71it/s]\u001b[A\n"," 99% 11812/11873 [00:41<00:00, 288.32it/s]\u001b[A\n","100% 11841/11873 [00:41<00:00, 287.47it/s]\u001b[A\n","100% 11873/11873 [00:41<00:00, 286.10it/s]\n","04/06/2022 06:56:18 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/eval_predictions.json.\n","04/06/2022 06:56:18 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/eval_nbest_predictions.json.\n","04/06/2022 06:56:21 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/bert-base-cased/back-trans-aug/eval_null_odds.json.\n","04/06/2022 06:56:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1525/1525 [04:03<00:00,  6.26it/s]\n","***** eval metrics *****\n","  epoch                  =     3.0\n","  eval_HasAns_exact      = 71.9636\n","  eval_HasAns_f1         = 78.7745\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 68.9151\n","  eval_NoAns_f1          = 68.9151\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 70.4371\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 73.8377\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 70.4371\n","  eval_f1                = 73.8377\n","  eval_samples           =   12199\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 06:56:25,684 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_back_trans_aug', 'type': 'sichenzhong/squad_v2_back_trans_aug', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path albert-base-v2 \\\n","  --dataset_name sichenzhong/squad_v2_back_trans_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 512 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29tuxIN6VnJx","executionInfo":{"status":"ok","timestamp":1649243557878,"user_tz":240,"elapsed":15367477,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"89557aff-ac79-4225-d5c9-ac672ef6dc0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["04/06/2022 06:56:38 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 06:56:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/runs/Apr06_06-56-38_2439d11b3fff,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 06:56:39 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b\n","04/06/2022 06:56:39 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/06/2022 06:56:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","04/06/2022 06:56:39 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","04/06/2022 06:56:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","100% 2/2 [00:00<00:00, 348.44it/s]\n","[INFO|hub.py:583] 2022-04-06 06:56:40,141 >> https://huggingface.co/albert-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppspm1msj\n","Downloading: 100% 684/684 [00:00<00:00, 598kB/s]\n","[INFO|hub.py:587] 2022-04-06 06:56:40,488 >> storing https://huggingface.co/albert-base-v2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|hub.py:595] 2022-04-06 06:56:40,488 >> creating metadata file for /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:654] 2022-04-06 06:56:40,488 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-06 06:56:40,491 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-06 06:56:40,838 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-06 06:56:41,184 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-06 06:56:41,185 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 06:56:41,884 >> https://huggingface.co/albert-base-v2/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwp6kbpgw\n","Downloading: 100% 742k/742k [00:00<00:00, 1.84MB/s]\n","[INFO|hub.py:587] 2022-04-06 06:56:42,649 >> storing https://huggingface.co/albert-base-v2/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:595] 2022-04-06 06:56:42,649 >> creating metadata file for /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|hub.py:583] 2022-04-06 06:56:42,998 >> https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphk0cnfa4\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 2.67MB/s]\n","[INFO|hub.py:587] 2022-04-06 06:56:43,857 >> storing https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|hub.py:595] 2022-04-06 06:56:43,857 >> creating metadata file for /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:56:44,902 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:56:44,902 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:56:44,902 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:56:44,902 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 06:56:44,902 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-06 06:56:45,247 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:690] 2022-04-06 06:56:45,248 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 06:56:45,745 >> https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzgrh3goa\n","Downloading: 100% 45.2M/45.2M [00:01<00:00, 42.6MB/s]\n","[INFO|hub.py:587] 2022-04-06 06:56:46,935 >> storing https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|hub.py:595] 2022-04-06 06:56:46,935 >> creating metadata file for /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1772] 2022-04-06 06:56:46,936 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[WARNING|modeling_utils.py:2050] 2022-04-06 06:56:47,076 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']\n","- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-06 06:56:47,076 >> Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/06/2022 06:56:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-1ee5f625ce0e2102.arrow\n","Running tokenizer on train dataset: 100% 131/131 [01:08<00:00,  1.91ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/06/2022 06:57:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-295017f4d85fe626.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:30<00:00,  7.55s/ba]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 06:59:31,012 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 06:59:31,012 >>   Num examples = 130546\n","[INFO|trainer.py:1292] 2022-04-06 06:59:31,012 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-06 06:59:31,012 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1294] 2022-04-06 06:59:31,012 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1295] 2022-04-06 06:59:31,012 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 06:59:31,012 >>   Total optimization steps = 16320\n","{'loss': 1.7587, 'learning_rate': 1.9387254901960785e-05, 'epoch': 0.06}\n","  3% 500/16320 [07:33<3:58:54,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:07:04,826 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:07:04,833 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:07:04,968 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:07:04,974 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:07:04,978 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 1.3166, 'learning_rate': 1.877450980392157e-05, 'epoch': 0.12}\n","  6% 1000/16320 [15:08<3:51:49,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:14:39,347 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 07:14:39,354 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:14:39,491 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:14:39,496 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:14:39,500 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.2159, 'learning_rate': 1.8161764705882355e-05, 'epoch': 0.18}\n","  9% 1500/16320 [22:42<3:44:23,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:22:13,900 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:22:13,906 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:22:14,040 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:22:14,045 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:22:14,049 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.1316, 'learning_rate': 1.7549019607843138e-05, 'epoch': 0.25}\n"," 12% 2000/16320 [30:17<3:36:57,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:29:48,679 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 07:29:48,686 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:29:48,824 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:29:48,830 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:29:48,835 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.0962, 'learning_rate': 1.693627450980392e-05, 'epoch': 0.31}\n"," 15% 2500/16320 [37:52<3:29:31,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:37:23,477 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:37:23,483 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:37:23,613 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:37:23,618 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:37:23,621 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.0851, 'learning_rate': 1.6323529411764708e-05, 'epoch': 0.37}\n"," 18% 3000/16320 [45:27<3:21:42,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:44:58,329 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 07:44:58,337 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:44:58,471 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:44:58,477 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:44:58,482 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.045, 'learning_rate': 1.571078431372549e-05, 'epoch': 0.43}\n"," 21% 3500/16320 [53:02<3:13:54,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 07:52:33,280 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 07:52:33,286 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 07:52:33,422 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 07:52:33,428 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 07:52:33,431 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.0578, 'learning_rate': 1.5098039215686276e-05, 'epoch': 0.49}\n"," 25% 4000/16320 [1:00:37<3:06:43,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:00:08,392 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:00:08,397 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:00:08,530 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:00:08,535 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:00:08,539 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.0297, 'learning_rate': 1.448529411764706e-05, 'epoch': 0.55}\n"," 28% 4500/16320 [1:08:12<2:59:23,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:07:43,304 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:07:43,311 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:07:43,440 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:07:43,445 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:07:43,448 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.9838, 'learning_rate': 1.3872549019607844e-05, 'epoch': 0.61}\n"," 31% 5000/16320 [1:15:47<2:51:24,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:15:18,335 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:15:18,341 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:15:18,470 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:15:18,475 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:15:18,479 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.0043, 'learning_rate': 1.3259803921568627e-05, 'epoch': 0.67}\n"," 34% 5500/16320 [1:23:22<2:44:04,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:22:53,527 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:22:53,533 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:22:53,660 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:22:53,666 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:22:53,670 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.9978, 'learning_rate': 1.2647058823529412e-05, 'epoch': 0.74}\n"," 37% 6000/16320 [1:30:57<2:36:22,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:30:28,642 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:30:28,648 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:30:28,777 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:30:28,783 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:30:28,786 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.9528, 'learning_rate': 1.2034313725490197e-05, 'epoch': 0.8}\n"," 40% 6500/16320 [1:38:32<2:28:57,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:38:03,743 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:38:03,749 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:38:03,882 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:38:03,888 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:38:03,892 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.9562, 'learning_rate': 1.142156862745098e-05, 'epoch': 0.86}\n"," 43% 7000/16320 [1:46:07<2:21:14,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:45:38,875 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 08:45:38,881 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:45:39,008 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:45:39,014 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:45:39,034 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.944, 'learning_rate': 1.0808823529411765e-05, 'epoch': 0.92}\n"," 46% 7500/16320 [1:53:42<2:14:03,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 08:53:14,040 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 08:53:14,045 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 08:53:14,194 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 08:53:14,199 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 08:53:14,203 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.9442, 'learning_rate': 1.0196078431372549e-05, 'epoch': 0.98}\n"," 49% 8000/16320 [2:01:18<2:06:03,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:00:49,324 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:00:49,349 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:00:49,480 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:00:49,485 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:00:49,489 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.779, 'learning_rate': 9.583333333333335e-06, 'epoch': 1.04}\n"," 52% 8500/16320 [2:08:52<1:58:26,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:08:23,946 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:08:23,953 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:08:24,088 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:08:24,098 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:08:24,102 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.7289, 'learning_rate': 8.970588235294119e-06, 'epoch': 1.1}\n"," 55% 9000/16320 [2:16:28<1:51:05,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:15:59,438 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:15:59,446 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:15:59,574 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:15:59,580 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:15:59,584 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.7513, 'learning_rate': 8.357843137254903e-06, 'epoch': 1.16}\n"," 58% 9500/16320 [2:24:03<1:43:37,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:23:34,842 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:23:34,849 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:23:34,980 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:23:34,985 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:23:34,988 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.7088, 'learning_rate': 7.745098039215687e-06, 'epoch': 1.23}\n"," 61% 10000/16320 [2:31:39<1:35:41,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:31:10,153 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:31:10,159 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:31:10,292 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:31:10,297 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:31:10,301 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.7458, 'learning_rate': 7.132352941176472e-06, 'epoch': 1.29}\n"," 64% 10500/16320 [2:39:14<1:28:16,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:38:45,350 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:38:45,356 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:38:45,490 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:38:45,495 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:38:45,499 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.7197, 'learning_rate': 6.519607843137256e-06, 'epoch': 1.35}\n"," 67% 11000/16320 [2:46:49<1:20:32,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:46:20,660 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-06 09:46:20,665 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:46:20,798 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:46:20,803 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:46:20,806 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.7042, 'learning_rate': 5.90686274509804e-06, 'epoch': 1.41}\n"," 70% 11500/16320 [2:54:24<1:13:00,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 09:53:55,942 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-11500\n","[INFO|configuration_utils.py:441] 2022-04-06 09:53:55,950 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 09:53:56,081 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 09:53:56,087 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 09:53:56,091 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.7153, 'learning_rate': 5.294117647058824e-06, 'epoch': 1.47}\n"," 74% 12000/16320 [3:02:00<1:05:26,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:01:31,120 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-12000\n","[INFO|configuration_utils.py:441] 2022-04-06 10:01:31,126 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:01:31,259 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:01:31,265 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:01:31,270 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.7101, 'learning_rate': 4.681372549019608e-06, 'epoch': 1.53}\n"," 77% 12500/16320 [3:09:35<57:55,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:09:06,310 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-12500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:09:06,318 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:09:06,446 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:09:06,451 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:09:06,455 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.7039, 'learning_rate': 4.068627450980392e-06, 'epoch': 1.59}\n"," 80% 13000/16320 [3:17:10<50:22,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:16:41,453 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-13000\n","[INFO|configuration_utils.py:441] 2022-04-06 10:16:41,460 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:16:41,594 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:16:41,600 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:16:41,604 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.7188, 'learning_rate': 3.4558823529411766e-06, 'epoch': 1.65}\n"," 83% 13500/16320 [3:24:45<42:46,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:24:16,703 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-13500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:24:16,709 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:24:16,842 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:24:16,848 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:24:16,852 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.6933, 'learning_rate': 2.843137254901961e-06, 'epoch': 1.72}\n"," 86% 14000/16320 [3:32:20<35:10,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:31:51,851 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-14000\n","[INFO|configuration_utils.py:441] 2022-04-06 10:31:51,857 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:31:51,987 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:31:51,993 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:31:51,997 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.6755, 'learning_rate': 2.2303921568627456e-06, 'epoch': 1.78}\n"," 89% 14500/16320 [3:39:55<27:34,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:39:26,868 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-14500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:39:26,874 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:39:27,003 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:39:27,008 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:39:27,011 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.6653, 'learning_rate': 1.6176470588235297e-06, 'epoch': 1.84}\n"," 92% 15000/16320 [3:47:30<19:59,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:47:01,990 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-15000\n","[INFO|configuration_utils.py:441] 2022-04-06 10:47:01,996 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:47:02,130 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:47:02,135 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:47:02,139 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.6772, 'learning_rate': 1.0049019607843138e-06, 'epoch': 1.9}\n"," 95% 15500/16320 [3:55:06<12:26,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 10:54:37,052 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-15500\n","[INFO|configuration_utils.py:441] 2022-04-06 10:54:37,059 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 10:54:37,193 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 10:54:37,198 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 10:54:37,201 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.6733, 'learning_rate': 3.921568627450981e-07, 'epoch': 1.96}\n"," 98% 16000/16320 [4:02:40<04:50,  1.10it/s][INFO|trainer.py:2166] 2022-04-06 11:02:12,013 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-16000\n","[INFO|configuration_utils.py:441] 2022-04-06 11:02:12,020 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:02:12,155 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:02:12,160 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:02:12,163 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/checkpoint-16000/special_tokens_map.json\n","100% 16320/16320 [4:07:31<00:00,  1.48it/s][INFO|trainer.py:1530] 2022-04-06 11:07:02,543 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 14851.5307, 'train_samples_per_second': 17.58, 'train_steps_per_second': 1.099, 'train_loss': 0.8982416134254605, 'epoch': 2.0}\n","100% 16320/16320 [4:07:31<00:00,  1.10it/s]\n","[INFO|trainer.py:2166] 2022-04-06 11:07:02,547 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug\n","[INFO|configuration_utils.py:441] 2022-04-06 11:07:02,553 >> Configuration saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:07:02,682 >> Model weights saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:07:02,687 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:07:02,691 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.8982\n","  train_runtime            = 4:07:31.53\n","  train_samples            =     130546\n","  train_samples_per_second =      17.58\n","  train_steps_per_second   =      1.099\n","04/06/2022 11:07:02 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 11:07:02,720 >> The following columns in the evaluation set  don't have a corresponding argument in `AlbertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `AlbertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 11:07:02,722 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 11:07:02,722 >>   Num examples = 11968\n","[INFO|trainer.py:2421] 2022-04-06 11:07:02,722 >>   Batch size = 8\n","100% 1496/1496 [04:20<00:00,  5.73it/s]04/06/2022 11:11:41 - INFO - utils_qa - Post-processing 11873 example predictions split into 11968 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 32/11873 [00:00<00:37, 317.81it/s]\u001b[A\n","  1% 65/11873 [00:00<00:37, 318.40it/s]\u001b[A\n","  1% 97/11873 [00:00<00:37, 312.81it/s]\u001b[A\n","  1% 130/11873 [00:00<00:37, 317.16it/s]\u001b[A\n","  1% 164/11873 [00:00<00:36, 324.34it/s]\u001b[A\n","  2% 197/11873 [00:00<00:36, 322.59it/s]\u001b[A\n","  2% 232/11873 [00:00<00:35, 330.79it/s]\u001b[A\n","  2% 266/11873 [00:00<00:35, 329.60it/s]\u001b[A\n","  3% 301/11873 [00:00<00:34, 333.57it/s]\u001b[A\n","  3% 335/11873 [00:01<00:35, 328.87it/s]\u001b[A\n","  3% 369/11873 [00:01<00:34, 330.17it/s]\u001b[A\n","  3% 403/11873 [00:01<00:34, 330.04it/s]\u001b[A\n","  4% 438/11873 [00:01<00:34, 334.34it/s]\u001b[A\n","  4% 472/11873 [00:01<00:34, 333.79it/s]\u001b[A\n","  4% 506/11873 [00:01<00:34, 330.83it/s]\u001b[A\n","  5% 540/11873 [00:01<00:34, 324.02it/s]\u001b[A\n","  5% 574/11873 [00:01<00:34, 325.83it/s]\u001b[A\n","  5% 608/11873 [00:01<00:34, 329.45it/s]\u001b[A\n","  5% 643/11873 [00:01<00:33, 334.34it/s]\u001b[A\n","  6% 678/11873 [00:02<00:33, 336.70it/s]\u001b[A\n","  6% 712/11873 [00:02<00:33, 329.08it/s]\u001b[A\n","  6% 745/11873 [00:02<00:34, 326.22it/s]\u001b[A\n","  7% 778/11873 [00:02<00:34, 325.03it/s]\u001b[A\n","  7% 813/11873 [00:02<00:33, 330.41it/s]\u001b[A\n","  7% 847/11873 [00:02<00:33, 327.74it/s]\u001b[A\n","  7% 884/11873 [00:02<00:32, 338.23it/s]\u001b[A\n","  8% 919/11873 [00:02<00:32, 340.00it/s]\u001b[A\n","  8% 955/11873 [00:02<00:31, 344.29it/s]\u001b[A\n","  8% 990/11873 [00:02<00:32, 337.70it/s]\u001b[A\n","  9% 1024/11873 [00:03<00:33, 320.78it/s]\u001b[A\n","  9% 1057/11873 [00:03<00:36, 298.72it/s]\u001b[A\n","  9% 1088/11873 [00:03<00:37, 284.18it/s]\u001b[A\n","  9% 1117/11873 [00:03<00:39, 273.44it/s]\u001b[A\n"," 10% 1145/11873 [00:03<00:40, 268.15it/s]\u001b[A\n"," 10% 1172/11873 [00:03<00:40, 264.89it/s]\u001b[A\n"," 10% 1199/11873 [00:03<00:40, 260.68it/s]\u001b[A\n"," 10% 1226/11873 [00:03<00:40, 259.83it/s]\u001b[A\n"," 11% 1253/11873 [00:04<00:41, 255.82it/s]\u001b[A\n"," 11% 1279/11873 [00:04<00:41, 255.01it/s]\u001b[A\n"," 11% 1305/11873 [00:04<00:41, 255.37it/s]\u001b[A\n"," 11% 1331/11873 [00:04<00:41, 255.27it/s]\u001b[A\n"," 11% 1357/11873 [00:04<00:41, 254.39it/s]\u001b[A\n"," 12% 1383/11873 [00:04<00:41, 252.67it/s]\u001b[A\n"," 12% 1410/11873 [00:04<00:40, 255.30it/s]\u001b[A\n"," 12% 1436/11873 [00:04<00:41, 254.18it/s]\u001b[A\n"," 12% 1462/11873 [00:04<00:40, 255.10it/s]\u001b[A\n"," 13% 1488/11873 [00:04<00:40, 255.92it/s]\u001b[A\n"," 13% 1514/11873 [00:05<00:40, 256.16it/s]\u001b[A\n"," 13% 1540/11873 [00:05<00:40, 253.98it/s]\u001b[A\n"," 13% 1566/11873 [00:05<00:40, 252.00it/s]\u001b[A\n"," 13% 1592/11873 [00:05<00:41, 249.70it/s]\u001b[A\n"," 14% 1617/11873 [00:05<00:41, 246.90it/s]\u001b[A\n"," 14% 1643/11873 [00:05<00:41, 248.83it/s]\u001b[A\n"," 14% 1668/11873 [00:05<00:41, 247.43it/s]\u001b[A\n"," 14% 1693/11873 [00:05<00:41, 247.55it/s]\u001b[A\n"," 14% 1719/11873 [00:05<00:40, 248.14it/s]\u001b[A\n"," 15% 1745/11873 [00:05<00:40, 251.15it/s]\u001b[A\n"," 15% 1772/11873 [00:06<00:39, 253.94it/s]\u001b[A\n"," 15% 1798/11873 [00:06<00:40, 251.77it/s]\u001b[A\n"," 15% 1824/11873 [00:06<00:40, 250.70it/s]\u001b[A\n"," 16% 1850/11873 [00:06<00:39, 251.79it/s]\u001b[A\n"," 16% 1876/11873 [00:06<00:40, 247.69it/s]\u001b[A\n"," 16% 1901/11873 [00:06<00:40, 246.04it/s]\u001b[A\n"," 16% 1926/11873 [00:06<00:41, 241.63it/s]\u001b[A\n"," 16% 1952/11873 [00:06<00:40, 245.01it/s]\u001b[A\n"," 17% 1977/11873 [00:06<00:40, 246.24it/s]\u001b[A\n"," 17% 2003/11873 [00:07<00:39, 249.50it/s]\u001b[A\n"," 17% 2029/11873 [00:07<00:39, 251.29it/s]\u001b[A\n"," 17% 2055/11873 [00:07<00:38, 251.79it/s]\u001b[A\n"," 18% 2081/11873 [00:07<00:39, 251.00it/s]\u001b[A\n"," 18% 2107/11873 [00:07<00:38, 252.30it/s]\u001b[A\n"," 18% 2133/11873 [00:07<00:38, 252.88it/s]\u001b[A\n"," 18% 2159/11873 [00:07<00:38, 254.23it/s]\u001b[A\n"," 18% 2185/11873 [00:07<00:38, 248.99it/s]\u001b[A\n"," 19% 2211/11873 [00:07<00:38, 252.03it/s]\u001b[A\n"," 19% 2237/11873 [00:07<00:38, 252.34it/s]\u001b[A\n"," 19% 2263/11873 [00:08<00:37, 253.14it/s]\u001b[A\n"," 19% 2289/11873 [00:08<00:38, 252.14it/s]\u001b[A\n"," 19% 2315/11873 [00:08<00:37, 253.41it/s]\u001b[A\n"," 20% 2341/11873 [00:08<00:37, 254.58it/s]\u001b[A\n"," 20% 2368/11873 [00:08<00:37, 256.34it/s]\u001b[A\n"," 20% 2394/11873 [00:08<00:36, 256.89it/s]\u001b[A\n"," 20% 2420/11873 [00:08<00:36, 256.36it/s]\u001b[A\n"," 21% 2446/11873 [00:08<00:37, 251.06it/s]\u001b[A\n"," 21% 2472/11873 [00:08<00:37, 248.73it/s]\u001b[A\n"," 21% 2497/11873 [00:08<00:37, 247.84it/s]\u001b[A\n"," 21% 2522/11873 [00:09<00:38, 245.81it/s]\u001b[A\n"," 21% 2548/11873 [00:09<00:37, 249.78it/s]\u001b[A\n"," 22% 2574/11873 [00:09<00:37, 250.12it/s]\u001b[A\n"," 22% 2600/11873 [00:09<00:37, 247.14it/s]\u001b[A\n"," 22% 2626/11873 [00:09<00:37, 249.09it/s]\u001b[A\n"," 22% 2652/11873 [00:09<00:36, 249.81it/s]\u001b[A\n"," 23% 2678/11873 [00:09<00:36, 251.42it/s]\u001b[A\n"," 23% 2704/11873 [00:09<00:36, 252.30it/s]\u001b[A\n"," 23% 2730/11873 [00:09<00:36, 251.50it/s]\u001b[A\n"," 23% 2756/11873 [00:10<00:36, 252.14it/s]\u001b[A\n"," 23% 2782/11873 [00:10<00:35, 252.55it/s]\u001b[A\n"," 24% 2809/11873 [00:10<00:35, 254.94it/s]\u001b[A\n"," 24% 2835/11873 [00:10<00:35, 254.87it/s]\u001b[A\n"," 24% 2861/11873 [00:10<00:35, 256.36it/s]\u001b[A\n"," 24% 2887/11873 [00:10<00:35, 254.53it/s]\u001b[A\n"," 25% 2913/11873 [00:10<00:35, 253.84it/s]\u001b[A\n"," 25% 2939/11873 [00:10<00:35, 252.76it/s]\u001b[A\n"," 25% 2965/11873 [00:10<00:35, 252.55it/s]\u001b[A\n"," 25% 2991/11873 [00:10<00:35, 249.74it/s]\u001b[A\n"," 25% 3016/11873 [00:11<00:35, 247.40it/s]\u001b[A\n"," 26% 3042/11873 [00:11<00:35, 249.54it/s]\u001b[A\n"," 26% 3067/11873 [00:11<00:36, 243.82it/s]\u001b[A\n"," 26% 3092/11873 [00:11<00:36, 242.33it/s]\u001b[A\n"," 26% 3117/11873 [00:11<00:40, 217.40it/s]\u001b[A\n"," 26% 3140/11873 [00:11<00:40, 217.00it/s]\u001b[A\n"," 27% 3163/11873 [00:11<00:42, 202.79it/s]\u001b[A\n"," 27% 3189/11873 [00:11<00:40, 215.88it/s]\u001b[A\n"," 27% 3215/11873 [00:11<00:38, 226.34it/s]\u001b[A\n"," 27% 3242/11873 [00:12<00:36, 236.73it/s]\u001b[A\n"," 28% 3266/11873 [00:12<00:36, 236.75it/s]\u001b[A\n"," 28% 3290/11873 [00:12<00:43, 197.25it/s]\u001b[A\n"," 28% 3311/11873 [00:12<00:45, 189.10it/s]\u001b[A\n"," 28% 3331/11873 [00:12<00:47, 181.72it/s]\u001b[A\n"," 28% 3351/11873 [00:12<00:46, 184.27it/s]\u001b[A\n"," 28% 3370/11873 [00:12<00:50, 168.32it/s]\u001b[A\n"," 29% 3395/11873 [00:12<00:44, 188.70it/s]\u001b[A\n"," 29% 3421/11873 [00:13<00:41, 205.82it/s]\u001b[A\n"," 29% 3447/11873 [00:13<00:38, 218.60it/s]\u001b[A\n"," 29% 3473/11873 [00:13<00:36, 228.09it/s]\u001b[A\n"," 29% 3498/11873 [00:13<00:36, 232.30it/s]\u001b[A\n"," 30% 3524/11873 [00:13<00:34, 239.42it/s]\u001b[A\n"," 30% 3550/11873 [00:13<00:33, 245.13it/s]\u001b[A\n"," 30% 3575/11873 [00:13<00:33, 245.80it/s]\u001b[A\n"," 30% 3601/11873 [00:13<00:33, 247.73it/s]\u001b[A\n"," 31% 3628/11873 [00:13<00:32, 250.58it/s]\u001b[A\n"," 31% 3654/11873 [00:13<00:32, 251.32it/s]\u001b[A\n"," 31% 3680/11873 [00:14<00:32, 248.72it/s]\u001b[A\n"," 31% 3705/11873 [00:14<00:33, 243.18it/s]\u001b[A\n"," 31% 3730/11873 [00:14<00:33, 244.06it/s]\u001b[A\n"," 32% 3756/11873 [00:14<00:32, 246.88it/s]\u001b[A\n"," 32% 3781/11873 [00:14<00:32, 245.94it/s]\u001b[A\n"," 32% 3806/11873 [00:14<00:32, 246.43it/s]\u001b[A\n"," 32% 3831/11873 [00:14<00:32, 244.56it/s]\u001b[A\n"," 32% 3857/11873 [00:14<00:32, 248.41it/s]\u001b[A\n"," 33% 3882/11873 [00:14<00:32, 243.44it/s]\u001b[A\n"," 33% 3908/11873 [00:14<00:32, 247.49it/s]\u001b[A\n"," 33% 3933/11873 [00:15<00:32, 244.49it/s]\u001b[A\n"," 33% 3958/11873 [00:15<00:32, 246.07it/s]\u001b[A\n"," 34% 3983/11873 [00:15<00:32, 244.26it/s]\u001b[A\n"," 34% 4010/11873 [00:15<00:31, 249.17it/s]\u001b[A\n"," 34% 4036/11873 [00:15<00:31, 251.00it/s]\u001b[A\n"," 34% 4062/11873 [00:15<00:31, 246.10it/s]\u001b[A\n"," 34% 4087/11873 [00:15<00:31, 245.55it/s]\u001b[A\n"," 35% 4112/11873 [00:15<00:32, 241.56it/s]\u001b[A\n"," 35% 4137/11873 [00:15<00:31, 243.00it/s]\u001b[A\n"," 35% 4162/11873 [00:16<00:31, 244.63it/s]\u001b[A\n"," 35% 4187/11873 [00:16<00:31, 241.27it/s]\u001b[A\n"," 35% 4213/11873 [00:16<00:31, 244.13it/s]\u001b[A\n"," 36% 4240/11873 [00:16<00:30, 249.31it/s]\u001b[A\n"," 36% 4265/11873 [00:16<00:30, 246.77it/s]\u001b[A\n"," 36% 4291/11873 [00:16<00:30, 250.01it/s]\u001b[A\n"," 36% 4318/11873 [00:16<00:29, 253.78it/s]\u001b[A\n"," 37% 4344/11873 [00:16<00:29, 252.51it/s]\u001b[A\n"," 37% 4371/11873 [00:16<00:29, 255.89it/s]\u001b[A\n"," 37% 4397/11873 [00:16<00:29, 251.40it/s]\u001b[A\n"," 37% 4423/11873 [00:17<00:34, 216.42it/s]\u001b[A\n"," 37% 4448/11873 [00:17<00:33, 224.70it/s]\u001b[A\n"," 38% 4475/11873 [00:17<00:31, 235.92it/s]\u001b[A\n"," 38% 4500/11873 [00:17<00:31, 237.24it/s]\u001b[A\n"," 38% 4526/11873 [00:17<00:30, 241.55it/s]\u001b[A\n"," 38% 4551/11873 [00:17<00:30, 242.85it/s]\u001b[A\n"," 39% 4578/11873 [00:17<00:29, 248.69it/s]\u001b[A\n"," 39% 4605/11873 [00:17<00:28, 252.75it/s]\u001b[A\n"," 39% 4631/11873 [00:17<00:28, 250.07it/s]\u001b[A\n"," 39% 4657/11873 [00:18<00:28, 250.90it/s]\u001b[A\n"," 39% 4683/11873 [00:18<00:28, 248.44it/s]\u001b[A\n"," 40% 4708/11873 [00:18<00:28, 247.88it/s]\u001b[A\n"," 40% 4733/11873 [00:18<00:29, 244.48it/s]\u001b[A\n"," 40% 4759/11873 [00:18<00:28, 246.29it/s]\u001b[A\n"," 40% 4784/11873 [00:18<00:28, 244.64it/s]\u001b[A\n"," 41% 4809/11873 [00:18<00:28, 244.70it/s]\u001b[A\n"," 41% 4834/11873 [00:18<00:28, 242.82it/s]\u001b[A\n"," 41% 4860/11873 [00:18<00:28, 246.18it/s]\u001b[A\n"," 41% 4886/11873 [00:18<00:28, 248.93it/s]\u001b[A\n"," 41% 4911/11873 [00:19<00:28, 245.79it/s]\u001b[A\n"," 42% 4937/11873 [00:19<00:28, 247.32it/s]\u001b[A\n"," 42% 4962/11873 [00:19<00:27, 247.16it/s]\u001b[A\n"," 42% 4988/11873 [00:19<00:27, 247.66it/s]\u001b[A\n"," 42% 5013/11873 [00:19<00:27, 245.07it/s]\u001b[A\n"," 42% 5038/11873 [00:19<00:27, 244.45it/s]\u001b[A\n"," 43% 5064/11873 [00:19<00:27, 247.51it/s]\u001b[A\n"," 43% 5089/11873 [00:19<00:27, 245.63it/s]\u001b[A\n"," 43% 5115/11873 [00:19<00:27, 249.51it/s]\u001b[A\n"," 43% 5141/11873 [00:19<00:26, 252.29it/s]\u001b[A\n"," 44% 5167/11873 [00:20<00:26, 250.54it/s]\u001b[A\n"," 44% 5193/11873 [00:20<00:26, 252.22it/s]\u001b[A\n"," 44% 5220/11873 [00:20<00:26, 255.10it/s]\u001b[A\n"," 44% 5246/11873 [00:20<00:25, 255.40it/s]\u001b[A\n"," 44% 5272/11873 [00:20<00:28, 232.98it/s]\u001b[A\n"," 45% 5297/11873 [00:20<00:27, 237.68it/s]\u001b[A\n"," 45% 5323/11873 [00:20<00:26, 243.86it/s]\u001b[A\n"," 45% 5349/11873 [00:20<00:26, 246.77it/s]\u001b[A\n"," 45% 5375/11873 [00:20<00:26, 248.06it/s]\u001b[A\n"," 45% 5400/11873 [00:21<00:26, 247.11it/s]\u001b[A\n"," 46% 5426/11873 [00:21<00:25, 248.92it/s]\u001b[A\n"," 46% 5451/11873 [00:21<00:25, 249.15it/s]\u001b[A\n"," 46% 5477/11873 [00:21<00:25, 249.72it/s]\u001b[A\n"," 46% 5503/11873 [00:21<00:25, 251.24it/s]\u001b[A\n"," 47% 5529/11873 [00:21<00:25, 252.13it/s]\u001b[A\n"," 47% 5555/11873 [00:21<00:25, 249.95it/s]\u001b[A\n"," 47% 5581/11873 [00:21<00:25, 248.57it/s]\u001b[A\n"," 47% 5606/11873 [00:21<00:25, 246.61it/s]\u001b[A\n"," 47% 5631/11873 [00:21<00:25, 247.47it/s]\u001b[A\n"," 48% 5656/11873 [00:22<00:25, 245.14it/s]\u001b[A\n"," 48% 5681/11873 [00:22<00:25, 245.51it/s]\u001b[A\n"," 48% 5707/11873 [00:22<00:24, 246.90it/s]\u001b[A\n"," 48% 5734/11873 [00:22<00:24, 250.72it/s]\u001b[A\n"," 49% 5760/11873 [00:22<00:24, 251.82it/s]\u001b[A\n"," 49% 5786/11873 [00:22<00:24, 251.32it/s]\u001b[A\n"," 49% 5813/11873 [00:22<00:23, 256.51it/s]\u001b[A\n"," 49% 5839/11873 [00:22<00:23, 257.44it/s]\u001b[A\n"," 49% 5866/11873 [00:22<00:23, 260.20it/s]\u001b[A\n"," 50% 5893/11873 [00:23<00:23, 255.40it/s]\u001b[A\n"," 50% 5919/11873 [00:23<00:23, 256.10it/s]\u001b[A\n"," 50% 5945/11873 [00:23<00:23, 255.46it/s]\u001b[A\n"," 50% 5971/11873 [00:23<00:23, 249.44it/s]\u001b[A\n"," 51% 5998/11873 [00:23<00:23, 253.04it/s]\u001b[A\n"," 51% 6025/11873 [00:23<00:22, 255.06it/s]\u001b[A\n"," 51% 6051/11873 [00:23<00:23, 251.22it/s]\u001b[A\n"," 51% 6077/11873 [00:23<00:23, 248.12it/s]\u001b[A\n"," 51% 6103/11873 [00:23<00:22, 251.00it/s]\u001b[A\n"," 52% 6129/11873 [00:23<00:22, 253.22it/s]\u001b[A\n"," 52% 6155/11873 [00:24<00:22, 249.72it/s]\u001b[A\n"," 52% 6180/11873 [00:24<00:22, 249.25it/s]\u001b[A\n"," 52% 6205/11873 [00:24<00:23, 246.27it/s]\u001b[A\n"," 52% 6231/11873 [00:24<00:22, 248.14it/s]\u001b[A\n"," 53% 6256/11873 [00:24<00:22, 246.89it/s]\u001b[A\n"," 53% 6282/11873 [00:24<00:22, 250.31it/s]\u001b[A\n"," 53% 6308/11873 [00:24<00:22, 251.55it/s]\u001b[A\n"," 53% 6335/11873 [00:24<00:21, 256.10it/s]\u001b[A\n"," 54% 6361/11873 [00:24<00:21, 252.72it/s]\u001b[A\n"," 54% 6387/11873 [00:24<00:21, 253.11it/s]\u001b[A\n"," 54% 6413/11873 [00:25<00:21, 251.86it/s]\u001b[A\n"," 54% 6439/11873 [00:25<00:21, 252.33it/s]\u001b[A\n"," 54% 6465/11873 [00:25<00:21, 251.77it/s]\u001b[A\n"," 55% 6491/11873 [00:25<00:21, 253.50it/s]\u001b[A\n"," 55% 6517/11873 [00:25<00:21, 244.89it/s]\u001b[A\n"," 55% 6542/11873 [00:25<00:22, 241.18it/s]\u001b[A\n"," 55% 6568/11873 [00:25<00:21, 245.13it/s]\u001b[A\n"," 56% 6594/11873 [00:25<00:21, 247.65it/s]\u001b[A\n"," 56% 6620/11873 [00:25<00:21, 249.83it/s]\u001b[A\n"," 56% 6646/11873 [00:26<00:21, 246.15it/s]\u001b[A\n"," 56% 6672/11873 [00:26<00:20, 247.97it/s]\u001b[A\n"," 56% 6697/11873 [00:26<00:21, 245.82it/s]\u001b[A\n"," 57% 6722/11873 [00:26<00:21, 241.34it/s]\u001b[A\n"," 57% 6748/11873 [00:26<00:20, 244.82it/s]\u001b[A\n"," 57% 6773/11873 [00:26<00:20, 246.10it/s]\u001b[A\n"," 57% 6798/11873 [00:26<00:21, 240.92it/s]\u001b[A\n"," 57% 6824/11873 [00:26<00:20, 245.24it/s]\u001b[A\n"," 58% 6850/11873 [00:26<00:20, 247.58it/s]\u001b[A\n"," 58% 6875/11873 [00:26<00:20, 244.27it/s]\u001b[A\n"," 58% 6901/11873 [00:27<00:20, 247.27it/s]\u001b[A\n"," 58% 6927/11873 [00:27<00:19, 249.69it/s]\u001b[A\n"," 59% 6952/11873 [00:27<00:19, 248.73it/s]\u001b[A\n"," 59% 6979/11873 [00:27<00:19, 253.44it/s]\u001b[A\n"," 59% 7005/11873 [00:27<00:19, 254.92it/s]\u001b[A\n"," 59% 7032/11873 [00:27<00:18, 256.88it/s]\u001b[A\n"," 59% 7059/11873 [00:27<00:18, 258.03it/s]\u001b[A\n"," 60% 7086/11873 [00:27<00:18, 260.47it/s]\u001b[A\n"," 60% 7113/11873 [00:27<00:18, 259.57it/s]\u001b[A\n"," 60% 7139/11873 [00:27<00:18, 259.20it/s]\u001b[A\n"," 60% 7166/11873 [00:28<00:18, 260.58it/s]\u001b[A\n"," 61% 7193/11873 [00:28<00:18, 255.70it/s]\u001b[A\n"," 61% 7219/11873 [00:28<00:18, 251.40it/s]\u001b[A\n"," 61% 7246/11873 [00:28<00:18, 254.96it/s]\u001b[A\n"," 61% 7272/11873 [00:28<00:18, 253.84it/s]\u001b[A\n"," 61% 7299/11873 [00:28<00:17, 256.01it/s]\u001b[A\n"," 62% 7325/11873 [00:28<00:17, 256.67it/s]\u001b[A\n"," 62% 7352/11873 [00:28<00:17, 257.59it/s]\u001b[A\n"," 62% 7379/11873 [00:28<00:17, 259.07it/s]\u001b[A\n"," 62% 7405/11873 [00:29<00:17, 259.17it/s]\u001b[A\n"," 63% 7431/11873 [00:29<00:17, 251.64it/s]\u001b[A\n"," 63% 7457/11873 [00:29<00:17, 252.66it/s]\u001b[A\n"," 63% 7483/11873 [00:29<00:17, 252.42it/s]\u001b[A\n"," 63% 7509/11873 [00:29<00:17, 254.62it/s]\u001b[A\n"," 63% 7536/11873 [00:29<00:16, 257.12it/s]\u001b[A\n"," 64% 7563/11873 [00:29<00:16, 258.05it/s]\u001b[A\n"," 64% 7589/11873 [00:29<00:16, 256.36it/s]\u001b[A\n"," 64% 7615/11873 [00:29<00:16, 253.70it/s]\u001b[A\n"," 64% 7641/11873 [00:29<00:16, 251.46it/s]\u001b[A\n"," 65% 7667/11873 [00:30<00:16, 252.33it/s]\u001b[A\n"," 65% 7693/11873 [00:30<00:16, 252.21it/s]\u001b[A\n"," 65% 7719/11873 [00:30<00:16, 247.56it/s]\u001b[A\n"," 65% 7744/11873 [00:30<00:16, 247.23it/s]\u001b[A\n"," 65% 7769/11873 [00:30<00:16, 247.36it/s]\u001b[A\n"," 66% 7795/11873 [00:30<00:16, 248.17it/s]\u001b[A\n"," 66% 7821/11873 [00:30<00:16, 250.71it/s]\u001b[A\n"," 66% 7847/11873 [00:30<00:16, 249.59it/s]\u001b[A\n"," 66% 7872/11873 [00:30<00:16, 249.71it/s]\u001b[A\n"," 67% 7898/11873 [00:30<00:15, 250.33it/s]\u001b[A\n"," 67% 7924/11873 [00:31<00:15, 253.00it/s]\u001b[A\n"," 67% 7950/11873 [00:31<00:15, 252.52it/s]\u001b[A\n"," 67% 7976/11873 [00:31<00:15, 249.98it/s]\u001b[A\n"," 67% 8003/11873 [00:31<00:15, 251.30it/s]\u001b[A\n"," 68% 8030/11873 [00:31<00:15, 254.60it/s]\u001b[A\n"," 68% 8056/11873 [00:31<00:15, 250.06it/s]\u001b[A\n"," 68% 8082/11873 [00:31<00:15, 250.79it/s]\u001b[A\n"," 68% 8108/11873 [00:31<00:15, 250.45it/s]\u001b[A\n"," 69% 8134/11873 [00:31<00:14, 250.62it/s]\u001b[A\n"," 69% 8160/11873 [00:32<00:15, 241.57it/s]\u001b[A\n"," 69% 8186/11873 [00:32<00:15, 243.74it/s]\u001b[A\n"," 69% 8211/11873 [00:32<00:15, 239.21it/s]\u001b[A\n"," 69% 8237/11873 [00:32<00:14, 243.70it/s]\u001b[A\n"," 70% 8263/11873 [00:32<00:14, 245.63it/s]\u001b[A\n"," 70% 8289/11873 [00:32<00:14, 248.83it/s]\u001b[A\n"," 70% 8315/11873 [00:32<00:14, 249.55it/s]\u001b[A\n"," 70% 8341/11873 [00:32<00:14, 250.95it/s]\u001b[A\n"," 70% 8367/11873 [00:32<00:14, 248.91it/s]\u001b[A\n"," 71% 8392/11873 [00:32<00:14, 246.29it/s]\u001b[A\n"," 71% 8417/11873 [00:33<00:14, 245.47it/s]\u001b[A\n"," 71% 8442/11873 [00:33<00:14, 244.17it/s]\u001b[A\n"," 71% 8467/11873 [00:33<00:14, 243.28it/s]\u001b[A\n"," 72% 8492/11873 [00:33<00:14, 237.94it/s]\u001b[A\n"," 72% 8519/11873 [00:33<00:13, 245.12it/s]\u001b[A\n"," 72% 8544/11873 [00:33<00:13, 245.24it/s]\u001b[A\n"," 72% 8570/11873 [00:33<00:13, 247.17it/s]\u001b[A\n"," 72% 8595/11873 [00:33<00:13, 245.58it/s]\u001b[A\n"," 73% 8620/11873 [00:33<00:13, 242.98it/s]\u001b[A\n"," 73% 8645/11873 [00:34<00:19, 163.70it/s]\u001b[A\n"," 73% 8671/11873 [00:34<00:17, 183.33it/s]\u001b[A\n"," 73% 8697/11873 [00:34<00:15, 199.68it/s]\u001b[A\n"," 73% 8722/11873 [00:34<00:14, 212.27it/s]\u001b[A\n"," 74% 8749/11873 [00:34<00:13, 225.96it/s]\u001b[A\n"," 74% 8775/11873 [00:34<00:13, 234.00it/s]\u001b[A\n"," 74% 8800/11873 [00:34<00:12, 237.12it/s]\u001b[A\n"," 74% 8826/11873 [00:34<00:12, 241.53it/s]\u001b[A\n"," 75% 8853/11873 [00:35<00:12, 246.95it/s]\u001b[A\n"," 75% 8879/11873 [00:35<00:11, 250.38it/s]\u001b[A\n"," 75% 8905/11873 [00:35<00:11, 251.51it/s]\u001b[A\n"," 75% 8931/11873 [00:35<00:11, 250.94it/s]\u001b[A\n"," 75% 8957/11873 [00:35<00:11, 249.64it/s]\u001b[A\n"," 76% 8984/11873 [00:35<00:11, 252.96it/s]\u001b[A\n"," 76% 9010/11873 [00:35<00:11, 246.52it/s]\u001b[A\n"," 76% 9035/11873 [00:35<00:11, 246.65it/s]\u001b[A\n"," 76% 9061/11873 [00:35<00:11, 248.86it/s]\u001b[A\n"," 77% 9088/11873 [00:35<00:10, 253.19it/s]\u001b[A\n"," 77% 9114/11873 [00:36<00:10, 254.25it/s]\u001b[A\n"," 77% 9141/11873 [00:36<00:10, 256.30it/s]\u001b[A\n"," 77% 9167/11873 [00:36<00:10, 256.09it/s]\u001b[A\n"," 77% 9193/11873 [00:36<00:10, 255.87it/s]\u001b[A\n"," 78% 9219/11873 [00:36<00:10, 252.93it/s]\u001b[A\n"," 78% 9245/11873 [00:36<00:10, 254.99it/s]\u001b[A\n"," 78% 9271/11873 [00:36<00:10, 254.42it/s]\u001b[A\n"," 78% 9297/11873 [00:36<00:10, 255.15it/s]\u001b[A\n"," 79% 9324/11873 [00:36<00:09, 257.46it/s]\u001b[A\n"," 79% 9350/11873 [00:36<00:09, 255.95it/s]\u001b[A\n"," 79% 9377/11873 [00:37<00:09, 257.64it/s]\u001b[A\n"," 79% 9403/11873 [00:37<00:09, 255.56it/s]\u001b[A\n"," 79% 9429/11873 [00:37<00:09, 253.13it/s]\u001b[A\n"," 80% 9455/11873 [00:37<00:09, 254.56it/s]\u001b[A\n"," 80% 9481/11873 [00:37<00:09, 254.82it/s]\u001b[A\n"," 80% 9507/11873 [00:37<00:09, 255.61it/s]\u001b[A\n"," 80% 9533/11873 [00:37<00:09, 253.13it/s]\u001b[A\n"," 81% 9560/11873 [00:37<00:09, 255.33it/s]\u001b[A\n"," 81% 9586/11873 [00:37<00:09, 252.49it/s]\u001b[A\n"," 81% 9612/11873 [00:37<00:08, 252.30it/s]\u001b[A\n"," 81% 9640/11873 [00:38<00:08, 257.72it/s]\u001b[A\n"," 81% 9666/11873 [00:38<00:08, 257.71it/s]\u001b[A\n"," 82% 9692/11873 [00:38<00:08, 258.10it/s]\u001b[A\n"," 82% 9718/11873 [00:38<00:08, 258.10it/s]\u001b[A\n"," 82% 9744/11873 [00:38<00:08, 257.57it/s]\u001b[A\n"," 82% 9770/11873 [00:38<00:08, 257.45it/s]\u001b[A\n"," 83% 9796/11873 [00:38<00:08, 257.44it/s]\u001b[A\n"," 83% 9822/11873 [00:38<00:08, 252.55it/s]\u001b[A\n"," 83% 9848/11873 [00:38<00:07, 253.72it/s]\u001b[A\n"," 83% 9874/11873 [00:39<00:07, 254.30it/s]\u001b[A\n"," 83% 9900/11873 [00:39<00:07, 253.48it/s]\u001b[A\n"," 84% 9926/11873 [00:39<00:07, 254.25it/s]\u001b[A\n"," 84% 9952/11873 [00:39<00:07, 254.10it/s]\u001b[A\n"," 84% 9978/11873 [00:39<00:07, 252.20it/s]\u001b[A\n"," 84% 10004/11873 [00:39<00:07, 253.89it/s]\u001b[A\n"," 84% 10030/11873 [00:39<00:07, 255.58it/s]\u001b[A\n"," 85% 10056/11873 [00:39<00:07, 253.91it/s]\u001b[A\n"," 85% 10082/11873 [00:39<00:07, 253.66it/s]\u001b[A\n"," 85% 10108/11873 [00:39<00:07, 251.10it/s]\u001b[A\n"," 85% 10134/11873 [00:40<00:06, 251.89it/s]\u001b[A\n"," 86% 10160/11873 [00:40<00:06, 253.45it/s]\u001b[A\n"," 86% 10186/11873 [00:40<00:06, 250.79it/s]\u001b[A\n"," 86% 10212/11873 [00:40<00:06, 250.42it/s]\u001b[A\n"," 86% 10238/11873 [00:40<00:06, 249.87it/s]\u001b[A\n"," 86% 10263/11873 [00:40<00:06, 243.56it/s]\u001b[A\n"," 87% 10288/11873 [00:40<00:06, 242.48it/s]\u001b[A\n"," 87% 10313/11873 [00:40<00:06, 237.93it/s]\u001b[A\n"," 87% 10339/11873 [00:40<00:06, 244.16it/s]\u001b[A\n"," 87% 10365/11873 [00:40<00:06, 247.22it/s]\u001b[A\n"," 88% 10392/11873 [00:41<00:05, 251.55it/s]\u001b[A\n"," 88% 10418/11873 [00:41<00:05, 249.46it/s]\u001b[A\n"," 88% 10444/11873 [00:41<00:05, 250.02it/s]\u001b[A\n"," 88% 10470/11873 [00:41<00:05, 249.91it/s]\u001b[A\n"," 88% 10496/11873 [00:41<00:05, 252.59it/s]\u001b[A\n"," 89% 10522/11873 [00:41<00:05, 250.94it/s]\u001b[A\n"," 89% 10548/11873 [00:41<00:05, 250.23it/s]\u001b[A\n"," 89% 10574/11873 [00:41<00:05, 250.04it/s]\u001b[A\n"," 89% 10600/11873 [00:41<00:05, 250.61it/s]\u001b[A\n"," 89% 10626/11873 [00:42<00:04, 250.17it/s]\u001b[A\n"," 90% 10652/11873 [00:42<00:04, 246.92it/s]\u001b[A\n"," 90% 10677/11873 [00:42<00:04, 247.64it/s]\u001b[A\n"," 90% 10703/11873 [00:42<00:04, 250.48it/s]\u001b[A\n"," 90% 10729/11873 [00:42<00:04, 249.99it/s]\u001b[A\n"," 91% 10755/11873 [00:42<00:04, 246.27it/s]\u001b[A\n"," 91% 10781/11873 [00:42<00:04, 247.83it/s]\u001b[A\n"," 91% 10807/11873 [00:42<00:04, 249.20it/s]\u001b[A\n"," 91% 10832/11873 [00:42<00:04, 244.89it/s]\u001b[A\n"," 91% 10857/11873 [00:42<00:04, 246.07it/s]\u001b[A\n"," 92% 10882/11873 [00:43<00:04, 245.32it/s]\u001b[A\n"," 92% 10907/11873 [00:43<00:03, 246.07it/s]\u001b[A\n"," 92% 10932/11873 [00:43<00:03, 240.27it/s]\u001b[A\n"," 92% 10958/11873 [00:43<00:03, 243.54it/s]\u001b[A\n"," 93% 10984/11873 [00:43<00:03, 248.03it/s]\u001b[A\n"," 93% 11010/11873 [00:43<00:03, 249.53it/s]\u001b[A\n"," 93% 11037/11873 [00:43<00:03, 252.87it/s]\u001b[A\n"," 93% 11063/11873 [00:43<00:03, 254.39it/s]\u001b[A\n"," 93% 11089/11873 [00:43<00:03, 246.33it/s]\u001b[A\n"," 94% 11115/11873 [00:43<00:03, 248.85it/s]\u001b[A\n"," 94% 11140/11873 [00:44<00:02, 248.45it/s]\u001b[A\n"," 94% 11166/11873 [00:44<00:02, 250.56it/s]\u001b[A\n"," 94% 11192/11873 [00:44<00:02, 244.81it/s]\u001b[A\n"," 94% 11217/11873 [00:44<00:02, 245.14it/s]\u001b[A\n"," 95% 11243/11873 [00:44<00:02, 247.91it/s]\u001b[A\n"," 95% 11268/11873 [00:44<00:02, 241.27it/s]\u001b[A\n"," 95% 11295/11873 [00:44<00:02, 247.76it/s]\u001b[A\n"," 95% 11321/11873 [00:44<00:02, 249.65it/s]\u001b[A\n"," 96% 11347/11873 [00:44<00:02, 251.19it/s]\u001b[A\n"," 96% 11373/11873 [00:45<00:02, 249.70it/s]\u001b[A\n"," 96% 11398/11873 [00:45<00:01, 248.71it/s]\u001b[A\n"," 96% 11423/11873 [00:45<00:01, 249.00it/s]\u001b[A\n"," 96% 11449/11873 [00:45<00:01, 249.58it/s]\u001b[A\n"," 97% 11474/11873 [00:45<00:01, 246.36it/s]\u001b[A\n"," 97% 11500/11873 [00:45<00:01, 248.38it/s]\u001b[A\n"," 97% 11525/11873 [00:45<00:01, 248.00it/s]\u001b[A\n"," 97% 11551/11873 [00:45<00:01, 249.95it/s]\u001b[A\n"," 98% 11578/11873 [00:45<00:01, 253.45it/s]\u001b[A\n"," 98% 11604/11873 [00:45<00:01, 252.71it/s]\u001b[A\n"," 98% 11630/11873 [00:46<00:00, 252.07it/s]\u001b[A\n"," 98% 11656/11873 [00:46<00:00, 250.60it/s]\u001b[A\n"," 98% 11682/11873 [00:46<00:00, 248.13it/s]\u001b[A\n"," 99% 11707/11873 [00:46<00:00, 248.31it/s]\u001b[A\n"," 99% 11732/11873 [00:46<00:00, 247.23it/s]\u001b[A\n"," 99% 11758/11873 [00:46<00:00, 250.81it/s]\u001b[A\n"," 99% 11784/11873 [00:46<00:00, 249.41it/s]\u001b[A\n"," 99% 11810/11873 [00:46<00:00, 249.91it/s]\u001b[A\n","100% 11836/11873 [00:46<00:00, 249.97it/s]\u001b[A\n","100% 11873/11873 [00:47<00:00, 252.43it/s]\n","04/06/2022 11:12:28 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/eval_predictions.json.\n","04/06/2022 11:12:28 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/eval_nbest_predictions.json.\n","04/06/2022 11:12:31 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/albert-base-v2/back-trans-aug/eval_null_odds.json.\n","04/06/2022 11:12:35 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1496/1496 [05:32<00:00,  4.49it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 76.2146\n","  eval_HasAns_f1         = 82.0608\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 80.9588\n","  eval_NoAns_f1          = 80.9588\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 78.5985\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 81.5174\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 78.5901\n","  eval_f1                =  81.509\n","  eval_samples           =   11968\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 11:12:36,462 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_back_trans_aug', 'type': 'sichenzhong/squad_v2_back_trans_aug', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path roberta-base \\\n","  --dataset_name sichenzhong/squad_v2_back_trans_aug \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKGXryQ5rMs-","executionInfo":{"status":"ok","timestamp":1649254169355,"user_tz":240,"elapsed":10611499,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"352027ac-05d6-42ac-bd06-a282466f3435"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["04/06/2022 11:12:42 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/06/2022 11:12:42 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/runs/Apr06_11-12-42_2439d11b3fff,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/06/2022 11:12:43 - WARNING - datasets.builder - Using custom data configuration sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b\n","04/06/2022 11:12:43 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","04/06/2022 11:12:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","04/06/2022 11:12:43 - WARNING - datasets.builder - Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n","04/06/2022 11:12:43 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901\n","100% 2/2 [00:00<00:00, 566.95it/s]\n","[INFO|hub.py:583] 2022-04-06 11:12:43,863 >> https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpcsxpq14d\n","Downloading: 100% 481/481 [00:00<00:00, 426kB/s]\n","[INFO|hub.py:587] 2022-04-06 11:12:44,209 >> storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|hub.py:595] 2022-04-06 11:12:44,210 >> creating metadata file for /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:654] 2022-04-06 11:12:44,210 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-06 11:12:44,212 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_auto.py:344] 2022-04-06 11:12:44,557 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-04-06 11:12:44,905 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-06 11:12:44,906 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 11:12:45,609 >> https://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpih9wuurq\n","Downloading: 100% 878k/878k [00:00<00:00, 1.84MB/s]\n","[INFO|hub.py:587] 2022-04-06 11:12:46,452 >> storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|hub.py:595] 2022-04-06 11:12:46,452 >> creating metadata file for /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|hub.py:583] 2022-04-06 11:12:46,805 >> https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpk7ol69a0\n","Downloading: 100% 446k/446k [00:00<00:00, 1.13MB/s]\n","[INFO|hub.py:587] 2022-04-06 11:12:47,559 >> storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|hub.py:595] 2022-04-06 11:12:47,559 >> creating metadata file for /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|hub.py:583] 2022-04-06 11:12:47,919 >> https://huggingface.co/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdu1fpt_7\n","Downloading: 100% 1.29M/1.29M [00:00<00:00, 2.73MB/s]\n","[INFO|hub.py:587] 2022-04-06 11:12:48,774 >> storing https://huggingface.co/roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|hub.py:595] 2022-04-06 11:12:48,774 >> creating metadata file for /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 11:12:49,815 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 11:12:49,815 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 11:12:49,815 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 11:12:49,815 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 11:12:49,815 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-06 11:12:49,815 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-04-06 11:12:50,159 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:690] 2022-04-06 11:12:50,160 >> Model config RobertaConfig {\n","  \"_name_or_path\": \"roberta-base\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|hub.py:583] 2022-04-06 11:12:50,621 >> https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptt9yoruo\n","Downloading: 100% 478M/478M [00:10<00:00, 46.8MB/s]\n","[INFO|hub.py:587] 2022-04-06 11:13:01,403 >> storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[INFO|hub.py:595] 2022-04-06 11:13:01,403 >> creating metadata file for /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[INFO|modeling_utils.py:1772] 2022-04-06 11:13:01,404 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","[WARNING|modeling_utils.py:2050] 2022-04-06 11:13:02,899 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2061] 2022-04-06 11:13:02,899 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/06/2022 11:13:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-d9c1ea7451f2f3a9.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:47<00:00,  2.74ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/06/2022 11:13:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/parquet/sichenzhong--squad_v2_back_trans_aug-ad615f9c3eda632b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-3e1a7223ad6bde00.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:14<00:00,  6.21s/ba]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-06 11:15:10,143 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-06 11:15:10,143 >>   Num examples = 131814\n","[INFO|trainer.py:1292] 2022-04-06 11:15:10,143 >>   Num Epochs = 2\n","[INFO|trainer.py:1293] 2022-04-06 11:15:10,143 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1294] 2022-04-06 11:15:10,143 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1295] 2022-04-06 11:15:10,143 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-06 11:15:10,143 >>   Total optimization steps = 10986\n","{'loss': 1.8394, 'learning_rate': 3.8179501183324234e-05, 'epoch': 0.09}\n","  5% 500/10986 [07:38<2:40:13,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 11:22:49,090 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-06 11:22:49,096 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:22:50,529 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:22:50,535 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:22:50,539 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-500/special_tokens_map.json\n","{'loss': 1.3407, 'learning_rate': 3.6359002366648465e-05, 'epoch': 0.18}\n","  9% 1000/10986 [15:25<2:32:33,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 11:30:36,011 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-06 11:30:36,017 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:30:37,451 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:30:37,456 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:30:37,460 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.2004, 'learning_rate': 3.4538503549972695e-05, 'epoch': 0.27}\n"," 14% 1500/10986 [23:11<2:25:05,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 11:38:21,985 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-06 11:38:21,992 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:38:23,452 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:38:23,457 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:38:23,461 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.1499, 'learning_rate': 3.2718004733296926e-05, 'epoch': 0.36}\n"," 18% 2000/10986 [30:57<2:17:41,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 11:46:07,741 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-06 11:46:07,748 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:46:09,219 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:46:09,225 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:46:09,229 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.0937, 'learning_rate': 3.0897505916621156e-05, 'epoch': 0.46}\n"," 23% 2500/10986 [38:42<2:10:05,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 11:53:52,178 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-06 11:53:52,184 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 11:53:53,586 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 11:53:53,591 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 11:53:53,596 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.0779, 'learning_rate': 2.9077007099945387e-05, 'epoch': 0.55}\n"," 27% 3000/10986 [46:26<2:02:20,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 12:01:36,473 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-06 12:01:36,480 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:01:37,877 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:01:37,883 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:01:37,887 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.0492, 'learning_rate': 2.7256508283269618e-05, 'epoch': 0.64}\n"," 32% 3500/10986 [54:10<1:54:36,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 12:09:20,782 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-06 12:09:20,788 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:09:22,242 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:09:22,248 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:09:22,253 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.0104, 'learning_rate': 2.543600946659385e-05, 'epoch': 0.73}\n"," 36% 4000/10986 [1:01:55<1:47:01,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 12:17:05,289 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-06 12:17:05,295 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:17:06,733 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:17:06,738 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:17:06,743 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.0001, 'learning_rate': 2.3615510649918082e-05, 'epoch': 0.82}\n"," 41% 4500/10986 [1:09:39<1:39:29,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 12:24:49,750 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-06 12:24:49,756 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:24:51,220 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:24:51,225 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:24:51,230 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-4500/special_tokens_map.json\n","{'loss': 0.9622, 'learning_rate': 2.1795011833242313e-05, 'epoch': 0.91}\n"," 46% 5000/10986 [1:17:25<1:31:43,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 12:32:35,223 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-06 12:32:35,229 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:32:36,701 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:32:36,707 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:32:36,711 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-5000/special_tokens_map.json\n","{'loss': 0.9571, 'learning_rate': 1.997451301656654e-05, 'epoch': 1.0}\n"," 50% 5500/10986 [1:25:10<1:22:36,  1.11it/s][INFO|trainer.py:2166] 2022-04-06 12:40:20,547 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-06 12:40:20,552 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:40:21,997 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:40:22,765 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:40:22,770 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.742, 'learning_rate': 1.8154014199890774e-05, 'epoch': 1.09}\n"," 55% 6000/10986 [1:32:55<1:16:19,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 12:48:05,995 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-06 12:48:06,001 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:48:07,452 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:48:07,458 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:48:07,462 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.7214, 'learning_rate': 1.6333515383215e-05, 'epoch': 1.18}\n"," 59% 6500/10986 [1:40:42<1:08:35,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 12:55:53,088 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-06 12:55:53,094 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 12:55:54,554 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 12:55:54,559 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 12:55:54,564 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.7314, 'learning_rate': 1.4513016566539234e-05, 'epoch': 1.27}\n"," 64% 7000/10986 [1:48:27<1:01:06,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 13:03:38,092 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-06 13:03:38,099 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:03:39,572 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:03:39,579 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:03:39,585 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.7222, 'learning_rate': 1.2692517749863464e-05, 'epoch': 1.37}\n"," 68% 7500/10986 [1:56:14<53:26,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 13:11:24,477 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-06 13:11:24,483 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:11:25,899 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:11:25,905 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:11:25,910 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.7053, 'learning_rate': 1.0872018933187693e-05, 'epoch': 1.46}\n"," 73% 8000/10986 [2:04:02<45:45,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 13:19:12,214 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-06 13:19:12,220 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:19:13,718 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:19:13,724 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:19:13,728 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.6964, 'learning_rate': 9.051520116511924e-06, 'epoch': 1.55}\n"," 77% 8500/10986 [2:11:47<38:08,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 13:26:57,672 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-06 13:26:57,679 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:26:59,058 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:26:59,063 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:27:00,019 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.6993, 'learning_rate': 7.231021299836156e-06, 'epoch': 1.64}\n"," 82% 9000/10986 [2:19:32<30:25,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 13:34:42,896 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-06 13:34:42,904 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:34:44,268 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:34:44,275 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:34:44,280 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.6783, 'learning_rate': 5.410522483160386e-06, 'epoch': 1.73}\n"," 86% 9500/10986 [2:27:17<22:45,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 13:42:27,215 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-06 13:42:27,222 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:42:28,596 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:42:28,602 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:42:28,606 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.6844, 'learning_rate': 3.5900236664846172e-06, 'epoch': 1.82}\n"," 91% 10000/10986 [2:35:01<15:07,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 13:50:11,450 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-06 13:50:11,457 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:50:12,816 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:50:12,822 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:50:12,826 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6763, 'learning_rate': 1.7695248498088476e-06, 'epoch': 1.91}\n"," 96% 10500/10986 [2:42:45<07:26,  1.09it/s][INFO|trainer.py:2166] 2022-04-06 13:57:55,690 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-06 13:57:55,696 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 13:57:57,072 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 13:57:57,077 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 13:57:57,081 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/checkpoint-10500/special_tokens_map.json\n","100% 10986/10986 [2:50:19<00:00,  1.38it/s][INFO|trainer.py:1530] 2022-04-06 14:05:29,921 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10219.7781, 'train_samples_per_second': 25.796, 'train_steps_per_second': 1.075, 'train_loss': 0.9279951939009022, 'epoch': 2.0}\n","100% 10986/10986 [2:50:19<00:00,  1.07it/s]\n","[INFO|trainer.py:2166] 2022-04-06 14:05:29,927 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug\n","[INFO|configuration_utils.py:441] 2022-04-06 14:05:29,933 >> Configuration saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-06 14:05:31,397 >> Model weights saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-06 14:05:31,403 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-06 14:05:31,408 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =      0.928\n","  train_runtime            = 2:50:19.77\n","  train_samples            =     131814\n","  train_samples_per_second =     25.796\n","  train_steps_per_second   =      1.075\n","04/06/2022 14:05:32 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-06 14:05:32,490 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-06 14:05:32,493 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-06 14:05:32,493 >>   Num examples = 12165\n","[INFO|trainer.py:2421] 2022-04-06 14:05:32,493 >>   Batch size = 8\n","100% 1520/1521 [02:56<00:00,  8.59it/s]04/06/2022 14:08:42 - INFO - utils_qa - Post-processing 11873 example predictions split into 12165 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 39/11873 [00:00<00:30, 384.17it/s]\u001b[A\n","  1% 78/11873 [00:00<00:33, 352.05it/s]\u001b[A\n","  1% 118/11873 [00:00<00:31, 369.51it/s]\u001b[A\n","  1% 159/11873 [00:00<00:30, 381.39it/s]\u001b[A\n","  2% 202/11873 [00:00<00:29, 397.22it/s]\u001b[A\n","  2% 245/11873 [00:00<00:28, 406.64it/s]\u001b[A\n","  2% 289/11873 [00:00<00:27, 417.32it/s]\u001b[A\n","  3% 331/11873 [00:00<00:28, 408.87it/s]\u001b[A\n","  3% 373/11873 [00:00<00:27, 411.87it/s]\u001b[A\n","  4% 417/11873 [00:01<00:27, 417.99it/s]\u001b[A\n","  4% 461/11873 [00:01<00:26, 423.32it/s]\u001b[A\n","  4% 504/11873 [00:01<00:27, 417.32it/s]\u001b[A\n","  5% 546/11873 [00:01<00:27, 409.77it/s]\u001b[A\n","  5% 588/11873 [00:01<00:27, 410.06it/s]\u001b[A\n","  5% 632/11873 [00:01<00:26, 418.66it/s]\u001b[A\n","  6% 677/11873 [00:01<00:26, 427.00it/s]\u001b[A\n","  6% 720/11873 [00:01<00:26, 416.27it/s]\u001b[A\n","  6% 762/11873 [00:01<00:27, 407.51it/s]\u001b[A\n","  7% 806/11873 [00:01<00:26, 414.29it/s]\u001b[A\n","  7% 849/11873 [00:02<00:26, 416.50it/s]\u001b[A\n","  8% 896/11873 [00:02<00:25, 430.17it/s]\u001b[A\n","  8% 941/11873 [00:02<00:25, 435.31it/s]\u001b[A\n","  8% 985/11873 [00:02<00:25, 428.54it/s]\u001b[A\n","  9% 1028/11873 [00:02<00:27, 401.47it/s]\u001b[A\n","  9% 1069/11873 [00:02<00:28, 374.01it/s]\u001b[A\n","  9% 1107/11873 [00:02<00:29, 360.63it/s]\u001b[A\n"," 10% 1144/11873 [00:02<00:30, 351.09it/s]\u001b[A\n"," 10% 1180/11873 [00:02<00:31, 341.31it/s]\u001b[A\n"," 10% 1215/11873 [00:03<00:31, 341.40it/s]\u001b[A\n"," 11% 1250/11873 [00:03<00:31, 337.78it/s]\u001b[A\n"," 11% 1284/11873 [00:03<00:31, 336.02it/s]\u001b[A\n"," 11% 1318/11873 [00:03<00:31, 336.67it/s]\u001b[A\n"," 11% 1352/11873 [00:03<00:31, 332.26it/s]\u001b[A\n"," 12% 1387/11873 [00:03<00:31, 334.98it/s]\u001b[A\n"," 12% 1421/11873 [00:03<00:31, 335.38it/s]\u001b[A\n"," 12% 1455/11873 [00:03<00:31, 334.99it/s]\u001b[A\n"," 13% 1489/11873 [00:03<00:31, 331.86it/s]\u001b[A\n"," 13% 1523/11873 [00:04<00:31, 333.17it/s]\u001b[A\n"," 13% 1557/11873 [00:04<00:30, 332.96it/s]\u001b[A\n"," 13% 1591/11873 [00:04<00:31, 331.49it/s]\u001b[A\n"," 14% 1625/11873 [00:04<00:31, 329.19it/s]\u001b[A\n"," 14% 1658/11873 [00:04<00:31, 329.42it/s]\u001b[A\n"," 14% 1691/11873 [00:04<00:31, 325.36it/s]\u001b[A\n"," 15% 1725/11873 [00:04<00:30, 327.53it/s]\u001b[A\n"," 15% 1759/11873 [00:04<00:30, 329.02it/s]\u001b[A\n"," 15% 1792/11873 [00:04<00:30, 327.02it/s]\u001b[A\n"," 15% 1825/11873 [00:04<00:31, 322.68it/s]\u001b[A\n"," 16% 1859/11873 [00:05<00:30, 325.23it/s]\u001b[A\n"," 16% 1892/11873 [00:05<00:30, 325.80it/s]\u001b[A\n"," 16% 1925/11873 [00:05<00:30, 322.41it/s]\u001b[A\n"," 16% 1959/11873 [00:05<00:30, 326.19it/s]\u001b[A\n"," 17% 1992/11873 [00:05<00:30, 323.85it/s]\u001b[A\n"," 17% 2025/11873 [00:05<00:30, 323.01it/s]\u001b[A\n"," 17% 2059/11873 [00:05<00:30, 326.09it/s]\u001b[A\n"," 18% 2093/11873 [00:05<00:29, 330.04it/s]\u001b[A\n"," 18% 2127/11873 [00:05<00:29, 331.94it/s]\u001b[A\n"," 18% 2162/11873 [00:05<00:29, 334.73it/s]\u001b[A\n"," 18% 2196/11873 [00:06<00:29, 329.96it/s]\u001b[A\n"," 19% 2230/11873 [00:06<00:28, 332.70it/s]\u001b[A\n"," 19% 2264/11873 [00:06<00:28, 331.42it/s]\u001b[A\n"," 19% 2298/11873 [00:06<00:28, 331.88it/s]\u001b[A\n"," 20% 2332/11873 [00:06<00:28, 330.76it/s]\u001b[A\n"," 20% 2366/11873 [00:06<00:28, 331.66it/s]\u001b[A\n"," 20% 2400/11873 [00:06<00:28, 333.02it/s]\u001b[A\n"," 21% 2434/11873 [00:06<00:28, 329.16it/s]\u001b[A\n"," 21% 2467/11873 [00:06<00:28, 328.27it/s]\u001b[A\n"," 21% 2500/11873 [00:06<00:28, 326.43it/s]\u001b[A\n"," 21% 2534/11873 [00:07<00:28, 328.99it/s]\u001b[A\n"," 22% 2568/11873 [00:07<00:28, 329.79it/s]\u001b[A\n"," 22% 2601/11873 [00:07<00:28, 329.37it/s]\u001b[A\n"," 22% 2634/11873 [00:07<00:28, 329.32it/s]\u001b[A\n"," 22% 2667/11873 [00:07<00:28, 328.59it/s]\u001b[A\n"," 23% 2701/11873 [00:07<00:27, 330.24it/s]\u001b[A\n"," 23% 2735/11873 [00:07<00:27, 327.89it/s]\u001b[A\n"," 23% 2768/11873 [00:07<00:27, 325.89it/s]\u001b[A\n"," 24% 2802/11873 [00:07<00:27, 328.00it/s]\u001b[A\n"," 24% 2836/11873 [00:08<00:27, 330.48it/s]\u001b[A\n"," 24% 2870/11873 [00:08<00:27, 330.84it/s]\u001b[A\n"," 24% 2904/11873 [00:08<00:27, 327.87it/s]\u001b[A\n"," 25% 2938/11873 [00:08<00:27, 329.21it/s]\u001b[A\n"," 25% 2972/11873 [00:08<00:26, 330.74it/s]\u001b[A\n"," 25% 3006/11873 [00:08<00:27, 320.98it/s]\u001b[A\n"," 26% 3039/11873 [00:08<00:27, 320.51it/s]\u001b[A\n"," 26% 3072/11873 [00:08<00:27, 316.77it/s]\u001b[A\n"," 26% 3104/11873 [00:08<00:28, 310.46it/s]\u001b[A\n"," 26% 3136/11873 [00:09<00:33, 261.75it/s]\u001b[A\n"," 27% 3164/11873 [00:09<00:35, 244.71it/s]\u001b[A\n"," 27% 3197/11873 [00:09<00:32, 265.16it/s]\u001b[A\n"," 27% 3230/11873 [00:09<00:30, 280.37it/s]\u001b[A\n"," 27% 3259/11873 [00:09<00:30, 279.52it/s]\u001b[A\n"," 28% 3288/11873 [00:09<00:39, 219.31it/s]\u001b[A\n"," 28% 3313/11873 [00:09<00:44, 192.75it/s]\u001b[A\n"," 28% 3335/11873 [00:09<00:46, 183.68it/s]\u001b[A\n"," 28% 3355/11873 [00:10<00:47, 178.25it/s]\u001b[A\n"," 28% 3375/11873 [00:10<00:46, 182.47it/s]\u001b[A\n"," 29% 3408/11873 [00:10<00:38, 218.28it/s]\u001b[A\n"," 29% 3440/11873 [00:10<00:34, 243.38it/s]\u001b[A\n"," 29% 3473/11873 [00:10<00:31, 264.86it/s]\u001b[A\n"," 30% 3504/11873 [00:10<00:30, 276.33it/s]\u001b[A\n"," 30% 3537/11873 [00:10<00:28, 290.60it/s]\u001b[A\n"," 30% 3571/11873 [00:10<00:27, 304.56it/s]\u001b[A\n"," 30% 3605/11873 [00:10<00:26, 314.69it/s]\u001b[A\n"," 31% 3638/11873 [00:11<00:25, 317.41it/s]\u001b[A\n"," 31% 3671/11873 [00:11<00:25, 318.65it/s]\u001b[A\n"," 31% 3704/11873 [00:11<00:25, 314.22it/s]\u001b[A\n"," 31% 3736/11873 [00:11<00:25, 315.35it/s]\u001b[A\n"," 32% 3769/11873 [00:11<00:25, 319.16it/s]\u001b[A\n"," 32% 3801/11873 [00:11<00:25, 317.02it/s]\u001b[A\n"," 32% 3833/11873 [00:11<00:27, 291.82it/s]\u001b[A\n"," 33% 3866/11873 [00:11<00:26, 300.98it/s]\u001b[A\n"," 33% 3900/11873 [00:11<00:25, 310.96it/s]\u001b[A\n"," 33% 3932/11873 [00:11<00:27, 287.86it/s]\u001b[A\n"," 33% 3964/11873 [00:12<00:26, 296.42it/s]\u001b[A\n"," 34% 3996/11873 [00:12<00:26, 301.68it/s]\u001b[A\n"," 34% 4030/11873 [00:12<00:25, 310.85it/s]\u001b[A\n"," 34% 4065/11873 [00:12<00:24, 320.25it/s]\u001b[A\n"," 35% 4099/11873 [00:12<00:24, 323.78it/s]\u001b[A\n"," 35% 4132/11873 [00:12<00:24, 320.66it/s]\u001b[A\n"," 35% 4165/11873 [00:12<00:25, 296.86it/s]\u001b[A\n"," 35% 4196/11873 [00:12<00:25, 299.31it/s]\u001b[A\n"," 36% 4231/11873 [00:12<00:24, 311.83it/s]\u001b[A\n"," 36% 4265/11873 [00:13<00:23, 318.22it/s]\u001b[A\n"," 36% 4299/11873 [00:13<00:23, 324.07it/s]\u001b[A\n"," 36% 4333/11873 [00:13<00:23, 325.76it/s]\u001b[A\n"," 37% 4368/11873 [00:13<00:22, 330.71it/s]\u001b[A\n"," 37% 4402/11873 [00:13<00:22, 326.14it/s]\u001b[A\n"," 37% 4435/11873 [00:13<00:28, 258.79it/s]\u001b[A\n"," 38% 4469/11873 [00:13<00:26, 278.72it/s]\u001b[A\n"," 38% 4502/11873 [00:13<00:25, 290.57it/s]\u001b[A\n"," 38% 4536/11873 [00:13<00:24, 301.99it/s]\u001b[A\n"," 38% 4570/11873 [00:14<00:23, 310.94it/s]\u001b[A\n"," 39% 4605/11873 [00:14<00:22, 319.99it/s]\u001b[A\n"," 39% 4638/11873 [00:14<00:22, 321.13it/s]\u001b[A\n"," 39% 4672/11873 [00:14<00:22, 324.09it/s]\u001b[A\n"," 40% 4705/11873 [00:14<00:22, 320.98it/s]\u001b[A\n"," 40% 4738/11873 [00:14<00:22, 319.95it/s]\u001b[A\n"," 40% 4771/11873 [00:14<00:22, 321.35it/s]\u001b[A\n"," 40% 4804/11873 [00:14<00:22, 313.95it/s]\u001b[A\n"," 41% 4836/11873 [00:14<00:22, 313.32it/s]\u001b[A\n"," 41% 4868/11873 [00:14<00:22, 315.14it/s]\u001b[A\n"," 41% 4900/11873 [00:15<00:22, 314.04it/s]\u001b[A\n"," 42% 4934/11873 [00:15<00:21, 320.93it/s]\u001b[A\n"," 42% 4969/11873 [00:15<00:21, 327.02it/s]\u001b[A\n"," 42% 5002/11873 [00:15<00:21, 324.20it/s]\u001b[A\n"," 42% 5035/11873 [00:15<00:21, 324.03it/s]\u001b[A\n"," 43% 5068/11873 [00:15<00:20, 324.39it/s]\u001b[A\n"," 43% 5101/11873 [00:15<00:20, 325.97it/s]\u001b[A\n"," 43% 5134/11873 [00:15<00:20, 326.35it/s]\u001b[A\n"," 44% 5167/11873 [00:15<00:20, 327.34it/s]\u001b[A\n"," 44% 5201/11873 [00:15<00:20, 330.53it/s]\u001b[A\n"," 44% 5235/11873 [00:16<00:20, 331.02it/s]\u001b[A\n"," 44% 5269/11873 [00:16<00:21, 301.55it/s]\u001b[A\n"," 45% 5303/11873 [00:16<00:21, 309.93it/s]\u001b[A\n"," 45% 5337/11873 [00:16<00:20, 317.72it/s]\u001b[A\n"," 45% 5370/11873 [00:16<00:20, 313.61it/s]\u001b[A\n"," 46% 5405/11873 [00:16<00:20, 321.92it/s]\u001b[A\n"," 46% 5438/11873 [00:16<00:19, 322.23it/s]\u001b[A\n"," 46% 5471/11873 [00:16<00:20, 319.71it/s]\u001b[A\n"," 46% 5505/11873 [00:16<00:19, 324.61it/s]\u001b[A\n"," 47% 5540/11873 [00:17<00:19, 329.11it/s]\u001b[A\n"," 47% 5573/11873 [00:17<00:19, 326.88it/s]\u001b[A\n"," 47% 5606/11873 [00:17<00:19, 322.08it/s]\u001b[A\n"," 47% 5639/11873 [00:17<00:19, 317.87it/s]\u001b[A\n"," 48% 5672/11873 [00:17<00:19, 319.74it/s]\u001b[A\n"," 48% 5705/11873 [00:17<00:19, 319.41it/s]\u001b[A\n"," 48% 5738/11873 [00:17<00:19, 321.94it/s]\u001b[A\n"," 49% 5771/11873 [00:17<00:19, 316.94it/s]\u001b[A\n"," 49% 5806/11873 [00:17<00:18, 323.86it/s]\u001b[A\n"," 49% 5839/11873 [00:17<00:18, 324.45it/s]\u001b[A\n"," 49% 5872/11873 [00:18<00:18, 325.70it/s]\u001b[A\n"," 50% 5906/11873 [00:18<00:18, 329.38it/s]\u001b[A\n"," 50% 5940/11873 [00:18<00:17, 330.17it/s]\u001b[A\n"," 50% 5974/11873 [00:18<00:17, 330.82it/s]\u001b[A\n"," 51% 6008/11873 [00:18<00:17, 331.34it/s]\u001b[A\n"," 51% 6042/11873 [00:18<00:17, 330.40it/s]\u001b[A\n"," 51% 6076/11873 [00:18<00:17, 329.29it/s]\u001b[A\n"," 51% 6109/11873 [00:18<00:17, 326.57it/s]\u001b[A\n"," 52% 6142/11873 [00:18<00:17, 324.54it/s]\u001b[A\n"," 52% 6176/11873 [00:19<00:17, 327.13it/s]\u001b[A\n"," 52% 6209/11873 [00:19<00:17, 323.30it/s]\u001b[A\n"," 53% 6242/11873 [00:19<00:17, 321.78it/s]\u001b[A\n"," 53% 6275/11873 [00:19<00:17, 320.86it/s]\u001b[A\n"," 53% 6308/11873 [00:19<00:17, 320.59it/s]\u001b[A\n"," 53% 6341/11873 [00:19<00:17, 319.45it/s]\u001b[A\n"," 54% 6375/11873 [00:19<00:17, 323.19it/s]\u001b[A\n"," 54% 6408/11873 [00:19<00:16, 322.11it/s]\u001b[A\n"," 54% 6441/11873 [00:19<00:17, 318.69it/s]\u001b[A\n"," 55% 6474/11873 [00:19<00:16, 321.49it/s]\u001b[A\n"," 55% 6507/11873 [00:20<00:16, 317.47it/s]\u001b[A\n"," 55% 6539/11873 [00:20<00:16, 317.59it/s]\u001b[A\n"," 55% 6571/11873 [00:20<00:16, 315.87it/s]\u001b[A\n"," 56% 6603/11873 [00:20<00:16, 316.69it/s]\u001b[A\n"," 56% 6636/11873 [00:20<00:16, 319.67it/s]\u001b[A\n"," 56% 6668/11873 [00:20<00:16, 312.73it/s]\u001b[A\n"," 56% 6700/11873 [00:20<00:16, 309.82it/s]\u001b[A\n"," 57% 6732/11873 [00:20<00:18, 273.82it/s]\u001b[A\n"," 57% 6765/11873 [00:20<00:17, 288.00it/s]\u001b[A\n"," 57% 6797/11873 [00:21<00:17, 296.32it/s]\u001b[A\n"," 58% 6830/11873 [00:21<00:16, 303.99it/s]\u001b[A\n"," 58% 6863/11873 [00:21<00:16, 308.97it/s]\u001b[A\n"," 58% 6895/11873 [00:21<00:16, 309.85it/s]\u001b[A\n"," 58% 6927/11873 [00:21<00:16, 308.51it/s]\u001b[A\n"," 59% 6959/11873 [00:21<00:15, 310.04it/s]\u001b[A\n"," 59% 6991/11873 [00:21<00:15, 312.04it/s]\u001b[A\n"," 59% 7025/11873 [00:21<00:15, 318.25it/s]\u001b[A\n"," 59% 7057/11873 [00:21<00:15, 316.90it/s]\u001b[A\n"," 60% 7089/11873 [00:21<00:15, 316.27it/s]\u001b[A\n"," 60% 7122/11873 [00:22<00:14, 318.86it/s]\u001b[A\n"," 60% 7154/11873 [00:22<00:14, 318.85it/s]\u001b[A\n"," 61% 7186/11873 [00:22<00:14, 319.04it/s]\u001b[A\n"," 61% 7220/11873 [00:22<00:14, 322.42it/s]\u001b[A\n"," 61% 7253/11873 [00:22<00:14, 324.54it/s]\u001b[A\n"," 61% 7286/11873 [00:22<00:14, 319.71it/s]\u001b[A\n"," 62% 7319/11873 [00:22<00:14, 320.38it/s]\u001b[A\n"," 62% 7352/11873 [00:22<00:14, 319.33it/s]\u001b[A\n"," 62% 7384/11873 [00:22<00:14, 317.56it/s]\u001b[A\n"," 62% 7416/11873 [00:22<00:15, 294.65it/s]\u001b[A\n"," 63% 7449/11873 [00:23<00:14, 302.61it/s]\u001b[A\n"," 63% 7482/11873 [00:23<00:14, 309.27it/s]\u001b[A\n"," 63% 7516/11873 [00:23<00:13, 317.22it/s]\u001b[A\n"," 64% 7550/11873 [00:23<00:13, 321.65it/s]\u001b[A\n"," 64% 7583/11873 [00:23<00:13, 323.85it/s]\u001b[A\n"," 64% 7616/11873 [00:23<00:13, 318.78it/s]\u001b[A\n"," 64% 7648/11873 [00:23<00:13, 316.83it/s]\u001b[A\n"," 65% 7680/11873 [00:23<00:13, 314.01it/s]\u001b[A\n"," 65% 7712/11873 [00:23<00:14, 292.05it/s]\u001b[A\n"," 65% 7743/11873 [00:24<00:13, 296.60it/s]\u001b[A\n"," 65% 7775/11873 [00:24<00:13, 301.13it/s]\u001b[A\n"," 66% 7807/11873 [00:24<00:13, 303.96it/s]\u001b[A\n"," 66% 7838/11873 [00:24<00:13, 303.60it/s]\u001b[A\n"," 66% 7869/11873 [00:24<00:13, 301.24it/s]\u001b[A\n"," 67% 7900/11873 [00:24<00:13, 292.93it/s]\u001b[A\n"," 67% 7933/11873 [00:24<00:13, 301.59it/s]\u001b[A\n"," 67% 7965/11873 [00:24<00:12, 304.61it/s]\u001b[A\n"," 67% 7997/11873 [00:24<00:12, 306.54it/s]\u001b[A\n"," 68% 8030/11873 [00:24<00:12, 311.32it/s]\u001b[A\n"," 68% 8062/11873 [00:25<00:12, 313.54it/s]\u001b[A\n"," 68% 8096/11873 [00:25<00:11, 320.48it/s]\u001b[A\n"," 68% 8129/11873 [00:25<00:11, 315.69it/s]\u001b[A\n"," 69% 8161/11873 [00:25<00:11, 315.97it/s]\u001b[A\n"," 69% 8193/11873 [00:25<00:11, 316.12it/s]\u001b[A\n"," 69% 8225/11873 [00:25<00:11, 313.75it/s]\u001b[A\n"," 70% 8259/11873 [00:25<00:11, 318.86it/s]\u001b[A\n"," 70% 8291/11873 [00:25<00:11, 318.48it/s]\u001b[A\n"," 70% 8323/11873 [00:25<00:11, 316.64it/s]\u001b[A\n"," 70% 8356/11873 [00:26<00:11, 319.21it/s]\u001b[A\n"," 71% 8388/11873 [00:26<00:10, 319.35it/s]\u001b[A\n"," 71% 8420/11873 [00:26<00:10, 319.17it/s]\u001b[A\n"," 71% 8452/11873 [00:26<00:10, 315.26it/s]\u001b[A\n"," 71% 8484/11873 [00:26<00:10, 316.14it/s]\u001b[A\n"," 72% 8518/11873 [00:26<00:10, 321.47it/s]\u001b[A\n"," 72% 8551/11873 [00:26<00:10, 320.55it/s]\u001b[A\n"," 72% 8585/11873 [00:26<00:10, 323.87it/s]\u001b[A\n"," 73% 8618/11873 [00:26<00:10, 320.57it/s]\u001b[A\n"," 73% 8651/11873 [00:26<00:10, 321.37it/s]\u001b[A\n"," 73% 8685/11873 [00:27<00:09, 324.25it/s]\u001b[A\n"," 73% 8718/11873 [00:27<00:09, 324.87it/s]\u001b[A\n"," 74% 8751/11873 [00:27<00:09, 325.13it/s]\u001b[A\n"," 74% 8785/11873 [00:27<00:09, 326.72it/s]\u001b[A\n"," 74% 8818/11873 [00:27<00:09, 321.83it/s]\u001b[A\n"," 75% 8852/11873 [00:27<00:09, 325.60it/s]\u001b[A\n"," 75% 8885/11873 [00:27<00:09, 320.92it/s]\u001b[A\n"," 75% 8919/11873 [00:27<00:09, 323.63it/s]\u001b[A\n"," 75% 8952/11873 [00:27<00:09, 323.54it/s]\u001b[A\n"," 76% 8985/11873 [00:27<00:08, 322.76it/s]\u001b[A\n"," 76% 9019/11873 [00:28<00:08, 326.39it/s]\u001b[A\n"," 76% 9052/11873 [00:28<00:08, 325.50it/s]\u001b[A\n"," 77% 9086/11873 [00:28<00:08, 326.97it/s]\u001b[A\n"," 77% 9119/11873 [00:28<00:08, 325.34it/s]\u001b[A\n"," 77% 9152/11873 [00:28<00:08, 319.12it/s]\u001b[A\n"," 77% 9186/11873 [00:28<00:08, 324.93it/s]\u001b[A\n"," 78% 9220/11873 [00:28<00:08, 326.88it/s]\u001b[A\n"," 78% 9253/11873 [00:28<00:08, 322.56it/s]\u001b[A\n"," 78% 9286/11873 [00:28<00:07, 323.98it/s]\u001b[A\n"," 78% 9320/11873 [00:28<00:07, 325.90it/s]\u001b[A\n"," 79% 9353/11873 [00:29<00:07, 320.64it/s]\u001b[A\n"," 79% 9388/11873 [00:29<00:07, 327.15it/s]\u001b[A\n"," 79% 9421/11873 [00:29<00:07, 325.09it/s]\u001b[A\n"," 80% 9455/11873 [00:29<00:07, 328.33it/s]\u001b[A\n"," 80% 9488/11873 [00:29<00:07, 328.39it/s]\u001b[A\n"," 80% 9522/11873 [00:29<00:07, 328.85it/s]\u001b[A\n"," 80% 9555/11873 [00:29<00:07, 325.98it/s]\u001b[A\n"," 81% 9589/11873 [00:29<00:06, 330.07it/s]\u001b[A\n"," 81% 9623/11873 [00:29<00:06, 329.11it/s]\u001b[A\n"," 81% 9656/11873 [00:30<00:06, 326.60it/s]\u001b[A\n"," 82% 9690/11873 [00:30<00:06, 329.47it/s]\u001b[A\n"," 82% 9724/11873 [00:30<00:06, 331.34it/s]\u001b[A\n"," 82% 9758/11873 [00:30<00:06, 331.35it/s]\u001b[A\n"," 82% 9792/11873 [00:30<00:06, 329.76it/s]\u001b[A\n"," 83% 9825/11873 [00:30<00:06, 325.79it/s]\u001b[A\n"," 83% 9860/11873 [00:30<00:06, 330.62it/s]\u001b[A\n"," 83% 9894/11873 [00:30<00:06, 328.77it/s]\u001b[A\n"," 84% 9928/11873 [00:30<00:05, 330.79it/s]\u001b[A\n"," 84% 9962/11873 [00:30<00:05, 331.21it/s]\u001b[A\n"," 84% 9996/11873 [00:31<00:05, 329.45it/s]\u001b[A\n"," 84% 10030/11873 [00:31<00:05, 330.23it/s]\u001b[A\n"," 85% 10064/11873 [00:31<00:05, 330.54it/s]\u001b[A\n"," 85% 10098/11873 [00:31<00:05, 331.31it/s]\u001b[A\n"," 85% 10132/11873 [00:31<00:05, 331.33it/s]\u001b[A\n"," 86% 10166/11873 [00:31<00:05, 331.95it/s]\u001b[A\n"," 86% 10200/11873 [00:31<00:05, 328.95it/s]\u001b[A\n"," 86% 10234/11873 [00:31<00:04, 330.52it/s]\u001b[A\n"," 86% 10268/11873 [00:31<00:04, 331.55it/s]\u001b[A\n"," 87% 10302/11873 [00:31<00:04, 325.68it/s]\u001b[A\n"," 87% 10336/11873 [00:32<00:04, 327.80it/s]\u001b[A\n"," 87% 10369/11873 [00:32<00:04, 328.28it/s]\u001b[A\n"," 88% 10402/11873 [00:32<00:04, 326.30it/s]\u001b[A\n"," 88% 10435/11873 [00:32<00:04, 306.84it/s]\u001b[A\n"," 88% 10469/11873 [00:32<00:04, 314.38it/s]\u001b[A\n"," 88% 10501/11873 [00:32<00:04, 314.54it/s]\u001b[A\n"," 89% 10535/11873 [00:32<00:04, 320.17it/s]\u001b[A\n"," 89% 10568/11873 [00:32<00:04, 316.78it/s]\u001b[A\n"," 89% 10600/11873 [00:32<00:04, 313.24it/s]\u001b[A\n"," 90% 10632/11873 [00:33<00:03, 314.62it/s]\u001b[A\n"," 90% 10664/11873 [00:33<00:03, 315.27it/s]\u001b[A\n"," 90% 10698/11873 [00:33<00:03, 320.22it/s]\u001b[A\n"," 90% 10732/11873 [00:33<00:03, 324.28it/s]\u001b[A\n"," 91% 10765/11873 [00:33<00:03, 324.68it/s]\u001b[A\n"," 91% 10798/11873 [00:33<00:03, 324.34it/s]\u001b[A\n"," 91% 10831/11873 [00:33<00:03, 297.30it/s]\u001b[A\n"," 92% 10865/11873 [00:33<00:03, 306.77it/s]\u001b[A\n"," 92% 10898/11873 [00:33<00:03, 310.71it/s]\u001b[A\n"," 92% 10930/11873 [00:33<00:03, 308.88it/s]\u001b[A\n"," 92% 10962/11873 [00:34<00:02, 304.42it/s]\u001b[A\n"," 93% 10995/11873 [00:34<00:02, 310.20it/s]\u001b[A\n"," 93% 11028/11873 [00:34<00:02, 313.35it/s]\u001b[A\n"," 93% 11062/11873 [00:34<00:02, 319.16it/s]\u001b[A\n"," 93% 11094/11873 [00:34<00:02, 315.38it/s]\u001b[A\n"," 94% 11128/11873 [00:34<00:02, 320.29it/s]\u001b[A\n"," 94% 11162/11873 [00:34<00:02, 323.66it/s]\u001b[A\n"," 94% 11196/11873 [00:34<00:02, 326.56it/s]\u001b[A\n"," 95% 11229/11873 [00:34<00:01, 325.14it/s]\u001b[A\n"," 95% 11262/11873 [00:34<00:01, 324.70it/s]\u001b[A\n"," 95% 11295/11873 [00:35<00:01, 326.02it/s]\u001b[A\n"," 95% 11329/11873 [00:35<00:01, 329.13it/s]\u001b[A\n"," 96% 11362/11873 [00:35<00:01, 327.61it/s]\u001b[A\n"," 96% 11395/11873 [00:35<00:01, 326.76it/s]\u001b[A\n"," 96% 11428/11873 [00:35<00:01, 326.88it/s]\u001b[A\n"," 97% 11463/11873 [00:35<00:01, 330.64it/s]\u001b[A\n"," 97% 11497/11873 [00:35<00:01, 326.32it/s]\u001b[A\n"," 97% 11530/11873 [00:35<00:01, 326.07it/s]\u001b[A\n"," 97% 11563/11873 [00:35<00:00, 323.59it/s]\u001b[A\n"," 98% 11596/11873 [00:36<00:00, 324.98it/s]\u001b[A\n"," 98% 11629/11873 [00:36<00:00, 323.55it/s]\u001b[A\n"," 98% 11662/11873 [00:36<00:00, 323.25it/s]\u001b[A\n"," 99% 11695/11873 [00:36<00:00, 320.31it/s]\u001b[A\n"," 99% 11729/11873 [00:36<00:00, 324.78it/s]\u001b[A\n"," 99% 11762/11873 [00:36<00:00, 314.11it/s]\u001b[A\n"," 99% 11795/11873 [00:36<00:00, 318.21it/s]\u001b[A\n","100% 11828/11873 [00:36<00:00, 320.41it/s]\u001b[A\n","100% 11873/11873 [00:36<00:00, 321.92it/s]\n","04/06/2022 14:09:19 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/eval_predictions.json.\n","04/06/2022 14:09:19 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/eval_nbest_predictions.json.\n","04/06/2022 14:09:22 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/roberta-base/back-trans-aug/eval_null_odds.json.\n","04/06/2022 14:09:26 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1521/1521 [03:54<00:00,  6.49it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 78.5762\n","  eval_HasAns_f1         = 84.4562\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       =  80.471\n","  eval_NoAns_f1          =  80.471\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        =  79.525\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 82.4608\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             =  79.525\n","  eval_f1                = 82.4608\n","  eval_samples           =   12165\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-06 14:09:27,476 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'sichenzhong/squad_v2_back_trans_aug', 'type': 'sichenzhong/squad_v2_back_trans_aug', 'args': 'squad_v2'}}\n"]}]}]}