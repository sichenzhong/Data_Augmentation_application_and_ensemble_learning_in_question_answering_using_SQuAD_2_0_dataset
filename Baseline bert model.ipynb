{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline bert model.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70oLi9mZP6oK","executionInfo":{"status":"ok","timestamp":1649311004430,"user_tz":240,"elapsed":355,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"3a476e00-ce90-4ea8-89d3-c42f9055bafe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Apr  7 05:56:43 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGqcN-zXTvvo","executionInfo":{"status":"ok","timestamp":1649311012383,"user_tz":240,"elapsed":7956,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"bc7e1bca-1d42-4b2e-8194-e58c37343bad"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n","\u001b[?25l\r\u001b[K     |█                               | 10 kB 38.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 42.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 81 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 102 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 112 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 122 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 133 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 143 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 153 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 163 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 174 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 184 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 194 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 204 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 215 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 225 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 235 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 245 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 256 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 266 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 276 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 286 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 296 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 307 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 317 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 325 kB 14.2 MB/s \n","\u001b[?25hCollecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n","\u001b[?25l\r\u001b[K     |██▍                             | 10 kB 54.7 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 20 kB 64.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 30 kB 76.5 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 40 kB 79.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51 kB 82.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 61 kB 85.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 71 kB 88.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 81 kB 86.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 92 kB 87.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 102 kB 90.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 112 kB 90.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 122 kB 90.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 133 kB 90.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 136 kB 90.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 73.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n","Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Collecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 6.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 54.3 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 87.5 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 88.8 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 84.9 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.3.0 huggingface-hub-0.5.1 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tI_RBT1FSotu","outputId":"5fd1436b-0fa4-40cd-8859-51834b6fc1b1","executionInfo":{"status":"ok","timestamp":1649311034520,"user_tz":240,"elapsed":22144,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-fui2xi9c\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-fui2xi9c\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (3.6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (1.21.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 14.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.63.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.5.1)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 82.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.11.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 84.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0.dev0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.0.dev0) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.0.dev0) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (1.25.11)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.15.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.19.0.dev0-py3-none-any.whl size=3965509 sha256=ac188e7915eb60c010fa903bf9db44196ae8ef8901bfba9fcc0151569ce4316e\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-0w1_yiuc/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.19.0.dev0\n"]}]},{"cell_type":"code","source":["import torch\n","import datetime\n","import json\n","import os\n","import time\n","import datasets\n","import pprint\n","import random\n","import string\n","import sys\n","import transformers"],"metadata":{"id":"DZ3Ma-pCRJDJ","executionInfo":{"status":"ok","timestamp":1649311042796,"user_tz":240,"elapsed":8280,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available()\n","                      else 'cpu')"],"metadata":{"id":"HNMUVyBpRGw8","executionInfo":{"status":"ok","timestamp":1649311042797,"user_tz":240,"elapsed":7,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/huggingface/transformers.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUVkgX-IQIiR","executionInfo":{"status":"ok","timestamp":1649311049820,"user_tz":240,"elapsed":7029,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"50d3d703-aa5a-487c-ee8f-19c5d9794ec8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 108786, done.\u001b[K\n","remote: Total 108786 (delta 0), reused 0 (delta 0), pack-reused 108786\u001b[K\n","Receiving objects: 100% (108786/108786), 95.61 MiB | 26.97 MiB/s, done.\n","Resolving deltas: 100% (79286/79286), done.\n"]}]},{"cell_type":"code","source":["%cd /content/transformers/examples/pytorch/question-answering/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uLaizsXQrk9","executionInfo":{"status":"ok","timestamp":1649311049820,"user_tz":240,"elapsed":8,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"e499fe60-61d7-4232-838a-8c1494d92cda"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/transformers/examples/pytorch/question-answering\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8O2U-C2qmOy8","executionInfo":{"status":"ok","timestamp":1649311065936,"user_tz":240,"elapsed":16122,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"dfb10862-77b2-4cf1-877c-4429cb6c87e9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path bert-base-cased \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 3e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOm5ck1RQqE_","executionInfo":{"status":"ok","timestamp":1647739682759,"user_tz":240,"elapsed":10431245,"user":{"displayName":"SICHEN ZHONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGziZ-Uu-PyjGuHsDy1aSyMZvJYEN6bO8qLBQyKw=s64","userId":"08427994781088390833"}},"outputId":"5a7bc212-b960-47ed-fbf5-dd9d96118812"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["03/19/2022 22:34:15 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","03/19/2022 22:34:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Mar19_22-34-15_4e419f9e067d,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","03/19/2022 22:34:15 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpch8_uye0\n","Downloading builder script: 5.28kB [00:00, 6.80MB/s]       \n","03/19/2022 22:34:15 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/19/2022 22:34:15 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/19/2022 22:34:15 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpeln7u2iz\n","Downloading metadata: 2.40kB [00:00, 4.09MB/s]       \n","03/19/2022 22:34:15 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/19/2022 22:34:15 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/19/2022 22:34:15 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","03/19/2022 22:34:15 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/19/2022 22:34:15 - INFO - datasets.builder - Generating dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","03/19/2022 22:34:15 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]03/19/2022 22:34:16 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpjvl3azx8\n","\n","Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n","Downloading data: 10.3MB [00:00, 103MB/s]        \u001b[A\n","Downloading data: 21.2MB [00:00, 107MB/s]\u001b[A\n","Downloading data: 42.1MB [00:00, 107MB/s]\n","03/19/2022 22:34:17 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","03/19/2022 22:34:17 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","Downloading data files:  50% 1/2 [00:01<00:01,  1.42s/it]03/19/2022 22:34:17 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpk4f1mfox\n","\n","Downloading data: 4.37MB [00:00, 114MB/s]       \n","03/19/2022 22:34:17 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","03/19/2022 22:34:17 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","Downloading data files: 100% 2/2 [00:01<00:00,  1.19it/s]\n","03/19/2022 22:34:17 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","03/19/2022 22:34:17 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1561.54it/s]\n","03/19/2022 22:34:17 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n","03/19/2022 22:34:17 - INFO - datasets.builder - Generating train split\n","03/19/2022 22:34:28 - INFO - datasets.builder - Generating validation split\n","03/19/2022 22:34:29 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 304.00it/s]\n","[INFO|file_utils.py:2241] 2022-03-19 22:34:29,885 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpdmj7lk4y\n","Downloading: 100% 570/570 [00:00<00:00, 564kB/s]\n","[INFO|file_utils.py:2245] 2022-03-19 22:34:30,250 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|file_utils.py:2253] 2022-03-19 22:34:30,250 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:649] 2022-03-19 22:34:30,250 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:685] 2022-03-19 22:34:30,251 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|file_utils.py:2241] 2022-03-19 22:34:30,608 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpj3gqkxtm\n","Downloading: 100% 29.0/29.0 [00:00<00:00, 26.2kB/s]\n","[INFO|file_utils.py:2245] 2022-03-19 22:34:30,968 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|file_utils.py:2253] 2022-03-19 22:34:30,968 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:649] 2022-03-19 22:34:31,327 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:685] 2022-03-19 22:34:31,328 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|file_utils.py:2241] 2022-03-19 22:34:32,043 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp642bxkxu\n","Downloading: 100% 208k/208k [00:00<00:00, 629kB/s]\n","[INFO|file_utils.py:2245] 2022-03-19 22:34:32,743 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|file_utils.py:2253] 2022-03-19 22:34:32,743 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|file_utils.py:2241] 2022-03-19 22:34:33,101 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf2d39ig_\n","Downloading: 100% 426k/426k [00:00<00:00, 1.03MB/s]\n","[INFO|file_utils.py:2245] 2022-03-19 22:34:33,886 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|file_utils.py:2253] 2022-03-19 22:34:33,886 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1786] 2022-03-19 22:34:34,969 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1786] 2022-03-19 22:34:34,969 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1786] 2022-03-19 22:34:34,969 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1786] 2022-03-19 22:34:34,969 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1786] 2022-03-19 22:34:34,969 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:649] 2022-03-19 22:34:35,327 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:685] 2022-03-19 22:34:35,328 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|file_utils.py:2241] 2022-03-19 22:34:35,730 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9dzeob69\n","Downloading: 100% 416M/416M [00:06<00:00, 64.3MB/s]\n","[INFO|file_utils.py:2245] 2022-03-19 22:34:42,577 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|file_utils.py:2253] 2022-03-19 22:34:42,577 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|modeling_utils.py:1432] 2022-03-19 22:34:42,577 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:1695] 2022-03-19 22:34:45,752 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:1706] 2022-03-19 22:34:45,752 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]03/19/2022 22:34:46 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-538cb096a17d202c.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:41<00:00,  3.14ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]03/19/2022 22:35:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-cc2890595de471da.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:03<00:00,  5.28s/ba]\n","03/19/2022 22:36:31 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpk_8wfnn9\n","Downloading builder script: 6.46kB [00:00, 8.31MB/s]       \n","03/19/2022 22:36:31 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/19/2022 22:36:31 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/19/2022 22:36:31 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpwrhd9bsz\n","Downloading extra modules: 11.3kB [00:00, 13.0MB/s]       \n","03/19/2022 22:36:31 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","03/19/2022 22:36:31 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1288] 2022-03-19 22:36:45,522 >> ***** Running training *****\n","[INFO|trainer.py:1289] 2022-03-19 22:36:45,522 >>   Num examples = 132079\n","[INFO|trainer.py:1290] 2022-03-19 22:36:45,522 >>   Num Epochs = 2\n","[INFO|trainer.py:1291] 2022-03-19 22:36:45,522 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1292] 2022-03-19 22:36:45,522 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1293] 2022-03-19 22:36:45,522 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1294] 2022-03-19 22:36:45,522 >>   Total optimization steps = 11008\n","{'loss': 2.1044, 'learning_rate': 2.863735465116279e-05, 'epoch': 0.09}\n","  5% 500/11008 [07:33<2:38:44,  1.10it/s][INFO|trainer.py:2162] 2022-03-19 22:44:18,660 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:440] 2022-03-19 22:44:18,661 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-19 22:44:19,495 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-19 22:44:19,495 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-19 22:44:19,495 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.4826, 'learning_rate': 2.727470930232558e-05, 'epoch': 0.18}\n","  9% 1000/11008 [15:09<2:31:17,  1.10it/s][INFO|trainer.py:2162] 2022-03-19 22:51:54,626 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:440] 2022-03-19 22:51:54,627 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-19 22:51:55,441 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-19 22:51:55,442 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-19 22:51:55,442 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.3569, 'learning_rate': 2.5912063953488374e-05, 'epoch': 0.27}\n"," 14% 1500/11008 [22:45<2:23:36,  1.10it/s][INFO|trainer.py:2162] 2022-03-19 22:59:30,584 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:440] 2022-03-19 22:59:30,585 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-19 22:59:31,395 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-19 22:59:31,396 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-19 22:59:31,396 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.2886, 'learning_rate': 2.4549418604651164e-05, 'epoch': 0.36}\n"," 18% 2000/11008 [30:20<2:15:57,  1.10it/s][INFO|trainer.py:2162] 2022-03-19 23:07:06,359 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:440] 2022-03-19 23:07:06,360 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-19 23:07:07,162 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-19 23:07:07,163 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-19 23:07:07,163 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.2181, 'learning_rate': 2.3186773255813954e-05, 'epoch': 0.45}\n"," 23% 2500/11008 [37:56<2:08:30,  1.10it/s][INFO|trainer.py:2162] 2022-03-19 23:14:42,200 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:440] 2022-03-19 23:14:42,201 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-19 23:14:43,044 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-19 23:14:43,045 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-19 23:14:43,045 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.1695, 'learning_rate': 2.1824127906976744e-05, 'epoch': 0.55}\n"," 27% 3000/11008 [45:32<2:01:01,  1.10it/s][INFO|trainer.py:2162] 2022-03-19 23:22:18,144 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:440] 2022-03-19 23:22:18,145 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-19 23:22:18,957 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-19 23:22:18,958 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-19 23:22:18,958 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.1279, 'learning_rate': 2.0461482558139534e-05, 'epoch': 0.64}\n"," 32% 3500/11008 [53:08<1:53:31,  1.10it/s][INFO|trainer.py:2162] 2022-03-19 23:29:54,281 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:440] 2022-03-19 23:29:54,282 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-19 23:29:55,090 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-19 23:29:55,091 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-19 23:29:55,091 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.0643, 'learning_rate': 1.9098837209302328e-05, 'epoch': 0.73}\n"," 36% 4000/11008 [1:00:44<1:45:47,  1.10it/s][INFO|trainer.py:2162] 2022-03-19 23:37:30,478 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:440] 2022-03-19 23:37:30,479 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-19 23:37:31,278 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-19 23:37:31,278 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-19 23:37:31,279 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.0724, 'learning_rate': 1.7736191860465118e-05, 'epoch': 0.82}\n"," 41% 4500/11008 [1:08:21<1:38:20,  1.10it/s][INFO|trainer.py:2162] 2022-03-19 23:45:06,817 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:440] 2022-03-19 23:45:06,817 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-19 23:45:07,622 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-19 23:45:07,622 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-19 23:45:07,622 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.047, 'learning_rate': 1.6373546511627908e-05, 'epoch': 0.91}\n"," 45% 5000/11008 [1:15:57<1:30:53,  1.10it/s][INFO|trainer.py:2162] 2022-03-19 23:52:43,262 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:440] 2022-03-19 23:52:43,263 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-19 23:52:44,067 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-19 23:52:44,067 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-19 23:52:44,067 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.0137, 'learning_rate': 1.50109011627907e-05, 'epoch': 1.0}\n"," 50% 5500/11008 [1:23:34<1:23:16,  1.10it/s][INFO|trainer.py:2162] 2022-03-20 00:00:19,830 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:440] 2022-03-20 00:00:19,830 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 00:00:20,639 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 00:00:20,640 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 00:00:20,640 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.778, 'learning_rate': 1.3648255813953488e-05, 'epoch': 1.09}\n"," 55% 6000/11008 [1:31:10<1:15:55,  1.10it/s][INFO|trainer.py:2162] 2022-03-20 00:07:55,830 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:440] 2022-03-20 00:07:55,831 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 00:07:56,651 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 00:07:56,651 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 00:07:56,651 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.7516, 'learning_rate': 1.2285610465116278e-05, 'epoch': 1.18}\n"," 59% 6500/11008 [1:38:47<1:08:02,  1.10it/s][INFO|trainer.py:2162] 2022-03-20 00:15:32,608 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:440] 2022-03-20 00:15:32,608 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 00:15:33,421 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 00:15:33,422 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 00:15:33,422 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.733, 'learning_rate': 1.092296511627907e-05, 'epoch': 1.27}\n"," 64% 7000/11008 [1:46:23<1:00:36,  1.10it/s][INFO|trainer.py:2162] 2022-03-20 00:23:09,008 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:440] 2022-03-20 00:23:09,009 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 00:23:09,874 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 00:23:09,875 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 00:23:09,875 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.7423, 'learning_rate': 9.56031976744186e-06, 'epoch': 1.36}\n"," 68% 7500/11008 [1:54:00<53:05,  1.10it/s][INFO|trainer.py:2162] 2022-03-20 00:30:45,897 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:440] 2022-03-20 00:30:45,898 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 00:30:46,720 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 00:30:46,721 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 00:30:46,721 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.7432, 'learning_rate': 8.197674418604652e-06, 'epoch': 1.45}\n"," 73% 8000/11008 [2:01:37<45:29,  1.10it/s][INFO|trainer.py:2162] 2022-03-20 00:38:22,584 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:440] 2022-03-20 00:38:22,585 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 00:38:23,423 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 00:38:23,424 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 00:38:23,424 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.7296, 'learning_rate': 6.835029069767442e-06, 'epoch': 1.54}\n"," 77% 8500/11008 [2:09:13<37:58,  1.10it/s][INFO|trainer.py:2162] 2022-03-20 00:45:59,513 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:440] 2022-03-20 00:45:59,514 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 00:46:00,386 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 00:46:00,386 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 00:46:00,386 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.7136, 'learning_rate': 5.4723837209302325e-06, 'epoch': 1.64}\n"," 82% 9000/11008 [2:16:50<30:25,  1.10it/s][INFO|trainer.py:2162] 2022-03-20 00:53:36,501 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:440] 2022-03-20 00:53:36,502 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 00:53:37,339 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 00:53:37,339 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 00:53:37,339 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.7425, 'learning_rate': 4.109738372093023e-06, 'epoch': 1.73}\n"," 86% 9500/11008 [2:24:27<22:49,  1.10it/s][INFO|trainer.py:2162] 2022-03-20 01:01:13,093 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:440] 2022-03-20 01:01:13,094 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 01:01:13,847 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 01:01:13,847 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 01:01:13,847 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.7061, 'learning_rate': 2.747093023255814e-06, 'epoch': 1.82}\n"," 91% 10000/11008 [2:32:03<15:16,  1.10it/s][INFO|trainer.py:2162] 2022-03-20 01:08:49,531 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:440] 2022-03-20 01:08:49,532 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 01:08:50,297 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 01:08:50,298 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 01:08:50,298 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.7088, 'learning_rate': 1.3844476744186047e-06, 'epoch': 1.91}\n"," 95% 10500/11008 [2:39:40<07:40,  1.10it/s][INFO|trainer.py:2162] 2022-03-20 01:16:26,085 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:440] 2022-03-20 01:16:26,086 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 01:16:26,810 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 01:16:26,811 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 01:16:26,811 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.7064, 'learning_rate': 2.1802325581395348e-08, 'epoch': 2.0}\n","100% 11000/11008 [2:47:16<00:07,  1.10it/s][INFO|trainer.py:2162] 2022-03-20 01:24:02,424 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11000\n","[INFO|configuration_utils.py:440] 2022-03-20 01:24:02,425 >> Configuration saved in /tmp/debug_squad/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 01:24:03,165 >> Model weights saved in /tmp/debug_squad/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 01:24:03,166 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 01:24:03,166 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11000/special_tokens_map.json\n","100% 11008/11008 [2:47:25<00:00,  1.27it/s][INFO|trainer.py:1526] 2022-03-20 01:24:11,404 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10045.8845, 'train_samples_per_second': 26.295, 'train_steps_per_second': 1.096, 'train_loss': 0.9997500386089087, 'epoch': 2.0}\n","100% 11008/11008 [2:47:25<00:00,  1.10it/s]\n","[INFO|trainer.py:2162] 2022-03-20 01:24:11,413 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:440] 2022-03-20 01:24:11,414 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1085] 2022-03-20 01:24:12,204 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2094] 2022-03-20 01:24:12,204 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2100] 2022-03-20 01:24:12,204 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.9998\n","  train_runtime            = 2:47:25.88\n","  train_samples            =     132079\n","  train_samples_per_second =     26.295\n","  train_steps_per_second   =      1.096\n","03/20/2022 01:24:12 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:571] 2022-03-20 01:24:12,238 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2412] 2022-03-20 01:24:12,241 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2414] 2022-03-20 01:24:12,241 >>   Num examples = 12199\n","[INFO|trainer.py:2417] 2022-03-20 01:24:12,241 >>   Batch size = 8\n","100% 1525/1525 [02:56<00:00,  8.78it/s]03/20/2022 01:27:21 - INFO - utils_qa - Post-processing 11873 example predictions split into 12199 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 44/11873 [00:00<00:27, 434.70it/s]\u001b[A\n","  1% 88/11873 [00:00<00:28, 411.42it/s]\u001b[A\n","  1% 134/11873 [00:00<00:27, 429.44it/s]\u001b[A\n","  2% 184/11873 [00:00<00:25, 452.38it/s]\u001b[A\n","  2% 232/11873 [00:00<00:25, 461.20it/s]\u001b[A\n","  2% 280/11873 [00:00<00:24, 466.34it/s]\u001b[A\n","  3% 327/11873 [00:00<00:24, 462.83it/s]\u001b[A\n","  3% 374/11873 [00:00<00:24, 463.20it/s]\u001b[A\n","  4% 422/11873 [00:00<00:24, 466.66it/s]\u001b[A\n","  4% 471/11873 [00:01<00:24, 471.49it/s]\u001b[A\n","  4% 519/11873 [00:01<00:24, 469.04it/s]\u001b[A\n","  5% 566/11873 [00:01<00:24, 468.95it/s]\u001b[A\n","  5% 614/11873 [00:01<00:23, 469.86it/s]\u001b[A\n","  6% 661/11873 [00:01<00:23, 469.18it/s]\u001b[A\n","  6% 709/11873 [00:01<00:23, 470.86it/s]\u001b[A\n","  6% 757/11873 [00:01<00:23, 464.50it/s]\u001b[A\n","  7% 805/11873 [00:01<00:23, 467.84it/s]\u001b[A\n","  7% 854/11873 [00:01<00:23, 474.03it/s]\u001b[A\n","  8% 905/11873 [00:01<00:22, 483.61it/s]\u001b[A\n","  8% 955/11873 [00:02<00:22, 486.08it/s]\u001b[A\n","  8% 1004/11873 [00:02<00:22, 474.70it/s]\u001b[A\n","  9% 1052/11873 [00:02<00:24, 437.19it/s]\u001b[A\n","  9% 1097/11873 [00:02<00:25, 418.16it/s]\u001b[A\n"," 10% 1140/11873 [00:02<00:26, 406.94it/s]\u001b[A\n"," 10% 1182/11873 [00:02<00:26, 397.16it/s]\u001b[A\n"," 10% 1222/11873 [00:02<00:27, 384.05it/s]\u001b[A\n"," 11% 1261/11873 [00:02<00:28, 378.06it/s]\u001b[A\n"," 11% 1299/11873 [00:02<00:28, 376.21it/s]\u001b[A\n"," 11% 1337/11873 [00:03<00:28, 374.13it/s]\u001b[A\n"," 12% 1375/11873 [00:03<00:28, 374.75it/s]\u001b[A\n"," 12% 1414/11873 [00:03<00:27, 378.74it/s]\u001b[A\n"," 12% 1452/11873 [00:03<00:27, 373.79it/s]\u001b[A\n"," 13% 1490/11873 [00:03<00:28, 368.11it/s]\u001b[A\n"," 13% 1527/11873 [00:03<00:28, 367.43it/s]\u001b[A\n"," 13% 1564/11873 [00:03<00:28, 366.07it/s]\u001b[A\n"," 14% 1603/11873 [00:03<00:27, 369.98it/s]\u001b[A\n"," 14% 1641/11873 [00:03<00:28, 357.60it/s]\u001b[A\n"," 14% 1677/11873 [00:03<00:28, 355.26it/s]\u001b[A\n"," 14% 1714/11873 [00:04<00:28, 359.51it/s]\u001b[A\n"," 15% 1751/11873 [00:04<00:27, 361.72it/s]\u001b[A\n"," 15% 1789/11873 [00:04<00:27, 365.72it/s]\u001b[A\n"," 15% 1826/11873 [00:04<00:27, 366.11it/s]\u001b[A\n"," 16% 1864/11873 [00:04<00:27, 370.20it/s]\u001b[A\n"," 16% 1902/11873 [00:04<00:26, 371.53it/s]\u001b[A\n"," 16% 1940/11873 [00:04<00:26, 371.17it/s]\u001b[A\n"," 17% 1978/11873 [00:04<00:26, 371.19it/s]\u001b[A\n"," 17% 2016/11873 [00:04<00:26, 368.26it/s]\u001b[A\n"," 17% 2054/11873 [00:05<00:26, 369.25it/s]\u001b[A\n"," 18% 2091/11873 [00:05<00:26, 366.92it/s]\u001b[A\n"," 18% 2128/11873 [00:05<00:26, 366.25it/s]\u001b[A\n"," 18% 2166/11873 [00:05<00:26, 368.75it/s]\u001b[A\n"," 19% 2203/11873 [00:05<00:27, 357.65it/s]\u001b[A\n"," 19% 2239/11873 [00:05<00:27, 351.02it/s]\u001b[A\n"," 19% 2275/11873 [00:05<00:27, 346.18it/s]\u001b[A\n"," 19% 2310/11873 [00:05<00:27, 345.74it/s]\u001b[A\n"," 20% 2345/11873 [00:05<00:27, 345.01it/s]\u001b[A\n"," 20% 2380/11873 [00:05<00:27, 342.73it/s]\u001b[A\n"," 20% 2417/11873 [00:06<00:27, 349.22it/s]\u001b[A\n"," 21% 2454/11873 [00:06<00:26, 353.86it/s]\u001b[A\n"," 21% 2491/11873 [00:06<00:26, 358.56it/s]\u001b[A\n"," 21% 2528/11873 [00:06<00:25, 360.62it/s]\u001b[A\n"," 22% 2565/11873 [00:06<00:26, 354.60it/s]\u001b[A\n"," 22% 2601/11873 [00:06<00:26, 350.21it/s]\u001b[A\n"," 22% 2637/11873 [00:06<00:26, 351.70it/s]\u001b[A\n"," 23% 2675/11873 [00:06<00:25, 357.60it/s]\u001b[A\n"," 23% 2713/11873 [00:06<00:25, 363.41it/s]\u001b[A\n"," 23% 2750/11873 [00:06<00:25, 362.93it/s]\u001b[A\n"," 23% 2787/11873 [00:07<00:25, 362.41it/s]\u001b[A\n"," 24% 2826/11873 [00:07<00:24, 368.21it/s]\u001b[A\n"," 24% 2864/11873 [00:07<00:24, 371.21it/s]\u001b[A\n"," 24% 2902/11873 [00:07<00:24, 367.38it/s]\u001b[A\n"," 25% 2939/11873 [00:07<00:24, 366.58it/s]\u001b[A\n"," 25% 2977/11873 [00:07<00:24, 368.90it/s]\u001b[A\n"," 25% 3014/11873 [00:07<00:26, 330.91it/s]\u001b[A\n"," 26% 3050/11873 [00:07<00:26, 336.79it/s]\u001b[A\n"," 26% 3085/11873 [00:07<00:25, 339.90it/s]\u001b[A\n"," 26% 3120/11873 [00:08<00:29, 298.46it/s]\u001b[A\n"," 27% 3151/11873 [00:08<00:30, 283.29it/s]\u001b[A\n"," 27% 3181/11873 [00:08<00:30, 284.40it/s]\u001b[A\n"," 27% 3216/11873 [00:08<00:28, 301.99it/s]\u001b[A\n"," 27% 3250/11873 [00:08<00:27, 312.05it/s]\u001b[A\n"," 28% 3282/11873 [00:08<00:32, 264.00it/s]\u001b[A\n"," 28% 3310/11873 [00:08<00:38, 221.81it/s]\u001b[A\n"," 28% 3335/11873 [00:08<00:40, 208.41it/s]\u001b[A\n"," 28% 3358/11873 [00:09<00:43, 197.17it/s]\u001b[A\n"," 29% 3387/11873 [00:09<00:38, 217.63it/s]\u001b[A\n"," 29% 3426/11873 [00:09<00:32, 259.18it/s]\u001b[A\n"," 29% 3462/11873 [00:09<00:29, 284.19it/s]\u001b[A\n"," 29% 3497/11873 [00:09<00:27, 300.93it/s]\u001b[A\n"," 30% 3533/11873 [00:09<00:26, 316.50it/s]\u001b[A\n"," 30% 3571/11873 [00:09<00:24, 332.55it/s]\u001b[A\n"," 30% 3610/11873 [00:09<00:23, 347.78it/s]\u001b[A\n"," 31% 3647/11873 [00:09<00:23, 353.78it/s]\u001b[A\n"," 31% 3683/11873 [00:10<00:23, 351.64it/s]\u001b[A\n"," 31% 3720/11873 [00:10<00:22, 356.28it/s]\u001b[A\n"," 32% 3758/11873 [00:10<00:22, 361.27it/s]\u001b[A\n"," 32% 3795/11873 [00:10<00:22, 360.86it/s]\u001b[A\n"," 32% 3832/11873 [00:10<00:24, 332.91it/s]\u001b[A\n"," 33% 3867/11873 [00:10<00:23, 335.21it/s]\u001b[A\n"," 33% 3903/11873 [00:10<00:23, 341.44it/s]\u001b[A\n"," 33% 3938/11873 [00:10<00:24, 323.52it/s]\u001b[A\n"," 33% 3971/11873 [00:10<00:24, 316.91it/s]\u001b[A\n"," 34% 4005/11873 [00:11<00:24, 323.26it/s]\u001b[A\n"," 34% 4042/11873 [00:11<00:23, 334.20it/s]\u001b[A\n"," 34% 4078/11873 [00:11<00:22, 340.51it/s]\u001b[A\n"," 35% 4113/11873 [00:11<00:22, 337.97it/s]\u001b[A\n"," 35% 4147/11873 [00:11<00:23, 330.82it/s]\u001b[A\n"," 35% 4181/11873 [00:11<00:23, 328.51it/s]\u001b[A\n"," 36% 4218/11873 [00:11<00:22, 338.58it/s]\u001b[A\n"," 36% 4256/11873 [00:11<00:21, 350.08it/s]\u001b[A\n"," 36% 4295/11873 [00:11<00:21, 359.40it/s]\u001b[A\n"," 37% 4334/11873 [00:11<00:20, 367.08it/s]\u001b[A\n"," 37% 4371/11873 [00:12<00:20, 365.03it/s]\u001b[A\n"," 37% 4408/11873 [00:12<00:21, 351.28it/s]\u001b[A\n"," 37% 4444/11873 [00:12<00:26, 282.98it/s]\u001b[A\n"," 38% 4478/11873 [00:12<00:24, 296.53it/s]\u001b[A\n"," 38% 4511/11873 [00:12<00:24, 304.49it/s]\u001b[A\n"," 38% 4546/11873 [00:12<00:23, 315.37it/s]\u001b[A\n"," 39% 4585/11873 [00:12<00:21, 335.13it/s]\u001b[A\n"," 39% 4621/11873 [00:12<00:21, 340.83it/s]\u001b[A\n"," 39% 4656/11873 [00:12<00:21, 334.31it/s]\u001b[A\n"," 40% 4691/11873 [00:13<00:21, 338.15it/s]\u001b[A\n"," 40% 4726/11873 [00:13<00:21, 340.09it/s]\u001b[A\n"," 40% 4762/11873 [00:13<00:20, 345.76it/s]\u001b[A\n"," 40% 4797/11873 [00:13<00:20, 346.46it/s]\u001b[A\n"," 41% 4832/11873 [00:13<00:20, 346.19it/s]\u001b[A\n"," 41% 4867/11873 [00:13<00:20, 346.00it/s]\u001b[A\n"," 41% 4902/11873 [00:13<00:20, 346.95it/s]\u001b[A\n"," 42% 4937/11873 [00:13<00:19, 346.94it/s]\u001b[A\n"," 42% 4973/11873 [00:13<00:19, 350.34it/s]\u001b[A\n"," 42% 5009/11873 [00:13<00:19, 351.14it/s]\u001b[A\n"," 42% 5045/11873 [00:14<00:19, 353.08it/s]\u001b[A\n"," 43% 5083/11873 [00:14<00:18, 360.38it/s]\u001b[A\n"," 43% 5121/11873 [00:14<00:18, 363.58it/s]\u001b[A\n"," 43% 5159/11873 [00:14<00:18, 366.10it/s]\u001b[A\n"," 44% 5197/11873 [00:14<00:18, 368.66it/s]\u001b[A\n"," 44% 5236/11873 [00:14<00:17, 372.93it/s]\u001b[A\n"," 44% 5274/11873 [00:14<00:18, 348.57it/s]\u001b[A\n"," 45% 5312/11873 [00:14<00:18, 356.21it/s]\u001b[A\n"," 45% 5351/11873 [00:14<00:17, 363.16it/s]\u001b[A\n"," 45% 5389/11873 [00:15<00:17, 365.59it/s]\u001b[A\n"," 46% 5426/11873 [00:15<00:17, 366.85it/s]\u001b[A\n"," 46% 5463/11873 [00:15<00:17, 361.87it/s]\u001b[A\n"," 46% 5502/11873 [00:15<00:17, 368.10it/s]\u001b[A\n"," 47% 5540/11873 [00:15<00:17, 371.54it/s]\u001b[A\n"," 47% 5578/11873 [00:15<00:17, 361.80it/s]\u001b[A\n"," 47% 5615/11873 [00:15<00:17, 353.04it/s]\u001b[A\n"," 48% 5651/11873 [00:15<00:17, 351.45it/s]\u001b[A\n"," 48% 5687/11873 [00:15<00:17, 347.44it/s]\u001b[A\n"," 48% 5722/11873 [00:15<00:17, 346.09it/s]\u001b[A\n"," 48% 5758/11873 [00:16<00:17, 348.88it/s]\u001b[A\n"," 49% 5797/11873 [00:16<00:16, 358.62it/s]\u001b[A\n"," 49% 5835/11873 [00:16<00:16, 363.27it/s]\u001b[A\n"," 49% 5872/11873 [00:16<00:16, 364.70it/s]\u001b[A\n"," 50% 5910/11873 [00:16<00:16, 367.56it/s]\u001b[A\n"," 50% 5948/11873 [00:16<00:16, 369.61it/s]\u001b[A\n"," 50% 5987/11873 [00:16<00:15, 372.92it/s]\u001b[A\n"," 51% 6026/11873 [00:16<00:15, 375.12it/s]\u001b[A\n"," 51% 6064/11873 [00:16<00:15, 374.97it/s]\u001b[A\n"," 51% 6102/11873 [00:16<00:15, 372.33it/s]\u001b[A\n"," 52% 6140/11873 [00:17<00:15, 358.47it/s]\u001b[A\n"," 52% 6176/11873 [00:17<00:16, 354.72it/s]\u001b[A\n"," 52% 6212/11873 [00:17<00:15, 354.75it/s]\u001b[A\n"," 53% 6248/11873 [00:17<00:15, 353.29it/s]\u001b[A\n"," 53% 6284/11873 [00:17<00:15, 350.67it/s]\u001b[A\n"," 53% 6320/11873 [00:17<00:15, 351.03it/s]\u001b[A\n"," 54% 6358/11873 [00:17<00:15, 356.55it/s]\u001b[A\n"," 54% 6396/11873 [00:17<00:15, 362.20it/s]\u001b[A\n"," 54% 6433/11873 [00:17<00:15, 361.74it/s]\u001b[A\n"," 54% 6470/11873 [00:18<00:14, 363.38it/s]\u001b[A\n"," 55% 6508/11873 [00:18<00:14, 365.74it/s]\u001b[A\n"," 55% 6546/11873 [00:18<00:14, 368.69it/s]\u001b[A\n"," 55% 6584/11873 [00:18<00:14, 370.01it/s]\u001b[A\n"," 56% 6622/11873 [00:18<00:14, 368.05it/s]\u001b[A\n"," 56% 6659/11873 [00:18<00:14, 367.50it/s]\u001b[A\n"," 56% 6696/11873 [00:18<00:14, 366.80it/s]\u001b[A\n"," 57% 6733/11873 [00:18<00:15, 330.17it/s]\u001b[A\n"," 57% 6770/11873 [00:18<00:14, 340.73it/s]\u001b[A\n"," 57% 6808/11873 [00:18<00:14, 349.46it/s]\u001b[A\n"," 58% 6845/11873 [00:19<00:14, 354.64it/s]\u001b[A\n"," 58% 6882/11873 [00:19<00:13, 358.57it/s]\u001b[A\n"," 58% 6920/11873 [00:19<00:13, 362.96it/s]\u001b[A\n"," 59% 6957/11873 [00:19<00:13, 363.18it/s]\u001b[A\n"," 59% 6995/11873 [00:19<00:13, 367.62it/s]\u001b[A\n"," 59% 7032/11873 [00:19<00:13, 356.39it/s]\u001b[A\n"," 60% 7068/11873 [00:19<00:13, 351.43it/s]\u001b[A\n"," 60% 7106/11873 [00:19<00:13, 359.19it/s]\u001b[A\n"," 60% 7144/11873 [00:19<00:13, 362.69it/s]\u001b[A\n"," 60% 7181/11873 [00:19<00:12, 362.82it/s]\u001b[A\n"," 61% 7219/11873 [00:20<00:12, 366.94it/s]\u001b[A\n"," 61% 7256/11873 [00:20<00:12, 365.85it/s]\u001b[A\n"," 61% 7293/11873 [00:20<00:12, 365.23it/s]\u001b[A\n"," 62% 7330/11873 [00:20<00:12, 363.82it/s]\u001b[A\n"," 62% 7367/11873 [00:20<00:12, 360.75it/s]\u001b[A\n"," 62% 7404/11873 [00:20<00:12, 351.06it/s]\u001b[A\n"," 63% 7440/11873 [00:20<00:13, 339.37it/s]\u001b[A\n"," 63% 7477/11873 [00:20<00:12, 347.13it/s]\u001b[A\n"," 63% 7515/11873 [00:20<00:12, 355.82it/s]\u001b[A\n"," 64% 7551/11873 [00:21<00:12, 356.93it/s]\u001b[A\n"," 64% 7587/11873 [00:21<00:12, 355.62it/s]\u001b[A\n"," 64% 7624/11873 [00:21<00:11, 357.35it/s]\u001b[A\n"," 65% 7660/11873 [00:21<00:11, 355.99it/s]\u001b[A\n"," 65% 7696/11873 [00:21<00:11, 356.38it/s]\u001b[A\n"," 65% 7732/11873 [00:21<00:12, 329.80it/s]\u001b[A\n"," 65% 7766/11873 [00:21<00:12, 326.49it/s]\u001b[A\n"," 66% 7801/11873 [00:21<00:12, 331.23it/s]\u001b[A\n"," 66% 7835/11873 [00:21<00:12, 331.59it/s]\u001b[A\n"," 66% 7869/11873 [00:21<00:12, 327.07it/s]\u001b[A\n"," 67% 7902/11873 [00:22<00:12, 317.68it/s]\u001b[A\n"," 67% 7939/11873 [00:22<00:11, 331.11it/s]\u001b[A\n"," 67% 7974/11873 [00:22<00:11, 335.42it/s]\u001b[A\n"," 67% 8012/11873 [00:22<00:11, 346.77it/s]\u001b[A\n"," 68% 8048/11873 [00:22<00:10, 350.37it/s]\u001b[A\n"," 68% 8084/11873 [00:22<00:10, 344.90it/s]\u001b[A\n"," 68% 8119/11873 [00:22<00:10, 343.58it/s]\u001b[A\n"," 69% 8154/11873 [00:22<00:10, 340.55it/s]\u001b[A\n"," 69% 8189/11873 [00:22<00:10, 340.39it/s]\u001b[A\n"," 69% 8224/11873 [00:23<00:10, 337.20it/s]\u001b[A\n"," 70% 8258/11873 [00:23<00:10, 336.83it/s]\u001b[A\n"," 70% 8292/11873 [00:23<00:10, 333.30it/s]\u001b[A\n"," 70% 8326/11873 [00:23<00:10, 329.10it/s]\u001b[A\n"," 70% 8360/11873 [00:23<00:10, 331.33it/s]\u001b[A\n"," 71% 8395/11873 [00:23<00:10, 335.44it/s]\u001b[A\n"," 71% 8431/11873 [00:23<00:10, 341.05it/s]\u001b[A\n"," 71% 8467/11873 [00:23<00:09, 345.34it/s]\u001b[A\n"," 72% 8505/11873 [00:23<00:09, 354.96it/s]\u001b[A\n"," 72% 8543/11873 [00:23<00:09, 360.85it/s]\u001b[A\n"," 72% 8580/11873 [00:24<00:09, 363.43it/s]\u001b[A\n"," 73% 8617/11873 [00:24<00:08, 362.94it/s]\u001b[A\n"," 73% 8654/11873 [00:24<00:08, 363.32it/s]\u001b[A\n"," 73% 8691/11873 [00:24<00:08, 364.95it/s]\u001b[A\n"," 74% 8728/11873 [00:24<00:08, 364.57it/s]\u001b[A\n"," 74% 8765/11873 [00:24<00:08, 364.65it/s]\u001b[A\n"," 74% 8802/11873 [00:24<00:08, 356.86it/s]\u001b[A\n"," 74% 8839/11873 [00:24<00:08, 360.00it/s]\u001b[A\n"," 75% 8877/11873 [00:24<00:08, 364.75it/s]\u001b[A\n"," 75% 8914/11873 [00:24<00:08, 363.44it/s]\u001b[A\n"," 75% 8951/11873 [00:25<00:08, 362.30it/s]\u001b[A\n"," 76% 8989/11873 [00:25<00:07, 365.82it/s]\u001b[A\n"," 76% 9026/11873 [00:25<00:07, 365.99it/s]\u001b[A\n"," 76% 9063/11873 [00:25<00:07, 365.55it/s]\u001b[A\n"," 77% 9101/11873 [00:25<00:07, 367.82it/s]\u001b[A\n"," 77% 9138/11873 [00:25<00:07, 366.48it/s]\u001b[A\n"," 77% 9176/11873 [00:25<00:07, 369.62it/s]\u001b[A\n"," 78% 9214/11873 [00:25<00:07, 371.78it/s]\u001b[A\n"," 78% 9252/11873 [00:25<00:07, 371.19it/s]\u001b[A\n"," 78% 9290/11873 [00:25<00:07, 367.61it/s]\u001b[A\n"," 79% 9328/11873 [00:26<00:06, 368.52it/s]\u001b[A\n"," 79% 9365/11873 [00:26<00:06, 366.23it/s]\u001b[A\n"," 79% 9403/11873 [00:26<00:06, 367.93it/s]\u001b[A\n"," 80% 9440/11873 [00:26<00:06, 367.17it/s]\u001b[A\n"," 80% 9477/11873 [00:26<00:06, 367.39it/s]\u001b[A\n"," 80% 9514/11873 [00:26<00:06, 363.64it/s]\u001b[A\n"," 80% 9551/11873 [00:26<00:06, 355.14it/s]\u001b[A\n"," 81% 9587/11873 [00:26<00:06, 351.51it/s]\u001b[A\n"," 81% 9623/11873 [00:26<00:06, 346.04it/s]\u001b[A\n"," 81% 9658/11873 [00:27<00:06, 344.00it/s]\u001b[A\n"," 82% 9696/11873 [00:27<00:06, 353.17it/s]\u001b[A\n"," 82% 9733/11873 [00:27<00:05, 357.52it/s]\u001b[A\n"," 82% 9772/11873 [00:27<00:05, 364.43it/s]\u001b[A\n"," 83% 9810/11873 [00:27<00:05, 366.69it/s]\u001b[A\n"," 83% 9847/11873 [00:27<00:05, 362.98it/s]\u001b[A\n"," 83% 9885/11873 [00:27<00:05, 367.93it/s]\u001b[A\n"," 84% 9923/11873 [00:27<00:05, 370.50it/s]\u001b[A\n"," 84% 9961/11873 [00:27<00:05, 368.90it/s]\u001b[A\n"," 84% 9999/11873 [00:27<00:05, 371.14it/s]\u001b[A\n"," 85% 10037/11873 [00:28<00:04, 372.92it/s]\u001b[A\n"," 85% 10075/11873 [00:28<00:04, 374.72it/s]\u001b[A\n"," 85% 10113/11873 [00:28<00:04, 367.31it/s]\u001b[A\n"," 85% 10150/11873 [00:28<00:04, 365.57it/s]\u001b[A\n"," 86% 10188/11873 [00:28<00:04, 368.35it/s]\u001b[A\n"," 86% 10226/11873 [00:28<00:04, 368.69it/s]\u001b[A\n"," 86% 10263/11873 [00:28<00:04, 365.47it/s]\u001b[A\n"," 87% 10300/11873 [00:28<00:04, 363.48it/s]\u001b[A\n"," 87% 10337/11873 [00:28<00:04, 365.33it/s]\u001b[A\n"," 87% 10374/11873 [00:28<00:04, 363.25it/s]\u001b[A\n"," 88% 10412/11873 [00:29<00:03, 365.44it/s]\u001b[A\n"," 88% 10449/11873 [00:29<00:04, 338.82it/s]\u001b[A\n"," 88% 10484/11873 [00:29<00:04, 339.15it/s]\u001b[A\n"," 89% 10519/11873 [00:29<00:03, 338.53it/s]\u001b[A\n"," 89% 10554/11873 [00:29<00:04, 317.08it/s]\u001b[A\n"," 89% 10589/11873 [00:29<00:03, 325.25it/s]\u001b[A\n"," 90% 10628/11873 [00:29<00:03, 341.09it/s]\u001b[A\n"," 90% 10663/11873 [00:29<00:03, 343.18it/s]\u001b[A\n"," 90% 10701/11873 [00:29<00:03, 352.71it/s]\u001b[A\n"," 90% 10739/11873 [00:30<00:03, 358.18it/s]\u001b[A\n"," 91% 10776/11873 [00:30<00:03, 359.79it/s]\u001b[A\n"," 91% 10813/11873 [00:30<00:02, 361.84it/s]\u001b[A\n"," 91% 10850/11873 [00:30<00:03, 332.70it/s]\u001b[A\n"," 92% 10888/11873 [00:30<00:02, 344.16it/s]\u001b[A\n"," 92% 10923/11873 [00:30<00:02, 339.14it/s]\u001b[A\n"," 92% 10960/11873 [00:30<00:02, 346.54it/s]\u001b[A\n"," 93% 10999/11873 [00:30<00:02, 356.87it/s]\u001b[A\n"," 93% 11036/11873 [00:30<00:02, 360.20it/s]\u001b[A\n"," 93% 11074/11873 [00:30<00:02, 365.32it/s]\u001b[A\n"," 94% 11111/11873 [00:31<00:02, 364.37it/s]\u001b[A\n"," 94% 11149/11873 [00:31<00:01, 367.43it/s]\u001b[A\n"," 94% 11186/11873 [00:31<00:01, 355.95it/s]\u001b[A\n"," 95% 11223/11873 [00:31<00:01, 358.93it/s]\u001b[A\n"," 95% 11259/11873 [00:31<00:01, 359.09it/s]\u001b[A\n"," 95% 11297/11873 [00:31<00:01, 363.96it/s]\u001b[A\n"," 95% 11334/11873 [00:31<00:01, 359.51it/s]\u001b[A\n"," 96% 11370/11873 [00:31<00:01, 348.37it/s]\u001b[A\n"," 96% 11405/11873 [00:31<00:01, 347.50it/s]\u001b[A\n"," 96% 11441/11873 [00:32<00:01, 350.65it/s]\u001b[A\n"," 97% 11478/11873 [00:32<00:01, 355.64it/s]\u001b[A\n"," 97% 11516/11873 [00:32<00:00, 360.19it/s]\u001b[A\n"," 97% 11553/11873 [00:32<00:00, 358.46it/s]\u001b[A\n"," 98% 11591/11873 [00:32<00:00, 362.27it/s]\u001b[A\n"," 98% 11629/11873 [00:32<00:00, 365.24it/s]\u001b[A\n"," 98% 11666/11873 [00:32<00:00, 357.97it/s]\u001b[A\n"," 99% 11702/11873 [00:32<00:00, 348.46it/s]\u001b[A\n"," 99% 11737/11873 [00:32<00:00, 344.59it/s]\u001b[A\n"," 99% 11772/11873 [00:32<00:00, 344.80it/s]\u001b[A\n"," 99% 11807/11873 [00:33<00:00, 344.18it/s]\u001b[A\n","100% 11873/11873 [00:33<00:00, 356.99it/s]\n","03/20/2022 01:27:54 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","03/20/2022 01:27:54 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","03/20/2022 01:27:57 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","03/20/2022 01:28:00 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1525/1525 [03:47<00:00,  6.70it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      =  72.554\n","  eval_HasAns_f1         = 78.8323\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 69.0833\n","  eval_NoAns_f1          = 69.0833\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 70.8161\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 73.9508\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 70.8161\n","  eval_f1                = 73.9508\n","  eval_samples           =   12199\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-03-20 01:28:00,426 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path bert-base-cased \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GBntN3fluB5-","executionInfo":{"status":"ok","timestamp":1648333561839,"user_tz":240,"elapsed":10305055,"user":{"displayName":"SICHEN ZHONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGziZ-Uu-PyjGuHsDy1aSyMZvJYEN6bO8qLBQyKw=s64","userId":"08427994781088390833"}},"outputId":"681a5828-4e23-4c31-dc2d-b4941e0c6b9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["03/26/2022 19:34:20 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","03/26/2022 19:34:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Mar26_19-34-20_56544ad519f9,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","03/26/2022 19:34:20 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","03/26/2022 19:34:20 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/26/2022 19:34:20 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","03/26/2022 19:34:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/26/2022 19:34:20 - WARNING - datasets.builder - Reusing dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","03/26/2022 19:34:20 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","100% 2/2 [00:00<00:00, 687.70it/s]\n","[INFO|configuration_utils.py:653] 2022-03-26 19:34:20,917 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-26 19:34:20,917 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:653] 2022-03-26 19:34:21,218 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-26 19:34:21,219 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1777] 2022-03-26 19:34:22,058 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1777] 2022-03-26 19:34:22,058 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1777] 2022-03-26 19:34:22,058 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1777] 2022-03-26 19:34:22,058 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1777] 2022-03-26 19:34:22,058 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:653] 2022-03-26 19:34:22,196 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-26 19:34:22,196 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:1771] 2022-03-26 19:34:22,376 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:2049] 2022-03-26 19:34:23,816 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-03-26 19:34:23,817 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/26/2022 19:34:23 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-538cb096a17d202c.arrow\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]03/26/2022 19:34:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-f1f6a455c564ba86.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:02<00:00,  5.24s/ba]\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1288] 2022-03-26 19:35:30,685 >> ***** Running training *****\n","[INFO|trainer.py:1289] 2022-03-26 19:35:30,685 >>   Num examples = 132079\n","[INFO|trainer.py:1290] 2022-03-26 19:35:30,685 >>   Num Epochs = 2\n","[INFO|trainer.py:1291] 2022-03-26 19:35:30,685 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1292] 2022-03-26 19:35:30,685 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1293] 2022-03-26 19:35:30,685 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1294] 2022-03-26 19:35:30,685 >>   Total optimization steps = 11008\n","{'loss': 2.2729, 'learning_rate': 1.9091569767441863e-05, 'epoch': 0.09}\n","  5% 500/11008 [07:32<2:38:21,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 19:43:02,699 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:440] 2022-03-26 19:43:02,699 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 19:43:03,357 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 19:43:03,358 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 19:43:03,358 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.5499, 'learning_rate': 1.8183139534883724e-05, 'epoch': 0.18}\n","  9% 1000/11008 [15:06<2:30:42,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 19:50:36,876 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:440] 2022-03-26 19:50:36,877 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 19:50:37,515 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 19:50:37,516 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 19:50:37,516 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.4113, 'learning_rate': 1.7274709302325582e-05, 'epoch': 0.27}\n"," 14% 1500/11008 [22:40<2:23:24,  1.10it/s][INFO|trainer.py:2162] 2022-03-26 19:58:11,106 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:440] 2022-03-26 19:58:11,107 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 19:58:11,719 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 19:58:11,720 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 19:58:11,720 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.3397, 'learning_rate': 1.6366279069767443e-05, 'epoch': 0.36}\n"," 18% 2000/11008 [30:14<2:15:39,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 20:05:45,487 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:440] 2022-03-26 20:05:45,488 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 20:05:46,124 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 20:05:46,125 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 20:05:46,125 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.2745, 'learning_rate': 1.5457848837209304e-05, 'epoch': 0.45}\n"," 23% 2500/11008 [37:49<2:08:19,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 20:13:19,866 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:440] 2022-03-26 20:13:19,867 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 20:13:20,515 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 20:13:20,515 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 20:13:20,515 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.2229, 'learning_rate': 1.4549418604651163e-05, 'epoch': 0.55}\n"," 27% 3000/11008 [45:23<2:00:25,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 20:20:54,190 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:440] 2022-03-26 20:20:54,191 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 20:20:54,839 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 20:20:54,840 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 20:20:54,840 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.1837, 'learning_rate': 1.3640988372093025e-05, 'epoch': 0.64}\n"," 32% 3500/11008 [52:57<1:53:11,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 20:28:28,659 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:440] 2022-03-26 20:28:28,660 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 20:28:29,309 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 20:28:29,310 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 20:28:29,310 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.1175, 'learning_rate': 1.2732558139534886e-05, 'epoch': 0.73}\n"," 36% 4000/11008 [1:00:32<1:45:45,  1.10it/s][INFO|trainer.py:2162] 2022-03-26 20:36:03,180 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:440] 2022-03-26 20:36:03,181 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 20:36:03,827 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 20:36:03,827 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 20:36:03,828 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.1184, 'learning_rate': 1.1824127906976745e-05, 'epoch': 0.82}\n"," 41% 4500/11008 [1:08:07<1:38:08,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 20:43:37,709 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:440] 2022-03-26 20:43:37,710 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 20:43:38,356 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 20:43:38,356 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 20:43:38,357 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.0933, 'learning_rate': 1.0915697674418606e-05, 'epoch': 0.91}\n"," 45% 5000/11008 [1:15:41<1:30:47,  1.10it/s][INFO|trainer.py:2162] 2022-03-26 20:51:12,394 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:440] 2022-03-26 20:51:12,395 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 20:51:13,049 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 20:51:13,050 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 20:51:13,050 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.0573, 'learning_rate': 1.0007267441860466e-05, 'epoch': 1.0}\n"," 50% 5500/11008 [1:23:16<1:23:09,  1.10it/s][INFO|trainer.py:2162] 2022-03-26 20:58:47,007 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:440] 2022-03-26 20:58:47,007 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 20:58:47,652 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 20:58:47,652 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 20:58:47,652 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.8735, 'learning_rate': 9.098837209302325e-06, 'epoch': 1.09}\n"," 55% 6000/11008 [1:30:50<1:15:30,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 21:06:20,795 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:440] 2022-03-26 21:06:20,796 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 21:06:21,446 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 21:06:21,447 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 21:06:21,447 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.8415, 'learning_rate': 8.190406976744186e-06, 'epoch': 1.18}\n"," 59% 6500/11008 [1:38:24<1:07:58,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 21:13:55,284 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:440] 2022-03-26 21:13:55,285 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 21:13:55,927 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 21:13:55,928 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 21:13:55,928 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.8246, 'learning_rate': 7.281976744186047e-06, 'epoch': 1.27}\n"," 64% 7000/11008 [1:45:58<1:00:26,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 21:21:29,683 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:440] 2022-03-26 21:21:29,683 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 21:21:30,326 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 21:21:30,326 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 21:21:30,327 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.8338, 'learning_rate': 6.373546511627907e-06, 'epoch': 1.36}\n"," 68% 7500/11008 [1:53:33<52:57,  1.10it/s][INFO|trainer.py:2162] 2022-03-26 21:29:04,026 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:440] 2022-03-26 21:29:04,027 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 21:29:04,671 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 21:29:04,671 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 21:29:04,671 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.8328, 'learning_rate': 5.465116279069767e-06, 'epoch': 1.45}\n"," 73% 8000/11008 [2:01:07<45:16,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 21:36:38,358 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:440] 2022-03-26 21:36:38,359 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 21:36:39,011 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 21:36:39,011 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 21:36:39,011 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.8229, 'learning_rate': 4.556686046511628e-06, 'epoch': 1.54}\n"," 77% 8500/11008 [2:08:41<37:51,  1.10it/s][INFO|trainer.py:2162] 2022-03-26 21:44:12,518 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:440] 2022-03-26 21:44:12,519 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 21:44:13,201 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 21:44:13,202 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 21:44:13,202 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.8106, 'learning_rate': 3.6482558139534885e-06, 'epoch': 1.64}\n"," 82% 9000/11008 [2:16:16<30:21,  1.10it/s][INFO|trainer.py:2162] 2022-03-26 21:51:47,163 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:440] 2022-03-26 21:51:47,164 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 21:51:47,859 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 21:51:47,860 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 21:51:47,860 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.8323, 'learning_rate': 2.7398255813953488e-06, 'epoch': 1.73}\n"," 86% 9500/11008 [2:23:51<22:45,  1.10it/s][INFO|trainer.py:2162] 2022-03-26 21:59:21,883 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:440] 2022-03-26 21:59:21,884 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 21:59:22,538 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 21:59:22,539 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 21:59:22,539 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.8, 'learning_rate': 1.8313953488372093e-06, 'epoch': 1.82}\n"," 91% 10000/11008 [2:31:25<15:11,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 22:06:56,355 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:440] 2022-03-26 22:06:56,356 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 22:06:57,016 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 22:06:57,016 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 22:06:57,017 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.7998, 'learning_rate': 9.229651162790698e-07, 'epoch': 1.91}\n"," 95% 10500/11008 [2:38:59<07:38,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 22:14:30,681 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:440] 2022-03-26 22:14:30,682 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 22:14:31,332 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 22:14:31,333 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 22:14:31,333 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.7945, 'learning_rate': 1.4534883720930234e-08, 'epoch': 2.0}\n","100% 11000/11008 [2:46:34<00:07,  1.10it/s][INFO|trainer.py:2162] 2022-03-26 22:22:05,097 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11000\n","[INFO|configuration_utils.py:440] 2022-03-26 22:22:05,098 >> Configuration saved in /tmp/debug_squad/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 22:22:05,735 >> Model weights saved in /tmp/debug_squad/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 22:22:05,736 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 22:22:05,736 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11000/special_tokens_map.json\n","100% 11008/11008 [2:46:43<00:00,  1.28it/s][INFO|trainer.py:1526] 2022-03-26 22:22:13,845 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10003.1627, 'train_samples_per_second': 26.407, 'train_steps_per_second': 1.1, 'train_loss': 1.0774169397042241, 'epoch': 2.0}\n","100% 11008/11008 [2:46:43<00:00,  1.10it/s]\n","[INFO|trainer.py:2162] 2022-03-26 22:22:13,849 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:440] 2022-03-26 22:22:13,850 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 22:22:14,472 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 22:22:14,473 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 22:22:14,473 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     1.0774\n","  train_runtime            = 2:46:43.16\n","  train_samples            =     132079\n","  train_samples_per_second =     26.407\n","  train_steps_per_second   =        1.1\n","03/26/2022 22:22:14 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:571] 2022-03-26 22:22:14,504 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2412] 2022-03-26 22:22:14,506 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2414] 2022-03-26 22:22:14,506 >>   Num examples = 12199\n","[INFO|trainer.py:2417] 2022-03-26 22:22:14,507 >>   Batch size = 8\n","100% 1525/1525 [02:56<00:00,  8.81it/s]03/26/2022 22:25:23 - INFO - utils_qa - Post-processing 11873 example predictions split into 12199 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 44/11873 [00:00<00:27, 431.37it/s]\u001b[A\n","  1% 88/11873 [00:00<00:28, 406.62it/s]\u001b[A\n","  1% 134/11873 [00:00<00:27, 428.89it/s]\u001b[A\n","  2% 183/11873 [00:00<00:25, 451.22it/s]\u001b[A\n","  2% 232/11873 [00:00<00:25, 463.04it/s]\u001b[A\n","  2% 280/11873 [00:00<00:24, 466.41it/s]\u001b[A\n","  3% 327/11873 [00:00<00:24, 464.85it/s]\u001b[A\n","  3% 375/11873 [00:00<00:24, 468.88it/s]\u001b[A\n","  4% 424/11873 [00:00<00:24, 472.54it/s]\u001b[A\n","  4% 473/11873 [00:01<00:23, 476.96it/s]\u001b[A\n","  4% 521/11873 [00:01<00:23, 475.83it/s]\u001b[A\n","  5% 569/11873 [00:01<00:24, 467.63it/s]\u001b[A\n","  5% 617/11873 [00:01<00:23, 469.27it/s]\u001b[A\n","  6% 665/11873 [00:01<00:23, 472.06it/s]\u001b[A\n","  6% 713/11873 [00:01<00:24, 454.36it/s]\u001b[A\n","  6% 759/11873 [00:01<00:24, 450.60it/s]\u001b[A\n","  7% 809/11873 [00:01<00:23, 463.57it/s]\u001b[A\n","  7% 858/11873 [00:01<00:23, 470.92it/s]\u001b[A\n","  8% 910/11873 [00:01<00:22, 482.44it/s]\u001b[A\n","  8% 961/11873 [00:02<00:22, 488.72it/s]\u001b[A\n","  9% 1010/11873 [00:02<00:23, 472.00it/s]\u001b[A\n","  9% 1058/11873 [00:02<00:24, 436.86it/s]\u001b[A\n","  9% 1103/11873 [00:02<00:25, 418.54it/s]\u001b[A\n"," 10% 1146/11873 [00:02<00:26, 403.49it/s]\u001b[A\n"," 10% 1187/11873 [00:02<00:26, 398.94it/s]\u001b[A\n"," 10% 1228/11873 [00:02<00:27, 393.25it/s]\u001b[A\n"," 11% 1268/11873 [00:02<00:27, 387.37it/s]\u001b[A\n"," 11% 1307/11873 [00:02<00:27, 385.30it/s]\u001b[A\n"," 11% 1346/11873 [00:03<00:27, 383.53it/s]\u001b[A\n"," 12% 1385/11873 [00:03<00:27, 384.21it/s]\u001b[A\n"," 12% 1424/11873 [00:03<00:27, 384.23it/s]\u001b[A\n"," 12% 1463/11873 [00:03<00:27, 375.34it/s]\u001b[A\n"," 13% 1502/11873 [00:03<00:27, 377.19it/s]\u001b[A\n"," 13% 1541/11873 [00:03<00:27, 379.27it/s]\u001b[A\n"," 13% 1580/11873 [00:03<00:27, 379.80it/s]\u001b[A\n"," 14% 1619/11873 [00:03<00:27, 373.88it/s]\u001b[A\n"," 14% 1657/11873 [00:03<00:27, 375.15it/s]\u001b[A\n"," 14% 1695/11873 [00:03<00:27, 371.10it/s]\u001b[A\n"," 15% 1734/11873 [00:04<00:26, 376.25it/s]\u001b[A\n"," 15% 1773/11873 [00:04<00:26, 378.66it/s]\u001b[A\n"," 15% 1811/11873 [00:04<00:26, 377.72it/s]\u001b[A\n"," 16% 1849/11873 [00:04<00:26, 377.01it/s]\u001b[A\n"," 16% 1887/11873 [00:04<00:26, 377.73it/s]\u001b[A\n"," 16% 1926/11873 [00:04<00:26, 380.29it/s]\u001b[A\n"," 17% 1965/11873 [00:04<00:25, 381.28it/s]\u001b[A\n"," 17% 2004/11873 [00:04<00:25, 381.67it/s]\u001b[A\n"," 17% 2043/11873 [00:04<00:25, 383.32it/s]\u001b[A\n"," 18% 2082/11873 [00:05<00:25, 383.01it/s]\u001b[A\n"," 18% 2121/11873 [00:05<00:25, 380.26it/s]\u001b[A\n"," 18% 2160/11873 [00:05<00:25, 382.33it/s]\u001b[A\n"," 19% 2199/11873 [00:05<00:25, 380.27it/s]\u001b[A\n"," 19% 2238/11873 [00:05<00:25, 381.44it/s]\u001b[A\n"," 19% 2277/11873 [00:05<00:25, 379.70it/s]\u001b[A\n"," 19% 2315/11873 [00:05<00:25, 379.75it/s]\u001b[A\n"," 20% 2354/11873 [00:05<00:24, 382.26it/s]\u001b[A\n"," 20% 2393/11873 [00:05<00:24, 382.16it/s]\u001b[A\n"," 20% 2432/11873 [00:05<00:24, 379.77it/s]\u001b[A\n"," 21% 2471/11873 [00:06<00:24, 380.00it/s]\u001b[A\n"," 21% 2510/11873 [00:06<00:24, 378.84it/s]\u001b[A\n"," 21% 2549/11873 [00:06<00:24, 379.38it/s]\u001b[A\n"," 22% 2587/11873 [00:06<00:24, 377.30it/s]\u001b[A\n"," 22% 2625/11873 [00:06<00:24, 375.95it/s]\u001b[A\n"," 22% 2663/11873 [00:06<00:24, 375.99it/s]\u001b[A\n"," 23% 2702/11873 [00:06<00:24, 379.43it/s]\u001b[A\n"," 23% 2740/11873 [00:06<00:24, 375.73it/s]\u001b[A\n"," 23% 2778/11873 [00:06<00:24, 375.19it/s]\u001b[A\n"," 24% 2817/11873 [00:06<00:23, 377.46it/s]\u001b[A\n"," 24% 2855/11873 [00:07<00:23, 376.94it/s]\u001b[A\n"," 24% 2893/11873 [00:07<00:23, 374.97it/s]\u001b[A\n"," 25% 2931/11873 [00:07<00:23, 375.75it/s]\u001b[A\n"," 25% 2970/11873 [00:07<00:23, 379.51it/s]\u001b[A\n"," 25% 3008/11873 [00:07<00:26, 339.65it/s]\u001b[A\n"," 26% 3045/11873 [00:07<00:25, 347.88it/s]\u001b[A\n"," 26% 3081/11873 [00:07<00:25, 351.02it/s]\u001b[A\n"," 26% 3117/11873 [00:07<00:28, 310.16it/s]\u001b[A\n"," 27% 3150/11873 [00:07<00:30, 286.61it/s]\u001b[A\n"," 27% 3180/11873 [00:08<00:30, 282.54it/s]\u001b[A\n"," 27% 3218/11873 [00:08<00:28, 306.76it/s]\u001b[A\n"," 27% 3256/11873 [00:08<00:26, 324.79it/s]\u001b[A\n"," 28% 3290/11873 [00:08<00:33, 257.85it/s]\u001b[A\n"," 28% 3319/11873 [00:08<00:38, 221.58it/s]\u001b[A\n"," 28% 3344/11873 [00:08<00:37, 226.53it/s]\u001b[A\n"," 28% 3369/11873 [00:08<00:41, 204.38it/s]\u001b[A\n"," 29% 3406/11873 [00:09<00:34, 242.45it/s]\u001b[A\n"," 29% 3445/11873 [00:09<00:30, 277.43it/s]\u001b[A\n"," 29% 3484/11873 [00:09<00:27, 305.09it/s]\u001b[A\n"," 30% 3522/11873 [00:09<00:25, 325.32it/s]\u001b[A\n"," 30% 3561/11873 [00:09<00:24, 341.89it/s]\u001b[A\n"," 30% 3600/11873 [00:09<00:23, 355.15it/s]\u001b[A\n"," 31% 3640/11873 [00:09<00:22, 365.30it/s]\u001b[A\n"," 31% 3678/11873 [00:09<00:22, 365.91it/s]\u001b[A\n"," 31% 3716/11873 [00:09<00:22, 364.22it/s]\u001b[A\n"," 32% 3754/11873 [00:09<00:22, 366.24it/s]\u001b[A\n"," 32% 3792/11873 [00:10<00:21, 368.42it/s]\u001b[A\n"," 32% 3830/11873 [00:10<00:23, 345.06it/s]\u001b[A\n"," 33% 3868/11873 [00:10<00:22, 354.65it/s]\u001b[A\n"," 33% 3905/11873 [00:10<00:22, 357.64it/s]\u001b[A\n"," 33% 3942/11873 [00:10<00:23, 335.57it/s]\u001b[A\n"," 33% 3976/11873 [00:10<00:23, 330.60it/s]\u001b[A\n"," 34% 4016/11873 [00:10<00:22, 347.62it/s]\u001b[A\n"," 34% 4055/11873 [00:10<00:21, 358.59it/s]\u001b[A\n"," 34% 4094/11873 [00:10<00:21, 366.65it/s]\u001b[A\n"," 35% 4132/11873 [00:11<00:20, 368.92it/s]\u001b[A\n"," 35% 4170/11873 [00:11<00:22, 343.99it/s]\u001b[A\n"," 35% 4207/11873 [00:11<00:21, 350.80it/s]\u001b[A\n"," 36% 4246/11873 [00:11<00:21, 361.38it/s]\u001b[A\n"," 36% 4285/11873 [00:11<00:20, 368.01it/s]\u001b[A\n"," 36% 4325/11873 [00:11<00:20, 375.94it/s]\u001b[A\n"," 37% 4365/11873 [00:11<00:19, 380.78it/s]\u001b[A\n"," 37% 4404/11873 [00:11<00:19, 379.88it/s]\u001b[A\n"," 37% 4443/11873 [00:11<00:23, 309.69it/s]\u001b[A\n"," 38% 4481/11873 [00:12<00:22, 325.54it/s]\u001b[A\n"," 38% 4518/11873 [00:12<00:21, 337.03it/s]\u001b[A\n"," 38% 4556/11873 [00:12<00:20, 348.59it/s]\u001b[A\n"," 39% 4595/11873 [00:12<00:20, 359.52it/s]\u001b[A\n"," 39% 4632/11873 [00:12<00:20, 359.61it/s]\u001b[A\n"," 39% 4671/11873 [00:12<00:19, 366.42it/s]\u001b[A\n"," 40% 4710/11873 [00:12<00:19, 370.41it/s]\u001b[A\n"," 40% 4748/11873 [00:12<00:19, 371.59it/s]\u001b[A\n"," 40% 4786/11873 [00:12<00:19, 369.79it/s]\u001b[A\n"," 41% 4824/11873 [00:12<00:19, 362.40it/s]\u001b[A\n"," 41% 4861/11873 [00:13<00:19, 363.51it/s]\u001b[A\n"," 41% 4898/11873 [00:13<00:19, 359.72it/s]\u001b[A\n"," 42% 4936/11873 [00:13<00:19, 364.29it/s]\u001b[A\n"," 42% 4975/11873 [00:13<00:18, 369.92it/s]\u001b[A\n"," 42% 5013/11873 [00:13<00:18, 369.67it/s]\u001b[A\n"," 43% 5051/11873 [00:13<00:18, 368.19it/s]\u001b[A\n"," 43% 5090/11873 [00:13<00:18, 374.36it/s]\u001b[A\n"," 43% 5129/11873 [00:13<00:17, 378.27it/s]\u001b[A\n"," 44% 5167/11873 [00:13<00:17, 375.33it/s]\u001b[A\n"," 44% 5205/11873 [00:13<00:17, 374.43it/s]\u001b[A\n"," 44% 5243/11873 [00:14<00:17, 372.81it/s]\u001b[A\n"," 44% 5281/11873 [00:14<00:19, 345.38it/s]\u001b[A\n"," 45% 5320/11873 [00:14<00:18, 354.97it/s]\u001b[A\n"," 45% 5359/11873 [00:14<00:17, 362.84it/s]\u001b[A\n"," 45% 5399/11873 [00:14<00:17, 370.98it/s]\u001b[A\n"," 46% 5438/11873 [00:14<00:17, 373.99it/s]\u001b[A\n"," 46% 5476/11873 [00:14<00:17, 373.39it/s]\u001b[A\n"," 46% 5515/11873 [00:14<00:16, 377.93it/s]\u001b[A\n"," 47% 5553/11873 [00:14<00:16, 377.82it/s]\u001b[A\n"," 47% 5591/11873 [00:15<00:16, 377.88it/s]\u001b[A\n"," 47% 5629/11873 [00:15<00:16, 376.18it/s]\u001b[A\n"," 48% 5668/11873 [00:15<00:16, 377.79it/s]\u001b[A\n"," 48% 5706/11873 [00:15<00:16, 375.99it/s]\u001b[A\n"," 48% 5745/11873 [00:15<00:16, 377.23it/s]\u001b[A\n"," 49% 5783/11873 [00:15<00:16, 377.34it/s]\u001b[A\n"," 49% 5823/11873 [00:15<00:15, 381.47it/s]\u001b[A\n"," 49% 5862/11873 [00:15<00:15, 380.64it/s]\u001b[A\n"," 50% 5901/11873 [00:15<00:15, 380.32it/s]\u001b[A\n"," 50% 5940/11873 [00:15<00:15, 380.23it/s]\u001b[A\n"," 50% 5979/11873 [00:16<00:15, 381.75it/s]\u001b[A\n"," 51% 6018/11873 [00:16<00:15, 382.77it/s]\u001b[A\n"," 51% 6057/11873 [00:16<00:15, 382.88it/s]\u001b[A\n"," 51% 6096/11873 [00:16<00:15, 375.51it/s]\u001b[A\n"," 52% 6135/11873 [00:16<00:15, 377.09it/s]\u001b[A\n"," 52% 6173/11873 [00:16<00:15, 377.13it/s]\u001b[A\n"," 52% 6211/11873 [00:16<00:15, 374.44it/s]\u001b[A\n"," 53% 6250/11873 [00:16<00:14, 378.38it/s]\u001b[A\n"," 53% 6289/11873 [00:16<00:14, 380.31it/s]\u001b[A\n"," 53% 6328/11873 [00:16<00:14, 381.71it/s]\u001b[A\n"," 54% 6367/11873 [00:17<00:14, 382.82it/s]\u001b[A\n"," 54% 6406/11873 [00:17<00:14, 381.15it/s]\u001b[A\n"," 54% 6445/11873 [00:17<00:14, 377.41it/s]\u001b[A\n"," 55% 6484/11873 [00:17<00:14, 378.29it/s]\u001b[A\n"," 55% 6522/11873 [00:17<00:14, 377.60it/s]\u001b[A\n"," 55% 6560/11873 [00:17<00:14, 375.22it/s]\u001b[A\n"," 56% 6598/11873 [00:17<00:14, 374.93it/s]\u001b[A\n"," 56% 6636/11873 [00:17<00:14, 372.45it/s]\u001b[A\n"," 56% 6675/11873 [00:17<00:13, 375.63it/s]\u001b[A\n"," 57% 6713/11873 [00:18<00:15, 342.33it/s]\u001b[A\n"," 57% 6750/11873 [00:18<00:14, 348.19it/s]\u001b[A\n"," 57% 6788/11873 [00:18<00:14, 355.96it/s]\u001b[A\n"," 58% 6827/11873 [00:18<00:13, 363.97it/s]\u001b[A\n"," 58% 6866/11873 [00:18<00:13, 369.06it/s]\u001b[A\n"," 58% 6904/11873 [00:18<00:13, 371.10it/s]\u001b[A\n"," 58% 6942/11873 [00:18<00:13, 368.76it/s]\u001b[A\n"," 59% 6979/11873 [00:18<00:13, 353.69it/s]\u001b[A\n"," 59% 7015/11873 [00:18<00:14, 341.59it/s]\u001b[A\n"," 59% 7051/11873 [00:18<00:13, 345.86it/s]\u001b[A\n"," 60% 7090/11873 [00:19<00:13, 357.32it/s]\u001b[A\n"," 60% 7129/11873 [00:19<00:12, 365.48it/s]\u001b[A\n"," 60% 7168/11873 [00:19<00:12, 371.18it/s]\u001b[A\n"," 61% 7206/11873 [00:19<00:12, 369.00it/s]\u001b[A\n"," 61% 7243/11873 [00:19<00:12, 368.20it/s]\u001b[A\n"," 61% 7280/11873 [00:19<00:12, 366.32it/s]\u001b[A\n"," 62% 7317/11873 [00:19<00:12, 366.92it/s]\u001b[A\n"," 62% 7354/11873 [00:19<00:12, 367.22it/s]\u001b[A\n"," 62% 7391/11873 [00:19<00:12, 367.53it/s]\u001b[A\n"," 63% 7428/11873 [00:20<00:12, 350.54it/s]\u001b[A\n"," 63% 7467/11873 [00:20<00:12, 359.38it/s]\u001b[A\n"," 63% 7506/11873 [00:20<00:11, 366.50it/s]\u001b[A\n"," 64% 7545/11873 [00:20<00:11, 372.84it/s]\u001b[A\n"," 64% 7583/11873 [00:20<00:11, 374.58it/s]\u001b[A\n"," 64% 7621/11873 [00:20<00:11, 372.53it/s]\u001b[A\n"," 65% 7659/11873 [00:20<00:11, 372.74it/s]\u001b[A\n"," 65% 7697/11873 [00:20<00:11, 371.70it/s]\u001b[A\n"," 65% 7735/11873 [00:20<00:11, 349.30it/s]\u001b[A\n"," 65% 7771/11873 [00:20<00:11, 351.87it/s]\u001b[A\n"," 66% 7809/11873 [00:21<00:11, 357.49it/s]\u001b[A\n"," 66% 7846/11873 [00:21<00:11, 358.79it/s]\u001b[A\n"," 66% 7882/11873 [00:21<00:11, 338.97it/s]\u001b[A\n"," 67% 7920/11873 [00:21<00:11, 349.52it/s]\u001b[A\n"," 67% 7958/11873 [00:21<00:10, 356.65it/s]\u001b[A\n"," 67% 7995/11873 [00:21<00:10, 360.16it/s]\u001b[A\n"," 68% 8034/11873 [00:21<00:10, 366.90it/s]\u001b[A\n"," 68% 8073/11873 [00:21<00:10, 371.24it/s]\u001b[A\n"," 68% 8111/11873 [00:21<00:10, 371.52it/s]\u001b[A\n"," 69% 8149/11873 [00:21<00:09, 372.50it/s]\u001b[A\n"," 69% 8187/11873 [00:22<00:09, 371.15it/s]\u001b[A\n"," 69% 8225/11873 [00:22<00:09, 370.72it/s]\u001b[A\n"," 70% 8263/11873 [00:22<00:09, 372.88it/s]\u001b[A\n"," 70% 8302/11873 [00:22<00:09, 376.83it/s]\u001b[A\n"," 70% 8341/11873 [00:22<00:09, 378.20it/s]\u001b[A\n"," 71% 8380/11873 [00:22<00:09, 378.99it/s]\u001b[A\n"," 71% 8418/11873 [00:22<00:09, 375.88it/s]\u001b[A\n"," 71% 8456/11873 [00:22<00:09, 372.36it/s]\u001b[A\n"," 72% 8494/11873 [00:22<00:09, 373.76it/s]\u001b[A\n"," 72% 8532/11873 [00:23<00:08, 374.92it/s]\u001b[A\n"," 72% 8570/11873 [00:23<00:08, 372.91it/s]\u001b[A\n"," 73% 8608/11873 [00:23<00:08, 370.21it/s]\u001b[A\n"," 73% 8646/11873 [00:23<00:08, 363.24it/s]\u001b[A\n"," 73% 8684/11873 [00:23<00:08, 366.11it/s]\u001b[A\n"," 73% 8721/11873 [00:23<00:08, 366.20it/s]\u001b[A\n"," 74% 8758/11873 [00:23<00:08, 364.26it/s]\u001b[A\n"," 74% 8795/11873 [00:23<00:08, 362.50it/s]\u001b[A\n"," 74% 8832/11873 [00:23<00:08, 358.23it/s]\u001b[A\n"," 75% 8869/11873 [00:23<00:08, 361.01it/s]\u001b[A\n"," 75% 8907/11873 [00:24<00:08, 366.41it/s]\u001b[A\n"," 75% 8945/11873 [00:24<00:07, 368.55it/s]\u001b[A\n"," 76% 8984/11873 [00:24<00:07, 372.18it/s]\u001b[A\n"," 76% 9023/11873 [00:24<00:07, 374.78it/s]\u001b[A\n"," 76% 9061/11873 [00:24<00:07, 372.44it/s]\u001b[A\n"," 77% 9099/11873 [00:24<00:07, 369.34it/s]\u001b[A\n"," 77% 9137/11873 [00:24<00:07, 370.30it/s]\u001b[A\n"," 77% 9175/11873 [00:24<00:07, 370.62it/s]\u001b[A\n"," 78% 9213/11873 [00:24<00:07, 371.73it/s]\u001b[A\n"," 78% 9251/11873 [00:24<00:07, 370.69it/s]\u001b[A\n"," 78% 9289/11873 [00:25<00:06, 369.31it/s]\u001b[A\n"," 79% 9327/11873 [00:25<00:06, 372.17it/s]\u001b[A\n"," 79% 9366/11873 [00:25<00:06, 374.88it/s]\u001b[A\n"," 79% 9404/11873 [00:25<00:06, 375.94it/s]\u001b[A\n"," 80% 9442/11873 [00:25<00:06, 374.85it/s]\u001b[A\n"," 80% 9480/11873 [00:25<00:06, 369.66it/s]\u001b[A\n"," 80% 9518/11873 [00:25<00:06, 370.19it/s]\u001b[A\n"," 80% 9557/11873 [00:25<00:06, 375.74it/s]\u001b[A\n"," 81% 9595/11873 [00:25<00:06, 376.87it/s]\u001b[A\n"," 81% 9633/11873 [00:25<00:05, 377.66it/s]\u001b[A\n"," 81% 9671/11873 [00:26<00:05, 378.24it/s]\u001b[A\n"," 82% 9711/11873 [00:26<00:05, 381.66it/s]\u001b[A\n"," 82% 9750/11873 [00:26<00:05, 383.40it/s]\u001b[A\n"," 82% 9789/11873 [00:26<00:05, 382.02it/s]\u001b[A\n"," 83% 9828/11873 [00:26<00:05, 377.52it/s]\u001b[A\n"," 83% 9867/11873 [00:26<00:05, 379.08it/s]\u001b[A\n"," 83% 9905/11873 [00:26<00:05, 379.03it/s]\u001b[A\n"," 84% 9943/11873 [00:26<00:05, 378.39it/s]\u001b[A\n"," 84% 9982/11873 [00:26<00:04, 379.03it/s]\u001b[A\n"," 84% 10021/11873 [00:26<00:04, 379.48it/s]\u001b[A\n"," 85% 10059/11873 [00:27<00:04, 379.01it/s]\u001b[A\n"," 85% 10098/11873 [00:27<00:04, 380.76it/s]\u001b[A\n"," 85% 10137/11873 [00:27<00:04, 370.50it/s]\u001b[A\n"," 86% 10175/11873 [00:27<00:04, 363.75it/s]\u001b[A\n"," 86% 10212/11873 [00:27<00:04, 360.25it/s]\u001b[A\n"," 86% 10249/11873 [00:27<00:04, 359.84it/s]\u001b[A\n"," 87% 10286/11873 [00:27<00:04, 355.75it/s]\u001b[A\n"," 87% 10324/11873 [00:27<00:04, 361.30it/s]\u001b[A\n"," 87% 10362/11873 [00:27<00:04, 365.12it/s]\u001b[A\n"," 88% 10400/11873 [00:28<00:03, 369.38it/s]\u001b[A\n"," 88% 10437/11873 [00:28<00:04, 348.92it/s]\u001b[A\n"," 88% 10475/11873 [00:28<00:03, 355.79it/s]\u001b[A\n"," 89% 10513/11873 [00:28<00:03, 361.03it/s]\u001b[A\n"," 89% 10550/11873 [00:28<00:03, 346.81it/s]\u001b[A\n"," 89% 10585/11873 [00:28<00:03, 347.36it/s]\u001b[A\n"," 89% 10621/11873 [00:28<00:03, 350.55it/s]\u001b[A\n"," 90% 10657/11873 [00:28<00:03, 347.08it/s]\u001b[A\n"," 90% 10694/11873 [00:28<00:03, 352.65it/s]\u001b[A\n"," 90% 10732/11873 [00:28<00:03, 359.79it/s]\u001b[A\n"," 91% 10769/11873 [00:29<00:03, 353.27it/s]\u001b[A\n"," 91% 10805/11873 [00:29<00:03, 352.88it/s]\u001b[A\n"," 91% 10841/11873 [00:29<00:03, 326.81it/s]\u001b[A\n"," 92% 10876/11873 [00:29<00:02, 332.84it/s]\u001b[A\n"," 92% 10912/11873 [00:29<00:02, 339.24it/s]\u001b[A\n"," 92% 10947/11873 [00:29<00:02, 337.16it/s]\u001b[A\n"," 93% 10984/11873 [00:29<00:02, 344.17it/s]\u001b[A\n"," 93% 11021/11873 [00:29<00:02, 350.70it/s]\u001b[A\n"," 93% 11058/11873 [00:29<00:02, 356.11it/s]\u001b[A\n"," 93% 11094/11873 [00:30<00:02, 353.97it/s]\u001b[A\n"," 94% 11130/11873 [00:30<00:02, 354.28it/s]\u001b[A\n"," 94% 11169/11873 [00:30<00:01, 364.06it/s]\u001b[A\n"," 94% 11208/11873 [00:30<00:01, 369.76it/s]\u001b[A\n"," 95% 11247/11873 [00:30<00:01, 373.26it/s]\u001b[A\n"," 95% 11286/11873 [00:30<00:01, 375.57it/s]\u001b[A\n"," 95% 11324/11873 [00:30<00:01, 376.48it/s]\u001b[A\n"," 96% 11363/11873 [00:30<00:01, 378.48it/s]\u001b[A\n"," 96% 11401/11873 [00:30<00:01, 378.70it/s]\u001b[A\n"," 96% 11440/11873 [00:30<00:01, 379.23it/s]\u001b[A\n"," 97% 11478/11873 [00:31<00:01, 377.79it/s]\u001b[A\n"," 97% 11516/11873 [00:31<00:00, 369.48it/s]\u001b[A\n"," 97% 11554/11873 [00:31<00:00, 369.88it/s]\u001b[A\n"," 98% 11593/11873 [00:31<00:00, 374.22it/s]\u001b[A\n"," 98% 11631/11873 [00:31<00:00, 373.86it/s]\u001b[A\n"," 98% 11669/11873 [00:31<00:00, 369.19it/s]\u001b[A\n"," 99% 11706/11873 [00:31<00:00, 361.25it/s]\u001b[A\n"," 99% 11743/11873 [00:31<00:00, 358.97it/s]\u001b[A\n"," 99% 11780/11873 [00:31<00:00, 361.85it/s]\u001b[A\n","100% 11818/11873 [00:31<00:00, 364.77it/s]\u001b[A\n","100% 11873/11873 [00:32<00:00, 369.49it/s]\n","03/26/2022 22:25:55 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","03/26/2022 22:25:55 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","03/26/2022 22:25:57 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","03/26/2022 22:26:00 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1525/1525 [03:45<00:00,  6.75it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 71.5756\n","  eval_HasAns_f1         =  77.869\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 67.6703\n","  eval_NoAns_f1          = 67.6703\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 69.6201\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 72.7623\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 69.6201\n","  eval_f1                = 72.7623\n","  eval_samples           =   12199\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-03-26 22:26:00,632 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path bert-base-cased \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 24 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BA5uvVZRuQVx","executionInfo":{"status":"ok","timestamp":1648344980105,"user_tz":240,"elapsed":9724394,"user":{"displayName":"SICHEN ZHONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGziZ-Uu-PyjGuHsDy1aSyMZvJYEN6bO8qLBQyKw=s64","userId":"08427994781088390833"}},"outputId":"7c86442b-d71e-4254-f8a4-27a8cff4c592"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["03/26/2022 22:45:31 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","03/26/2022 22:45:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Mar26_22-45-31_56544ad519f9,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=24,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","03/26/2022 22:45:31 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","03/26/2022 22:45:31 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/26/2022 22:45:31 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","03/26/2022 22:45:31 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/26/2022 22:45:31 - WARNING - datasets.builder - Reusing dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","03/26/2022 22:45:31 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","100% 2/2 [00:00<00:00, 606.07it/s]\n","[INFO|configuration_utils.py:653] 2022-03-26 22:45:32,079 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-26 22:45:32,080 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:653] 2022-03-26 22:45:32,352 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-26 22:45:32,352 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1777] 2022-03-26 22:45:33,159 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1777] 2022-03-26 22:45:33,159 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1777] 2022-03-26 22:45:33,159 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1777] 2022-03-26 22:45:33,159 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1777] 2022-03-26 22:45:33,159 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:653] 2022-03-26 22:45:33,289 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-26 22:45:33,289 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:1771] 2022-03-26 22:45:33,468 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:2049] 2022-03-26 22:45:34,830 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-03-26 22:45:34,830 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/26/2022 22:45:34 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-538cb096a17d202c.arrow\n","03/26/2022 22:45:34 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-f1f6a455c564ba86.arrow\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1288] 2022-03-26 22:45:38,714 >> ***** Running training *****\n","[INFO|trainer.py:1289] 2022-03-26 22:45:38,714 >>   Num examples = 132079\n","[INFO|trainer.py:1290] 2022-03-26 22:45:38,714 >>   Num Epochs = 2\n","[INFO|trainer.py:1291] 2022-03-26 22:45:38,714 >>   Instantaneous batch size per device = 24\n","[INFO|trainer.py:1292] 2022-03-26 22:45:38,714 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n","[INFO|trainer.py:1293] 2022-03-26 22:45:38,714 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1294] 2022-03-26 22:45:38,714 >>   Total optimization steps = 11008\n","{'loss': 2.012, 'learning_rate': 3.8183139534883726e-05, 'epoch': 0.09}\n","  5% 500/11008 [07:32<2:38:13,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 22:53:10,811 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:440] 2022-03-26 22:53:10,812 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 22:53:11,471 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 22:53:11,471 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 22:53:11,472 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.4501, 'learning_rate': 3.636627906976745e-05, 'epoch': 0.18}\n","  9% 1000/11008 [15:06<2:31:14,  1.10it/s][INFO|trainer.py:2162] 2022-03-26 23:00:45,517 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:440] 2022-03-26 23:00:45,518 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 23:00:46,158 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 23:00:46,159 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 23:00:46,159 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.3336, 'learning_rate': 3.4549418604651163e-05, 'epoch': 0.27}\n"," 14% 1500/11008 [22:41<2:23:20,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 23:08:20,294 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:440] 2022-03-26 23:08:20,295 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 23:08:20,945 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 23:08:20,946 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 23:08:20,946 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.2605, 'learning_rate': 3.2732558139534886e-05, 'epoch': 0.36}\n"," 18% 2000/11008 [30:15<2:15:40,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 23:15:54,711 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:440] 2022-03-26 23:15:54,712 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 23:15:55,345 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 23:15:55,346 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 23:15:55,346 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.1973, 'learning_rate': 3.091569767441861e-05, 'epoch': 0.45}\n"," 23% 2500/11008 [37:50<2:08:10,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 23:23:29,181 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:440] 2022-03-26 23:23:29,182 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 23:23:29,809 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 23:23:29,810 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 23:23:29,810 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.1501, 'learning_rate': 2.9098837209302327e-05, 'epoch': 0.55}\n"," 27% 3000/11008 [45:25<2:00:38,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 23:31:03,863 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:440] 2022-03-26 23:31:03,864 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 23:31:04,525 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 23:31:04,526 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 23:31:04,526 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.1173, 'learning_rate': 2.728197674418605e-05, 'epoch': 0.64}\n"," 32% 3500/11008 [53:00<1:53:28,  1.10it/s][INFO|trainer.py:2162] 2022-03-26 23:38:38,898 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:440] 2022-03-26 23:38:38,898 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 23:38:39,548 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 23:38:39,548 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 23:38:39,549 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.0504, 'learning_rate': 2.546511627906977e-05, 'epoch': 0.73}\n"," 36% 4000/11008 [1:00:35<1:45:42,  1.10it/s][INFO|trainer.py:2162] 2022-03-26 23:46:13,857 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:440] 2022-03-26 23:46:13,858 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 23:46:14,528 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 23:46:14,529 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 23:46:14,529 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.0534, 'learning_rate': 2.364825581395349e-05, 'epoch': 0.82}\n"," 41% 4500/11008 [1:08:10<1:38:04,  1.11it/s][INFO|trainer.py:2162] 2022-03-26 23:53:48,867 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:440] 2022-03-26 23:53:48,868 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-26 23:53:49,536 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-26 23:53:49,537 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-26 23:53:49,537 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.0329, 'learning_rate': 2.1831395348837213e-05, 'epoch': 0.91}\n"," 45% 5000/11008 [1:15:45<1:30:43,  1.10it/s][INFO|trainer.py:2162] 2022-03-27 00:01:23,865 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:440] 2022-03-27 00:01:23,866 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 00:01:24,538 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 00:01:24,539 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 00:01:24,539 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.002, 'learning_rate': 2.0014534883720932e-05, 'epoch': 1.0}\n"," 50% 5500/11008 [1:23:19<1:23:08,  1.10it/s][INFO|trainer.py:2162] 2022-03-27 00:08:58,709 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:440] 2022-03-27 00:08:58,710 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 00:08:59,381 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 00:08:59,382 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 00:08:59,382 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 0.7363, 'learning_rate': 1.819767441860465e-05, 'epoch': 1.09}\n"," 55% 6000/11008 [1:30:54<1:15:34,  1.10it/s][INFO|trainer.py:2162] 2022-03-27 00:16:33,292 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:440] 2022-03-27 00:16:33,293 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 00:16:33,958 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 00:16:33,958 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 00:16:33,958 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 0.721, 'learning_rate': 1.6380813953488373e-05, 'epoch': 1.18}\n"," 59% 6500/11008 [1:38:29<1:07:53,  1.11it/s][INFO|trainer.py:2162] 2022-03-27 00:24:08,123 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:440] 2022-03-27 00:24:08,124 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 00:24:08,796 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 00:24:08,797 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 00:24:08,797 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 0.697, 'learning_rate': 1.4563953488372094e-05, 'epoch': 1.27}\n"," 64% 7000/11008 [1:46:03<1:00:23,  1.11it/s][INFO|trainer.py:2162] 2022-03-27 00:31:42,664 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:440] 2022-03-27 00:31:42,665 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 00:31:43,332 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 00:31:43,332 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 00:31:43,332 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 0.7065, 'learning_rate': 1.2747093023255814e-05, 'epoch': 1.36}\n"," 68% 7500/11008 [1:53:38<52:55,  1.10it/s][INFO|trainer.py:2162] 2022-03-27 00:39:17,237 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:440] 2022-03-27 00:39:17,238 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 00:39:17,893 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 00:39:17,893 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 00:39:17,893 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.7041, 'learning_rate': 1.0930232558139535e-05, 'epoch': 1.45}\n"," 73% 8000/11008 [2:01:13<45:21,  1.11it/s][INFO|trainer.py:2162] 2022-03-27 00:46:51,870 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:440] 2022-03-27 00:46:51,871 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 00:46:52,530 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 00:46:52,531 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 00:46:52,531 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.6924, 'learning_rate': 9.113372093023255e-06, 'epoch': 1.54}\n"," 77% 8500/11008 [2:08:47<37:48,  1.11it/s][INFO|trainer.py:2162] 2022-03-27 00:54:26,522 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:440] 2022-03-27 00:54:26,522 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 00:54:27,195 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 00:54:27,196 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 00:54:27,196 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.6771, 'learning_rate': 7.296511627906977e-06, 'epoch': 1.64}\n"," 82% 9000/11008 [2:16:22<30:16,  1.11it/s][INFO|trainer.py:2162] 2022-03-27 01:02:01,233 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:440] 2022-03-27 01:02:01,234 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 01:02:01,909 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 01:02:01,909 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 01:02:01,910 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.7038, 'learning_rate': 5.4796511627906975e-06, 'epoch': 1.73}\n"," 86% 9500/11008 [2:23:56<22:45,  1.10it/s][INFO|trainer.py:2162] 2022-03-27 01:09:35,671 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:440] 2022-03-27 01:09:35,672 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 01:09:36,339 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 01:09:36,339 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 01:09:36,340 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.6637, 'learning_rate': 3.6627906976744186e-06, 'epoch': 1.82}\n"," 91% 10000/11008 [2:31:31<15:12,  1.10it/s][INFO|trainer.py:2162] 2022-03-27 01:17:10,289 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:440] 2022-03-27 01:17:10,290 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 01:17:10,970 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 01:17:10,970 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 01:17:10,971 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6711, 'learning_rate': 1.8459302325581396e-06, 'epoch': 1.91}\n"," 95% 10500/11008 [2:39:06<07:40,  1.10it/s][INFO|trainer.py:2162] 2022-03-27 01:24:44,870 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:440] 2022-03-27 01:24:44,871 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 01:24:45,533 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 01:24:45,534 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 01:24:45,534 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.6654, 'learning_rate': 2.9069767441860468e-08, 'epoch': 2.0}\n","100% 11000/11008 [2:46:42<00:07,  1.11it/s][INFO|trainer.py:2162] 2022-03-27 01:32:21,574 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11000\n","[INFO|configuration_utils.py:440] 2022-03-27 01:32:21,575 >> Configuration saved in /tmp/debug_squad/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 01:32:22,240 >> Model weights saved in /tmp/debug_squad/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 01:32:22,240 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 01:32:22,240 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11000/special_tokens_map.json\n","100% 11008/11008 [2:46:51<00:00,  1.28it/s][INFO|trainer.py:1526] 2022-03-27 01:32:30,479 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10011.7672, 'train_samples_per_second': 26.385, 'train_steps_per_second': 1.1, 'train_loss': 0.9678376120363557, 'epoch': 2.0}\n","100% 11008/11008 [2:46:51<00:00,  1.10it/s]\n","[INFO|trainer.py:2162] 2022-03-27 01:32:30,483 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:440] 2022-03-27 01:32:30,484 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 01:32:31,150 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 01:32:31,151 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 01:32:31,151 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.9678\n","  train_runtime            = 2:46:51.76\n","  train_samples            =     132079\n","  train_samples_per_second =     26.385\n","  train_steps_per_second   =        1.1\n","03/27/2022 01:32:31 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:571] 2022-03-27 01:32:31,189 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2412] 2022-03-27 01:32:31,191 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2414] 2022-03-27 01:32:31,191 >>   Num examples = 12199\n","[INFO|trainer.py:2417] 2022-03-27 01:32:31,191 >>   Batch size = 8\n","100% 1525/1525 [02:57<00:00,  8.78it/s]03/27/2022 01:35:40 - INFO - utils_qa - Post-processing 11873 example predictions split into 12199 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 45/11873 [00:00<00:26, 447.18it/s]\u001b[A\n","  1% 90/11873 [00:00<00:28, 419.30it/s]\u001b[A\n","  1% 137/11873 [00:00<00:26, 439.00it/s]\u001b[A\n","  2% 187/11873 [00:00<00:25, 459.60it/s]\u001b[A\n","  2% 237/11873 [00:00<00:24, 469.85it/s]\u001b[A\n","  2% 287/11873 [00:00<00:24, 479.28it/s]\u001b[A\n","  3% 335/11873 [00:00<00:24, 473.42it/s]\u001b[A\n","  3% 384/11873 [00:00<00:24, 477.57it/s]\u001b[A\n","  4% 434/11873 [00:00<00:23, 483.16it/s]\u001b[A\n","  4% 484/11873 [00:01<00:23, 486.55it/s]\u001b[A\n","  4% 533/11873 [00:01<00:23, 478.76it/s]\u001b[A\n","  5% 581/11873 [00:01<00:23, 473.22it/s]\u001b[A\n","  5% 633/11873 [00:01<00:23, 484.98it/s]\u001b[A\n","  6% 683/11873 [00:01<00:22, 487.16it/s]\u001b[A\n","  6% 732/11873 [00:01<00:23, 475.06it/s]\u001b[A\n","  7% 780/11873 [00:01<00:23, 470.08it/s]\u001b[A\n","  7% 828/11873 [00:01<00:23, 472.44it/s]\u001b[A\n","  7% 878/11873 [00:01<00:22, 480.37it/s]\u001b[A\n","  8% 928/11873 [00:01<00:22, 483.57it/s]\u001b[A\n","  8% 979/11873 [00:02<00:22, 489.67it/s]\u001b[A\n","  9% 1028/11873 [00:02<00:23, 465.58it/s]\u001b[A\n","  9% 1075/11873 [00:02<00:24, 444.49it/s]\u001b[A\n","  9% 1120/11873 [00:02<00:25, 425.85it/s]\u001b[A\n"," 10% 1163/11873 [00:02<00:25, 416.61it/s]\u001b[A\n"," 10% 1205/11873 [00:02<00:25, 410.39it/s]\u001b[A\n"," 11% 1247/11873 [00:02<00:26, 402.54it/s]\u001b[A\n"," 11% 1288/11873 [00:02<00:26, 398.44it/s]\u001b[A\n"," 11% 1328/11873 [00:02<00:26, 392.90it/s]\u001b[A\n"," 12% 1368/11873 [00:03<00:27, 388.56it/s]\u001b[A\n"," 12% 1407/11873 [00:03<00:26, 388.77it/s]\u001b[A\n"," 12% 1447/11873 [00:03<00:26, 389.28it/s]\u001b[A\n"," 13% 1486/11873 [00:03<00:26, 386.43it/s]\u001b[A\n"," 13% 1525/11873 [00:03<00:27, 382.90it/s]\u001b[A\n"," 13% 1564/11873 [00:03<00:27, 380.60it/s]\u001b[A\n"," 14% 1603/11873 [00:03<00:26, 382.28it/s]\u001b[A\n"," 14% 1642/11873 [00:03<00:26, 381.02it/s]\u001b[A\n"," 14% 1681/11873 [00:03<00:27, 377.41it/s]\u001b[A\n"," 14% 1719/11873 [00:03<00:27, 372.11it/s]\u001b[A\n"," 15% 1757/11873 [00:04<00:27, 373.83it/s]\u001b[A\n"," 15% 1795/11873 [00:04<00:26, 375.33it/s]\u001b[A\n"," 15% 1833/11873 [00:04<00:26, 376.26it/s]\u001b[A\n"," 16% 1872/11873 [00:04<00:26, 377.67it/s]\u001b[A\n"," 16% 1910/11873 [00:04<00:26, 373.40it/s]\u001b[A\n"," 16% 1949/11873 [00:04<00:26, 377.77it/s]\u001b[A\n"," 17% 1988/11873 [00:04<00:26, 379.43it/s]\u001b[A\n"," 17% 2027/11873 [00:04<00:25, 382.55it/s]\u001b[A\n"," 17% 2066/11873 [00:04<00:25, 381.56it/s]\u001b[A\n"," 18% 2105/11873 [00:04<00:25, 383.52it/s]\u001b[A\n"," 18% 2144/11873 [00:05<00:25, 383.65it/s]\u001b[A\n"," 18% 2183/11873 [00:05<00:25, 382.31it/s]\u001b[A\n"," 19% 2223/11873 [00:05<00:25, 384.80it/s]\u001b[A\n"," 19% 2262/11873 [00:05<00:24, 385.68it/s]\u001b[A\n"," 19% 2301/11873 [00:05<00:25, 381.84it/s]\u001b[A\n"," 20% 2340/11873 [00:05<00:24, 381.81it/s]\u001b[A\n"," 20% 2379/11873 [00:05<00:24, 382.05it/s]\u001b[A\n"," 20% 2418/11873 [00:05<00:24, 378.89it/s]\u001b[A\n"," 21% 2456/11873 [00:05<00:24, 378.23it/s]\u001b[A\n"," 21% 2494/11873 [00:06<00:24, 378.06it/s]\u001b[A\n"," 21% 2533/11873 [00:06<00:24, 379.47it/s]\u001b[A\n"," 22% 2573/11873 [00:06<00:24, 383.26it/s]\u001b[A\n"," 22% 2612/11873 [00:06<00:24, 380.33it/s]\u001b[A\n"," 22% 2651/11873 [00:06<00:24, 380.60it/s]\u001b[A\n"," 23% 2690/11873 [00:06<00:24, 382.32it/s]\u001b[A\n"," 23% 2729/11873 [00:06<00:23, 383.24it/s]\u001b[A\n"," 23% 2768/11873 [00:06<00:23, 380.17it/s]\u001b[A\n"," 24% 2807/11873 [00:06<00:23, 381.21it/s]\u001b[A\n"," 24% 2846/11873 [00:06<00:23, 383.60it/s]\u001b[A\n"," 24% 2885/11873 [00:07<00:23, 380.98it/s]\u001b[A\n"," 25% 2924/11873 [00:07<00:23, 381.69it/s]\u001b[A\n"," 25% 2963/11873 [00:07<00:23, 382.94it/s]\u001b[A\n"," 25% 3002/11873 [00:07<00:25, 349.03it/s]\u001b[A\n"," 26% 3039/11873 [00:07<00:24, 353.70it/s]\u001b[A\n"," 26% 3076/11873 [00:07<00:24, 355.99it/s]\u001b[A\n"," 26% 3112/11873 [00:07<00:26, 331.79it/s]\u001b[A\n"," 26% 3146/11873 [00:07<00:29, 296.45it/s]\u001b[A\n"," 27% 3177/11873 [00:07<00:29, 292.24it/s]\u001b[A\n"," 27% 3215/11873 [00:08<00:27, 315.38it/s]\u001b[A\n"," 27% 3254/11873 [00:08<00:25, 333.72it/s]\u001b[A\n"," 28% 3289/11873 [00:08<00:32, 266.79it/s]\u001b[A\n"," 28% 3319/11873 [00:08<00:37, 226.22it/s]\u001b[A\n"," 28% 3345/11873 [00:08<00:36, 230.87it/s]\u001b[A\n"," 28% 3371/11873 [00:08<00:39, 213.76it/s]\u001b[A\n"," 29% 3408/11873 [00:08<00:33, 249.58it/s]\u001b[A\n"," 29% 3446/11873 [00:09<00:30, 280.39it/s]\u001b[A\n"," 29% 3484/11873 [00:09<00:27, 305.54it/s]\u001b[A\n"," 30% 3521/11873 [00:09<00:25, 322.94it/s]\u001b[A\n"," 30% 3559/11873 [00:09<00:24, 338.85it/s]\u001b[A\n"," 30% 3599/11873 [00:09<00:23, 354.22it/s]\u001b[A\n"," 31% 3638/11873 [00:09<00:22, 364.42it/s]\u001b[A\n"," 31% 3676/11873 [00:09<00:22, 364.63it/s]\u001b[A\n"," 31% 3714/11873 [00:09<00:22, 366.53it/s]\u001b[A\n"," 32% 3752/11873 [00:09<00:21, 369.67it/s]\u001b[A\n"," 32% 3790/11873 [00:09<00:21, 372.17it/s]\u001b[A\n"," 32% 3828/11873 [00:10<00:23, 346.06it/s]\u001b[A\n"," 33% 3866/11873 [00:10<00:22, 354.94it/s]\u001b[A\n"," 33% 3905/11873 [00:10<00:21, 364.04it/s]\u001b[A\n"," 33% 3942/11873 [00:10<00:23, 341.33it/s]\u001b[A\n"," 33% 3977/11873 [00:10<00:23, 335.95it/s]\u001b[A\n"," 34% 4017/11873 [00:10<00:22, 353.44it/s]\u001b[A\n"," 34% 4057/11873 [00:10<00:21, 364.17it/s]\u001b[A\n"," 35% 4097/11873 [00:10<00:20, 372.44it/s]\u001b[A\n"," 35% 4136/11873 [00:10<00:20, 376.07it/s]\u001b[A\n"," 35% 4174/11873 [00:11<00:22, 348.62it/s]\u001b[A\n"," 35% 4212/11873 [00:11<00:21, 356.25it/s]\u001b[A\n"," 36% 4252/11873 [00:11<00:20, 367.38it/s]\u001b[A\n"," 36% 4292/11873 [00:11<00:20, 374.46it/s]\u001b[A\n"," 36% 4332/11873 [00:11<00:19, 381.23it/s]\u001b[A\n"," 37% 4371/11873 [00:11<00:19, 377.79it/s]\u001b[A\n"," 37% 4409/11873 [00:11<00:19, 378.30it/s]\u001b[A\n"," 37% 4447/11873 [00:11<00:23, 310.79it/s]\u001b[A\n"," 38% 4487/11873 [00:11<00:22, 332.38it/s]\u001b[A\n"," 38% 4525/11873 [00:12<00:21, 342.78it/s]\u001b[A\n"," 38% 4564/11873 [00:12<00:20, 355.66it/s]\u001b[A\n"," 39% 4604/11873 [00:12<00:19, 367.68it/s]\u001b[A\n"," 39% 4643/11873 [00:12<00:19, 372.42it/s]\u001b[A\n"," 39% 4683/11873 [00:12<00:18, 380.37it/s]\u001b[A\n"," 40% 4722/11873 [00:12<00:18, 378.65it/s]\u001b[A\n"," 40% 4762/11873 [00:12<00:18, 382.58it/s]\u001b[A\n"," 40% 4801/11873 [00:12<00:18, 377.58it/s]\u001b[A\n"," 41% 4839/11873 [00:12<00:18, 378.19it/s]\u001b[A\n"," 41% 4878/11873 [00:12<00:18, 379.43it/s]\u001b[A\n"," 41% 4917/11873 [00:13<00:18, 375.88it/s]\u001b[A\n"," 42% 4956/11873 [00:13<00:18, 379.10it/s]\u001b[A\n"," 42% 4994/11873 [00:13<00:18, 379.34it/s]\u001b[A\n"," 42% 5032/11873 [00:13<00:18, 364.87it/s]\u001b[A\n"," 43% 5069/11873 [00:13<00:18, 363.29it/s]\u001b[A\n"," 43% 5107/11873 [00:13<00:18, 365.87it/s]\u001b[A\n"," 43% 5144/11873 [00:13<00:18, 364.12it/s]\u001b[A\n"," 44% 5181/11873 [00:13<00:18, 361.00it/s]\u001b[A\n"," 44% 5218/11873 [00:13<00:18, 359.91it/s]\u001b[A\n"," 44% 5255/11873 [00:13<00:18, 350.02it/s]\u001b[A\n"," 45% 5291/11873 [00:14<00:19, 344.49it/s]\u001b[A\n"," 45% 5331/11873 [00:14<00:18, 358.43it/s]\u001b[A\n"," 45% 5371/11873 [00:14<00:17, 367.93it/s]\u001b[A\n"," 46% 5410/11873 [00:14<00:17, 372.14it/s]\u001b[A\n"," 46% 5449/11873 [00:14<00:17, 375.39it/s]\u001b[A\n"," 46% 5488/11873 [00:14<00:16, 377.43it/s]\u001b[A\n"," 47% 5528/11873 [00:14<00:16, 383.79it/s]\u001b[A\n"," 47% 5567/11873 [00:14<00:16, 385.46it/s]\u001b[A\n"," 47% 5606/11873 [00:14<00:16, 383.38it/s]\u001b[A\n"," 48% 5645/11873 [00:14<00:16, 381.13it/s]\u001b[A\n"," 48% 5684/11873 [00:15<00:16, 378.33it/s]\u001b[A\n"," 48% 5722/11873 [00:15<00:16, 378.12it/s]\u001b[A\n"," 49% 5761/11873 [00:15<00:16, 379.12it/s]\u001b[A\n"," 49% 5800/11873 [00:15<00:15, 380.63it/s]\u001b[A\n"," 49% 5839/11873 [00:15<00:15, 383.11it/s]\u001b[A\n"," 50% 5878/11873 [00:15<00:15, 381.00it/s]\u001b[A\n"," 50% 5917/11873 [00:15<00:15, 375.68it/s]\u001b[A\n"," 50% 5956/11873 [00:15<00:15, 378.41it/s]\u001b[A\n"," 50% 5994/11873 [00:15<00:15, 376.25it/s]\u001b[A\n"," 51% 6032/11873 [00:16<00:15, 374.51it/s]\u001b[A\n"," 51% 6070/11873 [00:16<00:15, 372.68it/s]\u001b[A\n"," 51% 6108/11873 [00:16<00:15, 368.43it/s]\u001b[A\n"," 52% 6145/11873 [00:16<00:15, 364.94it/s]\u001b[A\n"," 52% 6183/11873 [00:16<00:15, 366.63it/s]\u001b[A\n"," 52% 6222/11873 [00:16<00:15, 372.58it/s]\u001b[A\n"," 53% 6262/11873 [00:16<00:14, 378.30it/s]\u001b[A\n"," 53% 6302/11873 [00:16<00:14, 382.49it/s]\u001b[A\n"," 53% 6341/11873 [00:16<00:14, 382.47it/s]\u001b[A\n"," 54% 6380/11873 [00:16<00:14, 384.38it/s]\u001b[A\n"," 54% 6419/11873 [00:17<00:14, 380.42it/s]\u001b[A\n"," 54% 6458/11873 [00:17<00:14, 381.19it/s]\u001b[A\n"," 55% 6497/11873 [00:17<00:14, 380.11it/s]\u001b[A\n"," 55% 6536/11873 [00:17<00:13, 381.99it/s]\u001b[A\n"," 55% 6575/11873 [00:17<00:13, 382.33it/s]\u001b[A\n"," 56% 6614/11873 [00:17<00:13, 382.05it/s]\u001b[A\n"," 56% 6653/11873 [00:17<00:13, 383.79it/s]\u001b[A\n"," 56% 6692/11873 [00:17<00:13, 381.62it/s]\u001b[A\n"," 57% 6731/11873 [00:17<00:14, 343.21it/s]\u001b[A\n"," 57% 6771/11873 [00:18<00:14, 357.11it/s]\u001b[A\n"," 57% 6809/11873 [00:18<00:14, 361.15it/s]\u001b[A\n"," 58% 6848/11873 [00:18<00:13, 366.78it/s]\u001b[A\n"," 58% 6887/11873 [00:18<00:13, 371.15it/s]\u001b[A\n"," 58% 6926/11873 [00:18<00:13, 375.09it/s]\u001b[A\n"," 59% 6966/11873 [00:18<00:12, 379.46it/s]\u001b[A\n"," 59% 7005/11873 [00:18<00:12, 378.75it/s]\u001b[A\n"," 59% 7043/11873 [00:18<00:12, 378.63it/s]\u001b[A\n"," 60% 7082/11873 [00:18<00:12, 379.49it/s]\u001b[A\n"," 60% 7120/11873 [00:18<00:12, 379.25it/s]\u001b[A\n"," 60% 7159/11873 [00:19<00:12, 380.02it/s]\u001b[A\n"," 61% 7198/11873 [00:19<00:12, 374.91it/s]\u001b[A\n"," 61% 7237/11873 [00:19<00:12, 376.88it/s]\u001b[A\n"," 61% 7275/11873 [00:19<00:12, 375.33it/s]\u001b[A\n"," 62% 7313/11873 [00:19<00:12, 376.37it/s]\u001b[A\n"," 62% 7351/11873 [00:19<00:11, 377.28it/s]\u001b[A\n"," 62% 7390/11873 [00:19<00:11, 380.93it/s]\u001b[A\n"," 63% 7429/11873 [00:19<00:12, 363.30it/s]\u001b[A\n"," 63% 7468/11873 [00:19<00:11, 370.91it/s]\u001b[A\n"," 63% 7507/11873 [00:19<00:11, 374.45it/s]\u001b[A\n"," 64% 7545/11873 [00:20<00:11, 375.81it/s]\u001b[A\n"," 64% 7584/11873 [00:20<00:11, 378.65it/s]\u001b[A\n"," 64% 7622/11873 [00:20<00:11, 373.17it/s]\u001b[A\n"," 65% 7660/11873 [00:20<00:11, 374.28it/s]\u001b[A\n"," 65% 7698/11873 [00:20<00:11, 373.85it/s]\u001b[A\n"," 65% 7736/11873 [00:20<00:11, 351.76it/s]\u001b[A\n"," 65% 7774/11873 [00:20<00:11, 359.30it/s]\u001b[A\n"," 66% 7813/11873 [00:20<00:11, 365.66it/s]\u001b[A\n"," 66% 7851/11873 [00:20<00:10, 367.64it/s]\u001b[A\n"," 66% 7888/11873 [00:21<00:11, 348.04it/s]\u001b[A\n"," 67% 7927/11873 [00:21<00:10, 359.88it/s]\u001b[A\n"," 67% 7965/11873 [00:21<00:10, 364.49it/s]\u001b[A\n"," 67% 8005/11873 [00:21<00:10, 372.24it/s]\u001b[A\n"," 68% 8044/11873 [00:21<00:10, 375.30it/s]\u001b[A\n"," 68% 8084/11873 [00:21<00:09, 379.80it/s]\u001b[A\n"," 68% 8123/11873 [00:21<00:10, 372.75it/s]\u001b[A\n"," 69% 8161/11873 [00:21<00:10, 365.57it/s]\u001b[A\n"," 69% 8198/11873 [00:21<00:10, 354.43it/s]\u001b[A\n"," 69% 8236/11873 [00:21<00:10, 360.61it/s]\u001b[A\n"," 70% 8274/11873 [00:22<00:09, 364.58it/s]\u001b[A\n"," 70% 8312/11873 [00:22<00:09, 368.31it/s]\u001b[A\n"," 70% 8351/11873 [00:22<00:09, 372.12it/s]\u001b[A\n"," 71% 8389/11873 [00:22<00:09, 371.48it/s]\u001b[A\n"," 71% 8427/11873 [00:22<00:09, 371.47it/s]\u001b[A\n"," 71% 8465/11873 [00:22<00:09, 371.83it/s]\u001b[A\n"," 72% 8503/11873 [00:22<00:09, 373.21it/s]\u001b[A\n"," 72% 8543/11873 [00:22<00:08, 379.76it/s]\u001b[A\n"," 72% 8581/11873 [00:22<00:08, 378.49it/s]\u001b[A\n"," 73% 8619/11873 [00:22<00:08, 377.17it/s]\u001b[A\n"," 73% 8657/11873 [00:23<00:08, 375.99it/s]\u001b[A\n"," 73% 8695/11873 [00:23<00:08, 376.27it/s]\u001b[A\n"," 74% 8735/11873 [00:23<00:08, 380.53it/s]\u001b[A\n"," 74% 8774/11873 [00:23<00:08, 381.46it/s]\u001b[A\n"," 74% 8813/11873 [00:23<00:08, 380.28it/s]\u001b[A\n"," 75% 8852/11873 [00:23<00:07, 379.87it/s]\u001b[A\n"," 75% 8890/11873 [00:23<00:07, 377.08it/s]\u001b[A\n"," 75% 8928/11873 [00:23<00:07, 373.69it/s]\u001b[A\n"," 76% 8966/11873 [00:23<00:07, 368.36it/s]\u001b[A\n"," 76% 9003/11873 [00:23<00:07, 367.87it/s]\u001b[A\n"," 76% 9040/11873 [00:24<00:07, 367.46it/s]\u001b[A\n"," 76% 9078/11873 [00:24<00:07, 369.13it/s]\u001b[A\n"," 77% 9116/11873 [00:24<00:07, 370.16it/s]\u001b[A\n"," 77% 9154/11873 [00:24<00:07, 368.63it/s]\u001b[A\n"," 77% 9194/11873 [00:24<00:07, 377.57it/s]\u001b[A\n"," 78% 9233/11873 [00:24<00:06, 380.84it/s]\u001b[A\n"," 78% 9272/11873 [00:24<00:06, 380.49it/s]\u001b[A\n"," 78% 9311/11873 [00:24<00:06, 380.25it/s]\u001b[A\n"," 79% 9350/11873 [00:24<00:06, 380.23it/s]\u001b[A\n"," 79% 9389/11873 [00:25<00:06, 378.30it/s]\u001b[A\n"," 79% 9427/11873 [00:25<00:06, 374.69it/s]\u001b[A\n"," 80% 9465/11873 [00:25<00:06, 364.81it/s]\u001b[A\n"," 80% 9503/11873 [00:25<00:06, 368.95it/s]\u001b[A\n"," 80% 9542/11873 [00:25<00:06, 372.71it/s]\u001b[A\n"," 81% 9581/11873 [00:25<00:06, 377.77it/s]\u001b[A\n"," 81% 9619/11873 [00:25<00:05, 377.80it/s]\u001b[A\n"," 81% 9658/11873 [00:25<00:05, 381.15it/s]\u001b[A\n"," 82% 9698/11873 [00:25<00:05, 384.15it/s]\u001b[A\n"," 82% 9738/11873 [00:25<00:05, 388.58it/s]\u001b[A\n"," 82% 9777/11873 [00:26<00:05, 388.44it/s]\u001b[A\n"," 83% 9816/11873 [00:26<00:05, 386.42it/s]\u001b[A\n"," 83% 9856/11873 [00:26<00:05, 389.72it/s]\u001b[A\n"," 83% 9895/11873 [00:26<00:05, 389.30it/s]\u001b[A\n"," 84% 9934/11873 [00:26<00:05, 383.13it/s]\u001b[A\n"," 84% 9974/11873 [00:26<00:04, 386.92it/s]\u001b[A\n"," 84% 10013/11873 [00:26<00:04, 385.68it/s]\u001b[A\n"," 85% 10052/11873 [00:26<00:04, 378.07it/s]\u001b[A\n"," 85% 10090/11873 [00:26<00:04, 375.27it/s]\u001b[A\n"," 85% 10128/11873 [00:26<00:04, 375.02it/s]\u001b[A\n"," 86% 10166/11873 [00:27<00:04, 373.89it/s]\u001b[A\n"," 86% 10204/11873 [00:27<00:04, 362.52it/s]\u001b[A\n"," 86% 10241/11873 [00:27<00:04, 357.37it/s]\u001b[A\n"," 87% 10277/11873 [00:27<00:04, 354.20it/s]\u001b[A\n"," 87% 10313/11873 [00:27<00:04, 349.77it/s]\u001b[A\n"," 87% 10350/11873 [00:27<00:04, 354.11it/s]\u001b[A\n"," 88% 10389/11873 [00:27<00:04, 362.34it/s]\u001b[A\n"," 88% 10426/11873 [00:27<00:04, 352.62it/s]\u001b[A\n"," 88% 10464/11873 [00:27<00:03, 359.72it/s]\u001b[A\n"," 88% 10503/11873 [00:28<00:03, 367.59it/s]\u001b[A\n"," 89% 10542/11873 [00:28<00:03, 370.06it/s]\u001b[A\n"," 89% 10580/11873 [00:28<00:03, 343.56it/s]\u001b[A\n"," 89% 10618/11873 [00:28<00:03, 352.41it/s]\u001b[A\n"," 90% 10655/11873 [00:28<00:03, 355.30it/s]\u001b[A\n"," 90% 10694/11873 [00:28<00:03, 364.50it/s]\u001b[A\n"," 90% 10733/11873 [00:28<00:03, 370.54it/s]\u001b[A\n"," 91% 10771/11873 [00:28<00:04, 256.56it/s]\u001b[A\n"," 91% 10809/11873 [00:29<00:03, 283.83it/s]\u001b[A\n"," 91% 10842/11873 [00:29<00:03, 285.23it/s]\u001b[A\n"," 92% 10880/11873 [00:29<00:03, 308.29it/s]\u001b[A\n"," 92% 10918/11873 [00:29<00:02, 326.90it/s]\u001b[A\n"," 92% 10954/11873 [00:29<00:02, 335.77it/s]\u001b[A\n"," 93% 10994/11873 [00:29<00:02, 351.80it/s]\u001b[A\n"," 93% 11033/11873 [00:29<00:02, 362.67it/s]\u001b[A\n"," 93% 11071/11873 [00:29<00:02, 359.77it/s]\u001b[A\n"," 94% 11108/11873 [00:29<00:02, 360.54it/s]\u001b[A\n"," 94% 11145/11873 [00:29<00:02, 359.77it/s]\u001b[A\n"," 94% 11182/11873 [00:30<00:01, 356.84it/s]\u001b[A\n"," 94% 11218/11873 [00:30<00:01, 354.62it/s]\u001b[A\n"," 95% 11257/11873 [00:30<00:01, 362.59it/s]\u001b[A\n"," 95% 11296/11873 [00:30<00:01, 369.53it/s]\u001b[A\n"," 95% 11335/11873 [00:30<00:01, 375.14it/s]\u001b[A\n"," 96% 11374/11873 [00:30<00:01, 378.20it/s]\u001b[A\n"," 96% 11412/11873 [00:30<00:01, 375.66it/s]\u001b[A\n"," 96% 11450/11873 [00:30<00:01, 375.08it/s]\u001b[A\n"," 97% 11489/11873 [00:30<00:01, 377.59it/s]\u001b[A\n"," 97% 11527/11873 [00:30<00:00, 377.68it/s]\u001b[A\n"," 97% 11565/11873 [00:31<00:00, 373.88it/s]\u001b[A\n"," 98% 11604/11873 [00:31<00:00, 376.03it/s]\u001b[A\n"," 98% 11642/11873 [00:31<00:00, 376.83it/s]\u001b[A\n"," 98% 11680/11873 [00:31<00:00, 376.60it/s]\u001b[A\n"," 99% 11718/11873 [00:31<00:00, 377.25it/s]\u001b[A\n"," 99% 11756/11873 [00:31<00:00, 377.52it/s]\u001b[A\n"," 99% 11795/11873 [00:31<00:00, 380.70it/s]\u001b[A\n","100% 11834/11873 [00:31<00:00, 379.27it/s]\u001b[A\n","100% 11873/11873 [00:31<00:00, 372.50it/s]\n","03/27/2022 01:36:12 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","03/27/2022 01:36:12 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","03/27/2022 01:36:15 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","03/27/2022 01:36:18 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1525/1525 [03:46<00:00,  6.72it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      =  72.166\n","  eval_HasAns_f1         = 78.4134\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       =  69.857\n","  eval_NoAns_f1          =  69.857\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 71.0099\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 74.1291\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 71.0099\n","  eval_f1                = 74.1291\n","  eval_samples           =   12199\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-03-27 01:36:18,357 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path bert-base-cased \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJmqt3g2Oe7p","executionInfo":{"status":"ok","timestamp":1648359300435,"user_tz":240,"elapsed":10764861,"user":{"displayName":"SICHEN ZHONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGziZ-Uu-PyjGuHsDy1aSyMZvJYEN6bO8qLBQyKw=s64","userId":"08427994781088390833"}},"outputId":"e1619c61-d9e9-46ec-80f4-8b7af80e7575"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["03/27/2022 02:35:38 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","03/27/2022 02:35:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Mar27_02-35-38_56544ad519f9,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","03/27/2022 02:35:39 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","03/27/2022 02:35:39 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/27/2022 02:35:39 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","03/27/2022 02:35:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/27/2022 02:35:39 - WARNING - datasets.builder - Reusing dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","03/27/2022 02:35:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","100% 2/2 [00:00<00:00, 664.02it/s]\n","[INFO|configuration_utils.py:653] 2022-03-27 02:35:39,345 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-27 02:35:39,346 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|configuration_utils.py:653] 2022-03-27 02:35:39,618 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-27 02:35:39,618 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|tokenization_utils_base.py:1777] 2022-03-27 02:35:40,417 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1777] 2022-03-27 02:35:40,418 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1777] 2022-03-27 02:35:40,418 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1777] 2022-03-27 02:35:40,418 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1777] 2022-03-27 02:35:40,418 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:653] 2022-03-27 02:35:40,552 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-27 02:35:40,553 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|modeling_utils.py:1771] 2022-03-27 02:35:40,737 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:2049] 2022-03-27 02:35:42,110 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-03-27 02:35:42,110 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","03/27/2022 02:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-538cb096a17d202c.arrow\n","03/27/2022 02:35:42 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-f1f6a455c564ba86.arrow\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1288] 2022-03-27 02:35:45,957 >> ***** Running training *****\n","[INFO|trainer.py:1289] 2022-03-27 02:35:45,957 >>   Num examples = 132079\n","[INFO|trainer.py:1290] 2022-03-27 02:35:45,957 >>   Num Epochs = 2\n","[INFO|trainer.py:1291] 2022-03-27 02:35:45,957 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1292] 2022-03-27 02:35:45,957 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1293] 2022-03-27 02:35:45,957 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1294] 2022-03-27 02:35:45,957 >>   Total optimization steps = 16510\n","{'loss': 2.1395, 'learning_rate': 3.878861296184131e-05, 'epoch': 0.06}\n","  3% 500/16510 [05:16<2:48:55,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 02:41:02,537 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:440] 2022-03-27 02:41:02,538 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 02:41:03,220 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 02:41:03,220 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 02:41:03,221 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.5584, 'learning_rate': 3.7577225923682624e-05, 'epoch': 0.12}\n","  6% 1000/16510 [10:35<2:43:30,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 02:46:21,575 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:440] 2022-03-27 02:46:21,576 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 02:46:22,223 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 02:46:22,223 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 02:46:22,224 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.4252, 'learning_rate': 3.636583888552393e-05, 'epoch': 0.18}\n","  9% 1500/16510 [15:54<2:38:28,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 02:51:40,425 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:440] 2022-03-27 02:51:40,426 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 02:51:41,102 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 02:51:41,103 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 02:51:41,103 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.335, 'learning_rate': 3.515445184736523e-05, 'epoch': 0.24}\n"," 12% 2000/16510 [21:13<2:32:44,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 02:56:59,370 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:440] 2022-03-27 02:56:59,371 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 02:57:00,030 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 02:57:00,030 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 02:57:00,031 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.2897, 'learning_rate': 3.394306480920655e-05, 'epoch': 0.3}\n"," 15% 2500/16510 [26:32<2:27:49,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 03:02:18,247 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:440] 2022-03-27 03:02:18,248 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 03:02:18,940 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 03:02:18,940 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 03:02:18,941 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.246, 'learning_rate': 3.273167777104785e-05, 'epoch': 0.36}\n"," 18% 3000/16510 [31:50<2:22:39,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 03:07:36,889 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:440] 2022-03-27 03:07:36,890 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 03:07:37,540 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 03:07:37,541 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 03:07:37,541 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.1958, 'learning_rate': 3.152029073288916e-05, 'epoch': 0.42}\n"," 21% 3500/16510 [37:09<2:17:16,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 03:12:55,855 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:440] 2022-03-27 03:12:55,856 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 03:12:56,527 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 03:12:56,527 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 03:12:56,528 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.1581, 'learning_rate': 3.030890369473047e-05, 'epoch': 0.48}\n"," 24% 4000/16510 [42:28<2:11:52,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 03:18:14,724 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:440] 2022-03-27 03:18:14,724 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 03:18:15,396 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 03:18:15,397 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 03:18:15,397 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.143, 'learning_rate': 2.9097516656571776e-05, 'epoch': 0.55}\n"," 27% 4500/16510 [47:50<2:06:44,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 03:23:36,105 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:440] 2022-03-27 03:23:36,107 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 03:23:36,778 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 03:23:36,778 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 03:23:36,778 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.1075, 'learning_rate': 2.7886129618413086e-05, 'epoch': 0.61}\n"," 30% 5000/16510 [53:09<2:01:17,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 03:28:54,984 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:440] 2022-03-27 03:28:54,985 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 03:28:55,635 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 03:28:55,635 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 03:28:55,636 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.0888, 'learning_rate': 2.6674742580254393e-05, 'epoch': 0.67}\n"," 33% 5500/16510 [58:27<1:56:07,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 03:34:13,744 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:440] 2022-03-27 03:34:13,745 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 03:34:14,410 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 03:34:14,410 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 03:34:14,411 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.0395, 'learning_rate': 2.54633555420957e-05, 'epoch': 0.73}\n"," 36% 6000/16510 [1:03:46<1:51:01,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 03:39:32,647 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:440] 2022-03-27 03:39:32,648 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 03:39:33,317 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 03:39:33,318 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 03:39:33,318 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.066, 'learning_rate': 2.425196850393701e-05, 'epoch': 0.79}\n"," 39% 6500/16510 [1:09:05<1:45:34,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 03:44:51,531 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:440] 2022-03-27 03:44:51,532 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 03:44:52,204 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 03:44:52,205 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 03:44:52,205 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.0188, 'learning_rate': 2.3040581465778318e-05, 'epoch': 0.85}\n"," 42% 7000/16510 [1:14:24<1:40:11,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 03:50:10,170 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:440] 2022-03-27 03:50:10,170 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 03:50:10,823 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 03:50:10,823 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 03:50:10,823 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.0353, 'learning_rate': 2.1829194427619624e-05, 'epoch': 0.91}\n"," 45% 7500/16510 [1:19:43<1:34:54,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 03:55:29,146 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:440] 2022-03-27 03:55:29,147 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 03:55:29,802 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 03:55:29,803 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 03:55:29,803 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.9817, 'learning_rate': 2.0617807389460935e-05, 'epoch': 0.97}\n"," 48% 8000/16510 [1:25:02<1:29:42,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 04:00:47,990 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:440] 2022-03-27 04:00:47,991 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 04:00:48,643 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 04:00:48,644 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 04:00:48,644 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.8542, 'learning_rate': 1.9406420351302242e-05, 'epoch': 1.03}\n"," 51% 8500/16510 [1:30:20<1:24:31,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 04:06:06,895 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:440] 2022-03-27 04:06:06,896 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 04:06:07,562 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 04:06:07,562 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 04:06:07,563 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.7083, 'learning_rate': 1.8195033313143552e-05, 'epoch': 1.09}\n"," 55% 9000/16510 [1:35:39<1:19:13,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 04:11:25,569 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:440] 2022-03-27 04:11:25,570 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 04:11:26,231 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 04:11:26,231 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 04:11:26,231 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.7017, 'learning_rate': 1.698364627498486e-05, 'epoch': 1.15}\n"," 58% 9500/16510 [1:40:58<1:13:53,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 04:16:44,160 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:440] 2022-03-27 04:16:44,161 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 04:16:44,817 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 04:16:44,817 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 04:16:44,817 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.6799, 'learning_rate': 1.5772259236826166e-05, 'epoch': 1.21}\n"," 61% 10000/16510 [1:46:17<1:08:39,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 04:22:03,002 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:440] 2022-03-27 04:22:03,003 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 04:22:03,662 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 04:22:03,663 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 04:22:03,663 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.6867, 'learning_rate': 1.4560872198667475e-05, 'epoch': 1.27}\n"," 64% 10500/16510 [1:51:35<1:03:17,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 04:27:21,519 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:440] 2022-03-27 04:27:21,520 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 04:27:22,190 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 04:27:22,191 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 04:27:22,191 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.6884, 'learning_rate': 1.3349485160508783e-05, 'epoch': 1.33}\n"," 67% 11000/16510 [1:56:54<58:05,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 04:32:40,044 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11000\n","[INFO|configuration_utils.py:440] 2022-03-27 04:32:40,045 >> Configuration saved in /tmp/debug_squad/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 04:32:40,709 >> Model weights saved in /tmp/debug_squad/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 04:32:40,710 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 04:32:40,710 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.6836, 'learning_rate': 1.2138098122350092e-05, 'epoch': 1.39}\n"," 70% 11500/16510 [2:02:12<52:48,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 04:37:58,682 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11500\n","[INFO|configuration_utils.py:440] 2022-03-27 04:37:58,683 >> Configuration saved in /tmp/debug_squad/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 04:37:59,346 >> Model weights saved in /tmp/debug_squad/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 04:37:59,346 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 04:37:59,346 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.6882, 'learning_rate': 1.0926711084191399e-05, 'epoch': 1.45}\n"," 73% 12000/16510 [2:07:31<47:32,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 04:43:17,083 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12000\n","[INFO|configuration_utils.py:440] 2022-03-27 04:43:17,084 >> Configuration saved in /tmp/debug_squad/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 04:43:17,734 >> Model weights saved in /tmp/debug_squad/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 04:43:17,735 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 04:43:17,735 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.6809, 'learning_rate': 9.715324046032707e-06, 'epoch': 1.51}\n"," 76% 12500/16510 [2:12:49<42:15,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 04:48:35,435 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12500\n","[INFO|configuration_utils.py:440] 2022-03-27 04:48:35,436 >> Configuration saved in /tmp/debug_squad/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 04:48:36,083 >> Model weights saved in /tmp/debug_squad/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 04:48:36,083 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 04:48:36,084 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.6526, 'learning_rate': 8.503937007874016e-06, 'epoch': 1.57}\n"," 79% 13000/16510 [2:18:07<36:59,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 04:53:53,786 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13000\n","[INFO|configuration_utils.py:440] 2022-03-27 04:53:53,787 >> Configuration saved in /tmp/debug_squad/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 04:53:54,435 >> Model weights saved in /tmp/debug_squad/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 04:53:54,436 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 04:53:54,436 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.6635, 'learning_rate': 7.292549969715325e-06, 'epoch': 1.64}\n"," 82% 13500/16510 [2:23:26<31:41,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 04:59:12,182 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13500\n","[INFO|configuration_utils.py:440] 2022-03-27 04:59:12,183 >> Configuration saved in /tmp/debug_squad/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 04:59:12,838 >> Model weights saved in /tmp/debug_squad/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 04:59:12,839 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 04:59:12,839 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.678, 'learning_rate': 6.0811629315566326e-06, 'epoch': 1.7}\n"," 85% 14000/16510 [2:28:44<26:27,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 05:04:30,581 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14000\n","[INFO|configuration_utils.py:440] 2022-03-27 05:04:30,582 >> Configuration saved in /tmp/debug_squad/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 05:04:31,252 >> Model weights saved in /tmp/debug_squad/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 05:04:31,252 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 05:04:31,252 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.6556, 'learning_rate': 4.869775893397941e-06, 'epoch': 1.76}\n"," 88% 14500/16510 [2:34:02<21:11,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 05:09:48,919 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14500\n","[INFO|configuration_utils.py:440] 2022-03-27 05:09:48,920 >> Configuration saved in /tmp/debug_squad/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 05:09:49,594 >> Model weights saved in /tmp/debug_squad/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 05:09:49,595 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 05:09:49,595 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.636, 'learning_rate': 3.6583888552392494e-06, 'epoch': 1.82}\n"," 91% 15000/16510 [2:39:21<15:53,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 05:15:07,293 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15000\n","[INFO|configuration_utils.py:440] 2022-03-27 05:15:07,294 >> Configuration saved in /tmp/debug_squad/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 05:15:07,951 >> Model weights saved in /tmp/debug_squad/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 05:15:07,951 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 05:15:07,952 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.6399, 'learning_rate': 2.4470018170805572e-06, 'epoch': 1.88}\n"," 94% 15500/16510 [2:44:39<10:38,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 05:20:25,518 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15500\n","[INFO|configuration_utils.py:440] 2022-03-27 05:20:25,519 >> Configuration saved in /tmp/debug_squad/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 05:20:26,163 >> Model weights saved in /tmp/debug_squad/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 05:20:26,164 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 05:20:26,164 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.6404, 'learning_rate': 1.2356147789218657e-06, 'epoch': 1.94}\n"," 97% 16000/16510 [2:49:57<05:22,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 05:25:43,728 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-16000\n","[INFO|configuration_utils.py:440] 2022-03-27 05:25:43,729 >> Configuration saved in /tmp/debug_squad/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 05:25:44,380 >> Model weights saved in /tmp/debug_squad/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 05:25:44,380 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 05:25:44,381 >> Special tokens file saved in /tmp/debug_squad/checkpoint-16000/special_tokens_map.json\n","{'loss': 0.6523, 'learning_rate': 2.4227740763173837e-08, 'epoch': 2.0}\n","100% 16500/16510 [2:55:16<00:06,  1.58it/s][INFO|trainer.py:2162] 2022-03-27 05:31:01,999 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-16500\n","[INFO|configuration_utils.py:440] 2022-03-27 05:31:01,999 >> Configuration saved in /tmp/debug_squad/checkpoint-16500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 05:31:02,671 >> Model weights saved in /tmp/debug_squad/checkpoint-16500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 05:31:02,671 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-16500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 05:31:02,671 >> Special tokens file saved in /tmp/debug_squad/checkpoint-16500/special_tokens_map.json\n","100% 16510/16510 [2:55:24<00:00,  1.54it/s][INFO|trainer.py:1526] 2022-03-27 05:31:10,563 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 10524.6076, 'train_samples_per_second': 25.099, 'train_steps_per_second': 1.569, 'train_loss': 0.9518683873401275, 'epoch': 2.0}\n","100% 16510/16510 [2:55:24<00:00,  1.57it/s]\n","[INFO|trainer.py:2162] 2022-03-27 05:31:10,570 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:440] 2022-03-27 05:31:10,571 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 05:31:11,239 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 05:31:11,240 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 05:31:11,240 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        2.0\n","  train_loss               =     0.9519\n","  train_runtime            = 2:55:24.60\n","  train_samples            =     132079\n","  train_samples_per_second =     25.099\n","  train_steps_per_second   =      1.569\n","03/27/2022 05:31:11 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:571] 2022-03-27 05:31:11,276 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2412] 2022-03-27 05:31:11,289 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2414] 2022-03-27 05:31:11,289 >>   Num examples = 12199\n","[INFO|trainer.py:2417] 2022-03-27 05:31:11,289 >>   Batch size = 8\n","100% 1525/1525 [02:57<00:00,  8.78it/s]03/27/2022 05:34:20 - INFO - utils_qa - Post-processing 11873 example predictions split into 12199 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 42/11873 [00:00<00:28, 413.17it/s]\u001b[A\n","  1% 84/11873 [00:00<00:28, 407.96it/s]\u001b[A\n","  1% 130/11873 [00:00<00:27, 428.40it/s]\u001b[A\n","  1% 178/11873 [00:00<00:26, 445.80it/s]\u001b[A\n","  2% 226/11873 [00:00<00:25, 455.36it/s]\u001b[A\n","  2% 275/11873 [00:00<00:24, 465.85it/s]\u001b[A\n","  3% 324/11873 [00:00<00:24, 472.69it/s]\u001b[A\n","  3% 373/11873 [00:00<00:24, 477.37it/s]\u001b[A\n","  4% 424/11873 [00:00<00:23, 483.99it/s]\u001b[A\n","  4% 475/11873 [00:01<00:23, 490.42it/s]\u001b[A\n","  4% 525/11873 [00:01<00:23, 486.44it/s]\u001b[A\n","  5% 574/11873 [00:01<00:23, 479.64it/s]\u001b[A\n","  5% 625/11873 [00:01<00:23, 486.61it/s]\u001b[A\n","  6% 674/11873 [00:01<00:23, 481.33it/s]\u001b[A\n","  6% 723/11873 [00:01<00:23, 472.07it/s]\u001b[A\n","  6% 771/11873 [00:01<00:23, 467.78it/s]\u001b[A\n","  7% 818/11873 [00:01<00:23, 467.78it/s]\u001b[A\n","  7% 869/11873 [00:01<00:22, 478.51it/s]\u001b[A\n","  8% 921/11873 [00:01<00:22, 489.78it/s]\u001b[A\n","  8% 973/11873 [00:02<00:21, 495.89it/s]\u001b[A\n","  9% 1023/11873 [00:02<00:23, 469.05it/s]\u001b[A\n","  9% 1071/11873 [00:02<00:24, 445.00it/s]\u001b[A\n","  9% 1116/11873 [00:02<00:25, 425.72it/s]\u001b[A\n"," 10% 1159/11873 [00:02<00:26, 407.52it/s]\u001b[A\n"," 10% 1201/11873 [00:02<00:26, 401.96it/s]\u001b[A\n"," 10% 1242/11873 [00:02<00:26, 393.94it/s]\u001b[A\n"," 11% 1282/11873 [00:02<00:27, 390.99it/s]\u001b[A\n"," 11% 1322/11873 [00:02<00:27, 384.25it/s]\u001b[A\n"," 11% 1361/11873 [00:03<00:27, 379.09it/s]\u001b[A\n"," 12% 1399/11873 [00:03<00:28, 370.82it/s]\u001b[A\n"," 12% 1437/11873 [00:03<00:28, 368.57it/s]\u001b[A\n"," 12% 1474/11873 [00:03<00:28, 367.35it/s]\u001b[A\n"," 13% 1514/11873 [00:03<00:27, 376.10it/s]\u001b[A\n"," 13% 1554/11873 [00:03<00:27, 380.66it/s]\u001b[A\n"," 13% 1594/11873 [00:03<00:26, 384.81it/s]\u001b[A\n"," 14% 1633/11873 [00:03<00:26, 385.31it/s]\u001b[A\n"," 14% 1672/11873 [00:03<00:26, 385.01it/s]\u001b[A\n"," 14% 1711/11873 [00:03<00:26, 382.45it/s]\u001b[A\n"," 15% 1751/11873 [00:04<00:26, 386.19it/s]\u001b[A\n"," 15% 1790/11873 [00:04<00:26, 384.22it/s]\u001b[A\n"," 15% 1829/11873 [00:04<00:26, 383.68it/s]\u001b[A\n"," 16% 1869/11873 [00:04<00:25, 386.99it/s]\u001b[A\n"," 16% 1908/11873 [00:04<00:26, 382.86it/s]\u001b[A\n"," 16% 1948/11873 [00:04<00:25, 385.23it/s]\u001b[A\n"," 17% 1987/11873 [00:04<00:25, 386.37it/s]\u001b[A\n"," 17% 2027/11873 [00:04<00:25, 389.32it/s]\u001b[A\n"," 17% 2066/11873 [00:04<00:25, 387.77it/s]\u001b[A\n"," 18% 2105/11873 [00:05<00:25, 386.47it/s]\u001b[A\n"," 18% 2144/11873 [00:05<00:25, 387.50it/s]\u001b[A\n"," 18% 2183/11873 [00:05<00:25, 384.22it/s]\u001b[A\n"," 19% 2222/11873 [00:05<00:25, 385.30it/s]\u001b[A\n"," 19% 2261/11873 [00:05<00:25, 383.91it/s]\u001b[A\n"," 19% 2300/11873 [00:05<00:24, 383.65it/s]\u001b[A\n"," 20% 2340/11873 [00:05<00:24, 387.08it/s]\u001b[A\n"," 20% 2380/11873 [00:05<00:24, 389.14it/s]\u001b[A\n"," 20% 2419/11873 [00:05<00:24, 387.96it/s]\u001b[A\n"," 21% 2458/11873 [00:05<00:24, 380.57it/s]\u001b[A\n"," 21% 2497/11873 [00:06<00:25, 369.05it/s]\u001b[A\n"," 21% 2534/11873 [00:06<00:25, 363.58it/s]\u001b[A\n"," 22% 2571/11873 [00:06<00:25, 359.85it/s]\u001b[A\n"," 22% 2608/11873 [00:06<00:25, 358.54it/s]\u001b[A\n"," 22% 2645/11873 [00:06<00:25, 360.31it/s]\u001b[A\n"," 23% 2682/11873 [00:06<00:25, 361.57it/s]\u001b[A\n"," 23% 2721/11873 [00:06<00:24, 369.28it/s]\u001b[A\n"," 23% 2760/11873 [00:06<00:24, 372.78it/s]\u001b[A\n"," 24% 2799/11873 [00:06<00:24, 375.96it/s]\u001b[A\n"," 24% 2837/11873 [00:06<00:24, 375.18it/s]\u001b[A\n"," 24% 2875/11873 [00:07<00:23, 375.78it/s]\u001b[A\n"," 25% 2913/11873 [00:07<00:24, 366.00it/s]\u001b[A\n"," 25% 2951/11873 [00:07<00:24, 368.49it/s]\u001b[A\n"," 25% 2988/11873 [00:07<00:24, 368.62it/s]\u001b[A\n"," 25% 3025/11873 [00:07<00:26, 337.13it/s]\u001b[A\n"," 26% 3062/11873 [00:07<00:25, 346.24it/s]\u001b[A\n"," 26% 3098/11873 [00:07<00:25, 347.01it/s]\u001b[A\n"," 26% 3134/11873 [00:07<00:29, 299.79it/s]\u001b[A\n"," 27% 3166/11873 [00:08<00:31, 280.09it/s]\u001b[A\n"," 27% 3204/11873 [00:08<00:28, 305.45it/s]\u001b[A\n"," 27% 3241/11873 [00:08<00:26, 322.14it/s]\u001b[A\n"," 28% 3275/11873 [00:08<00:28, 306.46it/s]\u001b[A\n"," 28% 3307/11873 [00:08<00:36, 237.05it/s]\u001b[A\n"," 28% 3334/11873 [00:08<00:39, 213.99it/s]\u001b[A\n"," 28% 3358/11873 [00:08<00:43, 196.81it/s]\u001b[A\n"," 29% 3384/11873 [00:08<00:40, 209.08it/s]\u001b[A\n"," 29% 3418/11873 [00:09<00:35, 238.64it/s]\u001b[A\n"," 29% 3451/11873 [00:09<00:32, 260.16it/s]\u001b[A\n"," 29% 3485/11873 [00:09<00:29, 280.53it/s]\u001b[A\n"," 30% 3524/11873 [00:09<00:26, 310.29it/s]\u001b[A\n"," 30% 3563/11873 [00:09<00:25, 331.64it/s]\u001b[A\n"," 30% 3603/11873 [00:09<00:23, 349.67it/s]\u001b[A\n"," 31% 3643/11873 [00:09<00:22, 363.00it/s]\u001b[A\n"," 31% 3680/11873 [00:09<00:22, 364.47it/s]\u001b[A\n"," 31% 3718/11873 [00:09<00:22, 367.96it/s]\u001b[A\n"," 32% 3757/11873 [00:09<00:21, 373.16it/s]\u001b[A\n"," 32% 3796/11873 [00:10<00:21, 375.39it/s]\u001b[A\n"," 32% 3834/11873 [00:10<00:22, 352.21it/s]\u001b[A\n"," 33% 3873/11873 [00:10<00:22, 360.25it/s]\u001b[A\n"," 33% 3911/11873 [00:10<00:21, 362.29it/s]\u001b[A\n"," 33% 3948/11873 [00:10<00:23, 335.12it/s]\u001b[A\n"," 34% 3983/11873 [00:10<00:23, 331.17it/s]\u001b[A\n"," 34% 4022/11873 [00:10<00:22, 347.29it/s]\u001b[A\n"," 34% 4062/11873 [00:10<00:21, 360.01it/s]\u001b[A\n"," 35% 4102/11873 [00:10<00:21, 369.51it/s]\u001b[A\n"," 35% 4140/11873 [00:11<00:20, 370.66it/s]\u001b[A\n"," 35% 4178/11873 [00:11<00:22, 345.94it/s]\u001b[A\n"," 36% 4216/11873 [00:11<00:21, 353.00it/s]\u001b[A\n"," 36% 4256/11873 [00:11<00:20, 365.38it/s]\u001b[A\n"," 36% 4293/11873 [00:11<00:20, 365.78it/s]\u001b[A\n"," 36% 4331/11873 [00:11<00:20, 367.90it/s]\u001b[A\n"," 37% 4368/11873 [00:11<00:20, 368.21it/s]\u001b[A\n"," 37% 4405/11873 [00:11<00:20, 364.82it/s]\u001b[A\n"," 37% 4442/11873 [00:11<00:25, 294.85it/s]\u001b[A\n"," 38% 4480/11873 [00:12<00:23, 314.85it/s]\u001b[A\n"," 38% 4515/11873 [00:12<00:22, 322.44it/s]\u001b[A\n"," 38% 4550/11873 [00:12<00:22, 328.31it/s]\u001b[A\n"," 39% 4589/11873 [00:12<00:21, 343.67it/s]\u001b[A\n"," 39% 4628/11873 [00:12<00:20, 355.34it/s]\u001b[A\n"," 39% 4668/11873 [00:12<00:19, 365.63it/s]\u001b[A\n"," 40% 4707/11873 [00:12<00:19, 372.52it/s]\u001b[A\n"," 40% 4746/11873 [00:12<00:18, 375.45it/s]\u001b[A\n"," 40% 4785/11873 [00:12<00:18, 377.25it/s]\u001b[A\n"," 41% 4823/11873 [00:12<00:18, 375.24it/s]\u001b[A\n"," 41% 4862/11873 [00:13<00:18, 378.43it/s]\u001b[A\n"," 41% 4901/11873 [00:13<00:18, 381.13it/s]\u001b[A\n"," 42% 4940/11873 [00:13<00:18, 383.20it/s]\u001b[A\n"," 42% 4979/11873 [00:13<00:17, 384.14it/s]\u001b[A\n"," 42% 5018/11873 [00:13<00:18, 379.96it/s]\u001b[A\n"," 43% 5057/11873 [00:13<00:17, 379.58it/s]\u001b[A\n"," 43% 5098/11873 [00:13<00:17, 386.15it/s]\u001b[A\n"," 43% 5138/11873 [00:13<00:17, 387.88it/s]\u001b[A\n"," 44% 5178/11873 [00:13<00:17, 388.69it/s]\u001b[A\n"," 44% 5217/11873 [00:14<00:17, 378.60it/s]\u001b[A\n"," 44% 5255/11873 [00:14<00:18, 366.19it/s]\u001b[A\n"," 45% 5292/11873 [00:14<00:18, 347.77it/s]\u001b[A\n"," 45% 5327/11873 [00:14<00:19, 342.68it/s]\u001b[A\n"," 45% 5363/11873 [00:14<00:18, 347.11it/s]\u001b[A\n"," 45% 5402/11873 [00:14<00:18, 358.99it/s]\u001b[A\n"," 46% 5441/11873 [00:14<00:17, 367.09it/s]\u001b[A\n"," 46% 5479/11873 [00:14<00:17, 370.27it/s]\u001b[A\n"," 46% 5519/11873 [00:14<00:16, 376.31it/s]\u001b[A\n"," 47% 5558/11873 [00:14<00:16, 380.18it/s]\u001b[A\n"," 47% 5597/11873 [00:15<00:16, 380.75it/s]\u001b[A\n"," 47% 5636/11873 [00:15<00:16, 376.86it/s]\u001b[A\n"," 48% 5675/11873 [00:15<00:16, 379.86it/s]\u001b[A\n"," 48% 5714/11873 [00:15<00:16, 378.33it/s]\u001b[A\n"," 48% 5752/11873 [00:15<00:16, 376.17it/s]\u001b[A\n"," 49% 5790/11873 [00:15<00:16, 366.65it/s]\u001b[A\n"," 49% 5827/11873 [00:15<00:16, 361.13it/s]\u001b[A\n"," 49% 5864/11873 [00:15<00:16, 358.97it/s]\u001b[A\n"," 50% 5900/11873 [00:15<00:16, 351.42it/s]\u001b[A\n"," 50% 5937/11873 [00:16<00:16, 354.28it/s]\u001b[A\n"," 50% 5974/11873 [00:16<00:16, 356.99it/s]\u001b[A\n"," 51% 6012/11873 [00:16<00:16, 362.41it/s]\u001b[A\n"," 51% 6050/11873 [00:16<00:15, 367.41it/s]\u001b[A\n"," 51% 6087/11873 [00:16<00:15, 363.37it/s]\u001b[A\n"," 52% 6126/11873 [00:16<00:15, 369.36it/s]\u001b[A\n"," 52% 6165/11873 [00:16<00:15, 373.03it/s]\u001b[A\n"," 52% 6204/11873 [00:16<00:15, 377.27it/s]\u001b[A\n"," 53% 6243/11873 [00:16<00:14, 380.42it/s]\u001b[A\n"," 53% 6282/11873 [00:16<00:14, 382.66it/s]\u001b[A\n"," 53% 6321/11873 [00:17<00:14, 377.81it/s]\u001b[A\n"," 54% 6359/11873 [00:17<00:14, 374.91it/s]\u001b[A\n"," 54% 6397/11873 [00:17<00:14, 374.01it/s]\u001b[A\n"," 54% 6435/11873 [00:17<00:14, 372.80it/s]\u001b[A\n"," 55% 6473/11873 [00:17<00:14, 374.58it/s]\u001b[A\n"," 55% 6511/11873 [00:17<00:14, 369.57it/s]\u001b[A\n"," 55% 6550/11873 [00:17<00:14, 373.11it/s]\u001b[A\n"," 55% 6588/11873 [00:17<00:14, 370.53it/s]\u001b[A\n"," 56% 6626/11873 [00:17<00:14, 368.08it/s]\u001b[A\n"," 56% 6665/11873 [00:17<00:13, 373.49it/s]\u001b[A\n"," 56% 6703/11873 [00:18<00:14, 368.60it/s]\u001b[A\n"," 57% 6740/11873 [00:18<00:15, 340.95it/s]\u001b[A\n"," 57% 6779/11873 [00:18<00:14, 354.25it/s]\u001b[A\n"," 57% 6817/11873 [00:18<00:14, 360.16it/s]\u001b[A\n"," 58% 6856/11873 [00:18<00:13, 367.89it/s]\u001b[A\n"," 58% 6895/11873 [00:18<00:13, 373.67it/s]\u001b[A\n"," 58% 6934/11873 [00:18<00:13, 378.20it/s]\u001b[A\n"," 59% 6972/11873 [00:18<00:13, 366.75it/s]\u001b[A\n"," 59% 7009/11873 [00:18<00:13, 357.53it/s]\u001b[A\n"," 59% 7046/11873 [00:19<00:13, 360.77it/s]\u001b[A\n"," 60% 7085/11873 [00:19<00:13, 367.23it/s]\u001b[A\n"," 60% 7124/11873 [00:19<00:12, 372.92it/s]\u001b[A\n"," 60% 7162/11873 [00:19<00:12, 366.29it/s]\u001b[A\n"," 61% 7199/11873 [00:19<00:12, 365.69it/s]\u001b[A\n"," 61% 7236/11873 [00:19<00:12, 362.86it/s]\u001b[A\n"," 61% 7274/11873 [00:19<00:12, 367.30it/s]\u001b[A\n"," 62% 7313/11873 [00:19<00:12, 372.63it/s]\u001b[A\n"," 62% 7353/11873 [00:19<00:11, 378.22it/s]\u001b[A\n"," 62% 7392/11873 [00:19<00:11, 380.80it/s]\u001b[A\n"," 63% 7431/11873 [00:20<00:12, 358.81it/s]\u001b[A\n"," 63% 7471/11873 [00:20<00:11, 367.90it/s]\u001b[A\n"," 63% 7510/11873 [00:20<00:11, 373.87it/s]\u001b[A\n"," 64% 7548/11873 [00:20<00:11, 368.52it/s]\u001b[A\n"," 64% 7586/11873 [00:20<00:11, 371.63it/s]\u001b[A\n"," 64% 7624/11873 [00:20<00:11, 372.57it/s]\u001b[A\n"," 65% 7662/11873 [00:20<00:11, 371.49it/s]\u001b[A\n"," 65% 7700/11873 [00:20<00:11, 370.75it/s]\u001b[A\n"," 65% 7738/11873 [00:20<00:11, 354.50it/s]\u001b[A\n"," 65% 7776/11873 [00:20<00:11, 359.53it/s]\u001b[A\n"," 66% 7813/11873 [00:21<00:11, 360.72it/s]\u001b[A\n"," 66% 7850/11873 [00:21<00:11, 363.32it/s]\u001b[A\n"," 66% 7887/11873 [00:21<00:11, 343.23it/s]\u001b[A\n"," 67% 7926/11873 [00:21<00:11, 355.38it/s]\u001b[A\n"," 67% 7964/11873 [00:21<00:10, 360.70it/s]\u001b[A\n"," 67% 8003/11873 [00:21<00:10, 367.58it/s]\u001b[A\n"," 68% 8043/11873 [00:21<00:10, 374.56it/s]\u001b[A\n"," 68% 8081/11873 [00:21<00:10, 375.68it/s]\u001b[A\n"," 68% 8119/11873 [00:21<00:09, 376.58it/s]\u001b[A\n"," 69% 8157/11873 [00:22<00:09, 373.62it/s]\u001b[A\n"," 69% 8195/11873 [00:22<00:10, 364.08it/s]\u001b[A\n"," 69% 8232/11873 [00:22<00:10, 358.91it/s]\u001b[A\n"," 70% 8268/11873 [00:22<00:10, 354.72it/s]\u001b[A\n"," 70% 8304/11873 [00:22<00:10, 352.72it/s]\u001b[A\n"," 70% 8341/11873 [00:22<00:09, 357.03it/s]\u001b[A\n"," 71% 8380/11873 [00:22<00:09, 366.37it/s]\u001b[A\n"," 71% 8418/11873 [00:22<00:09, 368.98it/s]\u001b[A\n"," 71% 8455/11873 [00:22<00:09, 368.93it/s]\u001b[A\n"," 72% 8494/11873 [00:22<00:09, 372.79it/s]\u001b[A\n"," 72% 8532/11873 [00:23<00:08, 373.63it/s]\u001b[A\n"," 72% 8571/11873 [00:23<00:08, 377.58it/s]\u001b[A\n"," 73% 8610/11873 [00:23<00:08, 379.66it/s]\u001b[A\n"," 73% 8648/11873 [00:23<00:08, 375.14it/s]\u001b[A\n"," 73% 8687/11873 [00:23<00:08, 378.50it/s]\u001b[A\n"," 73% 8726/11873 [00:23<00:08, 379.86it/s]\u001b[A\n"," 74% 8766/11873 [00:23<00:08, 383.57it/s]\u001b[A\n"," 74% 8805/11873 [00:23<00:08, 382.49it/s]\u001b[A\n"," 74% 8844/11873 [00:23<00:08, 374.67it/s]\u001b[A\n"," 75% 8883/11873 [00:23<00:07, 378.86it/s]\u001b[A\n"," 75% 8921/11873 [00:24<00:07, 378.81it/s]\u001b[A\n"," 75% 8960/11873 [00:24<00:07, 380.31it/s]\u001b[A\n"," 76% 8999/11873 [00:24<00:07, 379.60it/s]\u001b[A\n"," 76% 9037/11873 [00:24<00:07, 377.05it/s]\u001b[A\n"," 76% 9076/11873 [00:24<00:07, 380.15it/s]\u001b[A\n"," 77% 9115/11873 [00:24<00:07, 378.66it/s]\u001b[A\n"," 77% 9154/11873 [00:24<00:07, 380.29it/s]\u001b[A\n"," 77% 9194/11873 [00:24<00:06, 384.41it/s]\u001b[A\n"," 78% 9233/11873 [00:24<00:06, 383.84it/s]\u001b[A\n"," 78% 9272/11873 [00:25<00:06, 383.52it/s]\u001b[A\n"," 78% 9311/11873 [00:25<00:06, 379.21it/s]\u001b[A\n"," 79% 9349/11873 [00:25<00:06, 374.12it/s]\u001b[A\n"," 79% 9387/11873 [00:25<00:06, 375.14it/s]\u001b[A\n"," 79% 9426/11873 [00:25<00:06, 378.02it/s]\u001b[A\n"," 80% 9464/11873 [00:25<00:06, 376.72it/s]\u001b[A\n"," 80% 9504/11873 [00:25<00:06, 380.90it/s]\u001b[A\n"," 80% 9543/11873 [00:25<00:06, 382.04it/s]\u001b[A\n"," 81% 9583/11873 [00:25<00:05, 384.56it/s]\u001b[A\n"," 81% 9622/11873 [00:25<00:05, 384.77it/s]\u001b[A\n"," 81% 9661/11873 [00:26<00:05, 384.47it/s]\u001b[A\n"," 82% 9700/11873 [00:26<00:05, 375.55it/s]\u001b[A\n"," 82% 9738/11873 [00:26<00:05, 372.20it/s]\u001b[A\n"," 82% 9776/11873 [00:26<00:05, 372.77it/s]\u001b[A\n"," 83% 9815/11873 [00:26<00:05, 375.15it/s]\u001b[A\n"," 83% 9853/11873 [00:26<00:05, 376.16it/s]\u001b[A\n"," 83% 9892/11873 [00:26<00:05, 379.50it/s]\u001b[A\n"," 84% 9931/11873 [00:26<00:05, 379.77it/s]\u001b[A\n"," 84% 9970/11873 [00:26<00:04, 380.97it/s]\u001b[A\n"," 84% 10010/11873 [00:26<00:04, 384.56it/s]\u001b[A\n"," 85% 10049/11873 [00:27<00:06, 265.86it/s]\u001b[A\n"," 85% 10089/11873 [00:27<00:06, 294.80it/s]\u001b[A\n"," 85% 10128/11873 [00:27<00:05, 316.31it/s]\u001b[A\n"," 86% 10166/11873 [00:27<00:05, 331.60it/s]\u001b[A\n"," 86% 10204/11873 [00:27<00:04, 343.27it/s]\u001b[A\n"," 86% 10243/11873 [00:27<00:04, 354.02it/s]\u001b[A\n"," 87% 10281/11873 [00:27<00:04, 360.99it/s]\u001b[A\n"," 87% 10319/11873 [00:27<00:04, 363.78it/s]\u001b[A\n"," 87% 10358/11873 [00:28<00:04, 369.85it/s]\u001b[A\n"," 88% 10396/11873 [00:28<00:04, 367.50it/s]\u001b[A\n"," 88% 10434/11873 [00:28<00:04, 336.84it/s]\u001b[A\n"," 88% 10469/11873 [00:28<00:04, 339.28it/s]\u001b[A\n"," 88% 10505/11873 [00:28<00:03, 344.04it/s]\u001b[A\n"," 89% 10542/11873 [00:28<00:03, 349.12it/s]\u001b[A\n"," 89% 10578/11873 [00:28<00:03, 335.15it/s]\u001b[A\n"," 89% 10617/11873 [00:28<00:03, 348.20it/s]\u001b[A\n"," 90% 10654/11873 [00:28<00:03, 353.61it/s]\u001b[A\n"," 90% 10691/11873 [00:28<00:03, 356.74it/s]\u001b[A\n"," 90% 10729/11873 [00:29<00:03, 362.49it/s]\u001b[A\n"," 91% 10766/11873 [00:29<00:03, 352.81it/s]\u001b[A\n"," 91% 10802/11873 [00:29<00:03, 349.42it/s]\u001b[A\n"," 91% 10838/11873 [00:29<00:03, 324.92it/s]\u001b[A\n"," 92% 10874/11873 [00:29<00:02, 333.64it/s]\u001b[A\n"," 92% 10911/11873 [00:29<00:02, 342.84it/s]\u001b[A\n"," 92% 10946/11873 [00:29<00:02, 343.03it/s]\u001b[A\n"," 93% 10985/11873 [00:29<00:02, 355.88it/s]\u001b[A\n"," 93% 11024/11873 [00:29<00:02, 363.73it/s]\u001b[A\n"," 93% 11064/11873 [00:30<00:02, 371.81it/s]\u001b[A\n"," 94% 11102/11873 [00:30<00:02, 370.79it/s]\u001b[A\n"," 94% 11140/11873 [00:30<00:01, 373.14it/s]\u001b[A\n"," 94% 11178/11873 [00:30<00:01, 373.68it/s]\u001b[A\n"," 94% 11216/11873 [00:30<00:01, 373.32it/s]\u001b[A\n"," 95% 11254/11873 [00:30<00:01, 374.18it/s]\u001b[A\n"," 95% 11292/11873 [00:30<00:01, 375.74it/s]\u001b[A\n"," 95% 11332/11873 [00:30<00:01, 380.91it/s]\u001b[A\n"," 96% 11371/11873 [00:30<00:01, 381.64it/s]\u001b[A\n"," 96% 11410/11873 [00:30<00:01, 381.89it/s]\u001b[A\n"," 96% 11449/11873 [00:31<00:01, 383.87it/s]\u001b[A\n"," 97% 11488/11873 [00:31<00:01, 379.35it/s]\u001b[A\n"," 97% 11527/11873 [00:31<00:00, 379.81it/s]\u001b[A\n"," 97% 11565/11873 [00:31<00:00, 378.25it/s]\u001b[A\n"," 98% 11604/11873 [00:31<00:00, 380.17it/s]\u001b[A\n"," 98% 11643/11873 [00:31<00:00, 373.05it/s]\u001b[A\n"," 98% 11681/11873 [00:31<00:00, 372.92it/s]\u001b[A\n"," 99% 11719/11873 [00:31<00:00, 373.99it/s]\u001b[A\n"," 99% 11758/11873 [00:31<00:00, 378.56it/s]\u001b[A\n"," 99% 11797/11873 [00:31<00:00, 380.59it/s]\u001b[A\n","100% 11873/11873 [00:32<00:00, 369.04it/s]\n","03/27/2022 05:34:53 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","03/27/2022 05:34:53 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","03/27/2022 05:34:55 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","03/27/2022 05:34:58 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1525/1525 [03:47<00:00,  6.72it/s]\n","***** eval metrics *****\n","  epoch                  =     2.0\n","  eval_HasAns_exact      = 71.6262\n","  eval_HasAns_f1         = 77.9828\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 73.4399\n","  eval_NoAns_f1          = 73.4399\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 72.5343\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 75.7081\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 72.5343\n","  eval_f1                = 75.7081\n","  eval_samples           =   12199\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-03-27 05:34:58,682 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path bert-base-cased \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 3 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2"],"metadata":{"id":"fe5yWRwX4UDp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649327390183,"user_tz":240,"elapsed":16292800,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"98643498-0b74-422b-e8fa-627990abb50d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["04/07/2022 05:58:20 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","04/07/2022 05:58:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/runs/Apr07_05-58-20_d675309a593b,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","04/07/2022 05:58:20 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpgnfcbnri\n","Downloading builder script: 5.28kB [00:00, 6.95MB/s]       \n","04/07/2022 05:58:20 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","04/07/2022 05:58:20 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","04/07/2022 05:58:21 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpuenfbjuy\n","Downloading metadata: 2.40kB [00:00, 4.21MB/s]       \n","04/07/2022 05:58:21 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","04/07/2022 05:58:21 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","04/07/2022 05:58:21 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","04/07/2022 05:58:21 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","04/07/2022 05:58:21 - INFO - datasets.builder - Generating dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","04/07/2022 05:58:21 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]04/07/2022 05:58:21 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp9lnkxxhq\n","\n","Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n","Downloading data: 10.6MB [00:00, 106MB/s]        \u001b[A\n","Downloading data: 21.6MB [00:00, 108MB/s]\u001b[A\n","Downloading data: 42.1MB [00:00, 108MB/s]\n","04/07/2022 05:58:21 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","04/07/2022 05:58:21 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","Downloading data files:  50% 1/2 [00:00<00:00,  1.88it/s]04/07/2022 05:58:21 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpff7hvfii\n","\n","Downloading data: 4.37MB [00:00, 114MB/s]       \n","04/07/2022 05:58:21 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","04/07/2022 05:58:21 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","Downloading data files: 100% 2/2 [00:00<00:00,  2.82it/s]\n","04/07/2022 05:58:21 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","04/07/2022 05:58:22 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1568.84it/s]\n","04/07/2022 05:58:22 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n","04/07/2022 05:58:22 - INFO - datasets.builder - Generating train split\n","04/07/2022 05:58:32 - INFO - datasets.builder - Generating validation split\n","04/07/2022 05:58:33 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 319.09it/s]\n","[INFO|hub.py:583] 2022-04-07 05:58:33,832 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmat40lxl\n","Downloading: 100% 570/570 [00:00<00:00, 611kB/s]\n","[INFO|hub.py:587] 2022-04-07 05:58:34,189 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|hub.py:595] 2022-04-07 05:58:34,189 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:654] 2022-04-07 05:58:34,189 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-07 05:58:34,190 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-07 05:58:34,548 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsjzvtcny\n","Downloading: 100% 29.0/29.0 [00:00<00:00, 27.4kB/s]\n","[INFO|hub.py:587] 2022-04-07 05:58:34,908 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|hub.py:595] 2022-04-07 05:58:34,908 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:654] 2022-04-07 05:58:35,266 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-07 05:58:35,267 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-07 05:58:35,981 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp39fiyu_f\n","Downloading: 100% 208k/208k [00:00<00:00, 632kB/s]\n","[INFO|hub.py:587] 2022-04-07 05:58:36,678 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:595] 2022-04-07 05:58:36,678 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:583] 2022-04-07 05:58:37,036 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqbzdoxec\n","Downloading: 100% 426k/426k [00:00<00:00, 1.04MB/s]\n","[INFO|hub.py:587] 2022-04-07 05:58:37,817 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|hub.py:595] 2022-04-07 05:58:37,817 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 05:58:38,888 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 05:58:38,889 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 05:58:38,889 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 05:58:38,889 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-04-07 05:58:38,889 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:654] 2022-04-07 05:58:39,251 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:690] 2022-04-07 05:58:39,252 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.19.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-04-07 05:58:39,651 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpddrtc04b\n","Downloading: 100% 416M/416M [00:05<00:00, 73.1MB/s]\n","[INFO|hub.py:587] 2022-04-07 05:58:45,665 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|hub.py:595] 2022-04-07 05:58:45,665 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|modeling_utils.py:1772] 2022-04-07 05:58:45,666 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:2049] 2022-04-07 05:58:47,110 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-04-07 05:58:47,110 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]04/07/2022 05:58:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-538cb096a17d202c.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:41<00:00,  3.12ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]04/07/2022 05:59:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-cc2890595de471da.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:01<00:00,  5.12s/ba]\n","04/07/2022 06:00:31 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp7uzb1j0g\n","Downloading builder script: 6.46kB [00:00, 6.95MB/s]       \n","04/07/2022 06:00:31 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/07/2022 06:00:31 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","04/07/2022 06:00:31 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp393ldact\n","Downloading extra modules: 11.3kB [00:00, 13.1MB/s]       \n","04/07/2022 06:00:31 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","04/07/2022 06:00:31 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1290] 2022-04-07 06:00:41,795 >> ***** Running training *****\n","[INFO|trainer.py:1291] 2022-04-07 06:00:41,795 >>   Num examples = 132079\n","[INFO|trainer.py:1292] 2022-04-07 06:00:41,795 >>   Num Epochs = 3\n","[INFO|trainer.py:1293] 2022-04-07 06:00:41,795 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1294] 2022-04-07 06:00:41,795 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1295] 2022-04-07 06:00:41,795 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1296] 2022-04-07 06:00:41,795 >>   Total optimization steps = 24765\n","{'loss': 2.1385, 'learning_rate': 3.9192408641227546e-05, 'epoch': 0.06}\n","  2% 500/24765 [05:16<4:15:59,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 06:05:58,654 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-500\n","[INFO|configuration_utils.py:441] 2022-04-07 06:05:58,658 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 06:05:59,808 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 06:05:59,812 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 06:05:59,815 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-500/special_tokens_map.json\n","{'loss': 1.5552, 'learning_rate': 3.838481728245508e-05, 'epoch': 0.12}\n","  4% 1000/24765 [10:40<4:10:46,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 06:11:22,276 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-1000\n","[INFO|configuration_utils.py:441] 2022-04-07 06:11:22,281 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 06:11:23,448 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 06:11:23,451 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 06:11:23,454 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.4269, 'learning_rate': 3.7577225923682624e-05, 'epoch': 0.18}\n","  6% 1500/24765 [16:04<4:06:04,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 06:16:45,838 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-1500\n","[INFO|configuration_utils.py:441] 2022-04-07 06:16:45,843 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 06:16:47,000 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 06:16:47,003 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 06:16:47,006 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.3389, 'learning_rate': 3.676963456491016e-05, 'epoch': 0.24}\n","  8% 2000/24765 [21:24<4:00:09,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 06:22:06,498 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-2000\n","[INFO|configuration_utils.py:441] 2022-04-07 06:22:06,503 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 06:22:07,669 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 06:22:07,672 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 06:22:07,675 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.2949, 'learning_rate': 3.5962043206137695e-05, 'epoch': 0.3}\n"," 10% 2500/24765 [26:45<3:55:03,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 06:27:26,990 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-2500\n","[INFO|configuration_utils.py:441] 2022-04-07 06:27:26,994 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 06:27:28,140 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 06:27:28,144 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 06:27:28,146 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.2545, 'learning_rate': 3.515445184736523e-05, 'epoch': 0.36}\n"," 12% 3000/24765 [32:05<3:49:56,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 06:32:47,414 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-3000\n","[INFO|configuration_utils.py:441] 2022-04-07 06:32:47,418 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 06:32:48,565 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 06:32:48,568 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 06:32:48,571 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.2032, 'learning_rate': 3.4346860488592774e-05, 'epoch': 0.42}\n"," 14% 3500/24765 [37:28<3:44:05,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 06:38:10,709 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-3500\n","[INFO|configuration_utils.py:441] 2022-04-07 06:38:10,713 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 06:38:11,867 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 06:38:11,871 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 06:38:11,874 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.1635, 'learning_rate': 3.3539269129820316e-05, 'epoch': 0.48}\n"," 16% 4000/24765 [42:52<3:39:21,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 06:43:34,283 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-4000\n","[INFO|configuration_utils.py:441] 2022-04-07 06:43:34,287 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 06:43:35,503 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 06:43:35,507 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 06:43:35,510 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.1532, 'learning_rate': 3.273167777104785e-05, 'epoch': 0.55}\n"," 18% 4500/24765 [48:15<3:33:45,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 06:48:57,766 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-4500\n","[INFO|configuration_utils.py:441] 2022-04-07 06:48:57,771 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 06:48:58,924 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 06:48:58,927 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 06:48:58,930 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.1104, 'learning_rate': 3.1924086412275394e-05, 'epoch': 0.61}\n"," 20% 5000/24765 [53:36<3:28:28,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 06:54:18,161 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-5000\n","[INFO|configuration_utils.py:441] 2022-04-07 06:54:18,165 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 06:54:19,318 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 06:54:19,322 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 06:54:19,324 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.1028, 'learning_rate': 3.111649505350293e-05, 'epoch': 0.67}\n"," 22% 5500/24765 [58:56<3:23:19,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 06:59:38,680 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-5500\n","[INFO|configuration_utils.py:441] 2022-04-07 06:59:38,684 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 06:59:39,838 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 06:59:39,841 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 06:59:39,843 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.0498, 'learning_rate': 3.030890369473047e-05, 'epoch': 0.73}\n"," 24% 6000/24765 [1:04:17<3:17:57,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 07:04:59,362 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-6000\n","[INFO|configuration_utils.py:441] 2022-04-07 07:04:59,366 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 07:05:00,521 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 07:05:00,524 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 07:05:00,526 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.0803, 'learning_rate': 2.9501312335958005e-05, 'epoch': 0.79}\n"," 26% 6500/24765 [1:09:38<3:12:38,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 07:10:19,982 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-6500\n","[INFO|configuration_utils.py:441] 2022-04-07 07:10:19,986 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 07:10:21,146 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 07:10:21,149 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 07:10:21,152 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.0376, 'learning_rate': 2.8693720977185547e-05, 'epoch': 0.85}\n"," 28% 7000/24765 [1:14:58<3:07:37,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 07:15:40,639 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-7000\n","[INFO|configuration_utils.py:441] 2022-04-07 07:15:40,644 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 07:15:41,798 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 07:15:41,802 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 07:15:41,805 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.036, 'learning_rate': 2.7886129618413086e-05, 'epoch': 0.91}\n"," 30% 7500/24765 [1:20:19<3:02:19,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 07:21:01,335 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-7500\n","[INFO|configuration_utils.py:441] 2022-04-07 07:21:01,339 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 07:21:02,490 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 07:21:02,493 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 07:21:02,496 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.9951, 'learning_rate': 2.7078538259640622e-05, 'epoch': 0.97}\n"," 32% 8000/24765 [1:25:39<2:56:35,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 07:26:21,747 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-8000\n","[INFO|configuration_utils.py:441] 2022-04-07 07:26:21,752 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 07:26:22,937 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 07:26:22,940 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 07:26:22,942 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.8684, 'learning_rate': 2.6270946900868165e-05, 'epoch': 1.03}\n"," 34% 8500/24765 [1:31:00<2:51:56,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 07:31:42,483 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-8500\n","[INFO|configuration_utils.py:441] 2022-04-07 07:31:42,488 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 07:31:43,727 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 07:31:43,730 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 07:31:43,732 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.7306, 'learning_rate': 2.54633555420957e-05, 'epoch': 1.09}\n"," 36% 9000/24765 [1:36:21<2:46:10,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 07:37:03,175 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-9000\n","[INFO|configuration_utils.py:441] 2022-04-07 07:37:03,179 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 07:37:04,371 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 07:37:04,374 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 07:37:04,377 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.7254, 'learning_rate': 2.465576418332324e-05, 'epoch': 1.15}\n"," 38% 9500/24765 [1:41:44<2:41:18,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 07:42:26,577 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-9500\n","[INFO|configuration_utils.py:441] 2022-04-07 07:42:26,582 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 07:42:27,720 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 07:42:27,724 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 07:42:27,727 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.71, 'learning_rate': 2.384817282455078e-05, 'epoch': 1.21}\n"," 40% 10000/24765 [1:47:05<2:35:46,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 07:47:47,054 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-10000\n","[INFO|configuration_utils.py:441] 2022-04-07 07:47:47,059 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 07:47:48,142 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 07:47:48,146 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 07:47:48,149 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.7143, 'learning_rate': 2.3040581465778318e-05, 'epoch': 1.27}\n"," 42% 10500/24765 [1:52:25<2:30:29,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 07:53:07,350 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-10500\n","[INFO|configuration_utils.py:441] 2022-04-07 07:53:07,354 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 07:53:08,430 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 07:53:08,434 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 07:53:08,437 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.7214, 'learning_rate': 2.223299010700586e-05, 'epoch': 1.33}\n"," 44% 11000/24765 [1:57:47<2:25:09,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 07:58:29,793 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-11000\n","[INFO|configuration_utils.py:441] 2022-04-07 07:58:29,797 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 07:58:30,885 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 07:58:30,889 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 07:58:30,891 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.7219, 'learning_rate': 2.1425398748233396e-05, 'epoch': 1.39}\n"," 46% 11500/24765 [2:03:11<2:19:56,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 08:03:53,199 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-11500\n","[INFO|configuration_utils.py:441] 2022-04-07 08:03:53,203 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 08:03:54,277 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 08:03:54,281 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 08:03:54,284 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.7122, 'learning_rate': 2.0617807389460935e-05, 'epoch': 1.45}\n"," 48% 12000/24765 [2:08:31<2:14:44,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 08:09:13,435 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-12000\n","[INFO|configuration_utils.py:441] 2022-04-07 08:09:13,440 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 08:09:14,534 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 08:09:14,537 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 08:09:14,540 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.7131, 'learning_rate': 1.9810216030688474e-05, 'epoch': 1.51}\n"," 50% 12500/24765 [2:13:54<2:09:26,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 08:14:36,626 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-12500\n","[INFO|configuration_utils.py:441] 2022-04-07 08:14:36,630 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 08:14:37,736 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 08:14:37,740 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 08:14:37,743 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.6926, 'learning_rate': 1.9002624671916013e-05, 'epoch': 1.57}\n"," 52% 13000/24765 [2:19:17<2:04:05,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 08:19:59,129 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-13000\n","[INFO|configuration_utils.py:441] 2022-04-07 08:19:59,133 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 08:20:00,217 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 08:20:00,221 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 08:20:00,223 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.7004, 'learning_rate': 1.8195033313143552e-05, 'epoch': 1.64}\n"," 55% 13500/24765 [2:24:37<1:58:52,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 08:25:19,267 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-13500\n","[INFO|configuration_utils.py:441] 2022-04-07 08:25:19,272 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 08:25:20,359 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 08:25:20,363 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 08:25:20,365 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.7217, 'learning_rate': 1.7387441954371088e-05, 'epoch': 1.7}\n"," 57% 14000/24765 [2:29:57<1:53:30,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 08:30:39,380 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-14000\n","[INFO|configuration_utils.py:441] 2022-04-07 08:30:39,385 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 08:30:40,489 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 08:30:40,492 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 08:30:40,494 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.7089, 'learning_rate': 1.6579850595598627e-05, 'epoch': 1.76}\n"," 59% 14500/24765 [2:35:17<1:48:17,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 08:35:59,600 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-14500\n","[INFO|configuration_utils.py:441] 2022-04-07 08:35:59,605 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 08:36:00,705 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 08:36:00,709 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 08:36:00,712 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.6782, 'learning_rate': 1.5772259236826166e-05, 'epoch': 1.82}\n"," 61% 15000/24765 [2:40:37<1:42:51,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 08:41:19,389 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-15000\n","[INFO|configuration_utils.py:441] 2022-04-07 08:41:19,393 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 08:41:20,482 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 08:41:20,485 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 08:41:20,487 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.6897, 'learning_rate': 1.4964667878053707e-05, 'epoch': 1.88}\n"," 63% 15500/24765 [2:45:57<1:37:37,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 08:46:39,280 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-15500\n","[INFO|configuration_utils.py:441] 2022-04-07 08:46:39,286 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 08:46:40,452 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 08:46:40,455 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 08:46:40,458 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.689, 'learning_rate': 1.4157076519281246e-05, 'epoch': 1.94}\n"," 65% 16000/24765 [2:51:17<1:32:17,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 08:51:59,612 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-16000\n","[INFO|configuration_utils.py:441] 2022-04-07 08:51:59,617 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 08:52:00,722 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 08:52:00,742 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 08:52:00,745 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-16000/special_tokens_map.json\n","{'loss': 0.6852, 'learning_rate': 1.3349485160508783e-05, 'epoch': 2.0}\n"," 67% 16500/24765 [2:56:37<1:27:15,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 08:57:19,702 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-16500\n","[INFO|configuration_utils.py:441] 2022-04-07 08:57:19,707 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-16500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 08:57:20,820 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-16500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 08:57:20,823 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-16500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 08:57:20,826 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-16500/special_tokens_map.json\n","{'loss': 0.4272, 'learning_rate': 1.2541893801736322e-05, 'epoch': 2.06}\n"," 69% 17000/24765 [3:01:58<1:21:44,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 09:02:40,257 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-17000\n","[INFO|configuration_utils.py:441] 2022-04-07 09:02:40,262 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-17000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 09:02:41,352 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-17000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 09:02:41,355 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-17000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 09:02:41,357 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-17000/special_tokens_map.json\n","{'loss': 0.4437, 'learning_rate': 1.1734302442963861e-05, 'epoch': 2.12}\n"," 71% 17500/24765 [3:07:18<1:16:36,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 09:08:00,310 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-17500\n","[INFO|configuration_utils.py:441] 2022-04-07 09:08:00,315 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-17500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 09:08:01,586 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-17500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 09:08:01,590 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-17500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 09:08:01,593 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-17500/special_tokens_map.json\n","{'loss': 0.4271, 'learning_rate': 1.0926711084191399e-05, 'epoch': 2.18}\n"," 73% 18000/24765 [3:12:39<1:11:16,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 09:13:21,396 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-18000\n","[INFO|configuration_utils.py:441] 2022-04-07 09:13:21,401 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-18000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 09:13:22,611 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-18000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 09:13:22,614 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-18000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 09:13:22,616 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-18000/special_tokens_map.json\n","{'loss': 0.4152, 'learning_rate': 1.011911972541894e-05, 'epoch': 2.24}\n"," 75% 18500/24765 [3:18:02<1:05:58,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 09:18:44,779 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-18500\n","[INFO|configuration_utils.py:441] 2022-04-07 09:18:44,784 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-18500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 09:18:46,030 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-18500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 09:18:46,033 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-18500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 09:18:46,036 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-18500/special_tokens_map.json\n","{'loss': 0.4285, 'learning_rate': 9.311528366646477e-06, 'epoch': 2.3}\n"," 77% 19000/24765 [3:23:26<1:00:52,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 09:24:08,163 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-19000\n","[INFO|configuration_utils.py:441] 2022-04-07 09:24:08,167 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-19000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 09:24:09,399 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-19000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 09:24:09,402 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-19000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 09:24:09,405 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-19000/special_tokens_map.json\n","{'loss': 0.407, 'learning_rate': 8.503937007874016e-06, 'epoch': 2.36}\n"," 79% 19500/24765 [3:28:47<55:32,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 09:29:28,882 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-19500\n","[INFO|configuration_utils.py:441] 2022-04-07 09:29:28,887 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-19500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 09:29:30,094 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-19500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 09:29:30,097 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-19500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 09:29:30,100 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-19500/special_tokens_map.json\n","{'loss': 0.4294, 'learning_rate': 7.696345649101555e-06, 'epoch': 2.42}\n"," 81% 20000/24765 [3:34:07<50:19,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 09:34:49,537 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-20000\n","[INFO|configuration_utils.py:441] 2022-04-07 09:34:49,542 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-20000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 09:34:50,743 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-20000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 09:34:50,747 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-20000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 09:34:50,750 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-20000/special_tokens_map.json\n","{'loss': 0.4263, 'learning_rate': 6.8887542903290935e-06, 'epoch': 2.48}\n"," 83% 20500/24765 [3:39:28<45:02,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 09:40:10,265 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-20500\n","[INFO|configuration_utils.py:441] 2022-04-07 09:40:10,270 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-20500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 09:40:11,474 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-20500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 09:40:11,478 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-20500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 09:40:11,480 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-20500/special_tokens_map.json\n","{'loss': 0.4206, 'learning_rate': 6.0811629315566326e-06, 'epoch': 2.54}\n"," 85% 21000/24765 [3:44:49<39:48,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 09:45:30,994 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-21000\n","[INFO|configuration_utils.py:441] 2022-04-07 09:45:31,000 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-21000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 09:45:32,212 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-21000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 09:45:32,215 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-21000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 09:45:32,217 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-21000/special_tokens_map.json\n","{'loss': 0.421, 'learning_rate': 5.273571572784172e-06, 'epoch': 2.6}\n"," 87% 21500/24765 [3:50:09<34:27,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 09:50:51,653 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-21500\n","[INFO|configuration_utils.py:441] 2022-04-07 09:50:51,658 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-21500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 09:50:52,912 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-21500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 09:50:52,916 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-21500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 09:50:52,919 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-21500/special_tokens_map.json\n","{'loss': 0.4135, 'learning_rate': 4.46598021401171e-06, 'epoch': 2.67}\n"," 89% 22000/24765 [3:55:30<29:09,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 09:56:12,422 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-22000\n","[INFO|configuration_utils.py:441] 2022-04-07 09:56:12,427 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-22000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 09:56:13,651 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-22000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 09:56:13,655 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-22000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 09:56:13,676 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-22000/special_tokens_map.json\n","{'loss': 0.4171, 'learning_rate': 3.6583888552392494e-06, 'epoch': 2.73}\n"," 91% 22500/24765 [4:00:51<23:54,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 10:01:32,827 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-22500\n","[INFO|configuration_utils.py:441] 2022-04-07 10:01:32,832 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-22500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 10:01:34,042 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-22500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 10:01:34,045 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-22500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 10:01:34,049 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-22500/special_tokens_map.json\n","{'loss': 0.4327, 'learning_rate': 2.8507974964667877e-06, 'epoch': 2.79}\n"," 93% 23000/24765 [4:06:11<18:37,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 10:06:53,694 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-23000\n","[INFO|configuration_utils.py:441] 2022-04-07 10:06:53,698 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-23000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 10:06:54,886 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-23000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 10:06:54,889 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-23000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 10:06:54,892 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-23000/special_tokens_map.json\n","{'loss': 0.4099, 'learning_rate': 2.0432061376943268e-06, 'epoch': 2.85}\n"," 95% 23500/24765 [4:11:34<13:22,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 10:12:16,403 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-23500\n","[INFO|configuration_utils.py:441] 2022-04-07 10:12:16,408 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-23500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 10:12:17,617 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-23500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 10:12:17,620 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-23500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 10:12:17,622 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-23500/special_tokens_map.json\n","{'loss': 0.4075, 'learning_rate': 1.2356147789218657e-06, 'epoch': 2.91}\n"," 97% 24000/24765 [4:16:57<08:04,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 10:17:39,795 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-24000\n","[INFO|configuration_utils.py:441] 2022-04-07 10:17:39,800 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-24000/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 10:17:41,022 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-24000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 10:17:41,025 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-24000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 10:17:41,028 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-24000/special_tokens_map.json\n","{'loss': 0.4042, 'learning_rate': 4.280234201494044e-07, 'epoch': 2.97}\n"," 99% 24500/24765 [4:22:21<02:47,  1.58it/s][INFO|trainer.py:2166] 2022-04-07 10:23:03,250 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-24500\n","[INFO|configuration_utils.py:441] 2022-04-07 10:23:03,254 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-24500/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 10:23:04,441 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-24500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 10:23:04,444 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-24500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 10:23:04,447 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/checkpoint-24500/special_tokens_map.json\n","100% 24765/24765 [4:25:13<00:00,  1.61it/s][INFO|trainer.py:1530] 2022-04-07 10:25:55,294 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 15913.5014, 'train_samples_per_second': 24.899, 'train_steps_per_second': 1.556, 'train_loss': 0.7886591919980434, 'epoch': 3.0}\n","100% 24765/24765 [4:25:13<00:00,  1.56it/s]\n","[INFO|trainer.py:2166] 2022-04-07 10:25:55,302 >> Saving model checkpoint to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2\n","[INFO|configuration_utils.py:441] 2022-04-07 10:25:55,306 >> Configuration saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/config.json\n","[INFO|modeling_utils.py:1378] 2022-04-07 10:25:56,539 >> Model weights saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-04-07 10:25:56,542 >> tokenizer config file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-04-07 10:25:56,545 >> Special tokens file saved in /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =     0.7887\n","  train_runtime            = 4:25:13.50\n","  train_samples            =     132079\n","  train_samples_per_second =     24.899\n","  train_steps_per_second   =      1.556\n","04/07/2022 10:25:56 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:567] 2022-04-07 10:25:56,595 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2416] 2022-04-07 10:25:56,610 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2418] 2022-04-07 10:25:56,610 >>   Num examples = 12199\n","[INFO|trainer.py:2421] 2022-04-07 10:25:56,610 >>   Batch size = 8\n","100% 1525/1525 [02:56<00:00,  8.80it/s]04/07/2022 10:29:05 - INFO - utils_qa - Post-processing 11873 example predictions split into 12199 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 36/11873 [00:00<00:33, 351.71it/s]\u001b[A\n","  1% 72/11873 [00:00<00:33, 354.86it/s]\u001b[A\n","  1% 117/11873 [00:00<00:29, 397.31it/s]\u001b[A\n","  1% 165/11873 [00:00<00:27, 426.65it/s]\u001b[A\n","  2% 209/11873 [00:00<00:27, 429.31it/s]\u001b[A\n","  2% 252/11873 [00:00<00:27, 421.23it/s]\u001b[A\n","  3% 298/11873 [00:00<00:26, 430.58it/s]\u001b[A\n","  3% 342/11873 [00:00<00:26, 427.11it/s]\u001b[A\n","  3% 389/11873 [00:00<00:26, 439.92it/s]\u001b[A\n","  4% 437/11873 [00:01<00:25, 451.96it/s]\u001b[A\n","  4% 484/11873 [00:01<00:24, 455.82it/s]\u001b[A\n","  4% 530/11873 [00:01<00:25, 452.51it/s]\u001b[A\n","  5% 576/11873 [00:01<00:25, 451.53it/s]\u001b[A\n","  5% 625/11873 [00:01<00:24, 460.90it/s]\u001b[A\n","  6% 673/11873 [00:01<00:24, 465.63it/s]\u001b[A\n","  6% 720/11873 [00:01<00:24, 462.42it/s]\u001b[A\n","  6% 767/11873 [00:01<00:24, 458.46it/s]\u001b[A\n","  7% 817/11873 [00:01<00:23, 467.60it/s]\u001b[A\n","  7% 867/11873 [00:01<00:23, 475.80it/s]\u001b[A\n","  8% 918/11873 [00:02<00:22, 483.25it/s]\u001b[A\n","  8% 968/11873 [00:02<00:22, 487.49it/s]\u001b[A\n","  9% 1017/11873 [00:02<00:23, 462.78it/s]\u001b[A\n","  9% 1064/11873 [00:02<00:24, 433.59it/s]\u001b[A\n","  9% 1108/11873 [00:02<00:26, 405.88it/s]\u001b[A\n"," 10% 1150/11873 [00:02<00:27, 389.48it/s]\u001b[A\n"," 10% 1190/11873 [00:02<00:28, 378.52it/s]\u001b[A\n"," 10% 1229/11873 [00:02<00:28, 378.52it/s]\u001b[A\n"," 11% 1268/11873 [00:02<00:28, 376.50it/s]\u001b[A\n"," 11% 1306/11873 [00:03<00:28, 375.88it/s]\u001b[A\n"," 11% 1344/11873 [00:03<00:28, 374.21it/s]\u001b[A\n"," 12% 1382/11873 [00:03<00:28, 366.63it/s]\u001b[A\n"," 12% 1419/11873 [00:03<00:29, 359.60it/s]\u001b[A\n"," 12% 1455/11873 [00:03<00:29, 350.15it/s]\u001b[A\n"," 13% 1491/11873 [00:03<00:30, 341.34it/s]\u001b[A\n"," 13% 1527/11873 [00:03<00:29, 345.47it/s]\u001b[A\n"," 13% 1564/11873 [00:03<00:29, 350.94it/s]\u001b[A\n"," 13% 1602/11873 [00:03<00:28, 358.70it/s]\u001b[A\n"," 14% 1639/11873 [00:04<00:28, 360.41it/s]\u001b[A\n"," 14% 1676/11873 [00:04<00:28, 362.71it/s]\u001b[A\n"," 14% 1713/11873 [00:04<00:28, 352.03it/s]\u001b[A\n"," 15% 1750/11873 [00:04<00:28, 355.75it/s]\u001b[A\n"," 15% 1787/11873 [00:04<00:28, 357.83it/s]\u001b[A\n"," 15% 1823/11873 [00:04<00:28, 357.01it/s]\u001b[A\n"," 16% 1860/11873 [00:04<00:27, 359.31it/s]\u001b[A\n"," 16% 1896/11873 [00:04<00:27, 359.29it/s]\u001b[A\n"," 16% 1933/11873 [00:04<00:27, 362.24it/s]\u001b[A\n"," 17% 1971/11873 [00:04<00:27, 364.98it/s]\u001b[A\n"," 17% 2009/11873 [00:05<00:26, 368.36it/s]\u001b[A\n"," 17% 2046/11873 [00:05<00:27, 363.57it/s]\u001b[A\n"," 18% 2084/11873 [00:05<00:26, 367.19it/s]\u001b[A\n"," 18% 2122/11873 [00:05<00:26, 368.79it/s]\u001b[A\n"," 18% 2160/11873 [00:05<00:26, 371.80it/s]\u001b[A\n"," 19% 2198/11873 [00:05<00:26, 365.63it/s]\u001b[A\n"," 19% 2235/11873 [00:05<00:27, 356.46it/s]\u001b[A\n"," 19% 2271/11873 [00:05<00:26, 356.63it/s]\u001b[A\n"," 19% 2307/11873 [00:05<00:26, 355.45it/s]\u001b[A\n"," 20% 2343/11873 [00:05<00:26, 356.66it/s]\u001b[A\n"," 20% 2380/11873 [00:06<00:26, 358.40it/s]\u001b[A\n"," 20% 2417/11873 [00:06<00:26, 359.05it/s]\u001b[A\n"," 21% 2453/11873 [00:06<00:26, 353.98it/s]\u001b[A\n"," 21% 2490/11873 [00:06<00:26, 358.26it/s]\u001b[A\n"," 21% 2527/11873 [00:06<00:26, 359.30it/s]\u001b[A\n"," 22% 2565/11873 [00:06<00:25, 363.29it/s]\u001b[A\n"," 22% 2602/11873 [00:06<00:26, 355.74it/s]\u001b[A\n"," 22% 2638/11873 [00:06<00:25, 356.87it/s]\u001b[A\n"," 23% 2676/11873 [00:06<00:25, 361.08it/s]\u001b[A\n"," 23% 2713/11873 [00:06<00:25, 357.53it/s]\u001b[A\n"," 23% 2749/11873 [00:07<00:25, 354.44it/s]\u001b[A\n"," 23% 2785/11873 [00:07<00:25, 355.19it/s]\u001b[A\n"," 24% 2823/11873 [00:07<00:25, 361.21it/s]\u001b[A\n"," 24% 2860/11873 [00:07<00:24, 361.62it/s]\u001b[A\n"," 24% 2897/11873 [00:07<00:25, 354.53it/s]\u001b[A\n"," 25% 2933/11873 [00:07<00:25, 353.87it/s]\u001b[A\n"," 25% 2971/11873 [00:07<00:24, 360.19it/s]\u001b[A\n"," 25% 3008/11873 [00:07<00:27, 325.47it/s]\u001b[A\n"," 26% 3045/11873 [00:07<00:26, 336.30it/s]\u001b[A\n"," 26% 3080/11873 [00:08<00:26, 333.88it/s]\u001b[A\n"," 26% 3114/11873 [00:08<00:28, 302.06it/s]\u001b[A\n"," 26% 3145/11873 [00:08<00:31, 277.90it/s]\u001b[A\n"," 27% 3174/11873 [00:08<00:31, 275.75it/s]\u001b[A\n"," 27% 3211/11873 [00:08<00:28, 299.99it/s]\u001b[A\n"," 27% 3247/11873 [00:08<00:27, 315.89it/s]\u001b[A\n"," 28% 3280/11873 [00:08<00:30, 281.75it/s]\u001b[A\n"," 28% 3310/11873 [00:08<00:37, 230.15it/s]\u001b[A\n"," 28% 3336/11873 [00:09<00:40, 212.16it/s]\u001b[A\n"," 28% 3359/11873 [00:09<00:41, 203.60it/s]\u001b[A\n"," 29% 3388/11873 [00:09<00:38, 221.63it/s]\u001b[A\n"," 29% 3426/11873 [00:09<00:32, 261.07it/s]\u001b[A\n"," 29% 3463/11873 [00:09<00:29, 287.87it/s]\u001b[A\n"," 29% 3500/11873 [00:09<00:27, 309.08it/s]\u001b[A\n"," 30% 3538/11873 [00:09<00:25, 328.60it/s]\u001b[A\n"," 30% 3575/11873 [00:09<00:24, 338.65it/s]\u001b[A\n"," 30% 3614/11873 [00:09<00:23, 351.90it/s]\u001b[A\n"," 31% 3651/11873 [00:10<00:23, 354.13it/s]\u001b[A\n"," 31% 3687/11873 [00:10<00:23, 346.90it/s]\u001b[A\n"," 31% 3722/11873 [00:10<00:23, 343.59it/s]\u001b[A\n"," 32% 3757/11873 [00:10<00:23, 343.76it/s]\u001b[A\n"," 32% 3793/11873 [00:10<00:23, 345.99it/s]\u001b[A\n"," 32% 3828/11873 [00:10<00:25, 319.58it/s]\u001b[A\n"," 33% 3863/11873 [00:10<00:24, 325.95it/s]\u001b[A\n"," 33% 3899/11873 [00:10<00:23, 334.81it/s]\u001b[A\n"," 33% 3933/11873 [00:10<00:25, 315.42it/s]\u001b[A\n"," 33% 3965/11873 [00:11<00:25, 313.57it/s]\u001b[A\n"," 34% 4001/11873 [00:11<00:24, 325.41it/s]\u001b[A\n"," 34% 4040/11873 [00:11<00:22, 342.92it/s]\u001b[A\n"," 34% 4077/11873 [00:11<00:22, 344.88it/s]\u001b[A\n"," 35% 4115/11873 [00:11<00:21, 354.54it/s]\u001b[A\n"," 35% 4151/11873 [00:11<00:23, 329.85it/s]\u001b[A\n"," 35% 4185/11873 [00:11<00:23, 326.24it/s]\u001b[A\n"," 36% 4220/11873 [00:11<00:22, 332.78it/s]\u001b[A\n"," 36% 4256/11873 [00:11<00:22, 339.19it/s]\u001b[A\n"," 36% 4293/11873 [00:11<00:21, 345.63it/s]\u001b[A\n"," 36% 4329/11873 [00:12<00:21, 347.52it/s]\u001b[A\n"," 37% 4364/11873 [00:12<00:21, 347.34it/s]\u001b[A\n"," 37% 4399/11873 [00:12<00:21, 344.98it/s]\u001b[A\n"," 37% 4434/11873 [00:12<00:26, 280.53it/s]\u001b[A\n"," 38% 4470/11873 [00:12<00:24, 299.14it/s]\u001b[A\n"," 38% 4504/11873 [00:12<00:23, 308.14it/s]\u001b[A\n"," 38% 4539/11873 [00:12<00:22, 319.05it/s]\u001b[A\n"," 39% 4577/11873 [00:12<00:21, 335.10it/s]\u001b[A\n"," 39% 4615/11873 [00:12<00:20, 346.04it/s]\u001b[A\n"," 39% 4652/11873 [00:13<00:20, 351.91it/s]\u001b[A\n"," 40% 4691/11873 [00:13<00:19, 360.21it/s]\u001b[A\n"," 40% 4728/11873 [00:13<00:19, 361.57it/s]\u001b[A\n"," 40% 4766/11873 [00:13<00:19, 364.08it/s]\u001b[A\n"," 40% 4803/11873 [00:13<00:19, 357.63it/s]\u001b[A\n"," 41% 4839/11873 [00:13<00:19, 355.77it/s]\u001b[A\n"," 41% 4875/11873 [00:13<00:19, 353.34it/s]\u001b[A\n"," 41% 4911/11873 [00:13<00:19, 353.85it/s]\u001b[A\n"," 42% 4949/11873 [00:13<00:19, 359.96it/s]\u001b[A\n"," 42% 4987/11873 [00:14<00:18, 364.68it/s]\u001b[A\n"," 42% 5024/11873 [00:14<00:19, 359.89it/s]\u001b[A\n"," 43% 5061/11873 [00:14<00:18, 359.29it/s]\u001b[A\n"," 43% 5100/11873 [00:14<00:18, 365.41it/s]\u001b[A\n"," 43% 5137/11873 [00:14<00:18, 363.87it/s]\u001b[A\n"," 44% 5174/11873 [00:14<00:18, 362.98it/s]\u001b[A\n"," 44% 5211/11873 [00:14<00:18, 359.56it/s]\u001b[A\n"," 44% 5248/11873 [00:14<00:18, 361.95it/s]\u001b[A\n"," 45% 5285/11873 [00:14<00:19, 336.19it/s]\u001b[A\n"," 45% 5322/11873 [00:14<00:19, 343.21it/s]\u001b[A\n"," 45% 5360/11873 [00:15<00:18, 350.82it/s]\u001b[A\n"," 45% 5396/11873 [00:15<00:18, 350.47it/s]\u001b[A\n"," 46% 5432/11873 [00:15<00:18, 346.96it/s]\u001b[A\n"," 46% 5468/11873 [00:15<00:18, 349.84it/s]\u001b[A\n"," 46% 5504/11873 [00:15<00:18, 348.51it/s]\u001b[A\n"," 47% 5542/11873 [00:15<00:17, 356.55it/s]\u001b[A\n"," 47% 5578/11873 [00:15<00:17, 357.04it/s]\u001b[A\n"," 47% 5614/11873 [00:15<00:17, 357.26it/s]\u001b[A\n"," 48% 5651/11873 [00:15<00:17, 359.85it/s]\u001b[A\n"," 48% 5688/11873 [00:15<00:17, 361.77it/s]\u001b[A\n"," 48% 5725/11873 [00:16<00:16, 363.11it/s]\u001b[A\n"," 49% 5762/11873 [00:16<00:16, 362.42it/s]\u001b[A\n"," 49% 5800/11873 [00:16<00:16, 366.83it/s]\u001b[A\n"," 49% 5839/11873 [00:16<00:16, 370.98it/s]\u001b[A\n"," 49% 5877/11873 [00:16<00:16, 367.60it/s]\u001b[A\n"," 50% 5914/11873 [00:16<00:16, 366.38it/s]\u001b[A\n"," 50% 5951/11873 [00:16<00:16, 365.31it/s]\u001b[A\n"," 50% 5989/11873 [00:16<00:16, 366.97it/s]\u001b[A\n"," 51% 6027/11873 [00:16<00:15, 368.00it/s]\u001b[A\n"," 51% 6064/11873 [00:17<00:16, 362.17it/s]\u001b[A\n"," 51% 6101/11873 [00:17<00:16, 352.35it/s]\u001b[A\n"," 52% 6137/11873 [00:17<00:16, 344.47it/s]\u001b[A\n"," 52% 6174/11873 [00:17<00:16, 351.73it/s]\u001b[A\n"," 52% 6211/11873 [00:17<00:15, 354.49it/s]\u001b[A\n"," 53% 6249/11873 [00:17<00:15, 361.45it/s]\u001b[A\n"," 53% 6286/11873 [00:17<00:15, 358.23it/s]\u001b[A\n"," 53% 6323/11873 [00:17<00:15, 360.44it/s]\u001b[A\n"," 54% 6361/11873 [00:17<00:15, 365.26it/s]\u001b[A\n"," 54% 6399/11873 [00:17<00:14, 366.98it/s]\u001b[A\n"," 54% 6436/11873 [00:18<00:14, 362.63it/s]\u001b[A\n"," 55% 6473/11873 [00:18<00:15, 357.94it/s]\u001b[A\n"," 55% 6510/11873 [00:18<00:14, 359.32it/s]\u001b[A\n"," 55% 6548/11873 [00:18<00:14, 364.03it/s]\u001b[A\n"," 55% 6585/11873 [00:18<00:14, 364.87it/s]\u001b[A\n"," 56% 6623/11873 [00:18<00:14, 367.27it/s]\u001b[A\n"," 56% 6660/11873 [00:18<00:14, 367.43it/s]\u001b[A\n"," 56% 6697/11873 [00:18<00:14, 365.45it/s]\u001b[A\n"," 57% 6734/11873 [00:18<00:15, 327.39it/s]\u001b[A\n"," 57% 6771/11873 [00:19<00:15, 338.51it/s]\u001b[A\n"," 57% 6808/11873 [00:19<00:14, 346.59it/s]\u001b[A\n"," 58% 6846/11873 [00:19<00:14, 355.44it/s]\u001b[A\n"," 58% 6883/11873 [00:19<00:13, 357.32it/s]\u001b[A\n"," 58% 6920/11873 [00:19<00:13, 360.09it/s]\u001b[A\n"," 59% 6958/11873 [00:19<00:13, 364.06it/s]\u001b[A\n"," 59% 6995/11873 [00:19<00:13, 363.29it/s]\u001b[A\n"," 59% 7032/11873 [00:19<00:13, 364.30it/s]\u001b[A\n"," 60% 7069/11873 [00:19<00:13, 364.50it/s]\u001b[A\n"," 60% 7107/11873 [00:19<00:12, 367.28it/s]\u001b[A\n"," 60% 7144/11873 [00:20<00:12, 364.02it/s]\u001b[A\n"," 60% 7181/11873 [00:20<00:12, 365.28it/s]\u001b[A\n"," 61% 7218/11873 [00:20<00:12, 360.67it/s]\u001b[A\n"," 61% 7256/11873 [00:20<00:12, 363.71it/s]\u001b[A\n"," 61% 7293/11873 [00:20<00:12, 362.76it/s]\u001b[A\n"," 62% 7330/11873 [00:20<00:12, 360.30it/s]\u001b[A\n"," 62% 7368/11873 [00:20<00:12, 364.75it/s]\u001b[A\n"," 62% 7405/11873 [00:20<00:12, 357.47it/s]\u001b[A\n"," 63% 7441/11873 [00:20<00:13, 337.01it/s]\u001b[A\n"," 63% 7479/11873 [00:20<00:12, 348.43it/s]\u001b[A\n"," 63% 7516/11873 [00:21<00:12, 352.11it/s]\u001b[A\n"," 64% 7552/11873 [00:21<00:12, 349.81it/s]\u001b[A\n"," 64% 7589/11873 [00:21<00:12, 354.47it/s]\u001b[A\n"," 64% 7625/11873 [00:21<00:12, 347.26it/s]\u001b[A\n"," 65% 7661/11873 [00:21<00:12, 350.88it/s]\u001b[A\n"," 65% 7698/11873 [00:21<00:11, 354.91it/s]\u001b[A\n"," 65% 7734/11873 [00:21<00:12, 331.03it/s]\u001b[A\n"," 65% 7769/11873 [00:21<00:12, 335.03it/s]\u001b[A\n"," 66% 7807/11873 [00:21<00:11, 345.23it/s]\u001b[A\n"," 66% 7843/11873 [00:22<00:11, 347.14it/s]\u001b[A\n"," 66% 7878/11873 [00:22<00:12, 330.72it/s]\u001b[A\n"," 67% 7913/11873 [00:22<00:11, 333.53it/s]\u001b[A\n"," 67% 7950/11873 [00:22<00:11, 343.19it/s]\u001b[A\n"," 67% 7985/11873 [00:22<00:11, 344.22it/s]\u001b[A\n"," 68% 8021/11873 [00:22<00:11, 347.32it/s]\u001b[A\n"," 68% 8058/11873 [00:22<00:10, 352.93it/s]\u001b[A\n"," 68% 8096/11873 [00:22<00:10, 358.56it/s]\u001b[A\n"," 68% 8133/11873 [00:22<00:10, 359.26it/s]\u001b[A\n"," 69% 8170/11873 [00:22<00:10, 362.11it/s]\u001b[A\n"," 69% 8207/11873 [00:23<00:10, 357.43it/s]\u001b[A\n"," 69% 8244/11873 [00:23<00:10, 361.02it/s]\u001b[A\n"," 70% 8282/11873 [00:23<00:09, 365.81it/s]\u001b[A\n"," 70% 8320/11873 [00:23<00:09, 367.04it/s]\u001b[A\n"," 70% 8357/11873 [00:23<00:09, 367.18it/s]\u001b[A\n"," 71% 8394/11873 [00:23<00:09, 362.17it/s]\u001b[A\n"," 71% 8431/11873 [00:23<00:09, 358.56it/s]\u001b[A\n"," 71% 8467/11873 [00:23<00:09, 347.82it/s]\u001b[A\n"," 72% 8504/11873 [00:23<00:09, 353.21it/s]\u001b[A\n"," 72% 8541/11873 [00:23<00:09, 356.53it/s]\u001b[A\n"," 72% 8578/11873 [00:24<00:09, 358.24it/s]\u001b[A\n"," 73% 8614/11873 [00:24<00:09, 352.81it/s]\u001b[A\n"," 73% 8650/11873 [00:24<00:09, 346.29it/s]\u001b[A\n"," 73% 8685/11873 [00:24<00:09, 345.00it/s]\u001b[A\n"," 73% 8720/11873 [00:24<00:09, 342.47it/s]\u001b[A\n"," 74% 8755/11873 [00:24<00:09, 343.78it/s]\u001b[A\n"," 74% 8791/11873 [00:24<00:08, 347.25it/s]\u001b[A\n"," 74% 8828/11873 [00:24<00:08, 353.67it/s]\u001b[A\n"," 75% 8866/11873 [00:24<00:08, 359.93it/s]\u001b[A\n"," 75% 8904/11873 [00:25<00:08, 364.57it/s]\u001b[A\n"," 75% 8942/11873 [00:25<00:08, 366.14it/s]\u001b[A\n"," 76% 8979/11873 [00:25<00:07, 364.75it/s]\u001b[A\n"," 76% 9016/11873 [00:25<00:07, 364.88it/s]\u001b[A\n"," 76% 9053/11873 [00:25<00:07, 361.64it/s]\u001b[A\n"," 77% 9090/11873 [00:25<00:07, 363.54it/s]\u001b[A\n"," 77% 9127/11873 [00:25<00:07, 365.11it/s]\u001b[A\n"," 77% 9164/11873 [00:25<00:07, 360.04it/s]\u001b[A\n"," 78% 9202/11873 [00:25<00:07, 363.03it/s]\u001b[A\n"," 78% 9240/11873 [00:25<00:07, 365.75it/s]\u001b[A\n"," 78% 9279/11873 [00:26<00:07, 370.35it/s]\u001b[A\n"," 78% 9317/11873 [00:26<00:06, 370.95it/s]\u001b[A\n"," 79% 9355/11873 [00:26<00:07, 352.00it/s]\u001b[A\n"," 79% 9391/11873 [00:26<00:07, 349.02it/s]\u001b[A\n"," 79% 9427/11873 [00:26<00:07, 343.78it/s]\u001b[A\n"," 80% 9462/11873 [00:26<00:07, 340.55it/s]\u001b[A\n"," 80% 9497/11873 [00:26<00:06, 341.59it/s]\u001b[A\n"," 80% 9533/11873 [00:26<00:06, 345.96it/s]\u001b[A\n"," 81% 9570/11873 [00:26<00:06, 352.20it/s]\u001b[A\n"," 81% 9608/11873 [00:26<00:06, 357.70it/s]\u001b[A\n"," 81% 9646/11873 [00:27<00:06, 362.96it/s]\u001b[A\n"," 82% 9685/11873 [00:27<00:05, 368.90it/s]\u001b[A\n"," 82% 9723/11873 [00:27<00:05, 369.85it/s]\u001b[A\n"," 82% 9761/11873 [00:27<00:05, 372.09it/s]\u001b[A\n"," 83% 9799/11873 [00:27<00:05, 370.86it/s]\u001b[A\n"," 83% 9837/11873 [00:27<00:05, 359.98it/s]\u001b[A\n"," 83% 9875/11873 [00:27<00:05, 363.51it/s]\u001b[A\n"," 83% 9913/11873 [00:27<00:05, 365.65it/s]\u001b[A\n"," 84% 9951/11873 [00:27<00:05, 369.02it/s]\u001b[A\n"," 84% 9989/11873 [00:28<00:05, 371.99it/s]\u001b[A\n"," 84% 10027/11873 [00:28<00:05, 364.56it/s]\u001b[A\n"," 85% 10064/11873 [00:28<00:05, 359.52it/s]\u001b[A\n"," 85% 10100/11873 [00:28<00:04, 355.24it/s]\u001b[A\n"," 85% 10136/11873 [00:28<00:05, 344.66it/s]\u001b[A\n"," 86% 10171/11873 [00:28<00:04, 345.38it/s]\u001b[A\n"," 86% 10206/11873 [00:28<00:04, 344.55it/s]\u001b[A\n"," 86% 10242/11873 [00:28<00:04, 347.03it/s]\u001b[A\n"," 87% 10277/11873 [00:28<00:04, 344.97it/s]\u001b[A\n"," 87% 10313/11873 [00:28<00:04, 348.80it/s]\u001b[A\n"," 87% 10348/11873 [00:29<00:06, 244.11it/s]\u001b[A\n"," 87% 10383/11873 [00:29<00:05, 268.03it/s]\u001b[A\n"," 88% 10419/11873 [00:29<00:05, 289.69it/s]\u001b[A\n"," 88% 10452/11873 [00:29<00:04, 291.30it/s]\u001b[A\n"," 88% 10488/11873 [00:29<00:04, 308.96it/s]\u001b[A\n"," 89% 10526/11873 [00:29<00:04, 326.43it/s]\u001b[A\n"," 89% 10561/11873 [00:29<00:04, 319.80it/s]\u001b[A\n"," 89% 10598/11873 [00:29<00:03, 333.70it/s]\u001b[A\n"," 90% 10633/11873 [00:30<00:03, 335.57it/s]\u001b[A\n"," 90% 10669/11873 [00:30<00:03, 341.06it/s]\u001b[A\n"," 90% 10706/11873 [00:30<00:03, 347.41it/s]\u001b[A\n"," 90% 10742/11873 [00:30<00:03, 350.74it/s]\u001b[A\n"," 91% 10778/11873 [00:30<00:03, 352.94it/s]\u001b[A\n"," 91% 10814/11873 [00:30<00:02, 354.57it/s]\u001b[A\n"," 91% 10850/11873 [00:30<00:03, 329.95it/s]\u001b[A\n"," 92% 10884/11873 [00:30<00:02, 332.63it/s]\u001b[A\n"," 92% 10918/11873 [00:30<00:02, 334.08it/s]\u001b[A\n"," 92% 10953/11873 [00:30<00:02, 338.52it/s]\u001b[A\n"," 93% 10991/11873 [00:31<00:02, 349.28it/s]\u001b[A\n"," 93% 11028/11873 [00:31<00:02, 352.89it/s]\u001b[A\n"," 93% 11066/11873 [00:31<00:02, 360.58it/s]\u001b[A\n"," 94% 11104/11873 [00:31<00:02, 365.46it/s]\u001b[A\n"," 94% 11141/11873 [00:31<00:02, 359.42it/s]\u001b[A\n"," 94% 11179/11873 [00:31<00:01, 364.36it/s]\u001b[A\n"," 94% 11216/11873 [00:31<00:01, 362.53it/s]\u001b[A\n"," 95% 11253/11873 [00:31<00:01, 356.40it/s]\u001b[A\n"," 95% 11290/11873 [00:31<00:01, 359.77it/s]\u001b[A\n"," 95% 11328/11873 [00:31<00:01, 365.25it/s]\u001b[A\n"," 96% 11365/11873 [00:32<00:01, 362.06it/s]\u001b[A\n"," 96% 11403/11873 [00:32<00:01, 365.39it/s]\u001b[A\n"," 96% 11440/11873 [00:32<00:01, 355.35it/s]\u001b[A\n"," 97% 11476/11873 [00:32<00:01, 351.95it/s]\u001b[A\n"," 97% 11512/11873 [00:32<00:01, 348.80it/s]\u001b[A\n"," 97% 11547/11873 [00:32<00:00, 343.75it/s]\u001b[A\n"," 98% 11582/11873 [00:32<00:00, 344.32it/s]\u001b[A\n"," 98% 11620/11873 [00:32<00:00, 353.37it/s]\u001b[A\n"," 98% 11657/11873 [00:32<00:00, 357.42it/s]\u001b[A\n"," 98% 11693/11873 [00:33<00:00, 356.89it/s]\u001b[A\n"," 99% 11729/11873 [00:33<00:00, 357.62it/s]\u001b[A\n"," 99% 11767/11873 [00:33<00:00, 361.66it/s]\u001b[A\n"," 99% 11806/11873 [00:33<00:00, 366.80it/s]\u001b[A\n","100% 11873/11873 [00:33<00:00, 354.22it/s]\n","04/07/2022 10:29:39 - INFO - utils_qa - Saving predictions to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/eval_predictions.json.\n","04/07/2022 10:29:39 - INFO - utils_qa - Saving nbest_preds to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/eval_nbest_predictions.json.\n","04/07/2022 10:29:41 - INFO - utils_qa - Saving null_odds to /content/drive/MyDrive/QA/model_results/bert-base-case/squad_v2/eval_null_odds.json.\n","04/07/2022 10:29:44 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1525/1525 [03:47<00:00,  6.70it/s]\n","***** eval metrics *****\n","  epoch                  =     3.0\n","  eval_HasAns_exact      = 72.5371\n","  eval_HasAns_f1         =  79.337\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 72.5652\n","  eval_NoAns_f1          = 72.5652\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 72.5512\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 75.9462\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 72.5512\n","  eval_f1                = 75.9462\n","  eval_samples           =   12199\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-04-07 10:29:44,808 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path bert-base-cased \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 4 \\\n","  --max_seq_length 384 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GyCOKXUWCeuj","executionInfo":{"status":"ok","timestamp":1648417787560,"user_tz":240,"elapsed":10088472,"user":{"displayName":"SICHEN ZHONG","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiGziZ-Uu-PyjGuHsDy1aSyMZvJYEN6bO8qLBQyKw=s64","userId":"08427994781088390833"}},"outputId":"b7dc3f76-ab51-4c7c-e765-e5f6bf4d1a0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["03/27/2022 15:42:25 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","03/27/2022 15:42:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Mar27_15-42-24_337df09552b5,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=4.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","03/27/2022 15:42:25 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmph2a7rx7e\n","Downloading builder script: 5.28kB [00:00, 6.86MB/s]       \n","03/27/2022 15:42:25 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/27/2022 15:42:25 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/27/2022 15:42:25 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpmb0hf38g\n","Downloading metadata: 2.40kB [00:00, 2.93MB/s]       \n","03/27/2022 15:42:25 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/27/2022 15:42:25 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/27/2022 15:42:26 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","03/27/2022 15:42:26 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/27/2022 15:42:26 - INFO - datasets.builder - Generating dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","03/27/2022 15:42:26 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]03/27/2022 15:42:27 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpw5js8x6u\n","\n","Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n","Downloading data:  78% 7.43M/9.55M [00:00<00:00, 74.3MB/s]\u001b[A\n","Downloading data: 15.6MB [00:00, 78.4MB/s]                \u001b[A\n","Downloading data: 23.4MB [00:00, 77.9MB/s]\u001b[A\n","Downloading data: 31.3MB [00:00, 78.2MB/s]\u001b[A\n","Downloading data: 42.1MB [00:00, 78.0MB/s]\n","03/27/2022 15:42:27 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","03/27/2022 15:42:27 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","Downloading data files:  50% 1/2 [00:01<00:01,  1.57s/it]03/27/2022 15:42:27 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp17vezarb\n","\n","Downloading data: 4.37MB [00:00, 83.2MB/s]      \n","03/27/2022 15:42:28 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","03/27/2022 15:42:28 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","Downloading data files: 100% 2/2 [00:01<00:00,  1.08it/s]\n","03/27/2022 15:42:28 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","03/27/2022 15:42:28 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1186.68it/s]\n","03/27/2022 15:42:28 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n","03/27/2022 15:42:28 - INFO - datasets.builder - Generating train split\n","03/27/2022 15:42:41 - INFO - datasets.builder - Generating validation split\n","03/27/2022 15:42:42 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 260.36it/s]\n","[INFO|hub.py:583] 2022-03-27 15:42:42,865 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps6112uvk\n","Downloading: 100% 570/570 [00:00<00:00, 535kB/s]\n","[INFO|hub.py:587] 2022-03-27 15:42:43,210 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|hub.py:595] 2022-03-27 15:42:43,210 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:653] 2022-03-27 15:42:43,210 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-27 15:42:43,211 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-03-27 15:42:43,558 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4zfwaefc\n","Downloading: 100% 29.0/29.0 [00:00<00:00, 36.0kB/s]\n","[INFO|hub.py:587] 2022-03-27 15:42:43,902 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|hub.py:595] 2022-03-27 15:42:43,902 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:653] 2022-03-27 15:42:44,251 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-27 15:42:44,252 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-03-27 15:42:44,939 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpyshfiiod\n","Downloading: 100% 208k/208k [00:00<00:00, 656kB/s]\n","[INFO|hub.py:587] 2022-03-27 15:42:45,612 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:595] 2022-03-27 15:42:45,613 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:583] 2022-03-27 15:42:45,960 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpuef72ihw\n","Downloading: 100% 426k/426k [00:00<00:00, 1.06MB/s]\n","[INFO|hub.py:587] 2022-03-27 15:42:46,726 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|hub.py:595] 2022-03-27 15:42:46,726 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1777] 2022-03-27 15:42:47,815 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1777] 2022-03-27 15:42:47,815 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1777] 2022-03-27 15:42:47,815 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1777] 2022-03-27 15:42:47,815 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1777] 2022-03-27 15:42:47,816 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:653] 2022-03-27 15:42:48,161 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-27 15:42:48,162 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-03-27 15:42:48,564 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn404ff7d\n","Downloading: 100% 416M/416M [00:08<00:00, 50.4MB/s]\n","[INFO|hub.py:587] 2022-03-27 15:42:57,289 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|hub.py:595] 2022-03-27 15:42:57,289 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|modeling_utils.py:1771] 2022-03-27 15:42:57,290 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:2049] 2022-03-27 15:42:59,723 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-03-27 15:42:59,723 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]03/27/2022 15:43:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-538cb096a17d202c.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:48<00:00,  2.68ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]03/27/2022 15:43:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-cc2890595de471da.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:12<00:00,  6.08s/ba]\n","03/27/2022 15:45:02 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmphzruy9na\n","Downloading builder script: 6.46kB [00:00, 8.53MB/s]       \n","03/27/2022 15:45:02 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/27/2022 15:45:02 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/27/2022 15:45:02 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp8wq40qnl\n","Downloading extra modules: 11.3kB [00:00, 11.2MB/s]       \n","03/27/2022 15:45:02 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","03/27/2022 15:45:02 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1288] 2022-03-27 15:45:13,864 >> ***** Running training *****\n","[INFO|trainer.py:1289] 2022-03-27 15:45:13,865 >>   Num examples = 132079\n","[INFO|trainer.py:1290] 2022-03-27 15:45:13,865 >>   Num Epochs = 4\n","[INFO|trainer.py:1291] 2022-03-27 15:45:13,865 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1292] 2022-03-27 15:45:13,865 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1293] 2022-03-27 15:45:13,865 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1294] 2022-03-27 15:45:13,865 >>   Total optimization steps = 33020\n","{'loss': 2.1381, 'learning_rate': 3.939430648092066e-05, 'epoch': 0.06}\n","  2% 500/33020 [05:24<5:51:48,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 15:50:38,367 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:440] 2022-03-27 15:50:38,368 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 15:50:39,242 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 15:50:39,242 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 15:50:39,243 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.5595, 'learning_rate': 3.878861296184131e-05, 'epoch': 0.12}\n","  3% 1000/33020 [10:51<5:45:31,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 15:56:05,371 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:440] 2022-03-27 15:56:05,372 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 15:56:06,221 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 15:56:06,222 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 15:56:06,222 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.4264, 'learning_rate': 3.8182919442761965e-05, 'epoch': 0.18}\n","  5% 1500/33020 [16:18<5:40:38,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 16:01:32,411 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:440] 2022-03-27 16:01:32,412 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 16:01:33,277 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 16:01:33,278 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 16:01:33,278 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.3383, 'learning_rate': 3.7577225923682624e-05, 'epoch': 0.24}\n","  6% 2000/33020 [21:45<5:34:56,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 16:06:59,437 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:440] 2022-03-27 16:06:59,438 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 16:07:00,287 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 16:07:00,288 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 16:07:00,288 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.2997, 'learning_rate': 3.697153240460327e-05, 'epoch': 0.3}\n","  8% 2500/33020 [27:12<5:30:44,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 16:12:26,502 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:440] 2022-03-27 16:12:26,504 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 16:12:27,369 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 16:12:27,369 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 16:12:27,370 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.2528, 'learning_rate': 3.636583888552393e-05, 'epoch': 0.36}\n","  9% 3000/33020 [32:39<5:24:14,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 16:17:53,699 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:440] 2022-03-27 16:17:53,700 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 16:17:54,549 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 16:17:54,549 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 16:17:54,549 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.207, 'learning_rate': 3.5760145366444586e-05, 'epoch': 0.42}\n"," 11% 3500/33020 [38:06<5:18:14,  1.55it/s][INFO|trainer.py:2162] 2022-03-27 16:23:20,825 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:440] 2022-03-27 16:23:20,826 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 16:23:21,666 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 16:23:21,666 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 16:23:21,667 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.1719, 'learning_rate': 3.515445184736523e-05, 'epoch': 0.48}\n"," 12% 4000/33020 [43:34<5:14:02,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 16:28:48,058 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:440] 2022-03-27 16:28:48,059 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 16:28:48,934 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 16:28:48,935 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 16:28:48,935 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.1557, 'learning_rate': 3.454875832828589e-05, 'epoch': 0.55}\n"," 14% 4500/33020 [49:01<5:09:05,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 16:34:15,401 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:440] 2022-03-27 16:34:15,402 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 16:34:16,275 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 16:34:16,275 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 16:34:16,276 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.1252, 'learning_rate': 3.394306480920655e-05, 'epoch': 0.61}\n"," 15% 5000/33020 [54:29<5:03:15,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 16:39:42,890 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:440] 2022-03-27 16:39:42,891 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 16:39:43,751 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 16:39:43,752 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 16:39:43,752 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.1058, 'learning_rate': 3.333737129012719e-05, 'epoch': 0.67}\n"," 17% 5500/33020 [59:56<4:57:44,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 16:45:10,302 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:440] 2022-03-27 16:45:10,303 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 16:45:11,158 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 16:45:11,158 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 16:45:11,159 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.056, 'learning_rate': 3.273167777104785e-05, 'epoch': 0.73}\n"," 18% 6000/33020 [1:05:24<4:52:47,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 16:50:38,056 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:440] 2022-03-27 16:50:38,057 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 16:50:38,922 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 16:50:38,923 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 16:50:38,923 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.0829, 'learning_rate': 3.212598425196851e-05, 'epoch': 0.79}\n"," 20% 6500/33020 [1:10:52<4:46:57,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 16:56:05,989 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:440] 2022-03-27 16:56:05,990 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 16:56:06,869 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 16:56:06,870 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 16:56:06,870 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.0448, 'learning_rate': 3.152029073288916e-05, 'epoch': 0.85}\n"," 21% 7000/33020 [1:16:20<4:41:31,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 17:01:33,929 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:440] 2022-03-27 17:01:33,930 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 17:01:34,811 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 17:01:34,812 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 17:01:34,812 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.0502, 'learning_rate': 3.0914597213809814e-05, 'epoch': 0.91}\n"," 23% 7500/33020 [1:21:47<4:35:35,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 17:07:01,865 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:440] 2022-03-27 17:07:01,866 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 17:07:02,743 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 17:07:02,744 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 17:07:02,744 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 0.9997, 'learning_rate': 3.030890369473047e-05, 'epoch': 0.97}\n"," 24% 8000/33020 [1:27:15<4:31:12,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 17:12:29,761 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:440] 2022-03-27 17:12:29,762 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 17:12:30,646 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 17:12:30,647 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 17:12:30,647 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.8791, 'learning_rate': 2.9703210175651124e-05, 'epoch': 1.03}\n"," 26% 8500/33020 [1:32:43<4:25:22,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 17:17:57,663 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:440] 2022-03-27 17:17:57,664 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 17:17:58,550 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 17:17:58,550 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 17:17:58,551 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.7373, 'learning_rate': 2.9097516656571776e-05, 'epoch': 1.09}\n"," 27% 9000/33020 [1:38:11<4:19:16,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 17:23:25,276 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:440] 2022-03-27 17:23:25,277 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 17:23:26,150 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 17:23:26,151 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 17:23:26,151 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.7337, 'learning_rate': 2.849182313749243e-05, 'epoch': 1.15}\n"," 29% 9500/33020 [1:43:38<4:14:36,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 17:28:52,846 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:440] 2022-03-27 17:28:52,847 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 17:28:53,630 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 17:28:53,631 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 17:28:53,631 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.7221, 'learning_rate': 2.7886129618413086e-05, 'epoch': 1.21}\n"," 30% 10000/33020 [1:49:06<4:09:44,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 17:34:20,685 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:440] 2022-03-27 17:34:20,687 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 17:34:21,468 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 17:34:21,468 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 17:34:21,469 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.7257, 'learning_rate': 2.7280436099333738e-05, 'epoch': 1.27}\n"," 32% 10500/33020 [1:54:34<4:04:00,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 17:39:48,064 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:440] 2022-03-27 17:39:48,065 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 17:39:48,850 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 17:39:48,851 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 17:39:48,851 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.738, 'learning_rate': 2.6674742580254393e-05, 'epoch': 1.33}\n"," 33% 11000/33020 [2:00:01<3:58:33,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 17:45:15,723 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11000\n","[INFO|configuration_utils.py:440] 2022-03-27 17:45:15,724 >> Configuration saved in /tmp/debug_squad/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 17:45:16,521 >> Model weights saved in /tmp/debug_squad/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 17:45:16,522 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 17:45:16,522 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.7419, 'learning_rate': 2.606904906117505e-05, 'epoch': 1.39}\n"," 35% 11500/33020 [2:05:29<3:53:48,  1.53it/s][INFO|trainer.py:2162] 2022-03-27 17:50:43,236 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11500\n","[INFO|configuration_utils.py:440] 2022-03-27 17:50:43,237 >> Configuration saved in /tmp/debug_squad/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 17:50:44,033 >> Model weights saved in /tmp/debug_squad/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 17:50:44,034 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 17:50:44,034 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.7388, 'learning_rate': 2.54633555420957e-05, 'epoch': 1.45}\n"," 36% 12000/33020 [2:10:57<3:48:25,  1.53it/s][INFO|trainer.py:2162] 2022-03-27 17:56:11,026 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12000\n","[INFO|configuration_utils.py:440] 2022-03-27 17:56:11,027 >> Configuration saved in /tmp/debug_squad/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 17:56:11,821 >> Model weights saved in /tmp/debug_squad/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 17:56:11,821 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 17:56:11,822 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.733, 'learning_rate': 2.4857662023016355e-05, 'epoch': 1.51}\n"," 38% 12500/33020 [2:16:24<3:42:18,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 18:01:38,726 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12500\n","[INFO|configuration_utils.py:440] 2022-03-27 18:01:38,727 >> Configuration saved in /tmp/debug_squad/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 18:01:39,506 >> Model weights saved in /tmp/debug_squad/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 18:01:39,507 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 18:01:39,507 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.7232, 'learning_rate': 2.425196850393701e-05, 'epoch': 1.57}\n"," 39% 13000/33020 [2:21:52<3:36:49,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 18:07:06,088 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13000\n","[INFO|configuration_utils.py:440] 2022-03-27 18:07:06,089 >> Configuration saved in /tmp/debug_squad/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 18:07:06,885 >> Model weights saved in /tmp/debug_squad/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 18:07:06,886 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 18:07:06,886 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.7183, 'learning_rate': 2.3646274984857662e-05, 'epoch': 1.64}\n"," 41% 13500/33020 [2:27:19<3:31:00,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 18:12:33,398 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13500\n","[INFO|configuration_utils.py:440] 2022-03-27 18:12:33,399 >> Configuration saved in /tmp/debug_squad/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 18:12:34,174 >> Model weights saved in /tmp/debug_squad/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 18:12:34,174 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 18:12:34,174 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.7456, 'learning_rate': 2.3040581465778318e-05, 'epoch': 1.7}\n"," 42% 14000/33020 [2:32:46<3:25:54,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 18:18:00,341 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14000\n","[INFO|configuration_utils.py:440] 2022-03-27 18:18:00,342 >> Configuration saved in /tmp/debug_squad/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 18:18:01,117 >> Model weights saved in /tmp/debug_squad/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 18:18:01,118 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 18:18:01,118 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.7354, 'learning_rate': 2.2434887946698973e-05, 'epoch': 1.76}\n"," 44% 14500/33020 [2:38:13<3:20:30,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 18:23:27,757 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14500\n","[INFO|configuration_utils.py:440] 2022-03-27 18:23:27,758 >> Configuration saved in /tmp/debug_squad/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 18:23:28,550 >> Model weights saved in /tmp/debug_squad/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 18:23:28,551 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 18:23:28,551 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.7134, 'learning_rate': 2.1829194427619624e-05, 'epoch': 1.82}\n"," 45% 15000/33020 [2:43:41<3:15:16,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 18:28:55,017 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15000\n","[INFO|configuration_utils.py:440] 2022-03-27 18:28:55,018 >> Configuration saved in /tmp/debug_squad/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 18:28:55,827 >> Model weights saved in /tmp/debug_squad/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 18:28:55,828 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 18:28:55,828 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.7174, 'learning_rate': 2.122350090854028e-05, 'epoch': 1.88}\n"," 47% 15500/33020 [2:49:08<3:10:00,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 18:34:22,732 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15500\n","[INFO|configuration_utils.py:440] 2022-03-27 18:34:22,734 >> Configuration saved in /tmp/debug_squad/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 18:34:23,539 >> Model weights saved in /tmp/debug_squad/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 18:34:23,540 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 18:34:23,540 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.7229, 'learning_rate': 2.0617807389460935e-05, 'epoch': 1.94}\n"," 48% 16000/33020 [2:54:36<3:04:03,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 18:39:50,274 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-16000\n","[INFO|configuration_utils.py:440] 2022-03-27 18:39:50,275 >> Configuration saved in /tmp/debug_squad/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 18:39:51,055 >> Model weights saved in /tmp/debug_squad/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 18:39:51,055 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 18:39:51,056 >> Special tokens file saved in /tmp/debug_squad/checkpoint-16000/special_tokens_map.json\n","{'loss': 0.718, 'learning_rate': 2.0012113870381587e-05, 'epoch': 2.0}\n"," 50% 16500/33020 [3:00:04<2:58:47,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 18:45:18,017 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-16500\n","[INFO|configuration_utils.py:440] 2022-03-27 18:45:18,019 >> Configuration saved in /tmp/debug_squad/checkpoint-16500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 18:45:18,820 >> Model weights saved in /tmp/debug_squad/checkpoint-16500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 18:45:18,821 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-16500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 18:45:18,821 >> Special tokens file saved in /tmp/debug_squad/checkpoint-16500/special_tokens_map.json\n","{'loss': 0.4498, 'learning_rate': 1.9406420351302242e-05, 'epoch': 2.06}\n"," 51% 17000/33020 [3:05:31<2:53:44,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 18:50:45,717 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-17000\n","[INFO|configuration_utils.py:440] 2022-03-27 18:50:45,718 >> Configuration saved in /tmp/debug_squad/checkpoint-17000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 18:50:46,514 >> Model weights saved in /tmp/debug_squad/checkpoint-17000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 18:50:46,515 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-17000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 18:50:46,515 >> Special tokens file saved in /tmp/debug_squad/checkpoint-17000/special_tokens_map.json\n","{'loss': 0.4586, 'learning_rate': 1.8800726832222897e-05, 'epoch': 2.12}\n"," 53% 17500/33020 [3:10:59<2:47:56,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 18:56:13,295 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-17500\n","[INFO|configuration_utils.py:440] 2022-03-27 18:56:13,296 >> Configuration saved in /tmp/debug_squad/checkpoint-17500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 18:56:14,087 >> Model weights saved in /tmp/debug_squad/checkpoint-17500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 18:56:14,088 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-17500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 18:56:14,088 >> Special tokens file saved in /tmp/debug_squad/checkpoint-17500/special_tokens_map.json\n","{'loss': 0.4479, 'learning_rate': 1.8195033313143552e-05, 'epoch': 2.18}\n"," 55% 18000/33020 [3:16:26<2:42:27,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 19:01:40,648 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-18000\n","[INFO|configuration_utils.py:440] 2022-03-27 19:01:40,649 >> Configuration saved in /tmp/debug_squad/checkpoint-18000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 19:01:41,440 >> Model weights saved in /tmp/debug_squad/checkpoint-18000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 19:01:41,440 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-18000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 19:01:41,441 >> Special tokens file saved in /tmp/debug_squad/checkpoint-18000/special_tokens_map.json\n","{'loss': 0.4331, 'learning_rate': 1.7589339794064204e-05, 'epoch': 2.24}\n"," 56% 18500/33020 [3:21:54<2:36:57,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 19:07:07,887 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-18500\n","[INFO|configuration_utils.py:440] 2022-03-27 19:07:07,888 >> Configuration saved in /tmp/debug_squad/checkpoint-18500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 19:07:08,677 >> Model weights saved in /tmp/debug_squad/checkpoint-18500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 19:07:08,678 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-18500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 19:07:08,678 >> Special tokens file saved in /tmp/debug_squad/checkpoint-18500/special_tokens_map.json\n","{'loss': 0.4428, 'learning_rate': 1.698364627498486e-05, 'epoch': 2.3}\n"," 58% 19000/33020 [3:27:21<2:31:53,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 19:12:34,960 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-19000\n","[INFO|configuration_utils.py:440] 2022-03-27 19:12:34,961 >> Configuration saved in /tmp/debug_squad/checkpoint-19000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 19:12:35,759 >> Model weights saved in /tmp/debug_squad/checkpoint-19000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 19:12:35,760 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-19000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 19:12:35,760 >> Special tokens file saved in /tmp/debug_squad/checkpoint-19000/special_tokens_map.json\n","{'loss': 0.4412, 'learning_rate': 1.6377952755905514e-05, 'epoch': 2.36}\n"," 59% 19500/33020 [3:32:48<2:26:21,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 19:18:02,069 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-19500\n","[INFO|configuration_utils.py:440] 2022-03-27 19:18:02,070 >> Configuration saved in /tmp/debug_squad/checkpoint-19500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 19:18:02,863 >> Model weights saved in /tmp/debug_squad/checkpoint-19500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 19:18:02,863 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-19500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 19:18:02,863 >> Special tokens file saved in /tmp/debug_squad/checkpoint-19500/special_tokens_map.json\n","{'loss': 0.4669, 'learning_rate': 1.5772259236826166e-05, 'epoch': 2.42}\n"," 61% 20000/33020 [3:38:15<2:20:57,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 19:23:29,628 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-20000\n","[INFO|configuration_utils.py:440] 2022-03-27 19:23:29,629 >> Configuration saved in /tmp/debug_squad/checkpoint-20000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 19:23:30,420 >> Model weights saved in /tmp/debug_squad/checkpoint-20000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 19:23:30,421 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-20000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 19:23:30,421 >> Special tokens file saved in /tmp/debug_squad/checkpoint-20000/special_tokens_map.json\n","{'loss': 0.4559, 'learning_rate': 1.5166565717746821e-05, 'epoch': 2.48}\n"," 62% 20500/33020 [3:43:43<2:15:10,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 19:28:56,952 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-20500\n","[INFO|configuration_utils.py:440] 2022-03-27 19:28:56,953 >> Configuration saved in /tmp/debug_squad/checkpoint-20500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 19:28:57,737 >> Model weights saved in /tmp/debug_squad/checkpoint-20500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 19:28:57,738 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-20500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 19:28:57,738 >> Special tokens file saved in /tmp/debug_squad/checkpoint-20500/special_tokens_map.json\n","{'loss': 0.4527, 'learning_rate': 1.4560872198667475e-05, 'epoch': 2.54}\n"," 64% 21000/33020 [3:49:10<2:09:53,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 19:34:24,059 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-21000\n","[INFO|configuration_utils.py:440] 2022-03-27 19:34:24,060 >> Configuration saved in /tmp/debug_squad/checkpoint-21000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 19:34:24,851 >> Model weights saved in /tmp/debug_squad/checkpoint-21000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 19:34:24,852 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-21000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 19:34:24,852 >> Special tokens file saved in /tmp/debug_squad/checkpoint-21000/special_tokens_map.json\n","{'loss': 0.4569, 'learning_rate': 1.395517867958813e-05, 'epoch': 2.6}\n"," 65% 21500/33020 [3:54:37<2:04:48,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 19:39:51,429 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-21500\n","[INFO|configuration_utils.py:440] 2022-03-27 19:39:51,430 >> Configuration saved in /tmp/debug_squad/checkpoint-21500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 19:39:52,230 >> Model weights saved in /tmp/debug_squad/checkpoint-21500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 19:39:52,231 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-21500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 19:39:52,231 >> Special tokens file saved in /tmp/debug_squad/checkpoint-21500/special_tokens_map.json\n","{'loss': 0.4519, 'learning_rate': 1.3349485160508783e-05, 'epoch': 2.67}\n"," 67% 22000/33020 [4:00:05<1:59:30,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 19:45:19,189 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-22000\n","[INFO|configuration_utils.py:440] 2022-03-27 19:45:19,190 >> Configuration saved in /tmp/debug_squad/checkpoint-22000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 19:45:19,981 >> Model weights saved in /tmp/debug_squad/checkpoint-22000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 19:45:19,982 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-22000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 19:45:19,982 >> Special tokens file saved in /tmp/debug_squad/checkpoint-22000/special_tokens_map.json\n","{'loss': 0.4738, 'learning_rate': 1.2743791641429437e-05, 'epoch': 2.73}\n"," 68% 22500/33020 [4:05:32<1:53:43,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 19:50:46,590 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-22500\n","[INFO|configuration_utils.py:440] 2022-03-27 19:50:46,591 >> Configuration saved in /tmp/debug_squad/checkpoint-22500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 19:50:47,401 >> Model weights saved in /tmp/debug_squad/checkpoint-22500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 19:50:47,401 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-22500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 19:50:47,401 >> Special tokens file saved in /tmp/debug_squad/checkpoint-22500/special_tokens_map.json\n","{'loss': 0.4717, 'learning_rate': 1.2138098122350092e-05, 'epoch': 2.79}\n"," 70% 23000/33020 [4:10:59<1:48:17,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 19:56:13,852 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-23000\n","[INFO|configuration_utils.py:440] 2022-03-27 19:56:13,853 >> Configuration saved in /tmp/debug_squad/checkpoint-23000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 19:56:14,638 >> Model weights saved in /tmp/debug_squad/checkpoint-23000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 19:56:14,639 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-23000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 19:56:14,639 >> Special tokens file saved in /tmp/debug_squad/checkpoint-23000/special_tokens_map.json\n","{'loss': 0.447, 'learning_rate': 1.1532404603270745e-05, 'epoch': 2.85}\n"," 71% 23500/33020 [4:16:27<1:43:06,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 20:01:41,248 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-23500\n","[INFO|configuration_utils.py:440] 2022-03-27 20:01:41,249 >> Configuration saved in /tmp/debug_squad/checkpoint-23500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 20:01:42,036 >> Model weights saved in /tmp/debug_squad/checkpoint-23500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 20:01:42,037 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-23500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 20:01:42,037 >> Special tokens file saved in /tmp/debug_squad/checkpoint-23500/special_tokens_map.json\n","{'loss': 0.4426, 'learning_rate': 1.0926711084191399e-05, 'epoch': 2.91}\n"," 73% 24000/33020 [4:21:54<1:37:39,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 20:07:08,283 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-24000\n","[INFO|configuration_utils.py:440] 2022-03-27 20:07:08,284 >> Configuration saved in /tmp/debug_squad/checkpoint-24000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 20:07:09,277 >> Model weights saved in /tmp/debug_squad/checkpoint-24000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 20:07:09,277 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-24000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 20:07:09,278 >> Special tokens file saved in /tmp/debug_squad/checkpoint-24000/special_tokens_map.json\n","{'loss': 0.4427, 'learning_rate': 1.0321017565112054e-05, 'epoch': 2.97}\n"," 74% 24500/33020 [4:27:22<1:32:04,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 20:12:36,096 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-24500\n","[INFO|configuration_utils.py:440] 2022-03-27 20:12:36,097 >> Configuration saved in /tmp/debug_squad/checkpoint-24500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 20:12:36,907 >> Model weights saved in /tmp/debug_squad/checkpoint-24500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 20:12:36,907 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-24500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 20:12:36,908 >> Special tokens file saved in /tmp/debug_squad/checkpoint-24500/special_tokens_map.json\n","{'loss': 0.3652, 'learning_rate': 9.715324046032707e-06, 'epoch': 3.03}\n"," 76% 25000/33020 [4:32:49<1:26:48,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 20:18:03,697 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-25000\n","[INFO|configuration_utils.py:440] 2022-03-27 20:18:03,698 >> Configuration saved in /tmp/debug_squad/checkpoint-25000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 20:18:04,501 >> Model weights saved in /tmp/debug_squad/checkpoint-25000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 20:18:04,501 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-25000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 20:18:04,501 >> Special tokens file saved in /tmp/debug_squad/checkpoint-25000/special_tokens_map.json\n","{'loss': 0.2762, 'learning_rate': 9.109630526953363e-06, 'epoch': 3.09}\n"," 77% 25500/33020 [4:38:17<1:21:30,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 20:23:31,333 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-25500\n","[INFO|configuration_utils.py:440] 2022-03-27 20:23:31,334 >> Configuration saved in /tmp/debug_squad/checkpoint-25500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 20:23:32,135 >> Model weights saved in /tmp/debug_squad/checkpoint-25500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 20:23:32,135 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-25500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 20:23:32,135 >> Special tokens file saved in /tmp/debug_squad/checkpoint-25500/special_tokens_map.json\n","{'loss': 0.2818, 'learning_rate': 8.503937007874016e-06, 'epoch': 3.15}\n"," 79% 26000/33020 [4:43:44<1:15:58,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 20:28:58,696 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-26000\n","[INFO|configuration_utils.py:440] 2022-03-27 20:28:58,697 >> Configuration saved in /tmp/debug_squad/checkpoint-26000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 20:28:59,496 >> Model weights saved in /tmp/debug_squad/checkpoint-26000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 20:28:59,497 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-26000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 20:28:59,497 >> Special tokens file saved in /tmp/debug_squad/checkpoint-26000/special_tokens_map.json\n","{'loss': 0.2747, 'learning_rate': 7.89824348879467e-06, 'epoch': 3.21}\n"," 80% 26500/33020 [4:49:12<1:10:33,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 20:34:26,253 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-26500\n","[INFO|configuration_utils.py:440] 2022-03-27 20:34:26,254 >> Configuration saved in /tmp/debug_squad/checkpoint-26500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 20:34:27,039 >> Model weights saved in /tmp/debug_squad/checkpoint-26500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 20:34:27,039 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-26500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 20:34:27,040 >> Special tokens file saved in /tmp/debug_squad/checkpoint-26500/special_tokens_map.json\n","{'loss': 0.2753, 'learning_rate': 7.292549969715325e-06, 'epoch': 3.27}\n"," 82% 27000/33020 [4:54:39<1:05:12,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 20:39:53,778 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-27000\n","[INFO|configuration_utils.py:440] 2022-03-27 20:39:53,780 >> Configuration saved in /tmp/debug_squad/checkpoint-27000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 20:39:54,573 >> Model weights saved in /tmp/debug_squad/checkpoint-27000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 20:39:54,573 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-27000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 20:39:54,573 >> Special tokens file saved in /tmp/debug_squad/checkpoint-27000/special_tokens_map.json\n","{'loss': 0.2771, 'learning_rate': 6.686856450635979e-06, 'epoch': 3.33}\n"," 83% 27500/33020 [5:00:07<59:49,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 20:45:21,692 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-27500\n","[INFO|configuration_utils.py:440] 2022-03-27 20:45:21,694 >> Configuration saved in /tmp/debug_squad/checkpoint-27500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 20:45:22,497 >> Model weights saved in /tmp/debug_squad/checkpoint-27500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 20:45:22,497 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-27500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 20:45:22,497 >> Special tokens file saved in /tmp/debug_squad/checkpoint-27500/special_tokens_map.json\n","{'loss': 0.2717, 'learning_rate': 6.0811629315566326e-06, 'epoch': 3.39}\n"," 85% 28000/33020 [5:05:35<54:27,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 20:50:49,561 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-28000\n","[INFO|configuration_utils.py:440] 2022-03-27 20:50:49,563 >> Configuration saved in /tmp/debug_squad/checkpoint-28000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 20:50:50,370 >> Model weights saved in /tmp/debug_squad/checkpoint-28000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 20:50:50,371 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-28000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 20:50:50,371 >> Special tokens file saved in /tmp/debug_squad/checkpoint-28000/special_tokens_map.json\n","{'loss': 0.2803, 'learning_rate': 5.475469412477287e-06, 'epoch': 3.45}\n"," 86% 28500/33020 [5:11:03<48:50,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 20:56:17,234 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-28500\n","[INFO|configuration_utils.py:440] 2022-03-27 20:56:17,235 >> Configuration saved in /tmp/debug_squad/checkpoint-28500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 20:56:18,054 >> Model weights saved in /tmp/debug_squad/checkpoint-28500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 20:56:18,055 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-28500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 20:56:18,055 >> Special tokens file saved in /tmp/debug_squad/checkpoint-28500/special_tokens_map.json\n","{'loss': 0.272, 'learning_rate': 4.869775893397941e-06, 'epoch': 3.51}\n"," 88% 29000/33020 [5:16:31<43:33,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 21:01:44,921 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-29000\n","[INFO|configuration_utils.py:440] 2022-03-27 21:01:44,922 >> Configuration saved in /tmp/debug_squad/checkpoint-29000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 21:01:45,731 >> Model weights saved in /tmp/debug_squad/checkpoint-29000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 21:01:45,732 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-29000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 21:01:45,732 >> Special tokens file saved in /tmp/debug_squad/checkpoint-29000/special_tokens_map.json\n","{'loss': 0.2682, 'learning_rate': 4.264082374318595e-06, 'epoch': 3.57}\n"," 89% 29500/33020 [5:21:58<38:06,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 21:07:12,376 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-29500\n","[INFO|configuration_utils.py:440] 2022-03-27 21:07:12,377 >> Configuration saved in /tmp/debug_squad/checkpoint-29500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 21:07:13,191 >> Model weights saved in /tmp/debug_squad/checkpoint-29500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 21:07:13,192 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-29500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 21:07:13,192 >> Special tokens file saved in /tmp/debug_squad/checkpoint-29500/special_tokens_map.json\n","{'loss': 0.2712, 'learning_rate': 3.6583888552392494e-06, 'epoch': 3.63}\n"," 91% 30000/33020 [5:27:25<32:41,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 21:12:39,875 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-30000\n","[INFO|configuration_utils.py:440] 2022-03-27 21:12:39,876 >> Configuration saved in /tmp/debug_squad/checkpoint-30000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 21:12:40,704 >> Model weights saved in /tmp/debug_squad/checkpoint-30000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 21:12:40,705 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-30000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 21:12:40,705 >> Special tokens file saved in /tmp/debug_squad/checkpoint-30000/special_tokens_map.json\n","{'loss': 0.2578, 'learning_rate': 3.0526953361599033e-06, 'epoch': 3.69}\n"," 92% 30500/33020 [5:32:53<27:14,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 21:18:07,354 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-30500\n","[INFO|configuration_utils.py:440] 2022-03-27 21:18:07,355 >> Configuration saved in /tmp/debug_squad/checkpoint-30500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 21:18:08,168 >> Model weights saved in /tmp/debug_squad/checkpoint-30500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 21:18:08,168 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-30500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 21:18:08,168 >> Special tokens file saved in /tmp/debug_squad/checkpoint-30500/special_tokens_map.json\n","{'loss': 0.2693, 'learning_rate': 2.4470018170805572e-06, 'epoch': 3.76}\n"," 94% 31000/33020 [5:38:20<21:51,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 21:23:34,810 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-31000\n","[INFO|configuration_utils.py:440] 2022-03-27 21:23:34,811 >> Configuration saved in /tmp/debug_squad/checkpoint-31000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 21:23:35,623 >> Model weights saved in /tmp/debug_squad/checkpoint-31000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 21:23:35,623 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-31000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 21:23:35,623 >> Special tokens file saved in /tmp/debug_squad/checkpoint-31000/special_tokens_map.json\n","{'loss': 0.2542, 'learning_rate': 1.8413082980012116e-06, 'epoch': 3.82}\n"," 95% 31500/33020 [5:43:48<16:27,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 21:29:02,277 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-31500\n","[INFO|configuration_utils.py:440] 2022-03-27 21:29:02,278 >> Configuration saved in /tmp/debug_squad/checkpoint-31500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 21:29:03,087 >> Model weights saved in /tmp/debug_squad/checkpoint-31500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 21:29:03,087 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-31500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 21:29:03,087 >> Special tokens file saved in /tmp/debug_squad/checkpoint-31500/special_tokens_map.json\n","{'loss': 0.2617, 'learning_rate': 1.2356147789218657e-06, 'epoch': 3.88}\n"," 97% 32000/33020 [5:49:15<11:02,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 21:34:29,821 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-32000\n","[INFO|configuration_utils.py:440] 2022-03-27 21:34:29,823 >> Configuration saved in /tmp/debug_squad/checkpoint-32000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 21:34:30,621 >> Model weights saved in /tmp/debug_squad/checkpoint-32000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 21:34:30,622 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-32000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 21:34:30,622 >> Special tokens file saved in /tmp/debug_squad/checkpoint-32000/special_tokens_map.json\n","{'loss': 0.2876, 'learning_rate': 6.299212598425198e-07, 'epoch': 3.94}\n"," 98% 32500/33020 [5:54:43<05:37,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 21:39:57,132 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-32500\n","[INFO|configuration_utils.py:440] 2022-03-27 21:39:57,133 >> Configuration saved in /tmp/debug_squad/checkpoint-32500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 21:39:57,942 >> Model weights saved in /tmp/debug_squad/checkpoint-32500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 21:39:57,942 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-32500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 21:39:57,943 >> Special tokens file saved in /tmp/debug_squad/checkpoint-32500/special_tokens_map.json\n","{'loss': 0.2548, 'learning_rate': 2.4227740763173837e-08, 'epoch': 4.0}\n","100% 33000/33020 [6:00:10<00:12,  1.54it/s][INFO|trainer.py:2162] 2022-03-27 21:45:24,549 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-33000\n","[INFO|configuration_utils.py:440] 2022-03-27 21:45:24,550 >> Configuration saved in /tmp/debug_squad/checkpoint-33000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 21:45:25,355 >> Model weights saved in /tmp/debug_squad/checkpoint-33000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 21:45:25,356 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-33000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 21:45:25,356 >> Special tokens file saved in /tmp/debug_squad/checkpoint-33000/special_tokens_map.json\n","100% 33020/33020 [6:00:26<00:00,  1.56it/s][INFO|trainer.py:1526] 2022-03-27 21:45:40,155 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 21626.2906, 'train_samples_per_second': 24.429, 'train_steps_per_second': 1.527, 'train_loss': 0.6738693125388031, 'epoch': 4.0}\n","100% 33020/33020 [6:00:26<00:00,  1.53it/s]\n","[INFO|trainer.py:2162] 2022-03-27 21:45:40,171 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:440] 2022-03-27 21:45:40,172 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-27 21:45:41,006 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2085] 2022-03-27 21:45:41,007 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2091] 2022-03-27 21:45:41,007 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        4.0\n","  train_loss               =     0.6739\n","  train_runtime            = 6:00:26.29\n","  train_samples            =     132079\n","  train_samples_per_second =     24.429\n","  train_steps_per_second   =      1.527\n","03/27/2022 21:45:41 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:571] 2022-03-27 21:45:41,059 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2412] 2022-03-27 21:45:41,109 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2414] 2022-03-27 21:45:41,109 >>   Num examples = 12199\n","[INFO|trainer.py:2417] 2022-03-27 21:45:41,109 >>   Batch size = 8\n","100% 1525/1525 [02:58<00:00,  8.71it/s]03/27/2022 21:48:55 - INFO - utils_qa - Post-processing 11873 example predictions split into 12199 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 5/11873 [00:00<13:12, 14.97it/s]\u001b[A\n","  0% 38/11873 [00:00<01:48, 108.96it/s]\u001b[A\n","  1% 69/11873 [00:00<01:10, 166.33it/s]\u001b[A\n","  1% 104/11873 [00:00<00:53, 219.65it/s]\u001b[A\n","  1% 141/11873 [00:00<00:44, 263.19it/s]\u001b[A\n","  2% 180/11873 [00:00<00:39, 299.43it/s]\u001b[A\n","  2% 217/11873 [00:00<00:36, 318.89it/s]\u001b[A\n","  2% 255/11873 [00:01<00:34, 335.29it/s]\u001b[A\n","  2% 294/11873 [00:01<00:33, 350.07it/s]\u001b[A\n","  3% 331/11873 [00:01<00:33, 347.87it/s]\u001b[A\n","  3% 370/11873 [00:01<00:32, 357.92it/s]\u001b[A\n","  3% 408/11873 [00:01<00:31, 363.26it/s]\u001b[A\n","  4% 446/11873 [00:01<00:31, 367.62it/s]\u001b[A\n","  4% 485/11873 [00:01<00:30, 372.61it/s]\u001b[A\n","  4% 523/11873 [00:01<00:30, 369.87it/s]\u001b[A\n","  5% 561/11873 [00:01<00:30, 366.16it/s]\u001b[A\n","  5% 598/11873 [00:01<00:30, 364.53it/s]\u001b[A\n","  5% 635/11873 [00:02<00:30, 362.77it/s]\u001b[A\n","  6% 673/11873 [00:02<00:30, 366.77it/s]\u001b[A\n","  6% 710/11873 [00:02<00:30, 366.56it/s]\u001b[A\n","  6% 747/11873 [00:02<00:30, 362.76it/s]\u001b[A\n","  7% 785/11873 [00:02<00:30, 366.43it/s]\u001b[A\n","  7% 824/11873 [00:02<00:29, 370.83it/s]\u001b[A\n","  7% 862/11873 [00:02<00:29, 372.80it/s]\u001b[A\n","  8% 902/11873 [00:02<00:28, 378.80it/s]\u001b[A\n","  8% 940/11873 [00:02<00:29, 376.06it/s]\u001b[A\n","  8% 979/11873 [00:02<00:28, 378.74it/s]\u001b[A\n","  9% 1017/11873 [00:03<00:29, 363.68it/s]\u001b[A\n","  9% 1054/11873 [00:03<00:31, 344.23it/s]\u001b[A\n","  9% 1089/11873 [00:03<00:32, 331.36it/s]\u001b[A\n","  9% 1123/11873 [00:03<00:33, 319.14it/s]\u001b[A\n"," 10% 1156/11873 [00:03<00:34, 311.83it/s]\u001b[A\n"," 10% 1188/11873 [00:03<00:34, 310.57it/s]\u001b[A\n"," 10% 1220/11873 [00:03<00:34, 308.32it/s]\u001b[A\n"," 11% 1251/11873 [00:03<00:34, 305.88it/s]\u001b[A\n"," 11% 1283/11873 [00:03<00:34, 306.98it/s]\u001b[A\n"," 11% 1314/11873 [00:04<00:34, 306.30it/s]\u001b[A\n"," 11% 1345/11873 [00:04<00:34, 301.86it/s]\u001b[A\n"," 12% 1376/11873 [00:04<00:34, 301.66it/s]\u001b[A\n"," 12% 1407/11873 [00:04<00:34, 301.49it/s]\u001b[A\n"," 12% 1438/11873 [00:04<00:35, 297.16it/s]\u001b[A\n"," 12% 1468/11873 [00:04<00:35, 295.81it/s]\u001b[A\n"," 13% 1499/11873 [00:04<00:34, 297.80it/s]\u001b[A\n"," 13% 1529/11873 [00:04<00:35, 292.85it/s]\u001b[A\n"," 13% 1559/11873 [00:04<00:35, 293.63it/s]\u001b[A\n"," 13% 1590/11873 [00:05<00:34, 296.29it/s]\u001b[A\n"," 14% 1620/11873 [00:05<00:34, 293.68it/s]\u001b[A\n"," 14% 1650/11873 [00:05<00:34, 294.93it/s]\u001b[A\n"," 14% 1681/11873 [00:05<00:34, 297.27it/s]\u001b[A\n"," 14% 1711/11873 [00:05<00:34, 295.56it/s]\u001b[A\n"," 15% 1743/11873 [00:05<00:33, 299.96it/s]\u001b[A\n"," 15% 1774/11873 [00:05<00:33, 297.87it/s]\u001b[A\n"," 15% 1804/11873 [00:05<00:34, 294.58it/s]\u001b[A\n"," 15% 1834/11873 [00:05<00:34, 294.37it/s]\u001b[A\n"," 16% 1864/11873 [00:05<00:33, 294.47it/s]\u001b[A\n"," 16% 1894/11873 [00:06<00:33, 294.52it/s]\u001b[A\n"," 16% 1925/11873 [00:06<00:33, 296.21it/s]\u001b[A\n"," 16% 1955/11873 [00:06<00:33, 295.67it/s]\u001b[A\n"," 17% 1985/11873 [00:06<00:33, 295.93it/s]\u001b[A\n"," 17% 2015/11873 [00:06<00:33, 293.10it/s]\u001b[A\n"," 17% 2045/11873 [00:06<00:33, 295.11it/s]\u001b[A\n"," 17% 2075/11873 [00:06<00:33, 294.98it/s]\u001b[A\n"," 18% 2105/11873 [00:06<00:33, 294.79it/s]\u001b[A\n"," 18% 2135/11873 [00:06<00:32, 295.24it/s]\u001b[A\n"," 18% 2165/11873 [00:06<00:33, 291.54it/s]\u001b[A\n"," 18% 2195/11873 [00:07<00:32, 293.46it/s]\u001b[A\n"," 19% 2225/11873 [00:07<00:32, 293.84it/s]\u001b[A\n"," 19% 2255/11873 [00:07<00:32, 295.57it/s]\u001b[A\n"," 19% 2285/11873 [00:07<00:33, 290.37it/s]\u001b[A\n"," 20% 2316/11873 [00:07<00:32, 293.99it/s]\u001b[A\n"," 20% 2346/11873 [00:07<00:32, 291.48it/s]\u001b[A\n"," 20% 2376/11873 [00:07<00:32, 290.33it/s]\u001b[A\n"," 20% 2406/11873 [00:07<00:32, 290.76it/s]\u001b[A\n"," 21% 2436/11873 [00:07<00:32, 291.91it/s]\u001b[A\n"," 21% 2466/11873 [00:08<00:32, 293.45it/s]\u001b[A\n"," 21% 2496/11873 [00:08<00:31, 294.38it/s]\u001b[A\n"," 21% 2527/11873 [00:08<00:31, 296.84it/s]\u001b[A\n"," 22% 2557/11873 [00:08<00:31, 294.29it/s]\u001b[A\n"," 22% 2587/11873 [00:08<00:31, 293.94it/s]\u001b[A\n"," 22% 2619/11873 [00:08<00:30, 298.85it/s]\u001b[A\n"," 22% 2649/11873 [00:08<00:30, 298.26it/s]\u001b[A\n"," 23% 2679/11873 [00:08<00:30, 298.27it/s]\u001b[A\n"," 23% 2710/11873 [00:08<00:30, 299.01it/s]\u001b[A\n"," 23% 2740/11873 [00:08<00:30, 295.52it/s]\u001b[A\n"," 23% 2770/11873 [00:09<00:30, 293.65it/s]\u001b[A\n"," 24% 2800/11873 [00:09<00:30, 293.38it/s]\u001b[A\n"," 24% 2830/11873 [00:09<00:31, 291.25it/s]\u001b[A\n"," 24% 2860/11873 [00:09<00:30, 293.50it/s]\u001b[A\n"," 24% 2890/11873 [00:09<00:31, 289.34it/s]\u001b[A\n"," 25% 2920/11873 [00:09<00:30, 292.43it/s]\u001b[A\n"," 25% 2951/11873 [00:09<00:30, 295.19it/s]\u001b[A\n"," 25% 2982/11873 [00:09<00:29, 296.84it/s]\u001b[A\n"," 25% 3012/11873 [00:09<00:34, 257.98it/s]\u001b[A\n"," 26% 3042/11873 [00:10<00:33, 266.71it/s]\u001b[A\n"," 26% 3071/11873 [00:10<00:32, 272.80it/s]\u001b[A\n"," 26% 3099/11873 [00:10<00:32, 268.43it/s]\u001b[A\n"," 26% 3127/11873 [00:10<00:39, 220.27it/s]\u001b[A\n"," 27% 3151/11873 [00:10<00:40, 217.52it/s]\u001b[A\n"," 27% 3174/11873 [00:10<00:40, 216.75it/s]\u001b[A\n"," 27% 3204/11873 [00:10<00:36, 237.74it/s]\u001b[A\n"," 27% 3234/11873 [00:10<00:33, 254.50it/s]\u001b[A\n"," 27% 3263/11873 [00:10<00:32, 263.52it/s]\u001b[A\n"," 28% 3290/11873 [00:11<00:44, 193.88it/s]\u001b[A\n"," 28% 3313/11873 [00:11<00:48, 175.22it/s]\u001b[A\n"," 28% 3333/11873 [00:11<00:50, 167.99it/s]\u001b[A\n"," 28% 3352/11873 [00:11<00:50, 167.93it/s]\u001b[A\n"," 28% 3370/11873 [00:11<00:52, 161.22it/s]\u001b[A\n"," 29% 3400/11873 [00:11<00:43, 194.73it/s]\u001b[A\n"," 29% 3431/11873 [00:11<00:37, 223.13it/s]\u001b[A\n"," 29% 3461/11873 [00:12<00:34, 242.01it/s]\u001b[A\n"," 29% 3491/11873 [00:12<00:32, 256.75it/s]\u001b[A\n"," 30% 3521/11873 [00:12<00:31, 267.12it/s]\u001b[A\n"," 30% 3552/11873 [00:12<00:29, 277.50it/s]\u001b[A\n"," 30% 3582/11873 [00:12<00:29, 283.24it/s]\u001b[A\n"," 30% 3612/11873 [00:12<00:28, 287.68it/s]\u001b[A\n"," 31% 3643/11873 [00:12<00:28, 292.56it/s]\u001b[A\n"," 31% 3673/11873 [00:12<00:27, 293.57it/s]\u001b[A\n"," 31% 3703/11873 [00:12<00:28, 289.39it/s]\u001b[A\n"," 31% 3733/11873 [00:12<00:27, 291.89it/s]\u001b[A\n"," 32% 3763/11873 [00:13<00:27, 293.85it/s]\u001b[A\n"," 32% 3793/11873 [00:13<00:27, 295.51it/s]\u001b[A\n"," 32% 3823/11873 [00:13<00:30, 267.77it/s]\u001b[A\n"," 32% 3854/11873 [00:13<00:28, 277.06it/s]\u001b[A\n"," 33% 3883/11873 [00:13<00:28, 278.73it/s]\u001b[A\n"," 33% 3912/11873 [00:13<00:29, 269.98it/s]\u001b[A\n"," 33% 3940/11873 [00:13<00:31, 254.44it/s]\u001b[A\n"," 33% 3966/11873 [00:13<00:31, 251.44it/s]\u001b[A\n"," 34% 3995/11873 [00:13<00:30, 262.02it/s]\u001b[A\n"," 34% 4025/11873 [00:14<00:28, 271.20it/s]\u001b[A\n"," 34% 4054/11873 [00:14<00:28, 274.66it/s]\u001b[A\n"," 34% 4083/11873 [00:14<00:27, 278.32it/s]\u001b[A\n"," 35% 4111/11873 [00:14<00:27, 278.35it/s]\u001b[A\n"," 35% 4141/11873 [00:14<00:27, 282.68it/s]\u001b[A\n"," 35% 4170/11873 [00:14<00:29, 261.45it/s]\u001b[A\n"," 35% 4198/11873 [00:14<00:28, 264.83it/s]\u001b[A\n"," 36% 4229/11873 [00:14<00:27, 275.13it/s]\u001b[A\n"," 36% 4260/11873 [00:14<00:26, 282.71it/s]\u001b[A\n"," 36% 4289/11873 [00:14<00:26, 283.49it/s]\u001b[A\n"," 36% 4321/11873 [00:15<00:25, 292.29it/s]\u001b[A\n"," 37% 4351/11873 [00:15<00:26, 288.08it/s]\u001b[A\n"," 37% 4381/11873 [00:15<00:25, 290.12it/s]\u001b[A\n"," 37% 4411/11873 [00:15<00:26, 285.54it/s]\u001b[A\n"," 37% 4440/11873 [00:15<00:32, 229.52it/s]\u001b[A\n"," 38% 4471/11873 [00:15<00:29, 249.45it/s]\u001b[A\n"," 38% 4502/11873 [00:15<00:27, 263.84it/s]\u001b[A\n"," 38% 4532/11873 [00:15<00:27, 271.65it/s]\u001b[A\n"," 38% 4563/11873 [00:15<00:26, 281.02it/s]\u001b[A\n"," 39% 4595/11873 [00:16<00:25, 289.51it/s]\u001b[A\n"," 39% 4626/11873 [00:16<00:24, 292.99it/s]\u001b[A\n"," 39% 4656/11873 [00:16<00:24, 294.95it/s]\u001b[A\n"," 39% 4686/11873 [00:16<00:24, 296.29it/s]\u001b[A\n"," 40% 4716/11873 [00:16<00:24, 293.72it/s]\u001b[A\n"," 40% 4746/11873 [00:16<00:24, 291.95it/s]\u001b[A\n"," 40% 4777/11873 [00:16<00:24, 294.40it/s]\u001b[A\n"," 40% 4807/11873 [00:16<00:24, 294.15it/s]\u001b[A\n"," 41% 4837/11873 [00:16<00:24, 291.66it/s]\u001b[A\n"," 41% 4867/11873 [00:16<00:23, 293.95it/s]\u001b[A\n"," 41% 4898/11873 [00:17<00:23, 295.92it/s]\u001b[A\n"," 42% 4928/11873 [00:17<00:23, 295.66it/s]\u001b[A\n"," 42% 4958/11873 [00:17<00:23, 295.38it/s]\u001b[A\n"," 42% 4988/11873 [00:17<00:23, 296.55it/s]\u001b[A\n"," 42% 5018/11873 [00:17<00:23, 291.22it/s]\u001b[A\n"," 43% 5048/11873 [00:17<00:23, 292.76it/s]\u001b[A\n"," 43% 5078/11873 [00:17<00:23, 290.71it/s]\u001b[A\n"," 43% 5109/11873 [00:17<00:22, 295.08it/s]\u001b[A\n"," 43% 5140/11873 [00:17<00:22, 297.18it/s]\u001b[A\n"," 44% 5170/11873 [00:18<00:22, 293.95it/s]\u001b[A\n"," 44% 5201/11873 [00:18<00:22, 296.49it/s]\u001b[A\n"," 44% 5231/11873 [00:18<00:22, 296.05it/s]\u001b[A\n"," 44% 5261/11873 [00:18<00:24, 268.28it/s]\u001b[A\n"," 45% 5291/11873 [00:18<00:23, 274.49it/s]\u001b[A\n"," 45% 5321/11873 [00:18<00:23, 279.63it/s]\u001b[A\n"," 45% 5351/11873 [00:18<00:22, 283.64it/s]\u001b[A\n"," 45% 5380/11873 [00:18<00:22, 282.41it/s]\u001b[A\n"," 46% 5411/11873 [00:18<00:22, 288.61it/s]\u001b[A\n"," 46% 5441/11873 [00:18<00:22, 290.58it/s]\u001b[A\n"," 46% 5471/11873 [00:19<00:21, 292.99it/s]\u001b[A\n"," 46% 5503/11873 [00:19<00:21, 299.05it/s]\u001b[A\n"," 47% 5533/11873 [00:19<00:21, 299.13it/s]\u001b[A\n"," 47% 5563/11873 [00:19<00:21, 298.46it/s]\u001b[A\n"," 47% 5593/11873 [00:19<00:21, 295.06it/s]\u001b[A\n"," 47% 5623/11873 [00:19<00:21, 288.13it/s]\u001b[A\n"," 48% 5652/11873 [00:19<00:21, 285.91it/s]\u001b[A\n"," 48% 5682/11873 [00:19<00:21, 288.31it/s]\u001b[A\n"," 48% 5711/11873 [00:19<00:21, 286.86it/s]\u001b[A\n"," 48% 5742/11873 [00:20<00:21, 291.11it/s]\u001b[A\n"," 49% 5772/11873 [00:20<00:20, 291.17it/s]\u001b[A\n"," 49% 5804/11873 [00:20<00:20, 297.75it/s]\u001b[A\n"," 49% 5835/11873 [00:20<00:20, 300.82it/s]\u001b[A\n"," 49% 5866/11873 [00:20<00:20, 298.06it/s]\u001b[A\n"," 50% 5896/11873 [00:20<00:20, 298.38it/s]\u001b[A\n"," 50% 5926/11873 [00:20<00:20, 293.42it/s]\u001b[A\n"," 50% 5958/11873 [00:20<00:19, 299.07it/s]\u001b[A\n"," 50% 5989/11873 [00:20<00:19, 300.89it/s]\u001b[A\n"," 51% 6020/11873 [00:20<00:19, 295.77it/s]\u001b[A\n"," 51% 6050/11873 [00:21<00:19, 295.94it/s]\u001b[A\n"," 51% 6080/11873 [00:21<00:19, 292.75it/s]\u001b[A\n"," 51% 6111/11873 [00:21<00:19, 295.62it/s]\u001b[A\n"," 52% 6143/11873 [00:21<00:19, 300.46it/s]\u001b[A\n"," 52% 6174/11873 [00:21<00:18, 301.49it/s]\u001b[A\n"," 52% 6206/11873 [00:21<00:18, 304.31it/s]\u001b[A\n"," 53% 6237/11873 [00:21<00:18, 303.76it/s]\u001b[A\n"," 53% 6268/11873 [00:21<00:18, 300.05it/s]\u001b[A\n"," 53% 6299/11873 [00:21<00:18, 299.51it/s]\u001b[A\n"," 53% 6329/11873 [00:21<00:18, 296.91it/s]\u001b[A\n"," 54% 6360/11873 [00:22<00:18, 298.47it/s]\u001b[A\n"," 54% 6391/11873 [00:22<00:18, 301.46it/s]\u001b[A\n"," 54% 6422/11873 [00:22<00:18, 296.59it/s]\u001b[A\n"," 54% 6452/11873 [00:22<00:18, 291.61it/s]\u001b[A\n"," 55% 6482/11873 [00:22<00:18, 292.34it/s]\u001b[A\n"," 55% 6512/11873 [00:22<00:18, 288.01it/s]\u001b[A\n"," 55% 6542/11873 [00:22<00:18, 291.24it/s]\u001b[A\n"," 55% 6572/11873 [00:22<00:18, 288.56it/s]\u001b[A\n"," 56% 6601/11873 [00:22<00:18, 287.98it/s]\u001b[A\n"," 56% 6631/11873 [00:23<00:18, 288.75it/s]\u001b[A\n"," 56% 6661/11873 [00:23<00:17, 290.75it/s]\u001b[A\n"," 56% 6691/11873 [00:23<00:18, 287.84it/s]\u001b[A\n"," 57% 6720/11873 [00:23<00:20, 251.00it/s]\u001b[A\n"," 57% 6750/11873 [00:23<00:19, 263.72it/s]\u001b[A\n"," 57% 6779/11873 [00:23<00:18, 269.89it/s]\u001b[A\n"," 57% 6808/11873 [00:23<00:18, 275.06it/s]\u001b[A\n"," 58% 6837/11873 [00:23<00:18, 276.93it/s]\u001b[A\n"," 58% 6867/11873 [00:23<00:17, 283.34it/s]\u001b[A\n"," 58% 6897/11873 [00:23<00:17, 287.00it/s]\u001b[A\n"," 58% 6926/11873 [00:24<00:17, 287.80it/s]\u001b[A\n"," 59% 6957/11873 [00:24<00:16, 291.85it/s]\u001b[A\n"," 59% 6988/11873 [00:24<00:16, 294.69it/s]\u001b[A\n"," 59% 7018/11873 [00:24<00:16, 295.49it/s]\u001b[A\n"," 59% 7048/11873 [00:24<00:16, 295.41it/s]\u001b[A\n"," 60% 7079/11873 [00:24<00:16, 298.64it/s]\u001b[A\n"," 60% 7109/11873 [00:24<00:16, 295.68it/s]\u001b[A\n"," 60% 7140/11873 [00:24<00:15, 299.19it/s]\u001b[A\n"," 60% 7171/11873 [00:24<00:15, 300.40it/s]\u001b[A\n"," 61% 7202/11873 [00:25<00:15, 297.56it/s]\u001b[A\n"," 61% 7232/11873 [00:25<00:15, 296.79it/s]\u001b[A\n"," 61% 7262/11873 [00:25<00:15, 292.75it/s]\u001b[A\n"," 61% 7293/11873 [00:25<00:15, 296.74it/s]\u001b[A\n"," 62% 7323/11873 [00:25<00:15, 292.83it/s]\u001b[A\n"," 62% 7353/11873 [00:25<00:15, 294.60it/s]\u001b[A\n"," 62% 7383/11873 [00:25<00:15, 294.75it/s]\u001b[A\n"," 62% 7413/11873 [00:25<00:15, 282.26it/s]\u001b[A\n"," 63% 7442/11873 [00:25<00:15, 279.00it/s]\u001b[A\n"," 63% 7473/11873 [00:25<00:15, 287.05it/s]\u001b[A\n"," 63% 7503/11873 [00:26<00:15, 289.69it/s]\u001b[A\n"," 63% 7534/11873 [00:26<00:14, 293.53it/s]\u001b[A\n"," 64% 7564/11873 [00:26<00:14, 290.02it/s]\u001b[A\n"," 64% 7594/11873 [00:26<00:14, 291.14it/s]\u001b[A\n"," 64% 7624/11873 [00:26<00:14, 291.92it/s]\u001b[A\n"," 64% 7654/11873 [00:26<00:14, 285.91it/s]\u001b[A\n"," 65% 7683/11873 [00:26<00:14, 285.03it/s]\u001b[A\n"," 65% 7712/11873 [00:26<00:15, 262.21it/s]\u001b[A\n"," 65% 7742/11873 [00:26<00:15, 272.28it/s]\u001b[A\n"," 65% 7770/11873 [00:27<00:14, 273.92it/s]\u001b[A\n"," 66% 7800/11873 [00:27<00:14, 279.08it/s]\u001b[A\n"," 66% 7830/11873 [00:27<00:14, 282.66it/s]\u001b[A\n"," 66% 7859/11873 [00:27<00:14, 283.87it/s]\u001b[A\n"," 66% 7888/11873 [00:27<00:15, 264.58it/s]\u001b[A\n"," 67% 7919/11873 [00:27<00:14, 275.84it/s]\u001b[A\n"," 67% 7950/11873 [00:27<00:13, 284.23it/s]\u001b[A\n"," 67% 7980/11873 [00:27<00:13, 287.51it/s]\u001b[A\n"," 67% 8011/11873 [00:27<00:13, 291.80it/s]\u001b[A\n"," 68% 8041/11873 [00:27<00:13, 294.00it/s]\u001b[A\n"," 68% 8072/11873 [00:28<00:12, 296.68it/s]\u001b[A\n"," 68% 8102/11873 [00:28<00:13, 285.50it/s]\u001b[A\n"," 68% 8132/11873 [00:28<00:13, 287.52it/s]\u001b[A\n"," 69% 8163/11873 [00:28<00:12, 291.40it/s]\u001b[A\n"," 69% 8193/11873 [00:28<00:12, 293.32it/s]\u001b[A\n"," 69% 8223/11873 [00:28<00:12, 287.67it/s]\u001b[A\n"," 70% 8253/11873 [00:28<00:12, 285.25it/s]\u001b[A\n"," 70% 8284/11873 [00:28<00:12, 290.24it/s]\u001b[A\n"," 70% 8314/11873 [00:28<00:12, 292.26it/s]\u001b[A\n"," 70% 8344/11873 [00:28<00:12, 293.66it/s]\u001b[A\n"," 71% 8374/11873 [00:29<00:12, 282.41it/s]\u001b[A\n"," 71% 8404/11873 [00:29<00:12, 284.99it/s]\u001b[A\n"," 71% 8434/11873 [00:29<00:11, 286.72it/s]\u001b[A\n"," 71% 8463/11873 [00:29<00:11, 285.87it/s]\u001b[A\n"," 72% 8493/11873 [00:29<00:11, 288.50it/s]\u001b[A\n"," 72% 8524/11873 [00:29<00:11, 292.13it/s]\u001b[A\n"," 72% 8554/11873 [00:29<00:11, 293.04it/s]\u001b[A\n"," 72% 8585/11873 [00:29<00:11, 295.52it/s]\u001b[A\n"," 73% 8615/11873 [00:29<00:11, 294.71it/s]\u001b[A\n"," 73% 8645/11873 [00:30<00:10, 295.97it/s]\u001b[A\n"," 73% 8675/11873 [00:30<00:10, 296.78it/s]\u001b[A\n"," 73% 8705/11873 [00:30<00:10, 292.18it/s]\u001b[A\n"," 74% 8736/11873 [00:30<00:10, 294.76it/s]\u001b[A\n"," 74% 8766/11873 [00:30<00:10, 296.28it/s]\u001b[A\n"," 74% 8796/11873 [00:30<00:10, 294.53it/s]\u001b[A\n"," 74% 8826/11873 [00:30<00:10, 294.91it/s]\u001b[A\n"," 75% 8856/11873 [00:30<00:10, 295.64it/s]\u001b[A\n"," 75% 8886/11873 [00:30<00:10, 296.43it/s]\u001b[A\n"," 75% 8917/11873 [00:30<00:09, 298.41it/s]\u001b[A\n"," 75% 8947/11873 [00:31<00:10, 289.45it/s]\u001b[A\n"," 76% 8978/11873 [00:31<00:09, 293.29it/s]\u001b[A\n"," 76% 9008/11873 [00:31<00:09, 287.72it/s]\u001b[A\n"," 76% 9037/11873 [00:31<00:09, 288.10it/s]\u001b[A\n"," 76% 9069/11873 [00:31<00:09, 295.46it/s]\u001b[A\n"," 77% 9099/11873 [00:31<00:09, 289.79it/s]\u001b[A\n"," 77% 9129/11873 [00:31<00:09, 288.98it/s]\u001b[A\n"," 77% 9160/11873 [00:31<00:09, 293.70it/s]\u001b[A\n"," 77% 9192/11873 [00:31<00:08, 298.58it/s]\u001b[A\n"," 78% 9222/11873 [00:31<00:08, 298.88it/s]\u001b[A\n"," 78% 9252/11873 [00:32<00:08, 299.00it/s]\u001b[A\n"," 78% 9282/11873 [00:32<00:08, 298.20it/s]\u001b[A\n"," 78% 9312/11873 [00:32<00:08, 297.54it/s]\u001b[A\n"," 79% 9342/11873 [00:32<00:08, 298.01it/s]\u001b[A\n"," 79% 9372/11873 [00:32<00:08, 296.45it/s]\u001b[A\n"," 79% 9403/11873 [00:32<00:08, 299.19it/s]\u001b[A\n"," 79% 9433/11873 [00:32<00:08, 297.78it/s]\u001b[A\n"," 80% 9463/11873 [00:32<00:08, 291.21it/s]\u001b[A\n"," 80% 9494/11873 [00:32<00:08, 295.19it/s]\u001b[A\n"," 80% 9525/11873 [00:33<00:07, 298.16it/s]\u001b[A\n"," 80% 9557/11873 [00:33<00:07, 302.12it/s]\u001b[A\n"," 81% 9588/11873 [00:33<00:07, 301.70it/s]\u001b[A\n"," 81% 9619/11873 [00:33<00:07, 296.53it/s]\u001b[A\n"," 81% 9650/11873 [00:33<00:07, 297.81it/s]\u001b[A\n"," 82% 9680/11873 [00:33<00:07, 295.67it/s]\u001b[A\n"," 82% 9711/11873 [00:33<00:07, 297.38it/s]\u001b[A\n"," 82% 9742/11873 [00:33<00:07, 298.74it/s]\u001b[A\n"," 82% 9772/11873 [00:33<00:07, 297.97it/s]\u001b[A\n"," 83% 9802/11873 [00:33<00:06, 297.62it/s]\u001b[A\n"," 83% 9832/11873 [00:34<00:06, 296.93it/s]\u001b[A\n"," 83% 9862/11873 [00:34<00:06, 296.64it/s]\u001b[A\n"," 83% 9892/11873 [00:34<00:06, 293.79it/s]\u001b[A\n"," 84% 9923/11873 [00:34<00:06, 296.40it/s]\u001b[A\n"," 84% 9954/11873 [00:34<00:06, 298.18it/s]\u001b[A\n"," 84% 9985/11873 [00:34<00:06, 300.10it/s]\u001b[A\n"," 84% 10016/11873 [00:34<00:06, 294.63it/s]\u001b[A\n"," 85% 10047/11873 [00:34<00:06, 296.39it/s]\u001b[A\n"," 85% 10078/11873 [00:34<00:06, 298.76it/s]\u001b[A\n"," 85% 10109/11873 [00:34<00:05, 300.18it/s]\u001b[A\n"," 85% 10140/11873 [00:35<00:05, 300.17it/s]\u001b[A\n"," 86% 10172/11873 [00:35<00:05, 305.43it/s]\u001b[A\n"," 86% 10203/11873 [00:35<00:05, 301.06it/s]\u001b[A\n"," 86% 10234/11873 [00:35<00:05, 300.47it/s]\u001b[A\n"," 86% 10265/11873 [00:35<00:05, 300.32it/s]\u001b[A\n"," 87% 10296/11873 [00:35<00:05, 297.02it/s]\u001b[A\n"," 87% 10326/11873 [00:35<00:05, 293.52it/s]\u001b[A\n"," 87% 10357/11873 [00:35<00:05, 297.07it/s]\u001b[A\n"," 88% 10389/11873 [00:35<00:04, 301.14it/s]\u001b[A\n"," 88% 10420/11873 [00:36<00:04, 296.31it/s]\u001b[A\n"," 88% 10450/11873 [00:36<00:05, 279.47it/s]\u001b[A\n"," 88% 10480/11873 [00:36<00:04, 283.48it/s]\u001b[A\n"," 89% 10510/11873 [00:36<00:04, 287.13it/s]\u001b[A\n"," 89% 10539/11873 [00:36<00:04, 285.29it/s]\u001b[A\n"," 89% 10568/11873 [00:36<00:04, 267.64it/s]\u001b[A\n"," 89% 10597/11873 [00:36<00:04, 272.09it/s]\u001b[A\n"," 89% 10625/11873 [00:36<00:04, 271.61it/s]\u001b[A\n"," 90% 10654/11873 [00:36<00:04, 276.51it/s]\u001b[A\n"," 90% 10684/11873 [00:36<00:04, 282.30it/s]\u001b[A\n"," 90% 10714/11873 [00:37<00:04, 286.47it/s]\u001b[A\n"," 90% 10744/11873 [00:37<00:03, 288.65it/s]\u001b[A\n"," 91% 10773/11873 [00:37<00:03, 285.69it/s]\u001b[A\n"," 91% 10802/11873 [00:37<00:03, 277.38it/s]\u001b[A\n"," 91% 10830/11873 [00:37<00:03, 261.59it/s]\u001b[A\n"," 91% 10860/11873 [00:37<00:03, 270.51it/s]\u001b[A\n"," 92% 10890/11873 [00:37<00:03, 276.93it/s]\u001b[A\n"," 92% 10919/11873 [00:37<00:03, 279.52it/s]\u001b[A\n"," 92% 10948/11873 [00:37<00:03, 278.23it/s]\u001b[A\n"," 92% 10978/11873 [00:38<00:03, 283.49it/s]\u001b[A\n"," 93% 11009/11873 [00:38<00:02, 289.61it/s]\u001b[A\n"," 93% 11041/11873 [00:38<00:02, 295.92it/s]\u001b[A\n"," 93% 11072/11873 [00:38<00:02, 297.91it/s]\u001b[A\n"," 94% 11103/11873 [00:38<00:02, 300.51it/s]\u001b[A\n"," 94% 11134/11873 [00:38<00:02, 293.89it/s]\u001b[A\n"," 94% 11165/11873 [00:38<00:02, 295.83it/s]\u001b[A\n"," 94% 11195/11873 [00:38<00:02, 294.90it/s]\u001b[A\n"," 95% 11225/11873 [00:38<00:02, 294.18it/s]\u001b[A\n"," 95% 11255/11873 [00:38<00:02, 293.49it/s]\u001b[A\n"," 95% 11287/11873 [00:39<00:01, 298.54it/s]\u001b[A\n"," 95% 11317/11873 [00:39<00:01, 298.67it/s]\u001b[A\n"," 96% 11347/11873 [00:39<00:01, 295.37it/s]\u001b[A\n"," 96% 11377/11873 [00:39<00:01, 295.80it/s]\u001b[A\n"," 96% 11408/11873 [00:39<00:01, 297.43it/s]\u001b[A\n"," 96% 11438/11873 [00:39<00:01, 297.25it/s]\u001b[A\n"," 97% 11469/11873 [00:39<00:01, 298.01it/s]\u001b[A\n"," 97% 11499/11873 [00:39<00:01, 297.74it/s]\u001b[A\n"," 97% 11529/11873 [00:39<00:01, 291.55it/s]\u001b[A\n"," 97% 11559/11873 [00:39<00:01, 290.65it/s]\u001b[A\n"," 98% 11589/11873 [00:40<00:00, 293.34it/s]\u001b[A\n"," 98% 11619/11873 [00:40<00:00, 287.90it/s]\u001b[A\n"," 98% 11649/11873 [00:40<00:00, 290.14it/s]\u001b[A\n"," 98% 11679/11873 [00:40<00:00, 289.60it/s]\u001b[A\n"," 99% 11708/11873 [00:40<00:00, 289.33it/s]\u001b[A\n"," 99% 11738/11873 [00:40<00:00, 291.56it/s]\u001b[A\n"," 99% 11768/11873 [00:40<00:00, 292.96it/s]\u001b[A\n"," 99% 11798/11873 [00:40<00:00, 294.16it/s]\u001b[A\n","100% 11828/11873 [00:40<00:00, 295.81it/s]\u001b[A\n","100% 11873/11873 [00:41<00:00, 289.20it/s]\n","03/27/2022 21:49:36 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","03/27/2022 21:49:36 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","03/27/2022 21:49:38 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","03/27/2022 21:49:42 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1525/1525 [04:01<00:00,  6.31it/s]\n","***** eval metrics *****\n","  epoch                  =     4.0\n","  eval_HasAns_exact      = 72.2335\n","  eval_HasAns_f1         = 79.5316\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 71.7578\n","  eval_NoAns_f1          = 71.7578\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 71.9953\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 75.6391\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 71.9953\n","  eval_f1                = 75.6391\n","  eval_samples           =   12199\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-03-27 21:49:43,288 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]},{"cell_type":"code","source":["!python run_qa.py \\\n","  --model_name_or_path bert-base-cased \\\n","  --dataset_name squad_v2 \\\n","  --do_train \\\n","  --do_eval \\\n","  --per_device_train_batch_size 16 \\\n","  --learning_rate 4e-5 \\\n","  --num_train_epochs 3 \\\n","  --max_seq_length 512 \\\n","  --doc_stride 128 \\\n","  --version_2_with_negative \\\n","  --output_dir /tmp/debug_squad/ \\\n","  --overwrite_output_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoQkdyqPNaTz","executionInfo":{"status":"ok","timestamp":1648588102922,"user_tz":240,"elapsed":21773768,"user":{"displayName":"SICHEN ZHONG","userId":"08427994781088390833"}},"outputId":"352b2ab0-99f7-47a7-d951-b1a19d978c9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["03/29/2022 15:05:32 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","03/29/2022 15:05:32 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=4e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=/tmp/debug_squad/runs/Mar29_15-05-32_cfcd475e920f,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=/tmp/debug_squad/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/tmp/debug_squad/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","03/29/2022 15:05:33 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmprzu9bbqy\n","Downloading builder script: 5.28kB [00:00, 5.84MB/s]       \n","03/29/2022 15:05:33 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/29/2022 15:05:33 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6525c306c8316971a45500e69d0b9a5ff055271fa08bc9ab4fa08594e4fc0047.4c3ea0f73d6316868385621317b47a919bac9ebe7b9807ac48c5fb2de204fa64.py\n","03/29/2022 15:05:33 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpuyktitbh\n","Downloading metadata: 2.40kB [00:00, 3.29MB/s]       \n","03/29/2022 15:05:33 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/squad_v2/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/29/2022 15:05:33 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/49043cf0b9a99222075dcad7af2ffa29ffb7ef9bf673b6f87c2ec206dffb3a9d.ef42503fd2a66b0c7ded94cf0581921a20d38420e5069d236039c8859a3c2e6a\n","03/29/2022 15:05:33 - INFO - datasets.builder - No config specified, defaulting to first: squad_v2/squad_v2\n","03/29/2022 15:05:33 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n","03/29/2022 15:05:33 - INFO - datasets.builder - Generating dataset squad_v2 (/root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n","Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n","03/29/2022 15:05:33 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n","Downloading data files:   0% 0/2 [00:00<?, ?it/s]03/29/2022 15:05:33 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp5h1nngqm\n","\n","Downloading data:   0% 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n","Downloading data: 10.4MB [00:00, 104MB/s]        \u001b[A\n","Downloading data: 21.2MB [00:00, 106MB/s]\u001b[A\n","Downloading data: 42.1MB [00:00, 106MB/s]\n","03/29/2022 15:05:33 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","03/29/2022 15:05:33 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n","Downloading data files:  50% 1/2 [00:00<00:00,  1.79it/s]03/29/2022 15:05:34 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpi7hh1utq\n","\n","Downloading data: 4.37MB [00:00, 115MB/s]       \n","03/29/2022 15:05:34 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","03/29/2022 15:05:34 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n","Downloading data files: 100% 2/2 [00:00<00:00,  2.64it/s]\n","03/29/2022 15:05:34 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","03/29/2022 15:05:34 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 1579.48it/s]\n","03/29/2022 15:05:34 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n","03/29/2022 15:05:34 - INFO - datasets.builder - Generating train split\n","03/29/2022 15:05:44 - INFO - datasets.builder - Generating validation split\n","03/29/2022 15:05:45 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n","Dataset squad_v2 downloaded and prepared to /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 329.17it/s]\n","[INFO|hub.py:583] 2022-03-29 15:05:46,270 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxz7du_gb\n","Downloading: 100% 570/570 [00:00<00:00, 847kB/s]\n","[INFO|hub.py:587] 2022-03-29 15:05:46,626 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|hub.py:595] 2022-03-29 15:05:46,626 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:653] 2022-03-29 15:05:46,626 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-29 15:05:46,627 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-03-29 15:05:46,981 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9us422rr\n","Downloading: 100% 29.0/29.0 [00:00<00:00, 36.3kB/s]\n","[INFO|hub.py:587] 2022-03-29 15:05:47,340 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|hub.py:595] 2022-03-29 15:05:47,340 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:653] 2022-03-29 15:05:47,698 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-29 15:05:47,698 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-03-29 15:05:48,408 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpumezj2lb\n","Downloading: 100% 208k/208k [00:00<00:00, 636kB/s]\n","[INFO|hub.py:587] 2022-03-29 15:05:49,101 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:595] 2022-03-29 15:05:49,101 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|hub.py:583] 2022-03-29 15:05:49,456 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp35_iqedo\n","Downloading: 100% 426k/426k [00:00<00:00, 1.04MB/s]\n","[INFO|hub.py:587] 2022-03-29 15:05:50,233 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|hub.py:595] 2022-03-29 15:05:50,233 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 15:05:51,313 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 15:05:51,313 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 15:05:51,313 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 15:05:51,313 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1778] 2022-03-29 15:05:51,313 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","[INFO|configuration_utils.py:653] 2022-03-29 15:05:51,668 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n","[INFO|configuration_utils.py:689] 2022-03-29 15:05:51,669 >> Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.18.0.dev0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 28996\n","}\n","\n","[INFO|hub.py:583] 2022-03-29 15:05:52,068 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfon1dtna\n","Downloading: 100% 416M/416M [00:06<00:00, 71.2MB/s]\n","[INFO|hub.py:587] 2022-03-29 15:05:58,248 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|hub.py:595] 2022-03-29 15:05:58,248 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[INFO|modeling_utils.py:1771] 2022-03-29 15:05:58,248 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n","[WARNING|modeling_utils.py:2049] 2022-03-29 15:06:01,634 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_utils.py:2060] 2022-03-29 15:06:01,634 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Running tokenizer on train dataset:   0% 0/131 [00:00<?, ?ba/s]03/29/2022 15:06:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-b5a3014a922610c1.arrow\n","Running tokenizer on train dataset: 100% 131/131 [00:48<00:00,  2.69ba/s]\n","Running tokenizer on validation dataset:   0% 0/12 [00:00<?, ?ba/s]03/29/2022 15:06:50 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-886299a6be65c3ab.arrow\n","Running tokenizer on validation dataset: 100% 12/12 [01:16<00:00,  6.38s/ba]\n","03/29/2022 15:08:07 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp6v3boikx\n","Downloading builder script: 6.46kB [00:00, 8.05MB/s]       \n","03/29/2022 15:08:07 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/squad_v2.py in cache at /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/29/2022 15:08:07 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/28e260e79373763d7435864d09ebeccbfa7b9903d7901d5283b0d6b7265e34c7.20ffda40aa962d94515737edbb5a7eda5c13e771416809a70e82ce2aee1820fd.py\n","03/29/2022 15:08:07 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpz_otp0jy\n","Downloading extra modules: 11.3kB [00:00, 10.5MB/s]       \n","03/29/2022 15:08:07 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/squad_v2/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","03/29/2022 15:08:07 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a1b3f17173d6daeea11b56052a09e38922a52847495c5c51da69b6024f7bc6c5.05a1750dbac0faed211b316ff86719fdec19d084c7ea3ae500a954598bcf548c.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1288] 2022-03-29 15:08:18,085 >> ***** Running training *****\n","[INFO|trainer.py:1289] 2022-03-29 15:08:18,085 >>   Num examples = 130544\n","[INFO|trainer.py:1290] 2022-03-29 15:08:18,086 >>   Num Epochs = 3\n","[INFO|trainer.py:1291] 2022-03-29 15:08:18,086 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1292] 2022-03-29 15:08:18,086 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1293] 2022-03-29 15:08:18,086 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1294] 2022-03-29 15:08:18,086 >>   Total optimization steps = 24477\n","{'loss': 2.1835, 'learning_rate': 3.9182906401928345e-05, 'epoch': 0.06}\n","  2% 500/24477 [07:11<5:44:56,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 15:15:29,681 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-500\n","[INFO|configuration_utils.py:440] 2022-03-29 15:15:29,682 >> Configuration saved in /tmp/debug_squad/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:15:30,462 >> Model weights saved in /tmp/debug_squad/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:15:30,462 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:15:30,463 >> Special tokens file saved in /tmp/debug_squad/checkpoint-500/special_tokens_map.json\n","{'loss': 1.6041, 'learning_rate': 3.836581280385669e-05, 'epoch': 0.12}\n","  4% 1000/24477 [14:25<5:37:34,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 15:22:44,077 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1000\n","[INFO|configuration_utils.py:440] 2022-03-29 15:22:44,078 >> Configuration saved in /tmp/debug_squad/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:22:44,873 >> Model weights saved in /tmp/debug_squad/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:22:44,873 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:22:44,873 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.4393, 'learning_rate': 3.754871920578503e-05, 'epoch': 0.18}\n","  6% 1500/24477 [21:40<5:31:21,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 15:29:58,530 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-1500\n","[INFO|configuration_utils.py:440] 2022-03-29 15:29:58,531 >> Configuration saved in /tmp/debug_squad/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:29:59,352 >> Model weights saved in /tmp/debug_squad/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:29:59,353 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:29:59,353 >> Special tokens file saved in /tmp/debug_squad/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.355, 'learning_rate': 3.673162560771337e-05, 'epoch': 0.25}\n","  8% 2000/24477 [28:55<5:23:18,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 15:37:13,267 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2000\n","[INFO|configuration_utils.py:440] 2022-03-29 15:37:13,268 >> Configuration saved in /tmp/debug_squad/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:37:14,074 >> Model weights saved in /tmp/debug_squad/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:37:14,074 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:37:14,074 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2000/special_tokens_map.json\n","{'loss': 1.283, 'learning_rate': 3.591453200964171e-05, 'epoch': 0.31}\n"," 10% 2500/24477 [36:10<5:16:11,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 15:44:28,163 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-2500\n","[INFO|configuration_utils.py:440] 2022-03-29 15:44:28,164 >> Configuration saved in /tmp/debug_squad/checkpoint-2500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:44:28,983 >> Model weights saved in /tmp/debug_squad/checkpoint-2500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:44:28,984 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-2500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:44:28,984 >> Special tokens file saved in /tmp/debug_squad/checkpoint-2500/special_tokens_map.json\n","{'loss': 1.2514, 'learning_rate': 3.509743841157005e-05, 'epoch': 0.37}\n"," 12% 3000/24477 [43:24<5:09:38,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 15:51:43,086 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3000\n","[INFO|configuration_utils.py:440] 2022-03-29 15:51:43,087 >> Configuration saved in /tmp/debug_squad/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:51:43,911 >> Model weights saved in /tmp/debug_squad/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:51:43,911 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:51:43,911 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3000/special_tokens_map.json\n","{'loss': 1.2211, 'learning_rate': 3.428034481349839e-05, 'epoch': 0.43}\n"," 14% 3500/24477 [50:40<5:01:52,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 15:58:58,315 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-3500\n","[INFO|configuration_utils.py:440] 2022-03-29 15:58:58,316 >> Configuration saved in /tmp/debug_squad/checkpoint-3500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 15:58:59,121 >> Model weights saved in /tmp/debug_squad/checkpoint-3500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 15:58:59,121 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-3500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 15:58:59,121 >> Special tokens file saved in /tmp/debug_squad/checkpoint-3500/special_tokens_map.json\n","{'loss': 1.1815, 'learning_rate': 3.346325121542673e-05, 'epoch': 0.49}\n"," 16% 4000/24477 [57:55<4:54:41,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 16:06:13,553 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4000\n","[INFO|configuration_utils.py:440] 2022-03-29 16:06:13,553 >> Configuration saved in /tmp/debug_squad/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:06:14,353 >> Model weights saved in /tmp/debug_squad/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:06:14,353 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:06:14,353 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4000/special_tokens_map.json\n","{'loss': 1.1448, 'learning_rate': 3.264615761735507e-05, 'epoch': 0.55}\n"," 18% 4500/24477 [1:05:10<4:47:46,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 16:13:28,631 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-4500\n","[INFO|configuration_utils.py:440] 2022-03-29 16:13:28,632 >> Configuration saved in /tmp/debug_squad/checkpoint-4500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:13:29,428 >> Model weights saved in /tmp/debug_squad/checkpoint-4500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:13:29,428 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-4500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:13:29,429 >> Special tokens file saved in /tmp/debug_squad/checkpoint-4500/special_tokens_map.json\n","{'loss': 1.1177, 'learning_rate': 3.182906401928341e-05, 'epoch': 0.61}\n"," 20% 5000/24477 [1:12:25<4:41:15,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 16:20:43,776 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5000\n","[INFO|configuration_utils.py:440] 2022-03-29 16:20:43,777 >> Configuration saved in /tmp/debug_squad/checkpoint-5000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:20:44,586 >> Model weights saved in /tmp/debug_squad/checkpoint-5000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:20:44,586 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:20:44,586 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5000/special_tokens_map.json\n","{'loss': 1.1124, 'learning_rate': 3.101197042121175e-05, 'epoch': 0.67}\n"," 22% 5500/24477 [1:19:40<4:33:34,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 16:27:58,981 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-5500\n","[INFO|configuration_utils.py:440] 2022-03-29 16:27:58,982 >> Configuration saved in /tmp/debug_squad/checkpoint-5500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:27:59,789 >> Model weights saved in /tmp/debug_squad/checkpoint-5500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:27:59,789 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-5500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:27:59,789 >> Special tokens file saved in /tmp/debug_squad/checkpoint-5500/special_tokens_map.json\n","{'loss': 1.0769, 'learning_rate': 3.0194876823140095e-05, 'epoch': 0.74}\n"," 25% 6000/24477 [1:26:56<4:26:41,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 16:35:14,303 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6000\n","[INFO|configuration_utils.py:440] 2022-03-29 16:35:14,304 >> Configuration saved in /tmp/debug_squad/checkpoint-6000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:35:15,105 >> Model weights saved in /tmp/debug_squad/checkpoint-6000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:35:15,105 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:35:15,106 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6000/special_tokens_map.json\n","{'loss': 1.0702, 'learning_rate': 2.9377783225068433e-05, 'epoch': 0.8}\n"," 27% 6500/24477 [1:34:11<4:18:46,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 16:42:29,578 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-6500\n","[INFO|configuration_utils.py:440] 2022-03-29 16:42:29,578 >> Configuration saved in /tmp/debug_squad/checkpoint-6500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:42:30,426 >> Model weights saved in /tmp/debug_squad/checkpoint-6500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:42:30,427 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-6500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:42:30,427 >> Special tokens file saved in /tmp/debug_squad/checkpoint-6500/special_tokens_map.json\n","{'loss': 1.0639, 'learning_rate': 2.8560689626996775e-05, 'epoch': 0.86}\n"," 29% 7000/24477 [1:41:26<4:11:59,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 16:49:44,901 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7000\n","[INFO|configuration_utils.py:440] 2022-03-29 16:49:44,902 >> Configuration saved in /tmp/debug_squad/checkpoint-7000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:49:45,745 >> Model weights saved in /tmp/debug_squad/checkpoint-7000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:49:45,746 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:49:45,746 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7000/special_tokens_map.json\n","{'loss': 1.0295, 'learning_rate': 2.7743596028925117e-05, 'epoch': 0.92}\n"," 31% 7500/24477 [1:48:42<4:04:59,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 16:57:00,378 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-7500\n","[INFO|configuration_utils.py:440] 2022-03-29 16:57:00,379 >> Configuration saved in /tmp/debug_squad/checkpoint-7500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 16:57:01,241 >> Model weights saved in /tmp/debug_squad/checkpoint-7500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 16:57:01,242 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-7500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 16:57:01,242 >> Special tokens file saved in /tmp/debug_squad/checkpoint-7500/special_tokens_map.json\n","{'loss': 1.0261, 'learning_rate': 2.692650243085346e-05, 'epoch': 0.98}\n"," 33% 8000/24477 [1:55:57<3:58:13,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 17:04:16,018 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8000\n","[INFO|configuration_utils.py:440] 2022-03-29 17:04:16,019 >> Configuration saved in /tmp/debug_squad/checkpoint-8000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:04:16,883 >> Model weights saved in /tmp/debug_squad/checkpoint-8000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:04:16,884 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:04:16,884 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8000/special_tokens_map.json\n","{'loss': 0.8375, 'learning_rate': 2.6109408832781794e-05, 'epoch': 1.04}\n"," 35% 8500/24477 [2:03:13<3:50:03,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 17:11:31,905 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-8500\n","[INFO|configuration_utils.py:440] 2022-03-29 17:11:31,906 >> Configuration saved in /tmp/debug_squad/checkpoint-8500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:11:32,726 >> Model weights saved in /tmp/debug_squad/checkpoint-8500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:11:32,727 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-8500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:11:32,727 >> Special tokens file saved in /tmp/debug_squad/checkpoint-8500/special_tokens_map.json\n","{'loss': 0.7584, 'learning_rate': 2.5292315234710136e-05, 'epoch': 1.1}\n"," 37% 9000/24477 [2:10:29<3:43:20,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 17:18:47,594 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9000\n","[INFO|configuration_utils.py:440] 2022-03-29 17:18:47,594 >> Configuration saved in /tmp/debug_squad/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:18:48,436 >> Model weights saved in /tmp/debug_squad/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:18:48,436 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:18:48,436 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9000/special_tokens_map.json\n","{'loss': 0.7314, 'learning_rate': 2.4475221636638477e-05, 'epoch': 1.16}\n"," 39% 9500/24477 [2:17:44<3:36:04,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 17:26:03,091 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-9500\n","[INFO|configuration_utils.py:440] 2022-03-29 17:26:03,092 >> Configuration saved in /tmp/debug_squad/checkpoint-9500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:26:03,850 >> Model weights saved in /tmp/debug_squad/checkpoint-9500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:26:03,850 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-9500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:26:03,850 >> Special tokens file saved in /tmp/debug_squad/checkpoint-9500/special_tokens_map.json\n","{'loss': 0.7269, 'learning_rate': 2.365812803856682e-05, 'epoch': 1.23}\n"," 41% 10000/24477 [2:25:00<3:28:26,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 17:33:18,334 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10000\n","[INFO|configuration_utils.py:440] 2022-03-29 17:33:18,335 >> Configuration saved in /tmp/debug_squad/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:33:19,081 >> Model weights saved in /tmp/debug_squad/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:33:19,081 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:33:19,081 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10000/special_tokens_map.json\n","{'loss': 0.7365, 'learning_rate': 2.284103444049516e-05, 'epoch': 1.29}\n"," 43% 10500/24477 [2:32:15<3:21:49,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 17:40:33,733 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-10500\n","[INFO|configuration_utils.py:440] 2022-03-29 17:40:33,734 >> Configuration saved in /tmp/debug_squad/checkpoint-10500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:40:34,493 >> Model weights saved in /tmp/debug_squad/checkpoint-10500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:40:34,493 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-10500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:40:34,493 >> Special tokens file saved in /tmp/debug_squad/checkpoint-10500/special_tokens_map.json\n","{'loss': 0.7331, 'learning_rate': 2.2023940842423503e-05, 'epoch': 1.35}\n"," 45% 11000/24477 [2:39:31<3:14:13,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 17:47:49,123 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11000\n","[INFO|configuration_utils.py:440] 2022-03-29 17:47:49,124 >> Configuration saved in /tmp/debug_squad/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:47:49,860 >> Model weights saved in /tmp/debug_squad/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:47:49,861 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:47:49,861 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11000/special_tokens_map.json\n","{'loss': 0.7174, 'learning_rate': 2.120684724435184e-05, 'epoch': 1.41}\n"," 47% 11500/24477 [2:46:46<3:07:12,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 17:55:04,444 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-11500\n","[INFO|configuration_utils.py:440] 2022-03-29 17:55:04,445 >> Configuration saved in /tmp/debug_squad/checkpoint-11500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 17:55:05,182 >> Model weights saved in /tmp/debug_squad/checkpoint-11500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 17:55:05,183 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-11500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 17:55:05,183 >> Special tokens file saved in /tmp/debug_squad/checkpoint-11500/special_tokens_map.json\n","{'loss': 0.7083, 'learning_rate': 2.0389753646280183e-05, 'epoch': 1.47}\n"," 49% 12000/24477 [2:54:01<3:00:02,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 18:02:19,715 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12000\n","[INFO|configuration_utils.py:440] 2022-03-29 18:02:19,715 >> Configuration saved in /tmp/debug_squad/checkpoint-12000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 18:02:20,441 >> Model weights saved in /tmp/debug_squad/checkpoint-12000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 18:02:20,442 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 18:02:20,442 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12000/special_tokens_map.json\n","{'loss': 0.6995, 'learning_rate': 1.9572660048208525e-05, 'epoch': 1.53}\n"," 51% 12500/24477 [3:01:16<2:52:58,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 18:09:34,976 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-12500\n","[INFO|configuration_utils.py:440] 2022-03-29 18:09:34,977 >> Configuration saved in /tmp/debug_squad/checkpoint-12500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 18:09:35,721 >> Model weights saved in /tmp/debug_squad/checkpoint-12500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 18:09:35,721 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-12500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 18:09:35,722 >> Special tokens file saved in /tmp/debug_squad/checkpoint-12500/special_tokens_map.json\n","{'loss': 0.7345, 'learning_rate': 1.8755566450136863e-05, 'epoch': 1.59}\n"," 53% 13000/24477 [3:08:32<2:45:41,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 18:16:50,287 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13000\n","[INFO|configuration_utils.py:440] 2022-03-29 18:16:50,288 >> Configuration saved in /tmp/debug_squad/checkpoint-13000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 18:16:51,037 >> Model weights saved in /tmp/debug_squad/checkpoint-13000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 18:16:51,037 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 18:16:51,037 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13000/special_tokens_map.json\n","{'loss': 0.7221, 'learning_rate': 1.7938472852065205e-05, 'epoch': 1.65}\n"," 55% 13500/24477 [3:15:47<2:38:23,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 18:24:05,618 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-13500\n","[INFO|configuration_utils.py:440] 2022-03-29 18:24:05,619 >> Configuration saved in /tmp/debug_squad/checkpoint-13500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 18:24:06,361 >> Model weights saved in /tmp/debug_squad/checkpoint-13500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 18:24:06,361 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-13500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 18:24:06,362 >> Special tokens file saved in /tmp/debug_squad/checkpoint-13500/special_tokens_map.json\n","{'loss': 0.742, 'learning_rate': 1.7121379253993547e-05, 'epoch': 1.72}\n"," 57% 14000/24477 [3:23:02<2:31:01,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 18:31:21,002 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14000\n","[INFO|configuration_utils.py:440] 2022-03-29 18:31:21,003 >> Configuration saved in /tmp/debug_squad/checkpoint-14000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 18:31:21,756 >> Model weights saved in /tmp/debug_squad/checkpoint-14000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 18:31:21,756 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 18:31:21,757 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14000/special_tokens_map.json\n","{'loss': 0.7096, 'learning_rate': 1.630428565592189e-05, 'epoch': 1.78}\n"," 59% 14500/24477 [3:30:18<2:23:38,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 18:38:36,233 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-14500\n","[INFO|configuration_utils.py:440] 2022-03-29 18:38:36,234 >> Configuration saved in /tmp/debug_squad/checkpoint-14500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 18:38:36,963 >> Model weights saved in /tmp/debug_squad/checkpoint-14500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 18:38:36,964 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-14500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 18:38:36,964 >> Special tokens file saved in /tmp/debug_squad/checkpoint-14500/special_tokens_map.json\n","{'loss': 0.6932, 'learning_rate': 1.548719205785023e-05, 'epoch': 1.84}\n"," 61% 15000/24477 [3:37:33<2:16:50,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 18:45:51,489 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15000\n","[INFO|configuration_utils.py:440] 2022-03-29 18:45:51,490 >> Configuration saved in /tmp/debug_squad/checkpoint-15000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 18:45:52,233 >> Model weights saved in /tmp/debug_squad/checkpoint-15000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 18:45:52,234 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 18:45:52,234 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15000/special_tokens_map.json\n","{'loss': 0.7034, 'learning_rate': 1.4670098459778567e-05, 'epoch': 1.9}\n"," 63% 15500/24477 [3:44:48<2:09:36,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 18:53:06,749 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-15500\n","[INFO|configuration_utils.py:440] 2022-03-29 18:53:06,750 >> Configuration saved in /tmp/debug_squad/checkpoint-15500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 18:53:07,486 >> Model weights saved in /tmp/debug_squad/checkpoint-15500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 18:53:07,486 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-15500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 18:53:07,486 >> Special tokens file saved in /tmp/debug_squad/checkpoint-15500/special_tokens_map.json\n","{'loss': 0.7334, 'learning_rate': 1.3853004861706909e-05, 'epoch': 1.96}\n"," 65% 16000/24477 [3:52:04<2:02:23,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 19:00:22,189 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-16000\n","[INFO|configuration_utils.py:440] 2022-03-29 19:00:22,190 >> Configuration saved in /tmp/debug_squad/checkpoint-16000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 19:00:22,961 >> Model weights saved in /tmp/debug_squad/checkpoint-16000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 19:00:22,961 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-16000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 19:00:22,962 >> Special tokens file saved in /tmp/debug_squad/checkpoint-16000/special_tokens_map.json\n","{'loss': 0.5993, 'learning_rate': 1.3035911263635251e-05, 'epoch': 2.02}\n"," 67% 16500/24477 [3:59:19<1:55:15,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 19:07:37,668 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-16500\n","[INFO|configuration_utils.py:440] 2022-03-29 19:07:37,669 >> Configuration saved in /tmp/debug_squad/checkpoint-16500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 19:07:38,424 >> Model weights saved in /tmp/debug_squad/checkpoint-16500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 19:07:38,425 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-16500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 19:07:38,425 >> Special tokens file saved in /tmp/debug_squad/checkpoint-16500/special_tokens_map.json\n","{'loss': 0.4547, 'learning_rate': 1.2218817665563591e-05, 'epoch': 2.08}\n"," 69% 17000/24477 [4:06:35<1:47:55,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 19:14:53,354 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-17000\n","[INFO|configuration_utils.py:440] 2022-03-29 19:14:53,355 >> Configuration saved in /tmp/debug_squad/checkpoint-17000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 19:14:54,089 >> Model weights saved in /tmp/debug_squad/checkpoint-17000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 19:14:54,090 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-17000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 19:14:54,090 >> Special tokens file saved in /tmp/debug_squad/checkpoint-17000/special_tokens_map.json\n","{'loss': 0.4475, 'learning_rate': 1.1401724067491933e-05, 'epoch': 2.14}\n"," 71% 17500/24477 [4:13:50<1:40:32,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 19:22:08,661 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-17500\n","[INFO|configuration_utils.py:440] 2022-03-29 19:22:08,662 >> Configuration saved in /tmp/debug_squad/checkpoint-17500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 19:22:09,397 >> Model weights saved in /tmp/debug_squad/checkpoint-17500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 19:22:09,398 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-17500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 19:22:09,398 >> Special tokens file saved in /tmp/debug_squad/checkpoint-17500/special_tokens_map.json\n","{'loss': 0.448, 'learning_rate': 1.0584630469420275e-05, 'epoch': 2.21}\n"," 74% 18000/24477 [4:21:05<1:33:30,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 19:29:24,065 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-18000\n","[INFO|configuration_utils.py:440] 2022-03-29 19:29:24,066 >> Configuration saved in /tmp/debug_squad/checkpoint-18000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 19:29:24,805 >> Model weights saved in /tmp/debug_squad/checkpoint-18000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 19:29:24,805 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-18000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 19:29:24,806 >> Special tokens file saved in /tmp/debug_squad/checkpoint-18000/special_tokens_map.json\n","{'loss': 0.4379, 'learning_rate': 9.767536871348615e-06, 'epoch': 2.27}\n"," 76% 18500/24477 [4:28:21<1:26:20,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 19:36:39,531 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-18500\n","[INFO|configuration_utils.py:440] 2022-03-29 19:36:39,532 >> Configuration saved in /tmp/debug_squad/checkpoint-18500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 19:36:40,284 >> Model weights saved in /tmp/debug_squad/checkpoint-18500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 19:36:40,284 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-18500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 19:36:40,285 >> Special tokens file saved in /tmp/debug_squad/checkpoint-18500/special_tokens_map.json\n","{'loss': 0.4384, 'learning_rate': 8.950443273276955e-06, 'epoch': 2.33}\n"," 78% 19000/24477 [4:35:37<1:19:00,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 19:43:55,160 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-19000\n","[INFO|configuration_utils.py:440] 2022-03-29 19:43:55,161 >> Configuration saved in /tmp/debug_squad/checkpoint-19000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 19:43:55,892 >> Model weights saved in /tmp/debug_squad/checkpoint-19000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 19:43:55,892 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-19000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 19:43:55,892 >> Special tokens file saved in /tmp/debug_squad/checkpoint-19000/special_tokens_map.json\n","{'loss': 0.4249, 'learning_rate': 8.133349675205295e-06, 'epoch': 2.39}\n"," 80% 19500/24477 [4:42:52<1:11:48,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 19:51:10,512 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-19500\n","[INFO|configuration_utils.py:440] 2022-03-29 19:51:10,513 >> Configuration saved in /tmp/debug_squad/checkpoint-19500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 19:51:11,254 >> Model weights saved in /tmp/debug_squad/checkpoint-19500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 19:51:11,254 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-19500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 19:51:11,255 >> Special tokens file saved in /tmp/debug_squad/checkpoint-19500/special_tokens_map.json\n","{'loss': 0.4423, 'learning_rate': 7.316256077133637e-06, 'epoch': 2.45}\n"," 82% 20000/24477 [4:50:07<1:04:39,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 19:58:25,837 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-20000\n","[INFO|configuration_utils.py:440] 2022-03-29 19:58:25,838 >> Configuration saved in /tmp/debug_squad/checkpoint-20000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 19:58:26,572 >> Model weights saved in /tmp/debug_squad/checkpoint-20000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 19:58:26,572 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-20000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 19:58:26,573 >> Special tokens file saved in /tmp/debug_squad/checkpoint-20000/special_tokens_map.json\n","{'loss': 0.4342, 'learning_rate': 6.499162479061977e-06, 'epoch': 2.51}\n"," 84% 20500/24477 [4:57:23<57:26,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 20:05:41,239 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-20500\n","[INFO|configuration_utils.py:440] 2022-03-29 20:05:41,240 >> Configuration saved in /tmp/debug_squad/checkpoint-20500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 20:05:41,993 >> Model weights saved in /tmp/debug_squad/checkpoint-20500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 20:05:41,994 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-20500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 20:05:41,994 >> Special tokens file saved in /tmp/debug_squad/checkpoint-20500/special_tokens_map.json\n","{'loss': 0.4295, 'learning_rate': 5.682068880990318e-06, 'epoch': 2.57}\n"," 86% 21000/24477 [5:04:38<50:06,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 20:12:56,634 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-21000\n","[INFO|configuration_utils.py:440] 2022-03-29 20:12:56,635 >> Configuration saved in /tmp/debug_squad/checkpoint-21000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 20:12:57,380 >> Model weights saved in /tmp/debug_squad/checkpoint-21000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 20:12:57,381 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-21000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 20:12:57,381 >> Special tokens file saved in /tmp/debug_squad/checkpoint-21000/special_tokens_map.json\n","{'loss': 0.4174, 'learning_rate': 4.864975282918659e-06, 'epoch': 2.64}\n"," 88% 21500/24477 [5:11:53<43:02,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 20:20:12,038 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-21500\n","[INFO|configuration_utils.py:440] 2022-03-29 20:20:12,039 >> Configuration saved in /tmp/debug_squad/checkpoint-21500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 20:20:12,773 >> Model weights saved in /tmp/debug_squad/checkpoint-21500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 20:20:12,773 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-21500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 20:20:12,774 >> Special tokens file saved in /tmp/debug_squad/checkpoint-21500/special_tokens_map.json\n","{'loss': 0.4316, 'learning_rate': 4.047881684846999e-06, 'epoch': 2.7}\n"," 90% 22000/24477 [5:19:09<35:46,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 20:27:27,493 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-22000\n","[INFO|configuration_utils.py:440] 2022-03-29 20:27:27,494 >> Configuration saved in /tmp/debug_squad/checkpoint-22000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 20:27:28,245 >> Model weights saved in /tmp/debug_squad/checkpoint-22000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 20:27:28,246 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-22000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 20:27:28,246 >> Special tokens file saved in /tmp/debug_squad/checkpoint-22000/special_tokens_map.json\n","{'loss': 0.4237, 'learning_rate': 3.2307880867753403e-06, 'epoch': 2.76}\n"," 92% 22500/24477 [5:26:25<28:32,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 20:34:43,134 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-22500\n","[INFO|configuration_utils.py:440] 2022-03-29 20:34:43,134 >> Configuration saved in /tmp/debug_squad/checkpoint-22500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 20:34:43,882 >> Model weights saved in /tmp/debug_squad/checkpoint-22500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 20:34:43,883 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-22500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 20:34:43,883 >> Special tokens file saved in /tmp/debug_squad/checkpoint-22500/special_tokens_map.json\n","{'loss': 0.4184, 'learning_rate': 2.4136944887036813e-06, 'epoch': 2.82}\n"," 94% 23000/24477 [5:33:40<21:19,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 20:41:58,747 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-23000\n","[INFO|configuration_utils.py:440] 2022-03-29 20:41:58,748 >> Configuration saved in /tmp/debug_squad/checkpoint-23000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 20:41:59,473 >> Model weights saved in /tmp/debug_squad/checkpoint-23000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 20:41:59,473 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-23000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 20:41:59,474 >> Special tokens file saved in /tmp/debug_squad/checkpoint-23000/special_tokens_map.json\n","{'loss': 0.4269, 'learning_rate': 1.596600890632022e-06, 'epoch': 2.88}\n"," 96% 23500/24477 [5:40:56<14:05,  1.16it/s][INFO|trainer.py:2162] 2022-03-29 20:49:14,473 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-23500\n","[INFO|configuration_utils.py:440] 2022-03-29 20:49:14,474 >> Configuration saved in /tmp/debug_squad/checkpoint-23500/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 20:49:15,198 >> Model weights saved in /tmp/debug_squad/checkpoint-23500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 20:49:15,198 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-23500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 20:49:15,199 >> Special tokens file saved in /tmp/debug_squad/checkpoint-23500/special_tokens_map.json\n","{'loss': 0.4271, 'learning_rate': 7.795072925603629e-07, 'epoch': 2.94}\n"," 98% 24000/24477 [5:48:11<06:53,  1.15it/s][INFO|trainer.py:2162] 2022-03-29 20:56:29,769 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-24000\n","[INFO|configuration_utils.py:440] 2022-03-29 20:56:29,770 >> Configuration saved in /tmp/debug_squad/checkpoint-24000/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 20:56:30,514 >> Model weights saved in /tmp/debug_squad/checkpoint-24000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 20:56:30,514 >> tokenizer config file saved in /tmp/debug_squad/checkpoint-24000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 20:56:30,515 >> Special tokens file saved in /tmp/debug_squad/checkpoint-24000/special_tokens_map.json\n","100% 24477/24477 [5:55:07<00:00,  1.16it/s][INFO|trainer.py:1526] 2022-03-29 21:03:25,334 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 21307.2492, 'train_samples_per_second': 18.38, 'train_steps_per_second': 1.149, 'train_loss': 0.8034816037041789, 'epoch': 3.0}\n","100% 24477/24477 [5:55:07<00:00,  1.15it/s]\n","[INFO|trainer.py:2162] 2022-03-29 21:03:25,337 >> Saving model checkpoint to /tmp/debug_squad/\n","[INFO|configuration_utils.py:440] 2022-03-29 21:03:25,337 >> Configuration saved in /tmp/debug_squad/config.json\n","[INFO|modeling_utils.py:1377] 2022-03-29 21:03:26,099 >> Model weights saved in /tmp/debug_squad/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2086] 2022-03-29 21:03:26,099 >> tokenizer config file saved in /tmp/debug_squad/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2092] 2022-03-29 21:03:26,099 >> Special tokens file saved in /tmp/debug_squad/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =     0.8035\n","  train_runtime            = 5:55:07.24\n","  train_samples            =     130544\n","  train_samples_per_second =      18.38\n","  train_steps_per_second   =      1.149\n","03/29/2022 21:03:26 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:571] 2022-03-29 21:03:26,135 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2412] 2022-03-29 21:03:26,144 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2414] 2022-03-29 21:03:26,144 >>   Num examples = 11974\n","[INFO|trainer.py:2417] 2022-03-29 21:03:26,144 >>   Batch size = 8\n","100% 1497/1497 [03:53<00:00,  6.87it/s]03/29/2022 21:07:34 - INFO - utils_qa - Post-processing 11873 example predictions split into 11974 features.\n","\n","  0% 0/11873 [00:00<?, ?it/s]\u001b[A\n","  0% 33/11873 [00:00<00:36, 328.39it/s]\u001b[A\n","  1% 71/11873 [00:00<00:33, 357.37it/s]\u001b[A\n","  1% 111/11873 [00:00<00:31, 373.91it/s]\u001b[A\n","  1% 152/11873 [00:00<00:30, 385.27it/s]\u001b[A\n","  2% 191/11873 [00:00<00:30, 385.86it/s]\u001b[A\n","  2% 232/11873 [00:00<00:29, 390.92it/s]\u001b[A\n","  2% 272/11873 [00:00<00:29, 392.12it/s]\u001b[A\n","  3% 314/11873 [00:00<00:29, 398.02it/s]\u001b[A\n","  3% 354/11873 [00:00<00:30, 383.07it/s]\u001b[A\n","  3% 393/11873 [00:01<00:30, 380.95it/s]\u001b[A\n","  4% 432/11873 [00:01<00:30, 378.25it/s]\u001b[A\n","  4% 470/11873 [00:01<00:30, 378.60it/s]\u001b[A\n","  4% 508/11873 [00:01<00:30, 374.23it/s]\u001b[A\n","  5% 548/11873 [00:01<00:29, 379.05it/s]\u001b[A\n","  5% 587/11873 [00:01<00:29, 382.13it/s]\u001b[A\n","  5% 628/11873 [00:01<00:28, 388.69it/s]\u001b[A\n","  6% 670/11873 [00:01<00:28, 395.46it/s]\u001b[A\n","  6% 710/11873 [00:01<00:28, 394.85it/s]\u001b[A\n","  6% 750/11873 [00:01<00:28, 394.75it/s]\u001b[A\n","  7% 791/11873 [00:02<00:27, 396.35it/s]\u001b[A\n","  7% 833/11873 [00:02<00:27, 400.77it/s]\u001b[A\n","  7% 877/11873 [00:02<00:26, 409.89it/s]\u001b[A\n","  8% 918/11873 [00:02<00:27, 400.40it/s]\u001b[A\n","  8% 959/11873 [00:02<00:27, 392.37it/s]\u001b[A\n","  8% 999/11873 [00:02<00:29, 370.37it/s]\u001b[A\n","  9% 1037/11873 [00:02<00:31, 340.08it/s]\u001b[A\n","  9% 1072/11873 [00:02<00:33, 320.13it/s]\u001b[A\n","  9% 1105/11873 [00:02<00:35, 304.95it/s]\u001b[A\n"," 10% 1136/11873 [00:03<00:35, 300.27it/s]\u001b[A\n"," 10% 1167/11873 [00:03<00:36, 296.20it/s]\u001b[A\n"," 10% 1198/11873 [00:03<00:35, 299.10it/s]\u001b[A\n"," 10% 1229/11873 [00:03<00:35, 300.95it/s]\u001b[A\n"," 11% 1260/11873 [00:03<00:35, 301.40it/s]\u001b[A\n"," 11% 1291/11873 [00:03<00:35, 301.01it/s]\u001b[A\n"," 11% 1322/11873 [00:03<00:34, 303.17it/s]\u001b[A\n"," 11% 1353/11873 [00:03<00:34, 301.85it/s]\u001b[A\n"," 12% 1384/11873 [00:03<00:34, 301.00it/s]\u001b[A\n"," 12% 1416/11873 [00:04<00:34, 303.79it/s]\u001b[A\n"," 12% 1447/11873 [00:04<00:34, 302.76it/s]\u001b[A\n"," 12% 1478/11873 [00:04<00:34, 302.45it/s]\u001b[A\n"," 13% 1510/11873 [00:04<00:33, 305.18it/s]\u001b[A\n"," 13% 1541/11873 [00:04<00:33, 306.21it/s]\u001b[A\n"," 13% 1572/11873 [00:04<00:33, 306.26it/s]\u001b[A\n"," 14% 1603/11873 [00:04<00:33, 307.12it/s]\u001b[A\n"," 14% 1634/11873 [00:04<00:33, 304.13it/s]\u001b[A\n"," 14% 1665/11873 [00:04<00:33, 305.13it/s]\u001b[A\n"," 14% 1696/11873 [00:04<00:33, 303.32it/s]\u001b[A\n"," 15% 1727/11873 [00:05<00:33, 299.74it/s]\u001b[A\n"," 15% 1758/11873 [00:05<00:33, 302.20it/s]\u001b[A\n"," 15% 1789/11873 [00:05<00:33, 301.28it/s]\u001b[A\n"," 15% 1820/11873 [00:05<00:33, 303.16it/s]\u001b[A\n"," 16% 1851/11873 [00:05<00:33, 302.99it/s]\u001b[A\n"," 16% 1882/11873 [00:05<00:33, 295.10it/s]\u001b[A\n"," 16% 1913/11873 [00:05<00:33, 297.97it/s]\u001b[A\n"," 16% 1945/11873 [00:05<00:32, 301.80it/s]\u001b[A\n"," 17% 1976/11873 [00:05<00:32, 302.33it/s]\u001b[A\n"," 17% 2007/11873 [00:05<00:33, 295.48it/s]\u001b[A\n"," 17% 2039/11873 [00:06<00:32, 300.74it/s]\u001b[A\n"," 17% 2070/11873 [00:06<00:32, 301.71it/s]\u001b[A\n"," 18% 2102/11873 [00:06<00:32, 304.48it/s]\u001b[A\n"," 18% 2133/11873 [00:06<00:32, 301.16it/s]\u001b[A\n"," 18% 2164/11873 [00:06<00:32, 302.74it/s]\u001b[A\n"," 18% 2195/11873 [00:06<00:31, 303.72it/s]\u001b[A\n"," 19% 2226/11873 [00:06<00:31, 303.63it/s]\u001b[A\n"," 19% 2258/11873 [00:06<00:31, 306.65it/s]\u001b[A\n"," 19% 2289/11873 [00:06<00:31, 303.97it/s]\u001b[A\n"," 20% 2320/11873 [00:06<00:31, 304.64it/s]\u001b[A\n"," 20% 2351/11873 [00:07<00:31, 303.30it/s]\u001b[A\n"," 20% 2382/11873 [00:07<00:31, 302.05it/s]\u001b[A\n"," 20% 2413/11873 [00:07<00:31, 301.94it/s]\u001b[A\n"," 21% 2444/11873 [00:07<00:31, 298.57it/s]\u001b[A\n"," 21% 2475/11873 [00:07<00:31, 301.33it/s]\u001b[A\n"," 21% 2506/11873 [00:07<00:31, 300.95it/s]\u001b[A\n"," 21% 2537/11873 [00:07<00:30, 303.25it/s]\u001b[A\n"," 22% 2568/11873 [00:07<00:30, 300.66it/s]\u001b[A\n"," 22% 2599/11873 [00:07<00:30, 302.68it/s]\u001b[A\n"," 22% 2630/11873 [00:08<00:30, 303.29it/s]\u001b[A\n"," 22% 2661/11873 [00:08<00:30, 303.96it/s]\u001b[A\n"," 23% 2692/11873 [00:08<00:30, 304.49it/s]\u001b[A\n"," 23% 2723/11873 [00:08<00:30, 304.37it/s]\u001b[A\n"," 23% 2754/11873 [00:08<00:30, 303.73it/s]\u001b[A\n"," 23% 2785/11873 [00:08<00:30, 299.34it/s]\u001b[A\n"," 24% 2816/11873 [00:08<00:29, 302.14it/s]\u001b[A\n"," 24% 2848/11873 [00:08<00:29, 304.80it/s]\u001b[A\n"," 24% 2879/11873 [00:08<00:29, 304.48it/s]\u001b[A\n"," 25% 2910/11873 [00:08<00:29, 304.56it/s]\u001b[A\n"," 25% 2941/11873 [00:09<00:29, 304.81it/s]\u001b[A\n"," 25% 2972/11873 [00:09<00:29, 305.74it/s]\u001b[A\n"," 25% 3003/11873 [00:09<00:29, 301.57it/s]\u001b[A\n"," 26% 3034/11873 [00:09<00:29, 301.18it/s]\u001b[A\n"," 26% 3065/11873 [00:09<00:29, 293.77it/s]\u001b[A\n"," 26% 3095/11873 [00:09<00:30, 291.87it/s]\u001b[A\n"," 26% 3125/11873 [00:09<00:33, 263.29it/s]\u001b[A\n"," 27% 3152/11873 [00:09<00:33, 260.92it/s]\u001b[A\n"," 27% 3179/11873 [00:09<00:34, 251.57it/s]\u001b[A\n"," 27% 3210/11873 [00:10<00:32, 265.93it/s]\u001b[A\n"," 27% 3241/11873 [00:10<00:31, 276.06it/s]\u001b[A\n"," 28% 3270/11873 [00:10<00:30, 279.66it/s]\u001b[A\n"," 28% 3299/11873 [00:10<00:38, 225.08it/s]\u001b[A\n"," 28% 3324/11873 [00:10<00:39, 215.23it/s]\u001b[A\n"," 28% 3350/11873 [00:10<00:37, 225.30it/s]\u001b[A\n"," 28% 3374/11873 [00:10<00:40, 208.88it/s]\u001b[A\n"," 29% 3405/11873 [00:10<00:36, 232.86it/s]\u001b[A\n"," 29% 3436/11873 [00:11<00:33, 253.16it/s]\u001b[A\n"," 29% 3467/11873 [00:11<00:31, 267.94it/s]\u001b[A\n"," 29% 3498/11873 [00:11<00:30, 277.16it/s]\u001b[A\n"," 30% 3529/11873 [00:11<00:29, 284.69it/s]\u001b[A\n"," 30% 3559/11873 [00:11<00:28, 287.86it/s]\u001b[A\n"," 30% 3590/11873 [00:11<00:28, 293.71it/s]\u001b[A\n"," 30% 3621/11873 [00:11<00:27, 297.61it/s]\u001b[A\n"," 31% 3652/11873 [00:11<00:27, 299.89it/s]\u001b[A\n"," 31% 3683/11873 [00:11<00:27, 296.57it/s]\u001b[A\n"," 31% 3713/11873 [00:11<00:27, 295.38it/s]\u001b[A\n"," 32% 3743/11873 [00:12<00:27, 293.34it/s]\u001b[A\n"," 32% 3774/11873 [00:12<00:27, 296.47it/s]\u001b[A\n"," 32% 3805/11873 [00:12<00:27, 298.66it/s]\u001b[A\n"," 32% 3835/11873 [00:12<00:26, 298.76it/s]\u001b[A\n"," 33% 3866/11873 [00:12<00:26, 299.81it/s]\u001b[A\n"," 33% 3898/11873 [00:12<00:26, 302.96it/s]\u001b[A\n"," 33% 3929/11873 [00:12<00:26, 295.37it/s]\u001b[A\n"," 33% 3959/11873 [00:12<00:26, 295.44it/s]\u001b[A\n"," 34% 3989/11873 [00:12<00:26, 295.05it/s]\u001b[A\n"," 34% 4020/11873 [00:12<00:26, 297.99it/s]\u001b[A\n"," 34% 4052/11873 [00:13<00:25, 301.77it/s]\u001b[A\n"," 34% 4084/11873 [00:13<00:25, 305.27it/s]\u001b[A\n"," 35% 4115/11873 [00:13<00:25, 304.02it/s]\u001b[A\n"," 35% 4146/11873 [00:13<00:25, 303.77it/s]\u001b[A\n"," 35% 4177/11873 [00:13<00:25, 297.37it/s]\u001b[A\n"," 35% 4207/11873 [00:13<00:25, 297.77it/s]\u001b[A\n"," 36% 4238/11873 [00:13<00:25, 299.85it/s]\u001b[A\n"," 36% 4269/11873 [00:13<00:25, 301.82it/s]\u001b[A\n"," 36% 4300/11873 [00:13<00:26, 288.61it/s]\u001b[A\n"," 36% 4332/11873 [00:13<00:25, 296.74it/s]\u001b[A\n"," 37% 4364/11873 [00:14<00:24, 302.29it/s]\u001b[A\n"," 37% 4395/11873 [00:14<00:24, 301.91it/s]\u001b[A\n"," 37% 4426/11873 [00:14<00:28, 263.40it/s]\u001b[A\n"," 38% 4456/11873 [00:14<00:27, 272.14it/s]\u001b[A\n"," 38% 4485/11873 [00:14<00:26, 276.56it/s]\u001b[A\n"," 38% 4514/11873 [00:14<00:26, 276.25it/s]\u001b[A\n"," 38% 4543/11873 [00:14<00:26, 277.04it/s]\u001b[A\n"," 39% 4572/11873 [00:14<00:26, 280.10it/s]\u001b[A\n"," 39% 4601/11873 [00:14<00:26, 279.33it/s]\u001b[A\n"," 39% 4630/11873 [00:15<00:25, 282.18it/s]\u001b[A\n"," 39% 4659/11873 [00:15<00:25, 283.88it/s]\u001b[A\n"," 39% 4688/11873 [00:15<00:25, 285.38it/s]\u001b[A\n"," 40% 4717/11873 [00:15<00:25, 281.26it/s]\u001b[A\n"," 40% 4746/11873 [00:15<00:25, 279.33it/s]\u001b[A\n"," 40% 4775/11873 [00:15<00:25, 279.54it/s]\u001b[A\n"," 40% 4803/11873 [00:15<00:25, 275.48it/s]\u001b[A\n"," 41% 4833/11873 [00:15<00:24, 282.45it/s]\u001b[A\n"," 41% 4863/11873 [00:15<00:24, 287.16it/s]\u001b[A\n"," 41% 4894/11873 [00:15<00:23, 291.64it/s]\u001b[A\n"," 41% 4925/11873 [00:16<00:23, 296.34it/s]\u001b[A\n"," 42% 4956/11873 [00:16<00:23, 299.82it/s]\u001b[A\n"," 42% 4987/11873 [00:16<00:22, 302.37it/s]\u001b[A\n"," 42% 5018/11873 [00:16<00:22, 300.54it/s]\u001b[A\n"," 43% 5049/11873 [00:16<00:23, 289.70it/s]\u001b[A\n"," 43% 5079/11873 [00:16<00:24, 282.91it/s]\u001b[A\n"," 43% 5109/11873 [00:16<00:23, 287.08it/s]\u001b[A\n"," 43% 5139/11873 [00:16<00:23, 289.67it/s]\u001b[A\n"," 44% 5170/11873 [00:16<00:22, 293.59it/s]\u001b[A\n"," 44% 5200/11873 [00:17<00:22, 290.99it/s]\u001b[A\n"," 44% 5230/11873 [00:17<00:23, 285.35it/s]\u001b[A\n"," 44% 5259/11873 [00:17<00:25, 259.17it/s]\u001b[A\n"," 45% 5286/11873 [00:17<00:25, 257.17it/s]\u001b[A\n"," 45% 5314/11873 [00:17<00:24, 262.97it/s]\u001b[A\n"," 45% 5344/11873 [00:17<00:23, 273.24it/s]\u001b[A\n"," 45% 5375/11873 [00:17<00:22, 283.59it/s]\u001b[A\n"," 46% 5406/11873 [00:17<00:22, 289.25it/s]\u001b[A\n"," 46% 5437/11873 [00:17<00:21, 292.98it/s]\u001b[A\n"," 46% 5467/11873 [00:18<00:21, 293.87it/s]\u001b[A\n"," 46% 5497/11873 [00:18<00:21, 294.38it/s]\u001b[A\n"," 47% 5528/11873 [00:18<00:21, 298.92it/s]\u001b[A\n"," 47% 5559/11873 [00:18<00:21, 299.42it/s]\u001b[A\n"," 47% 5590/11873 [00:18<00:20, 300.54it/s]\u001b[A\n"," 47% 5621/11873 [00:18<00:20, 299.85it/s]\u001b[A\n"," 48% 5652/11873 [00:18<00:20, 301.03it/s]\u001b[A\n"," 48% 5683/11873 [00:18<00:20, 302.14it/s]\u001b[A\n"," 48% 5714/11873 [00:18<00:20, 300.54it/s]\u001b[A\n"," 48% 5745/11873 [00:18<00:20, 302.09it/s]\u001b[A\n"," 49% 5776/11873 [00:19<00:20, 302.26it/s]\u001b[A\n"," 49% 5808/11873 [00:19<00:19, 305.97it/s]\u001b[A\n"," 49% 5840/11873 [00:19<00:19, 308.96it/s]\u001b[A\n"," 49% 5872/11873 [00:19<00:19, 311.04it/s]\u001b[A\n"," 50% 5904/11873 [00:19<00:19, 309.80it/s]\u001b[A\n"," 50% 5935/11873 [00:19<00:19, 301.01it/s]\u001b[A\n"," 50% 5966/11873 [00:19<00:20, 294.65it/s]\u001b[A\n"," 51% 5997/11873 [00:19<00:19, 297.28it/s]\u001b[A\n"," 51% 6028/11873 [00:19<00:19, 299.37it/s]\u001b[A\n"," 51% 6059/11873 [00:19<00:19, 302.34it/s]\u001b[A\n"," 51% 6090/11873 [00:20<00:19, 296.37it/s]\u001b[A\n"," 52% 6121/11873 [00:20<00:19, 298.10it/s]\u001b[A\n"," 52% 6151/11873 [00:20<00:20, 283.97it/s]\u001b[A\n"," 52% 6180/11873 [00:20<00:19, 285.35it/s]\u001b[A\n"," 52% 6209/11873 [00:20<00:19, 285.36it/s]\u001b[A\n"," 53% 6240/11873 [00:20<00:19, 291.13it/s]\u001b[A\n"," 53% 6270/11873 [00:20<00:19, 292.65it/s]\u001b[A\n"," 53% 6301/11873 [00:20<00:18, 295.99it/s]\u001b[A\n"," 53% 6332/11873 [00:20<00:18, 298.55it/s]\u001b[A\n"," 54% 6363/11873 [00:20<00:18, 301.60it/s]\u001b[A\n"," 54% 6394/11873 [00:21<00:18, 295.32it/s]\u001b[A\n"," 54% 6424/11873 [00:21<00:18, 291.97it/s]\u001b[A\n"," 54% 6454/11873 [00:21<00:18, 289.91it/s]\u001b[A\n"," 55% 6484/11873 [00:21<00:18, 290.42it/s]\u001b[A\n"," 55% 6514/11873 [00:21<00:18, 288.52it/s]\u001b[A\n"," 55% 6545/11873 [00:21<00:18, 294.20it/s]\u001b[A\n"," 55% 6576/11873 [00:21<00:17, 297.20it/s]\u001b[A\n"," 56% 6607/11873 [00:21<00:17, 299.46it/s]\u001b[A\n"," 56% 6637/11873 [00:21<00:17, 299.56it/s]\u001b[A\n"," 56% 6668/11873 [00:22<00:17, 301.94it/s]\u001b[A\n"," 56% 6699/11873 [00:22<00:17, 302.06it/s]\u001b[A\n"," 57% 6730/11873 [00:22<00:17, 298.58it/s]\u001b[A\n"," 57% 6762/11873 [00:22<00:16, 302.54it/s]\u001b[A\n"," 57% 6793/11873 [00:22<00:16, 303.09it/s]\u001b[A\n"," 57% 6824/11873 [00:22<00:16, 301.39it/s]\u001b[A\n"," 58% 6855/11873 [00:22<00:17, 291.36it/s]\u001b[A\n"," 58% 6886/11873 [00:22<00:16, 294.81it/s]\u001b[A\n"," 58% 6916/11873 [00:22<00:16, 293.64it/s]\u001b[A\n"," 59% 6947/11873 [00:22<00:16, 297.18it/s]\u001b[A\n"," 59% 6977/11873 [00:23<00:16, 297.95it/s]\u001b[A\n"," 59% 7009/11873 [00:23<00:16, 302.43it/s]\u001b[A\n"," 59% 7040/11873 [00:23<00:15, 303.07it/s]\u001b[A\n"," 60% 7071/11873 [00:23<00:15, 304.67it/s]\u001b[A\n"," 60% 7103/11873 [00:23<00:15, 306.64it/s]\u001b[A\n"," 60% 7134/11873 [00:23<00:15, 299.90it/s]\u001b[A\n"," 60% 7166/11873 [00:23<00:15, 303.04it/s]\u001b[A\n"," 61% 7197/11873 [00:23<00:15, 304.48it/s]\u001b[A\n"," 61% 7228/11873 [00:23<00:15, 305.65it/s]\u001b[A\n"," 61% 7259/11873 [00:23<00:15, 303.90it/s]\u001b[A\n"," 61% 7290/11873 [00:24<00:15, 303.13it/s]\u001b[A\n"," 62% 7321/11873 [00:24<00:15, 299.30it/s]\u001b[A\n"," 62% 7351/11873 [00:24<00:15, 299.04it/s]\u001b[A\n"," 62% 7382/11873 [00:24<00:14, 300.49it/s]\u001b[A\n"," 62% 7413/11873 [00:24<00:15, 297.06it/s]\u001b[A\n"," 63% 7443/11873 [00:24<00:15, 290.32it/s]\u001b[A\n"," 63% 7473/11873 [00:24<00:15, 288.04it/s]\u001b[A\n"," 63% 7502/11873 [00:24<00:15, 288.48it/s]\u001b[A\n"," 63% 7532/11873 [00:24<00:15, 289.01it/s]\u001b[A\n"," 64% 7561/11873 [00:25<00:15, 286.73it/s]\u001b[A\n"," 64% 7590/11873 [00:25<00:14, 287.64it/s]\u001b[A\n"," 64% 7620/11873 [00:25<00:14, 289.81it/s]\u001b[A\n"," 64% 7650/11873 [00:25<00:14, 290.45it/s]\u001b[A\n"," 65% 7681/11873 [00:25<00:14, 294.92it/s]\u001b[A\n"," 65% 7711/11873 [00:25<00:14, 292.72it/s]\u001b[A\n"," 65% 7742/11873 [00:25<00:14, 294.94it/s]\u001b[A\n"," 65% 7772/11873 [00:25<00:13, 296.10it/s]\u001b[A\n"," 66% 7802/11873 [00:25<00:13, 296.50it/s]\u001b[A\n"," 66% 7832/11873 [00:25<00:13, 296.54it/s]\u001b[A\n"," 66% 7862/11873 [00:26<00:13, 294.29it/s]\u001b[A\n"," 66% 7892/11873 [00:26<00:13, 293.86it/s]\u001b[A\n"," 67% 7923/11873 [00:26<00:13, 296.74it/s]\u001b[A\n"," 67% 7953/11873 [00:26<00:13, 290.24it/s]\u001b[A\n"," 67% 7983/11873 [00:26<00:13, 279.18it/s]\u001b[A\n"," 67% 8012/11873 [00:26<00:13, 278.23it/s]\u001b[A\n"," 68% 8043/11873 [00:26<00:13, 286.22it/s]\u001b[A\n"," 68% 8075/11873 [00:26<00:12, 293.47it/s]\u001b[A\n"," 68% 8105/11873 [00:26<00:12, 292.79it/s]\u001b[A\n"," 69% 8135/11873 [00:27<00:12, 294.36it/s]\u001b[A\n"," 69% 8165/11873 [00:27<00:12, 293.90it/s]\u001b[A\n"," 69% 8195/11873 [00:27<00:12, 289.00it/s]\u001b[A\n"," 69% 8224/11873 [00:27<00:18, 202.43it/s]\u001b[A\n"," 70% 8255/11873 [00:27<00:16, 225.62it/s]\u001b[A\n"," 70% 8286/11873 [00:27<00:14, 245.91it/s]\u001b[A\n"," 70% 8317/11873 [00:27<00:13, 261.05it/s]\u001b[A\n"," 70% 8348/11873 [00:27<00:12, 273.26it/s]\u001b[A\n"," 71% 8377/11873 [00:27<00:12, 275.03it/s]\u001b[A\n"," 71% 8408/11873 [00:28<00:12, 282.17it/s]\u001b[A\n"," 71% 8438/11873 [00:28<00:12, 280.95it/s]\u001b[A\n"," 71% 8467/11873 [00:28<00:12, 280.81it/s]\u001b[A\n"," 72% 8496/11873 [00:28<00:11, 283.05it/s]\u001b[A\n"," 72% 8527/11873 [00:28<00:11, 288.42it/s]\u001b[A\n"," 72% 8557/11873 [00:28<00:11, 288.54it/s]\u001b[A\n"," 72% 8587/11873 [00:28<00:11, 287.28it/s]\u001b[A\n"," 73% 8616/11873 [00:28<00:11, 287.70it/s]\u001b[A\n"," 73% 8645/11873 [00:28<00:11, 287.35it/s]\u001b[A\n"," 73% 8675/11873 [00:28<00:11, 289.44it/s]\u001b[A\n"," 73% 8704/11873 [00:29<00:10, 288.29it/s]\u001b[A\n"," 74% 8736/11873 [00:29<00:10, 296.27it/s]\u001b[A\n"," 74% 8766/11873 [00:29<00:10, 296.21it/s]\u001b[A\n"," 74% 8798/11873 [00:29<00:10, 301.27it/s]\u001b[A\n"," 74% 8829/11873 [00:29<00:10, 300.08it/s]\u001b[A\n"," 75% 8860/11873 [00:29<00:10, 300.26it/s]\u001b[A\n"," 75% 8891/11873 [00:29<00:10, 294.96it/s]\u001b[A\n"," 75% 8921/11873 [00:29<00:09, 295.34it/s]\u001b[A\n"," 75% 8951/11873 [00:29<00:09, 295.46it/s]\u001b[A\n"," 76% 8982/11873 [00:30<00:09, 299.37it/s]\u001b[A\n"," 76% 9012/11873 [00:30<00:09, 299.13it/s]\u001b[A\n"," 76% 9042/11873 [00:30<00:09, 296.28it/s]\u001b[A\n"," 76% 9074/11873 [00:30<00:09, 300.51it/s]\u001b[A\n"," 77% 9105/11873 [00:30<00:09, 301.38it/s]\u001b[A\n"," 77% 9136/11873 [00:30<00:09, 301.84it/s]\u001b[A\n"," 77% 9167/11873 [00:30<00:08, 302.21it/s]\u001b[A\n"," 77% 9198/11873 [00:30<00:08, 302.97it/s]\u001b[A\n"," 78% 9229/11873 [00:30<00:08, 304.90it/s]\u001b[A\n"," 78% 9260/11873 [00:30<00:08, 303.62it/s]\u001b[A\n"," 78% 9291/11873 [00:31<00:08, 299.35it/s]\u001b[A\n"," 79% 9322/11873 [00:31<00:08, 300.65it/s]\u001b[A\n"," 79% 9353/11873 [00:31<00:08, 294.75it/s]\u001b[A\n"," 79% 9383/11873 [00:31<00:08, 293.87it/s]\u001b[A\n"," 79% 9413/11873 [00:31<00:08, 291.65it/s]\u001b[A\n"," 80% 9444/11873 [00:31<00:08, 294.45it/s]\u001b[A\n"," 80% 9475/11873 [00:31<00:08, 298.57it/s]\u001b[A\n"," 80% 9507/11873 [00:31<00:07, 302.18it/s]\u001b[A\n"," 80% 9539/11873 [00:31<00:07, 305.14it/s]\u001b[A\n"," 81% 9570/11873 [00:31<00:07, 302.48it/s]\u001b[A\n"," 81% 9601/11873 [00:32<00:07, 303.55it/s]\u001b[A\n"," 81% 9632/11873 [00:32<00:07, 296.05it/s]\u001b[A\n"," 81% 9662/11873 [00:32<00:07, 289.69it/s]\u001b[A\n"," 82% 9694/11873 [00:32<00:07, 296.31it/s]\u001b[A\n"," 82% 9726/11873 [00:32<00:07, 301.78it/s]\u001b[A\n"," 82% 9758/11873 [00:32<00:06, 304.62it/s]\u001b[A\n"," 82% 9789/11873 [00:32<00:06, 305.49it/s]\u001b[A\n"," 83% 9820/11873 [00:32<00:06, 306.36it/s]\u001b[A\n"," 83% 9851/11873 [00:32<00:06, 303.64it/s]\u001b[A\n"," 83% 9882/11873 [00:33<00:06, 304.75it/s]\u001b[A\n"," 83% 9913/11873 [00:33<00:06, 303.83it/s]\u001b[A\n"," 84% 9944/11873 [00:33<00:06, 305.60it/s]\u001b[A\n"," 84% 9976/11873 [00:33<00:06, 307.55it/s]\u001b[A\n"," 84% 10007/11873 [00:33<00:06, 305.89it/s]\u001b[A\n"," 85% 10039/11873 [00:33<00:05, 308.51it/s]\u001b[A\n"," 85% 10071/11873 [00:33<00:05, 310.20it/s]\u001b[A\n"," 85% 10103/11873 [00:33<00:05, 311.75it/s]\u001b[A\n"," 85% 10135/11873 [00:33<00:05, 310.89it/s]\u001b[A\n"," 86% 10167/11873 [00:33<00:05, 306.47it/s]\u001b[A\n"," 86% 10198/11873 [00:34<00:05, 304.22it/s]\u001b[A\n"," 86% 10230/11873 [00:34<00:05, 306.34it/s]\u001b[A\n"," 86% 10261/11873 [00:34<00:05, 299.27it/s]\u001b[A\n"," 87% 10291/11873 [00:34<00:05, 290.96it/s]\u001b[A\n"," 87% 10321/11873 [00:34<00:05, 288.70it/s]\u001b[A\n"," 87% 10351/11873 [00:34<00:05, 289.45it/s]\u001b[A\n"," 87% 10380/11873 [00:34<00:05, 289.18it/s]\u001b[A\n"," 88% 10412/11873 [00:34<00:04, 296.24it/s]\u001b[A\n"," 88% 10442/11873 [00:34<00:04, 296.17it/s]\u001b[A\n"," 88% 10474/11873 [00:34<00:04, 301.63it/s]\u001b[A\n"," 88% 10505/11873 [00:35<00:04, 301.80it/s]\u001b[A\n"," 89% 10536/11873 [00:35<00:04, 303.02it/s]\u001b[A\n"," 89% 10567/11873 [00:35<00:04, 302.42it/s]\u001b[A\n"," 89% 10598/11873 [00:35<00:04, 302.40it/s]\u001b[A\n"," 90% 10629/11873 [00:35<00:04, 302.02it/s]\u001b[A\n"," 90% 10660/11873 [00:35<00:04, 301.92it/s]\u001b[A\n"," 90% 10691/11873 [00:35<00:03, 300.87it/s]\u001b[A\n"," 90% 10722/11873 [00:35<00:03, 303.40it/s]\u001b[A\n"," 91% 10753/11873 [00:35<00:03, 303.72it/s]\u001b[A\n"," 91% 10784/11873 [00:36<00:03, 302.52it/s]\u001b[A\n"," 91% 10815/11873 [00:36<00:03, 302.63it/s]\u001b[A\n"," 91% 10846/11873 [00:36<00:03, 294.61it/s]\u001b[A\n"," 92% 10876/11873 [00:36<00:03, 292.76it/s]\u001b[A\n"," 92% 10906/11873 [00:36<00:03, 290.45it/s]\u001b[A\n"," 92% 10936/11873 [00:36<00:03, 282.53it/s]\u001b[A\n"," 92% 10965/11873 [00:36<00:03, 283.76it/s]\u001b[A\n"," 93% 10994/11873 [00:36<00:03, 284.88it/s]\u001b[A\n"," 93% 11023/11873 [00:36<00:02, 284.46it/s]\u001b[A\n"," 93% 11052/11873 [00:36<00:02, 284.86it/s]\u001b[A\n"," 93% 11081/11873 [00:37<00:02, 284.12it/s]\u001b[A\n"," 94% 11110/11873 [00:37<00:02, 283.31it/s]\u001b[A\n"," 94% 11140/11873 [00:37<00:02, 286.76it/s]\u001b[A\n"," 94% 11172/11873 [00:37<00:02, 296.47it/s]\u001b[A\n"," 94% 11204/11873 [00:37<00:02, 301.15it/s]\u001b[A\n"," 95% 11235/11873 [00:37<00:02, 303.61it/s]\u001b[A\n"," 95% 11266/11873 [00:37<00:01, 304.88it/s]\u001b[A\n"," 95% 11297/11873 [00:37<00:01, 305.75it/s]\u001b[A\n"," 95% 11329/11873 [00:37<00:01, 308.66it/s]\u001b[A\n"," 96% 11360/11873 [00:37<00:01, 308.33it/s]\u001b[A\n"," 96% 11391/11873 [00:38<00:01, 305.67it/s]\u001b[A\n"," 96% 11422/11873 [00:38<00:01, 306.37it/s]\u001b[A\n"," 96% 11453/11873 [00:38<00:01, 301.69it/s]\u001b[A\n"," 97% 11484/11873 [00:38<00:01, 292.30it/s]\u001b[A\n"," 97% 11514/11873 [00:38<00:01, 291.87it/s]\u001b[A\n"," 97% 11544/11873 [00:38<00:01, 289.51it/s]\u001b[A\n"," 97% 11574/11873 [00:38<00:01, 290.76it/s]\u001b[A\n"," 98% 11605/11873 [00:38<00:00, 295.52it/s]\u001b[A\n"," 98% 11636/11873 [00:38<00:00, 297.64it/s]\u001b[A\n"," 98% 11667/11873 [00:39<00:00, 299.37it/s]\u001b[A\n"," 99% 11697/11873 [00:39<00:00, 292.52it/s]\u001b[A\n"," 99% 11727/11873 [00:39<00:00, 293.16it/s]\u001b[A\n"," 99% 11757/11873 [00:39<00:00, 293.58it/s]\u001b[A\n"," 99% 11787/11873 [00:39<00:00, 293.85it/s]\u001b[A\n","100% 11817/11873 [00:39<00:00, 292.82it/s]\u001b[A\n","100% 11873/11873 [00:39<00:00, 299.00it/s]\n","03/29/2022 21:08:14 - INFO - utils_qa - Saving predictions to /tmp/debug_squad/eval_predictions.json.\n","03/29/2022 21:08:14 - INFO - utils_qa - Saving nbest_preds to /tmp/debug_squad/eval_nbest_predictions.json.\n","03/29/2022 21:08:16 - INFO - utils_qa - Saving null_odds to /tmp/debug_squad/eval_null_odds.json.\n","03/29/2022 21:08:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad_v2/default/default_experiment-1-0.arrow\n","100% 1497/1497 [04:53<00:00,  5.10it/s]\n","***** eval metrics *****\n","  epoch                  =     3.0\n","  eval_HasAns_exact      = 72.3178\n","  eval_HasAns_f1         = 79.1162\n","  eval_HasAns_total      =    5928\n","  eval_NoAns_exact       = 71.0849\n","  eval_NoAns_f1          = 71.0849\n","  eval_NoAns_total       =    5945\n","  eval_best_exact        = 71.7005\n","  eval_best_exact_thresh =     0.0\n","  eval_best_f1           = 75.0948\n","  eval_best_f1_thresh    =     0.0\n","  eval_exact             = 71.7005\n","  eval_f1                = 75.0948\n","  eval_samples           =   11974\n","  eval_total             =   11873\n","[INFO|modelcard.py:460] 2022-03-29 21:08:20,318 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'args': 'squad_v2'}}\n"]}]}]}